[
  {
    "URL": "http://ascilite.org.au/conferences/perth04/procs/pdf/wirski.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Wirski",
        "given": "Ralph"
      },
      {
        "family": "Brownfield",
        "given": "Grame"
      },
      {
        "family": "Oliver",
        "given": "Ron"
      }
    ],
    "editor": [
      {
        "family": "Atkinson",
        "given": "Roger"
      },
      {
        "family": "McBeath",
        "given": "Clare"
      },
      {
        "family": "Jonas-Dwyer",
        "given": "Diana"
      },
      {
        "family": "Phillips",
        "given": "Rob"
      }
    ],
    "event": "21<sup>st</sup> ASCILITE conference",
    "event-date": {
      "date-parts": [
        [
          2004,
          12,
          5
        ],
        [
          2004,
          12,
          8
        ]
      ]
    },
    "event-place": "Perth, Western Australia",
    "id": "Wirski2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "938-947",
    "publisher": "ASCILITE",
    "publisher-place": "Tugun",
    "title": "Exploring SCORM and the national flexible learning toolboxes",
    "type": "paper-conference"
  },
  {
    "URL": "http://ilk.uvt.nl/LaTeCH2010/LPF/ws16.pdf",
    "author": [
      {
        "family": "Volk",
        "given": "Martin"
      },
      {
        "family": "Marek",
        "given": "Torsten"
      },
      {
        "family": "Sennrich",
        "given": "Rico"
      }
    ],
    "container-title": "Proceedings of the ECAI 2010 workshop on language technology for cultural heritage, social sciences, and humanities (LaTeCH 2010)",
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      }
    ],
    "id": "Volk2010c",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "language": "en-US",
    "page": "61-65",
    "title": "Reducing OCR errors by combining two OCR systems",
    "type": "paper-conference"
  },
  {
    "URL": "http://dharchive.org/paper/DH2014/Paper-61.xml",
    "author": [
      {
        "family": "Sperberg-McQueen",
        "given": "C. M."
      },
      {
        "family": "Marcoux",
        "given": "Yves"
      },
      {
        "family": "Huitfeldt",
        "given": "Claus"
      }
    ],
    "container-title": "Digital humanities 2014 conference abstracts",
    "id": "Sperberg-McQueen2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "publisher": "Alliance of Digital Humanities Organizations (ADHO)",
    "title": "Transcriptional implicature: A contribution to markup semantics",
    "title-short": "Transcriptional implicature",
    "type": "paper-conference"
  },
  {
    "ISBN": "978-0-9565793-0-0",
    "URL": "http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/html/ab-663.html",
    "author": [
      {
        "family": "Sperberg-McQueen",
        "given": "C. M."
      },
      {
        "family": "Marcoux",
        "given": "Yves"
      },
      {
        "family": "Huitfeldt",
        "given": "Claus"
      }
    ],
    "container-title": "Digital humanities 2010 conference abstracts",
    "id": "Sperberg-McQueen2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "page": "244-245",
    "publisher": "ADHO",
    "title": "Two representations of the semantics of TEI lite",
    "type": "paper-conference"
  },
  {
    "URL": "http://ilk.uvt.nl/LaTeCH2010/LPF/ws16.pdf",
    "author": [
      {
        "family": "Schwarte",
        "given": "Andreas"
      },
      {
        "family": "Haccius",
        "given": "Christopher"
      },
      {
        "family": "Steenbuck",
        "given": "Sebastian"
      },
      {
        "family": "Steudter",
        "given": "Sven"
      }
    ],
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      }
    ],
    "id": "Schwarte2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "title": "Usability enhancement by mining, processing and visualizing data from the federal german archive",
    "type": "paper-conference"
  },
  {
    "URL": "http://ilk.uvt.nl/LaTeCH2010/LPF/ws16.pdf",
    "author": [
      {
        "family": "Oravecz",
        "given": "Csaba"
      },
      {
        "family": "Sass",
        "given": "Bálint"
      },
      {
        "family": "Simon",
        "given": "Eszter"
      }
    ],
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      }
    ],
    "id": "Oravecz2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "title": "Semi-automatic normalization of old hungarian codices",
    "type": "paper-conference"
  },
  {
    "URL": "http://ilk.uvt.nl/LaTeCH2010/LPF/ws16.pdf",
    "author": [
      {
        "family": "Hendrickx",
        "given": "Iris"
      },
      {
        "family": "Généreux",
        "given": "Michel"
      },
      {
        "family": "Marquilhas",
        "given": "Rita"
      }
    ],
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      }
    ],
    "id": "Hendrickx2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "title": "Automatic pragmatic text segmentation of historical letters",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/workshops/W22_Proceedings.pdf",
    "author": [
      {
        "family": "Haug",
        "given": "Dag"
      },
      {
        "family": "Jøhndal",
        "given": "Marius L."
      }
    ],
    "editor": [
      {
        "family": "Ribarov",
        "given": "Kiril"
      },
      {
        "family": "Sporleder",
        "given": "Caroline"
      }
    ],
    "id": "Haug2008",
    "issued": {
      "date-parts": [
        [
          2008,
          6
        ]
      ]
    },
    "title": "Creating a parallel treebank of the old Indo-European Bible translations",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Graziadei",
        "given": "William D."
      }
    ],
    "collection-title": "SUNY series in computer-mediated communication",
    "container-title": "Computer networking and scholarly communication in twenty-first-century university",
    "editor": [
      {
        "family": "Harrison",
        "given": "Teresa M."
      },
      {
        "family": "Stephen",
        "given": "Timothy D."
      }
    ],
    "id": "Graziadei1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "page": "257-276",
    "publisher": "SUNY Press",
    "publisher-place": "Albany, NY",
    "title": "VICE in REST",
    "type": "chapter"
  },
  {
    "URL": "http://www.lt-world.org/HLT_Survey/master.pdf",
    "abstract": "Languages, in all their forms, are the more efficient and natural means for people to communicate. Enormous quantities of information are produced, distributed and consumed using languages. Human language technology’s main purpose is to allow the use of automatic systems and tools to assist humans in producing and accessing information, to improve communication between humans, and to assist humans in communicating with machines. This book, sponsored by the Directorate General XIII of the European Union and the Information Science and Engineering Directorate of the National Science Foundation, USA, offers the first comprehensive overview of the human language technology field.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Dale",
        "given": "Robert"
      }
    ],
    "container-title": "Survey of the state of the art in human language technology",
    "editor": [
      {
        "family": "Cole",
        "given": "Ronald"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Uszkoreit",
        "given": "Hans"
      },
      {
        "family": "Zaenen",
        "given": "Annie"
      },
      {
        "family": "Zue",
        "given": "Viktor"
      }
    ],
    "id": "Dale1997:SSAHLT",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "page": "235-237",
    "publisher": "Cambridge University Press",
    "title": "Computer assistance in text creation and editing",
    "type": "chapter"
  },
  {
    "abstract": "E-learning is at an exciting point in its development; its potential in terms of research is great and its impact on institutional practices is fully recognised. This book aims to define e-learning as a field of research, highlighting the complex issues, activities and tensions that characterize the area. Written by a team of experienced researchers and commented upon by internationally recognised experts, this book engages researchers and practitioners in critical discussion and debate of findings emerging from the field and the associated impact on practice. Key topics examined include: access and inclusion; the social-cultural contexts of e-learning; organisational structures, processes and identities; technical aspects of learning research - using tools and resources; approaches to learning and teaching practices and associated learning theories; designing for e-learning and the management of educational resources; professional roles and identities; the evolution of e-assessment; and, collaboration, motivation and educational evaluation.  provides a synthesis of research, giving a grounding in contemporary e-learning scholarship whilst identifying the debates that make it such a lively and fast-moving area. A landmark text in an evolving field, this book will prove invaluable for all researchers, practitioners, policy makers and students engaging with e-learning.",
    "author": [
      {
        "family": "Cook",
        "given": "John"
      },
      {
        "family": "White",
        "given": "Su"
      },
      {
        "family": "Sharples",
        "given": "Mike"
      },
      {
        "family": "Sclater",
        "given": "Niall"
      },
      {
        "family": "Davis",
        "given": "Hugh"
      }
    ],
    "chapter-number": "4",
    "collection-title": "Open and flexible learning series",
    "container-title": "Contemporary perspectives in e-learning research: Themes, methods and impact on practice",
    "editor": [
      {
        "family": "Conole",
        "given": "Gráinne"
      },
      {
        "family": "Oliver",
        "given": "Martin"
      }
    ],
    "id": "Cook2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "e-learning, pedagogy",
    "page": "55-68",
    "publisher": "Routledge",
    "publisher-place": "Abingdon",
    "title": "The design of learning technologies",
    "type": "chapter"
  },
  {
    "abstract": "E-learning is at an exciting point in its development; its potential in terms of research is great and its impact on institutional practices is fully recognised. This book aims to define e-learning as a field of research, highlighting the complex issues, activities and tensions that characterize the area. Written by a team of experienced researchers and commented upon by internationally recognised experts, this book engages researchers and practitioners in critical discussion and debate of findings emerging from the field and the associated impact on practice. Key topics examined include: access and inclusion; the social-cultural contexts of e-learning; organisational structures, processes and identities; technical aspects of learning research - using tools and resources; approaches to learning and teaching practices and associated learning theories; designing for e-learning and the management of educational resources; professional roles and identities; the evolution of e-assessment; and, collaboration, motivation and educational evaluation.  provides a synthesis of research, giving a grounding in contemporary e-learning scholarship whilst identifying the debates that make it such a lively and fast-moving area. A landmark text in an evolving field, this book will prove invaluable for all researchers, practitioners, policy makers and students engaging with e-learning.",
    "author": [
      {
        "family": "Conole",
        "given": "Gráinne"
      },
      {
        "family": "Smith",
        "given": "Janice"
      },
      {
        "family": "White",
        "given": "Su"
      }
    ],
    "chapter-number": "3",
    "collection-title": "Open and flexible learning series",
    "container-title": "Contemporary perspectives in e-learning research: Themes, methods and impact on practice",
    "editor": [
      {
        "family": "Conole",
        "given": "Gráinne"
      },
      {
        "family": "Oliver",
        "given": "Martin"
      }
    ],
    "id": "Conole2007a",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "e-learning, pedagogy",
    "page": "38-54",
    "publisher": "Routledge",
    "publisher-place": "Abingdon",
    "title": "A critique of the impact of policy and funding",
    "type": "chapter"
  },
  {
    "abstract": "This Standard presents guidelines and conventions for the contents, display, construction, testing, maintenance, and management of monolingual controlled vocabularies. This Standard focuses on controlled vocabularies that are used for the representation of content objects in knowledge organization systems including lists, synonym rings, taxonomies, and thesauri. This Standard should be regarded as a set of recommendations based on preferred techniques and procedures. Optional procedures are, however, sometimes described, e.g., for the display of terms in a controlled vocabulary. The primary purpose of vocabulary control is to achieve consistency in the description of content objects and to facilitate retrieval. Vocabulary control is accomplished by three principal methods: defining the scope, or meaning, of terms; using the equivalence relationship to link synonymous and nearly synonymous terms; and distinguishing among homographs.",
    "author": [
      {
        "literal": "ANSI/NISO"
      }
    ],
    "id": "ANSI-Z39-19",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "standard, terminology",
    "number": "Z39.19-2005",
    "publisher": "NISO",
    "publisher-place": "Bethesda, MD, USA",
    "title": "Guidelines for the construction, format, and management of monolingual controlled vocabularies",
    "type": "report"
  },
  {
    "DOI": "10.1145/1385989.1386007",
    "ISBN": "978-1-60558-090-6",
    "abstract": "This industry paper is about a new type of event database architecture that makes it efficient to reason about things, people, companies, relationships between people and companies, and about places and events. This event database is built on top of a scalable distributed RDF triple store that can handle literally billions of events. Like objects, events have at least one actor, but usually more, a start-time and possibly an end-time, a place where the event happened, and the type of the event. An event can have many additional properties and annotations. For example, telephone call detail records, email records, financial transactions, purchases, hospital visits, insurance claims, library records, etc. can all be viewed as events. On top of this event database we implemented very efficient geospatial and temporal queries, an extensive social network analysis library and simplified description logic. This paper focuses on a query framework that makes it easy to combine all of the aforementioned capabilities in a user friendly query language.",
    "author": [
      {
        "family": "Aasman",
        "given": "Jans"
      }
    ],
    "container-title": "Proceedings of the second international conference on distributed event-based systems (DEBS ’08)",
    "id": "Aasman2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "database, lisp, rdf, temporal_data",
    "page": "139-145",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Unification of geospatial reasoning, temporal logic, & social network analysis in event-based systems",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/2209249.2209267",
    "ISSN": "0001-0782",
    "abstract": "A searchable meta-graph can connect even troublesome house elves and other supernatural beings to scholarly folk categories.",
    "author": [
      {
        "family": "Abello",
        "given": "James"
      },
      {
        "family": "Broadwell",
        "given": "Peter"
      },
      {
        "family": "Tangherlini",
        "given": "Timothy R."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Abello2012",
    "issue": "7",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_humanities, geocoding",
    "page": "60-70",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Computational folkloristics",
    "type": "article-journal",
    "volume": "55"
  },
  {
    "DOI": "10.2200/s00338ed1v01y201104dtm015",
    "abstract": "This lecture introduces systematically into the problem of managing large data collections in peer-to-peer systems. Search over large datasets has always been a key problem in peer-to-peer systems and the peer-to-peer paradigm has incited novel directions in the field of data management. This resulted in many novel peer-to-peer data management concepts and algorithms, for supporting data management tasks in a wider sense, including data integration, document management and text retrieval. The lecture covers four different types of peer-to-peer data management systems that are characterized by the type of data they manage and the search capabilities they support. The first type are structured peer-to-peer data management systems which support structured query capabilities for standard data models. The second type are peer-to-peer data integration systems for querying of heterogeneous databases without requiring a common global schema. The third type are peer-to-peer document retrieval systems that enable document search based both on the textual content and the document structure. Finally, we introduce semantic overlay networks, which support similarity search on information represented in hierarchically organized and multi-dimensional semantic spaces. Topics that go beyond data representation and search are summarized at the end of the lecture.",
    "author": [
      {
        "family": "Aberer",
        "given": "Karl"
      }
    ],
    "collection-title": "Synthesis lectures on data management",
    "id": "Aberer2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "distributed_systems",
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "publisher-place": "San Rafael, CA, USA",
    "title": "Peer-to-peer data management",
    "type": "book"
  },
  {
    "URL": "https://nbn-resolving.org/urn:nbn:de:0074-1486-1",
    "abstract": "This paper introduces DOREMUS—a semantic web project aiming to provide common knowledge models and shared multilingual vocabularies to cultural institutions, publishers, distributors and users in the musical domain. The project develops methods to describe, publish, connect and contextualize music catalogs on the web of data. Our focus is on the description of classical and traditional musical works as well as their interpretations (events).",
    "author": [
      {
        "family": "Achichi",
        "given": "Manel"
      },
      {
        "family": "Bailly",
        "given": "Rodolphe"
      },
      {
        "family": "Cecconi",
        "given": "Cécile"
      },
      {
        "family": "Destandau",
        "given": "Marie"
      },
      {
        "family": "Todorov",
        "given": "Konstantin"
      },
      {
        "family": "Troncy",
        "given": "Raphaël"
      }
    ],
    "container-title": "Proceedings of the ISWC 2015 posters & demonstrations track",
    "id": "Achichi2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "music, ontologies",
    "title": "DOREMUS: Doing reusable musical data",
    "title-short": "DOREMUS",
    "type": "paper-conference"
  },
  {
    "abstract": "In this thesis, strategies are explored to retrieve 17th century Dutch documents for modern Dutch queries. Three approaches are proposed, based on different views on historic languages. In addition, a new topic set is constructed. The first approach treats Historic Document Retrieval as monolingual IR. Tokenization methods, including stemming, decompounding and n-gramming, are used to deal with the linguistic variations. The second approach uses the SoundEx algorithm to translate words into phonetic codes. The final approach treats HDR as a form of Cross-Language Information Retrieval (CLIR). Language resources, created by M. Koolen in a related research, are used to translate the documents into the query language and vice versa. The main finding is that Historic Document Retrieval for 17th century Dutch should be treated as a form of Cross-Language Information Retrieval. The best results are obtained by combining high-quality translation resources with effective tokenization methods.",
    "author": [
      {
        "family": "Adriaans",
        "given": "Frans"
      }
    ],
    "genre": "Master’s thesis",
    "id": "Adriaans2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "cultural_heritage, dutch, ir",
    "publisher": "Universiteit van Amsterdam",
    "title": "Historic document retrieval: Exploring strategies for 17th century Dutch",
    "title-short": "Historic document retrieval",
    "type": "thesis"
  },
  {
    "DOI": "10.1109/tkde.2008.190",
    "ISSN": "1041-4347",
    "abstract": "In recent years, a number of indirect data collection methodologies have lead to the proliferation of uncertain data. Such data points are often represented in the form of a probabilistic function, since the corresponding deterministic value is not known. This increases the challenge of mining and managing uncertain data, since the precise behavior of the underlying data is no longer known. In this paper, we provide a survey of uncertain data mining and management applications. In the field of uncertain data management, we will examine traditional methods such as join processing, query processing, selectivity estimation, OLAP queries, and indexing. In the field of uncertain data mining, we will examine traditional mining problems such as classification and clustering. We will also examine a general transform based technique for mining uncertain data. We discuss the models for uncertain data, and how they can be leveraged in a variety of applications. We discuss different methodologies to process and mine uncertain data in a variety of forms.",
    "author": [
      {
        "family": "Aggarwal",
        "given": "Charu C."
      },
      {
        "family": "Yu",
        "given": "Philip S."
      }
    ],
    "container-title": "IEEE Transactions on Knowledge and Data Engineering",
    "id": "Aggarwal2009a",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "database, uncertainty",
    "page": "609-623",
    "title": "A survey of uncertain data algorithms and applications",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "DOI": "10.1007/978-0-387-09690-2",
    "ISBN": "978-0-387-09689-6",
    "abstract": "Managing and Mining Uncertain Data contains surveys by well known researchers in the field of uncertain databases. The book presents the most recent models, algorithms, and applications in the uncertain data field in a structured and concise way. This book is organized so as to cover the most important management and mining topics in the field. The idea is to make it accessible not only to researchers, but also to application-driven practitioners for solving real problems. Given the lack of structurally organized information on the new and emerging area of uncertain data, this book provides insights which are not easily accessible elsewhere. Managing and Mining Uncertain Data is designed for a varied audience composed of professors, researchers and practitioners in industry. This book is also suitable as a reference book for advanced-level database students in computer science and engineering.",
    "collection-title": "Advances in database systems",
    "editor": [
      {
        "family": "Aggarwal",
        "given": "Charu C."
      }
    ],
    "id": "Aggarwal2009b",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "database, uncertainty",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "Boston, MA, USA",
    "title": "Managing and mining uncertain data",
    "type": "book",
    "volume": "35"
  },
  {
    "DOI": "10.1145/1924475.1924477",
    "ISSN": "0163-5840",
    "abstract": "An abstract is not available.",
    "author": [
      {
        "family": "Agosti",
        "given": "Maristella"
      },
      {
        "family": "Braschler",
        "given": "Martin"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Ferro",
        "given": "Nicola"
      },
      {
        "family": "Harman",
        "given": "Donna"
      },
      {
        "family": "Peters",
        "given": "Carol"
      },
      {
        "family": "Pianta",
        "given": "Emanuele"
      },
      {
        "dropping-particle": "de",
        "family": "Rijke",
        "given": "Maarten"
      },
      {
        "family": "Smeaton",
        "given": "Alan"
      }
    ],
    "container-title": "SIGIR Forum",
    "id": "Agosti2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "evaluation, ir",
    "page": "8-12",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "CLEF 2010 conference on multilingual and multimodal information access evaluation",
    "type": "article-journal",
    "volume": "44"
  },
  {
    "abstract": "This paper addresses the problem of computational terminology evaluation not per se but in a specific application context. This paper describes the evaluation procedure that has been used to assess the validity of our overall indexing approach and the quality of the IndDoc indexing tool. Even if user-oriented extended evaluation is irreplaceable, we argue that early evaluations are possible and they are useful for development guidance.",
    "author": [
      {
        "family": "Aït El Mekki",
        "given": "Touria"
      },
      {
        "family": "Nazarenko",
        "given": "Adeline"
      }
    ],
    "container-title": "Proceedings of the workshop on terminology design: Quality criteria and evaluation methods (TermEval). In association with LREC 2006",
    "id": "AitElMekki2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "indices",
    "page": "18-21",
    "title": "An application-oriented terminology evaluation: The case of back-of-the book indexes",
    "title-short": "An application-oriented terminology evaluation",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-322-84139-1",
    "ISBN": "978-3-531-12080-5",
    "editor": [
      {
        "family": "Albrecht",
        "given": "Ulrich"
      },
      {
        "family": "Altvater",
        "given": "Elmar"
      },
      {
        "family": "Krippendorff",
        "given": "Ekkehart"
      }
    ],
    "id": "Albrecht1989",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "philosophy_of_science",
    "language": "de-DE",
    "publisher": "Westdeutscher Verlag",
    "publisher-place": "Opladen",
    "title": "Was heißt und zu welchem Ende betreiben wir Politikwissenschaft?",
    "type": "book"
  },
  {
    "DOI": "10.1145/964058.964059",
    "ISSN": "0163-5735",
    "author": [
      {
        "family": "Alderman",
        "given": "Donald L."
      }
    ],
    "container-title": "SIGCUE Outlook",
    "id": "Alderman1979",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1979,
          7
        ]
      ]
    },
    "keyword": "e-learning, ticcit",
    "page": "5-17",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Evaluation of the TICCIT computer-assisted instructional system in the community college",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "abstract": "Quantitative modelling and analysis is common in safety engineering, but it is often criticised. Objections include the difficulty in acquiring probabilities (e.g. for human error), the dubious assumptions often needed to manipulate them (e.g. independence of events), and the inherent uncertainty involved in making decisions based on probabilistic predictions. Clearly, poor predictions are of little value and may be dangerous. Faced with this danger, many people respond by eliminating quantities altogether. This is a trap, as we have no guarantee that the resulting model or predictions will be better; indeed, the subtlety of expression offered by numerical probabilities has been lost. This paper discusses some alternatives to the non-quantitative trap, and explores their significance for the issue of safety case assurance.",
    "author": [
      {
        "family": "Alexander",
        "given": "Robert D."
      },
      {
        "family": "Kelly",
        "given": "Tim P."
      }
    ],
    "container-title": "27<sup>th</sup> international system safety conference and joint weapons system safety conference 2009 (ISSC/JWSSC 2009)",
    "id": "Alexander2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "formal_models, uncertainty",
    "page": "115-120",
    "title": "Escaping the Non-Quantitative trap",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1016/b978-044451548-3/50010-0",
    "ISBN": "9780444515483",
    "abstract": "This chapter focuses on the logical, historical and computational approaches to the philosophy of science. It discusses how the logical approach to philosophy of science was introduced by the Vienna Circle, and developed by them and their followers and associates. The logical approach to philosophy of science remained the dominant subject throughout the 1950s; but, from the early 1960s, it was challenged by a striking development of the historical approach. The historical approach was not introduced for the first time in the 1960s. On the contrary, it had been developed by Mach and Duhem much earlier. Although, Mach and Duhem are cited by the Vienna Circle as important influences on their philosophy, the Vienna Circle did not adopt the historical features of these two thinkers. In the excitement generated by the new logic of Frege and Russell, history of science seems to have been temporarily forgotten. The general idea of the historical approach is not new in the 1960s, however, that decade saw striking developments in this approach. After Kuhn, the analysis of scientific revolutions became a major problem for philosophy of science, while Lakatos applied the historical approach to mathematics for the first time.",
    "author": [
      {
        "family": "Aliseda",
        "given": "Atocha"
      },
      {
        "family": "Gillies",
        "given": "Donald"
      }
    ],
    "container-title": "General philosophy of science",
    "id": "Aliseda2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "philosophy_of_science",
    "page": "431-513",
    "publisher": "Elsevier",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "Logical, historical and computational approaches",
    "type": "chapter"
  },
  {
    "ISBN": "9780123859662",
    "URL": "https://learning.acm.org/books/book_detail.cfm?id=2016697;type=24",
    "abstract": "Semantic Web models and technologies provide information in machine-readable languages that enable computers to access the Web more intelligently and perform tasks automatically without the direction of users. These technologies are relatively recent and advancing rapidly, creating a set of unique challenges for those developing applications. Semantic Web for the Working Ontologist is the essential, comprehensive resource on semantic modeling, for practitioners in health care, artificial intelligence, finance, engineering, military intelligence, enterprise architecture, and more. Focused on developing useful and reusable models, this market-leading book explains how to build semantic content (ontologies) and how to build applications that access that content.",
    "author": [
      {
        "family": "Allemang",
        "given": "Dean"
      },
      {
        "family": "Hendler",
        "given": "James"
      }
    ],
    "edition": "Second",
    "id": "Allemang2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "semantic_web",
    "language": "en-US",
    "publisher": "Morgan Kaufmann",
    "publisher-place": "Waltham, MA, USA",
    "title": "Semantic Web for the working ontologist",
    "type": "book"
  },
  {
    "URL": "https://lareviewofbooks.org/article/neoliberal-tools-archives-political-history-digital-humanities/",
    "abstract": "Advocates position Digital Humanities as a corrective to the “traditional” and outmoded approaches to literary study that supposedly plague English departments. Like much of the rhetoric surrounding Silicon Valley today, this discourse sees technological innovation as an end in itself and equates the development of disruptive business models with political progress. Yet despite the aggressive promotion of Digital Humanities as a radical insurgency, its institutional success has for the most part involved the displacement of politically progressive humanities scholarship and activism in favor of the manufacture of digital tools and archives. Advocates characterize the development of such tools as revolutionary and claim that other literary scholars fail to see their political import due to fear or ignorance of technology. But the unparalleled level of material support that Digital Humanities has received suggests that its most significant contribution to academic politics may lie in its (perhaps unintentional) facilitation of the neoliberal takeover of the university.",
    "author": [
      {
        "family": "Allington",
        "given": "Daniel"
      },
      {
        "family": "Brouillette",
        "given": "Sarah"
      },
      {
        "family": "Golumbia",
        "given": "David"
      }
    ],
    "container-title": "Los Angeles Review of Books",
    "id": "Allington2016",
    "issued": {
      "date-parts": [
        [
          2016,
          5,
          1
        ]
      ]
    },
    "keyword": "digital_humanities",
    "title": "Neoliberal tools (and archives): A political history of digital humanities",
    "title-short": "Neoliberal tools (and archives)",
    "type": "article-journal"
  },
  {
    "DOI": "10.1145/1516512.1516518",
    "ISSN": "0004-5411",
    "abstract": "We propose the model of nested words for representation of data with both a linear ordering and a hierarchically nested matching of items. Examples of data with such dual linear-hierarchical structure include executions of structured programs, annotated linguistic data, and HTML/XML documents. Nested words generalize both words and ordered trees, and allow both word and tree operations. We define nested word automata—finite-state acceptors for nested words, and show that the resulting class of regular languages of nested words has all the appealing theoretical properties that the classical regular word languages enjoys: deterministic nested word automata are as expressive as their nondeterministic counterparts; the class is closed under union, intersection, complementation, concatenation, Kleene-&ast;, prefixes, and language homomorphisms; membership, emptiness, language inclusion, and language equivalence are all decidable; and definability in monadic second order logic corresponds exactly to finite-state recognizability. We also consider regular languages of infinite nested words and show that the closure properties, MSO-characterization, and decidability of decision problems carry over. The linear encodings of nested words give the class of visibly pushdown languages of words, and this class lies between balanced languages and deterministic context-free languages. We argue that for algorithmic verification of structured programs, instead of viewing the program as a context-free language over words, one should view it as a regular language of nested words (or equivalently, a visibly pushdown language), and this would allow model checking of many properties (such as stack inspection, pre-post conditions) that are not expressible in existing specification logics. We also study the relationship between ordered trees and nested words, and the corresponding automata: while the analysis complexity of nested word automata is the same as that of classical tree automata, they combine both bottom-up and top-down traversals, and enjoy expressiveness and succinctness benefits over tree automata.",
    "author": [
      {
        "family": "Alur",
        "given": "Rajeev"
      },
      {
        "family": "Madhusudan",
        "given": "P."
      }
    ],
    "container-title": "J. ACM",
    "id": "Alur2009",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "document_engineering, xml",
    "page": "1-43",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Adding nesting structure to words",
    "type": "article-journal",
    "volume": "56"
  },
  {
    "DOI": "10.1145/1140124.1140150",
    "abstract": "To reduce the workload of teachers and to improve the effectiveness of face-to-face courses, it is desirable to supplement them with Web-based tools. This paper presents our approach for supporting computer science education with software components which support the creation, management, submission, and assessment of assignments and tests, including the automatic assessment of programming exercises. These components are integrated into a general-purpose content management system (CMS) and can combinde with other components to create tailored learning environments. We describe the design and implementation of these components, and we report on our practical experience with deploying the software in our courses.",
    "author": [
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "container-title": "ITiCSE ’06: Proceedings of the 11<sup>th</sup> annual conference on innovation and technology in computer science education",
    "id": "Amelung2006",
    "issued": {
      "date-parts": [
        [
          2006,
          6
        ]
      ]
    },
    "page": "88-92",
    "publisher": "ACM",
    "publisher-place": "Bologna, Italy",
    "title": "EduComponents: Experiences in e-assessment in computer science education",
    "title-short": "EduComponents",
    "type": "paper-conference"
  },
  {
    "abstract": "We present the eduComponents, a component-based approach to e-learning system architecture. In contrast to typical “integrated” platforms, the eduComponents are implemented as extension modules for a general-purpose content management system (CMS). The components can be used individually, together, and in combination with other modules. The use of a general-purpose (i.e., not e-learning-specific) CMS means that a single platform can be used for e-learning and other Web content, providing the advantages of a uniform user interface, reduced system administration overhead, and extensive code reuse.",
    "author": [
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "container-title": "ITiCSE ’07: Proceedings of the 12<sup>th</sup> annual SIGCSE conference on innovation and technology in computer science education",
    "id": "Amelung2007",
    "issued": {
      "date-parts": [
        [
          2007,
          6
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "page": "352-352",
    "publisher": "ACM",
    "publisher-place": "Dundee, U.K.",
    "title": "eduComponents: A component-based e-learning environment",
    "title-short": "eduComponents",
    "type": "paper-conference"
  },
  {
    "abstract": "Zum Erwerb von Programmierfähigkeiten ist neben einem theoretischen Verständnis vor allem praktische Übung notwendig. Die automatische Überprüfung von Programmieraufgaben hilft, Studierenden mehr Übungsmöglichkeiten mit schnellerer Rückmeldung zur Verfügung zu stellen und Lehrende gleichzeitig zu entlasten, so dass sie sich auf inhaltliche und didaktische Fragen konzentrieren können. Wir stellen eine schlanke, dienstebasierte Architektur und ihre Implementierung für die automatische Überprüfung von studentischen Einreichungen vor und berichten über praktische Erfahrungen beim Einsatz dieser Software in unseren Lehrveranstaltungen.",
    "author": [
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "collection-title": "Lecture Notes in Informatics",
    "container-title": "2. Workshop Pervasive University im Rahmen der GI-Jahrestagung 2007",
    "id": "Amelung2007b",
    "issued": {
      "date-parts": [
        [
          2007,
          9
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "language": "de-DE",
    "publisher": "GI; GI-Verlag",
    "publisher-place": "Bremen, Germany",
    "title": "Webbasierte Dienste für das E-Assessment",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Amelung",
        "given": "Mario"
      }
    ],
    "genre": "PhD thesis",
    "id": "Amelung2009",
    "issued": {
      "date-parts": []
    },
    "language": "de-DE",
    "publisher": "Otto-von-Guericke-Universität Magdeburg",
    "publisher-place": "Magdeburg",
    "title": "Web-Services für das E-Assessment",
    "type": "thesis"
  },
  {
    "edition": "Abridged edition",
    "editor": [
      {
        "family": "Anderson",
        "given": "Lorin W."
      },
      {
        "family": "Krathwohl",
        "given": "David R."
      }
    ],
    "id": "Anderson2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Longman",
    "publisher-place": "New York, NY, USA",
    "title": "A taxonomy for learning, teaching, and assessing: A revision of bloom’s taxonomy of educational objectives",
    "title-short": "A taxonomy for learning, teaching, and assessing",
    "type": "book"
  },
  {
    "DOI": "10.4135/9781848607958.n14",
    "ISBN": "9781412901192",
    "abstract": "Quantitative history is the term for an array of skills and techniques used to apply the methods of statistical data analysis to the study of history. Sometimes also called cliometrics by economic historians, the term was popularized in the 1950s and 1960s as social, political and economic historians called for the development of a “social science history”, adopted methods from the social sciences and applied them to historical problems.",
    "author": [
      {
        "family": "Anderson",
        "given": "Margo"
      }
    ],
    "chapter-number": "14",
    "container-title": "The SAGE handbook of social science methodology",
    "id": "Anderson2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "formal_models, history",
    "language": "en-US",
    "page": "248-264",
    "publisher": "SAGE",
    "publisher-place": "London",
    "title": "Quantitative history",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Anklam",
        "given": "Patti"
      }
    ],
    "container-title": "Knowledge Management Magazine",
    "id": "Anklam2001",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "title": "The Camelot of collaboration: The case of VAX Notes",
    "title-short": "The Camelot of collaboration",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.1007/s10032-007-0045-1",
    "ISSN": "1433-2833",
    "author": [
      {
        "family": "Antonacopoulos",
        "given": "Apostolos"
      },
      {
        "family": "Downton",
        "given": "Andy C."
      }
    ],
    "container-title": "International Journal on Document Analysis and Recognition",
    "id": "Antonacopoulos2007",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage, handwriting_recognition, ocr",
    "page": "75-77",
    "title": "Special issue on the analysis of historical documents",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "DOI": "10.1007/978-94-010-3667-2_1",
    "ISBN": "978-94-010-3669-6",
    "annote": "The contributions of this volume first appeared as special issue of Synthese (Volume 12, Issue 2-3, September 1960) https://link.springer.com/journal/11229/12/2/page/1",
    "author": [
      {
        "family": "Apostel",
        "given": "Leo"
      }
    ],
    "chapter-number": "1",
    "container-title": "The concept and the role of the model in mathematics and natural and social sciences",
    "editor": [
      {
        "family": "Freudenthal",
        "given": "Hans"
      }
    ],
    "id": "Apostel1961",
    "issued": {
      "date-parts": [
        [
          1961
        ]
      ]
    },
    "keyword": "formal_models, models_in_general",
    "language": "en-US",
    "page": "1-37",
    "publisher": "D. Reidel",
    "publisher-place": "Dordrecht",
    "title": "Towards the formal study of models in the non-formal sciences",
    "type": "paper-conference"
  },
  {
    "ISBN": "3-540-66694-X",
    "URL": "http://portal.acm.org/citation.cfm?id=712532",
    "author": [
      {
        "family": "Appelt",
        "given": "Wolfgang"
      }
    ],
    "collection-number": "1725",
    "collection-title": "Lecture notes in computer science",
    "container-title": "SOFSEM ’99: Proceedings of the 26th conference on current trends in theory and practice of informatics",
    "id": "Appelt1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "cscw",
    "page": "66-78",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "WWW based collaboration with the BSCW system",
    "type": "paper-conference"
  },
  {
    "abstract": "Niupepa is a collection of 42 newspaper title published in New Zealand from 1842–1933, comprising a total of 21,000 pages in 1,750 issues. This collection forms a unique historical record of the language of the indigenous Māori people, the evolution of the written form of this language, and of events and developments during the formative colonial history of our country. Using the greenstone software from the New Zealand Digital Library (see article in this special section), this collection is now publicly available with full-text search capability.",
    "author": [
      {
        "family": "Apperley",
        "given": "Mark"
      },
      {
        "family": "Cunningham",
        "given": "Sally Jo"
      },
      {
        "family": "Keegan",
        "given": "Te Taka"
      },
      {
        "family": "Witten",
        "given": "Ian H."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Apperley2001",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "page": "86-87",
    "title": "Niupepa: A historical newspaper collection",
    "title-short": "Niupepa",
    "type": "article-journal",
    "volume": "44"
  },
  {
    "DOI": "10.1007/978-3-642-03754-2_4",
    "ISBN": "978-3-642-03753-5",
    "abstract": "The goal of this paper is to give an overview of the basics of the theory of RDF databases. We provide a formal definition of RDF that includes the features that distinguish this model from other graph data models. We then move into the fundamental issue of querying RDF data. We start by considering the RDF query language SPARQL, which is a W3C Recommendation since January 2008. We provide an algebraic syntax and a compositional semantics for this language, study the complexity of the evaluation problem for different fragments of SPARQL, and consider the problem of optimizing the evaluation of SPARQL queries, showing that a natural fragment of this language has some good properties in this respect. We furthermore study the expressive power of SPARQL, by comparing it with some well-known query languages such as relational algebra. We conclude by considering the issue of querying RDF data in the presence of RDFS vocabulary. In particular, we present a recently proposed extension of SPARQL with navigational capabilities.",
    "author": [
      {
        "family": "Arenas",
        "given": "Marcelo"
      },
      {
        "family": "Gutierrez",
        "given": "Claudio"
      },
      {
        "family": "Pérez",
        "given": "Jorge"
      }
    ],
    "chapter-number": "4",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Reasoning web. Semantic technologies for information systems",
    "editor": [
      {
        "family": "Tessaris",
        "given": "Sergio"
      },
      {
        "family": "Franconi",
        "given": "Enrico"
      },
      {
        "family": "Eiter",
        "given": "Thomas"
      },
      {
        "family": "Gutierrez",
        "given": "Claudio"
      },
      {
        "family": "Handschuh",
        "given": "Siegfried"
      },
      {
        "family": "Rousset",
        "given": "Marie-Christine"
      },
      {
        "family": "Schmidt",
        "given": "Renate A."
      }
    ],
    "id": "Arenas2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "database, rdf",
    "page": "158-204",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Foundations of RDF databases",
    "type": "chapter",
    "volume": "5689"
  },
  {
    "DOI": "10.1145/2024288.2024303",
    "ISBN": "978-1-4503-0732-1",
    "abstract": "For the canon of important works of art, lots of information is available on the Internet. Different Web platforms exist that show artworks with additional information and an Art historical description. While this information is suitable for experts in Art history, users without this expert knowledge may find this information hard to access because of a vocabulary mismatch between experts and laypersons. In order to allow users with different expertise to communicate and comprehend Art in their own vocabulary we have created the explorARTorium (www.explorARTorium.info), which visualises artworks in context and allows users to browse along different narratives. The explorARTorium also offers the possibility to annotate artworks. We have therefore collected a Folksonomy about artworks. Applying Data Mining algorithms on this Folksonomy, we show that the reproduction of Art historical facts is possible. Moreover, we are able to analyse the extent of Art historical knowledge among people of the 21st century.",
    "author": [
      {
        "family": "Arends",
        "given": "Max"
      },
      {
        "family": "Froschauer",
        "given": "Josef"
      },
      {
        "family": "Goldfarb",
        "given": "Doron"
      },
      {
        "family": "Merkl",
        "given": "Dieter"
      }
    ],
    "container-title": "Proceedings of the 11th international conference on knowledge management and knowledge technologies (i-KNOW ’11)",
    "id": "Arends2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "semantic_web, vocabularies",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Analysing user generated content related to art history",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/5326.941845",
    "ISSN": "10946977",
    "abstract": "Character recognition (CR) has been extensively studied in the last half century and has progressed to a level that is sufficient to produce technology-driven applications. Now, rapidly growing computational power is enabling the implementation of the present CR methodologies and is creating an increasing demand in many emerging application domains which require more advanced methodologies. This paper serves as a guide and update for readers working in the CR area. First, the historical evolution of CR systems is presented. Then, the available CR techniques, with their superiorities and weaknesses, are reviewed. Finally, the current status of CR is discussed and directions for future research are suggested. Special attention is given to off-line handwriting recognition, since this area requires more research in order to reach the ultimate goal of machine simulation of human reading",
    "author": [
      {
        "family": "Arica",
        "given": "Nafiz"
      },
      {
        "family": "Yarman-Vural",
        "given": "Fatos T."
      }
    ],
    "container-title": "IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)",
    "id": "Arica2001",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "handwriting_recognition, ocr",
    "page": "216-233",
    "publisher": "IEEE",
    "title": "An overview of character recognition focused on off-line handwriting",
    "type": "article-journal",
    "volume": "31"
  },
  {
    "DOI": "10.1007/978-3-540-36728-4_17",
    "ISBN": "978-3-540-36727-7",
    "abstract": "This paper proposes an advanced method of geocoding for walking directions in documents using daily local expressions. Walking directions are usually described in casual expressions, but not in a formal address. Thus, it has been considered practically impossible to geocode walking directions. However, sidewalk network databases in major cities of Japan have been available for pedestrian navigation services with GPS-equipped mobile phones since 2003. The databases can be expected to enable more advanced geocoding for larger scale natural language expressions. We first introduce the core schema of sidewalk network databases. Then, we explain a structure of walking directions expressed in Japanese, and propose FRS (Formal Route Statement) to represent and process walking directions by means of a computer. Finally, prototype systems which have been developed based on our proposed framework are presented.",
    "author": [
      {
        "family": "Arikawa",
        "given": "Masatoshi"
      },
      {
        "family": "Noaki",
        "given": "Kouzou"
      }
    ],
    "collection-title": "Lecture notes in geoinformation and cartography",
    "container-title": "Location based services and TeleCartography",
    "editor": [
      {
        "family": "Gartner",
        "given": "Georg"
      },
      {
        "family": "Cartwright",
        "given": "William"
      },
      {
        "family": "Peterson",
        "given": "Michael P."
      }
    ],
    "id": "Arikawa2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "geocoding",
    "page": "217-229",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Geocoding Japanese walking directions using sidewalk network databases",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Aron",
        "given": "Raymond"
      }
    ],
    "id": "Aron1938",
    "issued": {
      "date-parts": [
        [
          1938
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Gallimard",
    "publisher-place": "Paris",
    "title": "Introduction à la philosophie de l’histoire: Essai sur les limites de l’objectivité historique",
    "title-short": "Introduction à la philosophie de l’histoire",
    "type": "book"
  },
  {
    "ISSN": "1091-8280",
    "PMCID": "PMC2233202",
    "PMID": "8947691",
    "URL": "http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2233202/",
    "abstract": "Accounting for textual variation in the documents and queries processed by information retrieval systems is considered essential for achieving good retrieval. Recent research has called into question several of the techniques used to support this endeavor. This paper reports on experiments with a concept based information retrieval system which relies on a program called MetaMap to account for textual variation in the process of mapping biomedical text such as MEDLINE bibliographic citations to the UMLS Metathesaurus. The experiments confirm that the effort expended in handling textual variation is well-spent for at least one type of concept based information retrieval.",
    "author": [
      {
        "family": "Aronson",
        "given": "Alan R."
      }
    ],
    "container-title": "Proceedings of the 1996 AMIA annual fall symposium",
    "id": "Aronson1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "ir",
    "page": "373-377",
    "publisher": "AMIA",
    "publisher-place": "Bethesda, MD, USA",
    "title": "The effect of textual variation on concept based information retrieval",
    "type": "paper-conference"
  },
  {
    "ISSN": "1531-605X",
    "PMCID": "PMC2243666",
    "PMID": "11825149",
    "URL": "http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2243666/",
    "abstract": "The UMLS Metathesaurus, the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classified by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map biomedical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM’s Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomedical literature at the library.",
    "author": [
      {
        "family": "Aronson",
        "given": "Alan R."
      }
    ],
    "container-title": "Proceedings of the 2001 AMIA Annual Symposium",
    "id": "Aronson2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "ir",
    "page": "17-21",
    "publisher": "AMIA",
    "publisher-place": "Bethesda, MD, USA",
    "title": "Effective mapping of biomedical text to the UMLS Metathesaurus: The MetaMap program.",
    "title-short": "Effective mapping of biomedical text to the UMLS Metathesaurus",
    "type": "article-journal"
  },
  {
    "DOI": "10.1136/jamia.2009.002733",
    "ISSN": "1527-974X",
    "PMCID": "PMC2995713",
    "PMID": "20442139",
    "abstract": "MetaMap is a widely available program providing access to the concepts in the unified medical language system (UMLS) Metathesaurus from biomedical text. This study reports on MetaMap’s evolution over more than a decade, concentrating on those features arising out of the research needs of the biomedical informatics community both within and outside of the National Library of Medicine. Such features include the detection of author-defined acronyms/abbreviations, the ability to browse the Metathesaurus for concepts even tenuously related to input text, the detection of negation in situations in which the polarity of predications is important, word sense disambiguation (WSD), and various technical and algorithmic features. Near-term plans for MetaMap development include the incorporation of chemical name recognition and enhanced WSD.",
    "author": [
      {
        "family": "Aronson",
        "given": "Alan R."
      },
      {
        "family": "Lang",
        "given": "François-Michel"
      }
    ],
    "container-title": "Journal of the American Medical Informatics Association (JAMIA)",
    "id": "Aronson2010",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "ir",
    "page": "229-236",
    "title": "An overview of MetaMap: Historical perspective and recent advances",
    "title-short": "An overview of MetaMap",
    "type": "article-journal",
    "volume": "17"
  },
  {
    "ISBN": "9780262527811",
    "URL": "http://www.worldcat.org/isbn/9780262527811",
    "abstract": "In the era of \"big data,\" science is increasingly information driven, and the potential for computers to store, manage, and integrate massive amounts of data has given rise to such new disciplinary fields as biomedical informatics. Applied ontology offers a strategy for the organization of scientific information in computer-tractable form, drawing on concepts not only from computer and information science but also from linguistics, logic, and philosophy. This book provides an introduction to the field of applied ontology that is of particular relevance to biomedicine, covering theoretical components of ontologies, best practices for ontology design, and examples of biomedical ontologies in use. After defining an ontology as a representation of the types of entities in a given domain, the book distinguishes between different kinds of ontologies and taxonomies, and shows how applied ontology draws on more traditional ideas from metaphysics. It presents the core features of the Basic Formal Ontology (BFO), now used by over one hundred ontology projects around the world, and offers examples of domain ontologies that utilize BFO. The book also describes Web Ontology Language (OWL), a common framework for Semantic Web technologies. Throughout, the book provides concrete recommendations for the design and construction of domain ontologies",
    "author": [
      {
        "family": "Arp",
        "given": "Robert"
      },
      {
        "family": "Smith",
        "given": "Barry"
      },
      {
        "family": "Spear",
        "given": "Andrew D."
      }
    ],
    "id": "Arp2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "ontologies, semantic_web",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Building ontologies with Basic Formal Ontology",
    "type": "book"
  },
  {
    "DOI": "10.1145/1141277.1141473",
    "ISBN": "1-59593-108-2",
    "abstract": "Language identification is an important task for web information retrieval. This paper presents the implementation of a tool for language identification in mono- and multi-lingual documents. The tool implements four algorithms for language identification. Furthermore, we present a n-gram approach for the identification of languages in multi-lingual documents. An evaluation for monolingual texts of varied length is presented. Results for eight languages including Ukrainian and Russian are presented. It could be shown that n-gram-based approaches outperform word-based algorithms for short texts. For longer texts, the performance is comparable. The evaluation for multi-lingual documents is based on real world web documents. Our tool is able to recognize the languages present in a document with reasonable accuracy.",
    "author": [
      {
        "family": "Artemenko",
        "given": "Olga"
      },
      {
        "family": "Mandl",
        "given": "Thomas"
      },
      {
        "family": "Shramko",
        "given": "Margaryta"
      },
      {
        "family": "Womser-Hacker",
        "given": "Christa"
      }
    ],
    "container-title": "Proceedings of the 2006 ACM symposium on applied computing (SAC ’06)",
    "id": "Artemenko2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "language_identification",
    "page": "859-860",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Evaluation of a language identification system for mono- and multilingual text documents",
    "type": "paper-conference"
  },
  {
    "DOI": "10.2307/2687821",
    "ISSN": "1079-8986",
    "abstract": "In 1933 Gödel introduced a calculus of provability (also known as modal logic S4) and left open the question of its exact intended semantics. In this paper we give a solution to this problem. We find the logic LP of propositions and proofs and show that Gödel’s provability calculus is nothing but the forgetful projection of LP. This also achieves Gödel’s objective of defining intuitionistic propositional logic Int via classical proofs and provides a Brouwer-Heyting-Kolmogorov style provability semantics for Int which resisted formalization since the early 1930s. LP may be regarded as a unified underlying structure for intuitionistic, modal logics, typed combinatory logic and λ-calculus.",
    "author": [
      {
        "family": "Artemov",
        "given": "Sergei N."
      }
    ],
    "container-title": "Bulletin of Symbolic Logic",
    "id": "Artemov2001",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "logic, uncertainty",
    "language": "en-US",
    "page": "1-36",
    "publisher": "Association for Symbolic Logic",
    "title": "Explicit provability and constructive semantics",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "DOI": "10.1016/j.tcs.2006.03.009",
    "ISSN": "03043975",
    "abstract": "In this paper we introduce the justified knowledge operator J with the intended meaning of Jφ as ’there is a justification for φ.’ Though justified knowledge appears here in a case study of common knowledge systems, a similar approach is applicable in more general situations. First we consider evidence-based common knowledge systems obtained by augmenting a multi-agent logic of knowledge with a system of evidence assertions t:φ, reflecting the notion ’t is an evidence for φ,’ such that evidence is respected by all agents. Justified common knowledge is obtained by collapsing all evidence terms into one modality J. We show that in standard situations, when the base epistemic systems are T, S4, and S5, the resulting justified common knowledge systems are normal modal logics, which places them within the scope of well-developed machinery applicable to modal logic: Kripke-style epistemic models, normalized proofs, automated proof search, etc. In the aforementioned situations, the intended semantics of justified knowledge is supported by a realization theorem stating that for any valid fact about justified knowledge, one could recover its constructive meaning by realizing all occurrences of justified knowledge modalities Jφ by appropriate evidence terms t:φ.",
    "author": [
      {
        "family": "Artemov",
        "given": "Sergei N."
      }
    ],
    "container-title": "Theoretical Computer Science",
    "id": "Artemov2006",
    "issue": "1–3",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "logic, uncertainty",
    "language": "en-US",
    "page": "4-22",
    "title": "Justified common knowledge",
    "type": "article-journal",
    "volume": "357"
  },
  {
    "DOI": "10.1057/9781137337016",
    "editor": [
      {
        "family": "Arthur",
        "given": "Paul Longley"
      },
      {
        "family": "Bode",
        "given": "Katherine"
      }
    ],
    "id": "Arthur2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Palgrave Macmillan UK",
    "title": "Advancing digital humanities",
    "type": "book"
  },
  {
    "DOI": "10.1057/9781137337016",
    "author": [
      {
        "family": "Bode",
        "given": "Katherine"
      },
      {
        "family": "Arthur",
        "given": "Paul Longley"
      }
    ],
    "chapter-number": "1",
    "container-title": "Advancing digital humanities",
    "editor": [
      {
        "family": "Arthur",
        "given": "Paul Longley"
      },
      {
        "family": "Bode",
        "given": "Katherine"
      }
    ],
    "id": "Bode2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "page": "1-12",
    "publisher": "Palgrave Macmillan UK",
    "title": "Collecting ourselves",
    "type": "chapter"
  },
  {
    "DOI": "10.1057/9781137337016",
    "author": [
      {
        "family": "Flanders",
        "given": "Julia"
      }
    ],
    "chapter-number": "11",
    "container-title": "Advancing digital humanities",
    "editor": [
      {
        "family": "Arthur",
        "given": "Paul Longley"
      },
      {
        "family": "Bode",
        "given": "Katherine"
      }
    ],
    "id": "Flanders2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "page": "163-174",
    "publisher": "Palgrave Macmillan UK",
    "title": "Rethinking collections",
    "type": "chapter"
  },
  {
    "DOI": "10.1057/9781137337016",
    "author": [
      {
        "family": "Turnbull",
        "given": "Paul"
      }
    ],
    "chapter-number": "17",
    "container-title": "Advancing digital humanities",
    "editor": [
      {
        "family": "Arthur",
        "given": "Paul Longley"
      },
      {
        "family": "Bode",
        "given": "Katherine"
      }
    ],
    "id": "Turnbull2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "page": "258-273",
    "publisher": "Palgrave Macmillan UK",
    "title": "Margins, mainstreams and the mission of digital humanities",
    "type": "chapter"
  },
  {
    "DOI": "10.1215/00166928-2392348",
    "ISSN": "2160-0228",
    "abstract": "Does Franco Moretti’s notion of “distant reading” really provide a liberating and democratic approach to literature, as it promises? Far from opening new perspectives, distant reading may actually blunt our critical faculties, inviting us to inadvertently adopt biased views of literature under the mask of objectivity. This essay aims to reveal the pseudoscientific nature of distant reading as applied to the analysis of literary genres. In “Conjectures on World Literature” (2000) Moretti discusses two basic cognitive metaphors: the tree and the wave. These and other metaphors are at the core of the three essays Moretti published in the New Left Review in 2003 and 2004 under the common heading “Graphs, Maps, Trees: Abstract Models for Literary History.” I argue that although Moretti draws his models from scientific disciplines (graphs from quantitative history, maps from geography, and trees from evolutionary theory), he actually uses them to support theories that lack proper verification, incurring the danger of bending reality to theory.",
    "author": [
      {
        "family": "Ascari",
        "given": "Maurizio"
      }
    ],
    "container-title": "Genre",
    "id": "Ascari2014",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities, literature",
    "language": "en-US",
    "page": "1-19",
    "publisher": "Duke University Press",
    "title": "The dangers of distant reading: Reassessing Moretti’s approach to literary genres",
    "title-short": "The dangers of distant reading",
    "type": "article-journal",
    "volume": "47"
  },
  {
    "author": [
      {
        "family": "Ascheri",
        "given": "Mario"
      }
    ],
    "id": "Ascheri1982",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "keyword": "classic, digital_humanities",
    "language": "it-IT",
    "publisher": "Il Leccio",
    "publisher-place": "Siena",
    "title": "I consilia dei giuristi medievali: per un repertorio-incipitario computerizzato",
    "title-short": "I consilia dei giuristi medievali",
    "type": "book"
  },
  {
    "DOI": "10.1336/1573565512",
    "abstract": "In this detailed study, Astin examines why assessment activity has produced such meager results and, just as important, how existing activities can be improved. The author also discusses what new assessment practices can be implemented and shares specific and sometimes startling ideas on: How assessment information can most effectively be used for evaluation How results can be used to enlighten and inform the practitioner How practical, technical, and political problems can be overcome when building an assessment database from student and faculty input How the movement of externally mandated assessments in various states is having a negative impact on higher education",
    "author": [
      {
        "family": "Astin",
        "given": "Alexander W."
      }
    ],
    "collection-title": "American council on education oryx press series on higher education",
    "id": "Astin1991",
    "issued": {
      "date-parts": [
        [
          1991,
          1
        ]
      ]
    },
    "keyword": "assessment, e-learning",
    "publisher": "Oryx Press",
    "title": "Assessment for excellence: The philosophy and practice of assessment and evaluation in higher education",
    "title-short": "Assessment for excellence",
    "type": "book"
  },
  {
    "URL": "http://www.ascilite.org/conferences/perth04/procs/",
    "accessed": {
      "date-parts": [
        [
          2021,
          2,
          13
        ]
      ]
    },
    "editor": [
      {
        "family": "Atkinson",
        "given": "Roger"
      },
      {
        "family": "McBeath",
        "given": "Clare"
      },
      {
        "family": "Jonas-Dwyer",
        "given": "Diana"
      },
      {
        "family": "Phillips",
        "given": "Rob"
      }
    ],
    "event": "21<sup>st</sup> ASCILITE conference",
    "event-date": {
      "date-parts": [
        [
          2004,
          12,
          5
        ],
        [
          2004,
          12,
          8
        ]
      ]
    },
    "event-place": "Perth, Western Australia",
    "id": "Atkinson2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "publisher": "ASCILITE",
    "publisher-place": "Tugun",
    "title": "Beyond the comfort zone: Proceedings of the 21<sup>st</sup> ASCILITE conference",
    "title-short": "Beyond the comfort zone",
    "type": "book"
  },
  {
    "DOI": "10.1145/1816123.1816166",
    "ISBN": "978-1-4503-0085-8",
    "abstract": "Despite the growing prominence of digital libraries as tools to support humanities scholars, little is known about the work practices and needs of these scholars as they pertain to working with source documents. In this paper we present our findings from a formative user study consisting of semi-structured interviews with eight scholars. We find that the use of source materials (by which we mean the original physical documents or digital facsimiles with minimal editorial intervention) in scholarship is not a simple, straight-forward examination of a document in isolation. Instead, scholars study source materials as an integral part of a complex ecosystem of inquiry that seeks to understand both the text being studied and the context in which that text was created, transmitted and used. Drawing examples from our interviews, we address critical questions of why scholars use source documents and what information they hope to gain by studying them. We also briefly summarize key note-taking practices as a means for assessing the potential to design user interfaces that support scholarly work-practices.",
    "author": [
      {
        "family": "Audenaert",
        "given": "Neal"
      },
      {
        "family": "Furuta",
        "given": "Richard"
      }
    ],
    "container-title": "Proceedings of the 10th annual joint conference on digital libraries (JCDL ’10)",
    "id": "Audenaert2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_library, hci, ir",
    "page": "283-292",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "What humanists want: How scholars use source materials",
    "title-short": "What humanists want",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/3-540-38076-0_13",
    "abstract": "There exists an urgent demand on defining architectures for Learning Management Systems, so that high-level frameworks for understanding these systems can be discovered, and quality attributes like portability, interoperability, reusability and modifiability can be achieved. In this paper we propose a prototype architecture aimed to engineer Open Learning Management Systems, that professes state-of-the-art software engineering techniques such as layered structure and component-based nature. Our work is based upon standards and practices from international standardization bodies, on the empirical results of designing, developing and evaluating Learning Management Systems and on the practices of well-established software engineering techniques.",
    "author": [
      {
        "family": "Avgeriou",
        "given": "Paris"
      },
      {
        "family": "Retalis",
        "given": "Simos"
      },
      {
        "family": "Skordalakis",
        "given": "Manolis"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Advances in informatics",
    "id": "Avgeriou2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "e-learning",
    "page": "83-99",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "An architecture for open learning management systems",
    "type": "chapter",
    "volume": "2563"
  },
  {
    "abstract": "The TUTOR logic-building language to be used with the PLATO (Programmed Logic for Automated Teaching Operations) system is explained in this manual. TUTOR is designed to transcend the difficulties of FORTRAN for a computer-based educational system utilizing graphical screen displays. The language consists of about seventy words or \"commands\" which can be used in various combinations to produce the desired effect. It was designed specifically for use by lesson authors lacking prior knowledge of and experience with computers. Although authors are able to write parts of useful lessons after approximately one hour of introduction to TUTOR, the ultimate complexity and flexibility of TUTOR lessons is limited largely by the ingenuity and experience of lesson authors. A sample TUTOR program which allows the student to construct geometric shapes on his television screen demonstrates how a TUTOR lesson appears to the student. A complete description of the structure and elements of the language is presented, as well as a description of methods for inputting lessons and obtaining output. The manual is intended to be used as a textbook by the beginning lesson author and as a reference tool by the experienced TUTOR user.",
    "author": [
      {
        "family": "Avner",
        "given": "R. A."
      },
      {
        "family": "Tenczar",
        "given": "Paul"
      }
    ],
    "genre": "CERL Report",
    "id": "Avner1969",
    "issued": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "number": "X-4",
    "publisher": "University of Illinois",
    "publisher-place": "Urbana, IL",
    "title": "The TUTOR manual",
    "type": "report"
  },
  {
    "URL": "http://www.bmbf.de/foerderungen/2576.php",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "Federal Republic of Germany. Bundesministerium für Bildung und Forschung (Federal Ministry for Education and Research)"
      }
    ],
    "id": "BMBF2004",
    "issued": {
      "date-parts": [
        [
          2004,
          7
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Bekanntmachung des Bundesministeriums für Bildung und Forschung",
    "title": ". Richtlinien über die Förderung der Entwicklung und Erprobung von Maßnahmen der Strukturentwicklung zur Etablierung von eLearning in der Hochschullehre im Rahmen des Förderschwerpunkts ",
    "type": ""
  },
  {
    "URL": "http://technologische-leistungsfaehigkeit.de/en/4281.php",
    "accessed": {
      "date-parts": [
        [
          2008,
          12,
          9
        ]
      ]
    },
    "author": [
      {
        "literal": "Federal Republic of Germany. Bundesministerium für Bildung und Forschung (Federal Ministry for Education and Research)"
      }
    ],
    "id": "BMBF2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "title": "2005 report on Germany’s technological performance: Main statements from the federal government’s point of view",
    "title-short": "2005 report on Germany’s technological performance",
    "type": ""
  },
  {
    "DOI": "10.1145/502716.502722",
    "ISBN": "1-58113-459-2",
    "abstract": "In traditional human-computer interfaces, a human master directs a computer system as a servant, telling it not only what to do, but also how to do it. Collaborative interfaces attempt to realign the roles, making the participants collaborators in solving the person’s problem. This paper describes Writer’s Aid, a system that deploys AI planning techniques to enable it to serve as an author’s collaborative assistant. Writer’s Aid differs from previous collaborative interfaces in both the kinds of actions the system partner takes and the underlying technology it uses to do so. While an author writes a document, Writer’s Aid helps in identifying and inserting citation keys and by autonomously finding and caching potentially relevant papers and their associated bibliographic information from various on-line sources. This autonomy, enabled by the use of a planning system at the core of Writer’s Aid, distinguishes this system from other collaborative interfaces. The collaborative design and its division of labor result in more efficient operation: faster and easier writing on the user’s part and more effective information gathering on the part of the system. Subjects in our laboratory user study found the system effective and the interface intuitive and easy to use.",
    "author": [
      {
        "family": "Babaian",
        "given": "Tamara"
      },
      {
        "family": "Grosz",
        "given": "Barbara J."
      },
      {
        "family": "Shieber",
        "given": "Stuart M."
      }
    ],
    "container-title": "Proceedings of the 7th international conference on intelligent user interfaces (IUI ’02)",
    "id": "Babaian2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "authoring, emacs, interactive_editing",
    "page": "7-14",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A writer’s collaborative assistant",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Bachelard",
        "given": "Suzanne"
      }
    ],
    "container-title": "Élaboration et justification des modelès",
    "editor": [
      {
        "family": "Delattre",
        "given": "Pierre"
      },
      {
        "family": "Thellier",
        "given": "Michel"
      }
    ],
    "id": "Bachelard1979",
    "issued": {
      "date-parts": [
        [
          1979
        ]
      ]
    },
    "language": "fr-FR",
    "page": "3-19",
    "publisher": "Maloine",
    "publisher-place": "Paris",
    "title": "Quelques aspects historiques des notion de modèle et de justification des modèles",
    "type": "chapter",
    "volume": "I"
  },
  {
    "DOI": "10.1145/1860559.1860622",
    "ISBN": "978-1-4503-0231-9",
    "abstract": "Medieval manuscript layouts are quite complex. Additionally to their main text flow, which can spread over one or several columns, such manuscripts contain also other textual elements such as insertions, annotations, and corrections. They are often richly decorated with ornaments, illustrations, and drop capitals making their layout even more complex. In this paper we propose a generic layout model to represent their physical structure. To achieve this goal we propose to use four layers in order to distinguish between the different graphical elements. In this paper we show how this model is used to represent automatic segmentation results and how it allows a quantitative measure of their accuracy.",
    "author": [
      {
        "family": "Baechler",
        "given": "Micheal"
      },
      {
        "family": "Ingold",
        "given": "Rolf"
      }
    ],
    "collection-title": "DocEng ’10",
    "container-title": "Proceedings of the 10th ACM symposium on document engineering",
    "id": "Baechler2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "page": "275-278",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Medieval manuscript layout model",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/icfhr.2010.36",
    "ISBN": "978-0-7695-4221-8",
    "abstract": "Medieval manuscript layouts are quite complex. They contain textual elements such as insertions, annotations, and corrections. They may be richly decorated with ornaments, illustrations, and decorative initials making their layout even more complex. In this paper we describe a semi-automatic tool which annotates medieval manuscripts using our generic format. This format allows to represent the physical structure of such manuscripts. Our semi-automatic tool is composed of two parts. The first part achieves a layout analysis which automatically segments manuscripts into text blocks and text lines. That is, a Multi-Layer Perceptron (MLP) identifies layout elements by using color features, it extracts the textual content image of the manuscript. Then, a segmentation based on Connected Component (CC) is performed on the textual content in order to retrieve text blocks and lines. The second part provides an interactive interface allowing the user to customize the automatic analysis, to visualize its results, and to correct them. Our tool is still a prototype, nevertheless, first experiments give encouraging results. Thus, in this paper, we show how to generate a ground truth for medieval manuscripts layouts.",
    "author": [
      {
        "family": "Baechler",
        "given": "Micheal"
      },
      {
        "family": "Bloechle",
        "given": "Jean L."
      },
      {
        "family": "Ingold",
        "given": "Rolf"
      }
    ],
    "container-title": "Frontiers in handwriting recognition (ICFHR), 2010 international conference on",
    "id": "Baechler2010b",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, document_analysis, ocr",
    "page": "182-187",
    "publisher": "IEEE Computer Society",
    "publisher-place": "Washington, DC, USA",
    "title": "Semi-automatic annotation tool for medieval manuscripts",
    "type": "paper-conference"
  },
  {
    "abstract": "Blackletter type, also known as Fraktur or German Gothic, originated with Gutenberg’s moveable type, and was based on the contemporary calligraphy of that time. From the sixteenth century on, it shared the spotlight with roman type in German-speaking countries and was even adopted for the printing of Martin Luther’s writings. Yet by the twentieth century it was increasingly spurned by both commercial artists, who embraced roman type for its classical associations, and modernist designers, who championed sanserif type for its universal and democratic qualities. At the close of the Second World War, the identification of blackletter with failed Nazi ideology was inescapable, thus effectively ending the four-hundred-year tradition of blackletter as a distinctive national script. The essays in Blackletter investigate the rise and fall of blackletter type, examining its uses and cultural significance at various points throughout history, including the Reformation, the Weimar Republic, the Nazi regime, and the post-Berlin Wall period. This title, illustrated with numerous color examples of blackletter typefaces and their implementation, is a necessity for anyone interested in the history of type.",
    "editor": [
      {
        "family": "Bain",
        "given": "Peter"
      },
      {
        "family": "Shaw",
        "given": "Paul"
      }
    ],
    "id": "Bain1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "typography",
    "publisher": "Paperback; Princeton Architectural Press",
    "publisher-place": "New York, NY, USA",
    "title": "Blackletter: Type and national identity",
    "title-short": "Blackletter",
    "type": "book"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/pdf/295_paper.pdf",
    "abstract": "This paper describes an accurate, extensible method for automatically classifying unknown foreign words that requires minimal monolingual resources and no bilingual training data (which is often difficult to obtain for an arbitrary language pair). We use a small set of phonologically-based transliteration rules to generate a potentially unlimited amount of pseudo-data that can be used to train a classifier to distinguish etymological classes of actual words. We ran a series of experiments on identifying English loanwords in Korean, in order to explore the consequences of using pseudo-data in place of the original training data. Results show that a sufficient quantity of automatically generated training data, even produced by fairly low precision transliteration rules, can be used to train a classifier that performs within 0.3% of one trained on actual English loanwords (~ 96% accuracy).",
    "author": [
      {
        "family": "Baker",
        "given": "Kirk"
      },
      {
        "family": "Brew",
        "given": "Chris"
      }
    ],
    "container-title": "Proceedings of the sixth international language resources and evaluation (LREC’08)",
    "id": "Baker2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "page": "1159-1163",
    "title": "Statistical identification of english loanwords in korean using automatically generated training data",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1016/j.pss.2012.10.008",
    "ISSN": "00320633",
    "abstract": "Analogical reasoning is critical to planetary geology, but its role can be misconstrued by those unfamiliar with the practice of that science. The methodological importance of analogy to geology lies in the formulation of genetic hypotheses, an absolutely essential component of geological reasoning that was either ignored or denigrated by most 20th century philosophers of science, who took the theoretical/experimental methodology of physics to be the sole model for all of scientific inquiry. Following the seminal 19th century work of Grove Karl Gilbert, an early pioneer of planetary geology, it has long been recognized that broad experience with and understanding of terrestrial geological phenomena provide geologists with their most effective resource for the invention of potentially fruitful, working hypotheses. The actions of (1) forming such hypotheses, (2) following their consequences, and (3) testing those consequences comprise integral parts of effective geological practice in regard to the understanding of planetary surfaces. Nevertheless, the logical terminology and philosophical bases for such practice will be unfamiliar to most planetary scientists, both geologists and nongeologists. The invention of geological hypotheses involves both inductive inferences of the type Gilbert termed ” empiric classification” and abductive inferences of a logical form made famous by the 19th century American logician Charles Sanders Peirce. The testing and corroboration of geological hypotheses relies less on the correspondence logic of theoretical/ experimental sciences, like physics, and more on the logic of consistency, coherence, and consilience that characterizes the investigative and historical sciences of interpretation exemplified by geology. Highlights: ▶︎ planetary geological hypotheses derive from analogical reasoning. ▶︎ known Earth phenomena serve as the most effective analogs for planetary geology. ▶︎ analogical reasoning in geology has logical basis that promotes productive inquiry. ▶︎ analogical inferences in geology should tested for consistency, coherence and consilience.",
    "author": [
      {
        "family": "Baker",
        "given": "Victor R."
      }
    ],
    "container-title": "Planetary and Space Science",
    "id": "Baker2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "philosophy_of_science, uncertainty",
    "page": "5-10",
    "title": "Terrestrial analogs, planetary geology, and the nature of geological reasoning",
    "type": "article-journal",
    "volume": "95"
  },
  {
    "URL": "https://aclanthology.org/N10-1027",
    "abstract": "Language identification is the task of identifying the language a given document is written in. This paper describes a detailed examination of what models perform best under different conditions, based on experiments across three separate datasets and a range of tokenisation strategies. We demonstrate that the task becomes increasingly difficult as we increase the number of languages, reduce the amount of training data and reduce the length of documents. We also show that it is possible to perform language identification without having to perform explicit character encoding detection.",
    "author": [
      {
        "family": "Baldwin",
        "given": "Timothy"
      },
      {
        "family": "Lui",
        "given": "Marco"
      }
    ],
    "container-title": "Human language technologies: The 2010 annual conference of the north american chapter of the association for computational linguistics",
    "event-place": "Los Angeles, CA, USA",
    "id": "Baldwin2010",
    "issued": {
      "date-parts": [
        [
          2010,
          6
        ]
      ]
    },
    "keyword": "language_identification",
    "page": "229-237",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Language identification: The long and the short of the matter",
    "title-short": "Language identification",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1860559.1860624",
    "ISBN": "978-1-4503-0231-9",
    "abstract": "Keyword extraction is a fundamental problem in text data mining and document processing. A large number of document processing applications directly depend on the quality and speed of keyword extraction algorithms. In this article, a novel approach to rapid change detection in data stream. and documents is developed. It is based on ideas from image processing and especially on the Helmholtz Principle from the Gestalt Theory of human perception. Applied to the problem of keywords extraction, it delivers fast and effective tools to identify meaningful keywords using parameter-free methods. We also define a level of meaningfulness of the keywords which can be used to modify the set of keywords depending on application needs.",
    "author": [
      {
        "family": "Balinsky",
        "given": "Alexander A."
      },
      {
        "family": "Balinsky",
        "given": "Helen Y."
      },
      {
        "family": "Simske",
        "given": "Steven J."
      }
    ],
    "collection-title": "DocEng ’10",
    "container-title": "Proceedings of the 10th ACM symposium on document engineering",
    "id": "Balinsky2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "document_engineering, ir",
    "page": "283-286",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "On Helmholtz’s principle for documents processing",
    "type": "paper-conference"
  },
  {
    "ISSN": "0049-3155",
    "URL": "http://www.ingentaconnect.com/content/stc/tc/2010/00000057/00000001/art00003",
    "abstract": "Purpose: This article studies and determines the benefits for technical communicators using narrative to compose and edit software requirements specifications. Specifically, this article is an examination of requirements specifications written for a Web-based radiology application serving the medical industry. Method: The study adheres to the usability principle that successful design accommodates complex problem solving. Requirements specifications, the application, and the application’s code are examined as part of the study. Results: The first determination is that composing detailed narratives within the requirements specifications can ensure flexible spaces for users, in this case doctors, to view, study, and manipulate data as they see fit. The article also acknowledges and accounts for the reality of low-level or code-level procedural programming required for creating such flexible spaces. The second determination is that employing narratological structures within requirements specifications also leads to technical inventions at the code level. Practitioners will have a better understanding of how their work facilitates the development of a software application’s functionality, design, and even code. Conclusion: Ultimately, narrative is the suggested method for developing the flexible affordances desired by usability specialists and it simultaneously helps negotiate low-level code.",
    "author": [
      {
        "family": "Ballentine",
        "given": "Brian D."
      }
    ],
    "container-title": "Technical Communication",
    "id": "Ballentine2010",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2010,
          2
        ]
      ]
    },
    "keyword": "hci, wrabii, writing_research",
    "page": "26-43",
    "publisher": "Society for Technical Communication",
    "title": "Requirements specifications and anticipating user needs: Methods and warnings on writing development narratives for new software",
    "title-short": "Requirements specifications and anticipating user needs",
    "type": "article-journal",
    "volume": "57"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/workshops/W22_Proceedings.pdf",
    "abstract": "We describe here a method for discovering imitative textual allusions in a large collection of Classical Latin poetry. In translating the logic of literary allusion into computa- tional terms, we include not only traditional IR variables such as token similarity and n-grams, but also incorporate a comparison of syntactic structure as well. This provides a more robust search method for Classical languages since it accomodates their relatively free word order and rich inflection, and has the potential to improve fuzzy string searching in other languages as well.",
    "author": [
      {
        "family": "Bamman",
        "given": "David"
      },
      {
        "family": "Crane",
        "given": "Gregory"
      }
    ],
    "container-title": "Proceedings of the LREC 2008 workshop on language technology for cultural heritage data (LaTeCH 2008)",
    "editor": [
      {
        "family": "Ribarov",
        "given": "Kiril"
      },
      {
        "family": "Sporleder",
        "given": "Caroline"
      }
    ],
    "id": "Bamman2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "digital_humanities, intertextuality, plagiarism",
    "page": "1-8",
    "title": "The logic and discovery of textual allusion",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/pdf/25_paper.pdf",
    "abstract": "The paper describes the treatment of some specific syntactic constructions in two treebanks of Latin according to a common set of annotation guidelines. Both projects work within the theoretical framework of Dependency Grammar, which has been demonstrated to be an especially appropriate framework for the representation of languages with a moderately free word order, where the linear order of constituents is broken up with elements of other constituents. The two projects are the first of their kind for Latin, so no prior established guidelines for syntactic annotation are available to rely on. The general model for the adopted style of representation is that used by the Prague Dependency Treebank, with departures arising from the Latin grammar of Pinkster, specifically in the traditional grammatical categories of the ablative absolute, the accusative + infinitive, and gerunds/gerundives. Sharing common annotation guidelines allows us to compare the datasets of the two treebanks for tasks such as mutually checking annotation consistency, diachronically studying specific syntactic constructions, and training statistical dependency parsers.",
    "author": [
      {
        "family": "Bamman",
        "given": "David"
      },
      {
        "family": "Passarotti",
        "given": "Marco"
      },
      {
        "family": "Busa",
        "given": "Roberto"
      },
      {
        "family": "Crane",
        "given": "Gregory"
      }
    ],
    "container-title": "Proceedings of the sixth international conference on language resources and evaluation (LREC’08)",
    "editor": [
      {
        "family": "Calzolari",
        "given": "Nicoletta"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Maegaard",
        "given": "Bente"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Odijk",
        "given": "Jan"
      },
      {
        "family": "Piperidis",
        "given": "Stelios"
      },
      {
        "family": "Tapias",
        "given": "Daniel"
      }
    ],
    "id": "Bamman2008a",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, latin",
    "publisher": "European Language Resources Association (ELRA)",
    "title": "The annotation guidelines of the Latin Dependency Treebank and Index Thomisticus Treebank: The treatment of some specific syntactic constructions in Latin",
    "title-short": "The annotation guidelines of the Latin Dependency Treebank and Index Thomisticus Treebank",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1378889.1378892",
    "ISBN": "978-1-59593-998-2",
    "abstract": "We describe here in detail our work toward creating a dynamic lexicon from the texts in a large digital library. By leveraging a small structured knowledge source (a 30,457 word treebank), we are able to extract selectional preferences for words from a 3.5 million word Latin corpus. This is promising news for low-resource languages and digital collections seeking to leverage a small human investment into much larger gain. The library architecture in which this work is developed allows us to query customized subcorpora to report on lexical usage by author, genre or era and allows us to continually update the lexicon as new texts are added to the collection.",
    "author": [
      {
        "family": "Bamman",
        "given": "David"
      },
      {
        "family": "Crane",
        "given": "Gregory"
      }
    ],
    "container-title": "Proceedings of the 8th ACM/IEEE-CS joint conference on digital libraries (JCDL ’08)",
    "id": "Bamman2008b",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_library, latin, pos_tagging",
    "page": "11-20",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Building a dynamic lexicon from a digital library",
    "type": "paper-conference"
  },
  {
    "URL": "http://tlt8.unicatt.it/allegati/Proceedings_TLT8.pdf",
    "abstract": "We describe here the first release of the Ancient Greek Dependency Treebank (AGDT), a 190,903-word syntactically annotated corpus of literary texts including the works of Hesiod, Homer and Aeschylus. While the far larger works of Hesiod and Homer (142,705 words) have been annotated under a standard treebank production method of soliciting annotations from two independent reviewers and then reconciling their differences, we also put forth with Aeschylus (48,198 words) a new model of treebank production that draws on the methods of classical philology to take into account the personal responsibility of the annotator in the publication and ownership of a “scholarly” treebank.",
    "author": [
      {
        "family": "Bamman",
        "given": "David"
      },
      {
        "family": "Mambrini",
        "given": "Francesco"
      },
      {
        "family": "Crane",
        "given": "Gregory"
      }
    ],
    "container-title": "Proceedings of the eighth international workshop on treebanks and linguistic theories (TLT 2009)",
    "editor": [
      {
        "family": "Passarotti",
        "given": "Marco"
      },
      {
        "family": "Przepiórkowski",
        "given": "Adam"
      },
      {
        "family": "Raynaud",
        "given": "Savina"
      },
      {
        "family": "Van Eynde",
        "given": "Frank"
      }
    ],
    "id": "Bamman2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, greek",
    "page": "5-15",
    "publisher": "EDUCatt",
    "publisher-place": "Milan, Italy",
    "title": "An ownership model of annotation: The Ancient Greek Dependency Treebank",
    "title-short": "An ownership model of annotation",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-20227-8_5",
    "ISBN": "978-3-642-20226-1",
    "abstract": "This paper describes the development, composition, and several uses of the Ancient Greek and Latin Dependency Treebanks, large collections of Classical texts in which the syntactic, morphological and lexical information for each word is made explicit. To date, over 200 individuals from around the world have collaborated to annotate over 350,000 words, including the entirety of Homer’s Iliad and Odyssey, Sophocles’ Ajax, all of the extant works of Hesiod and Aeschylus, and selections from Caesar, Cicero, Jerome, Ovid, Petronius, Propertius, Sallust and Vergil. While perhaps the most straightforward value of such an annotated corpus for Classical philology is the morphosyntactic searching it makes possible, it also enables a large number of downstream tasks as well, such as inducing the syntactic behavior of lexemes and automatically identifying similar passages between texts.",
    "author": [
      {
        "family": "Bamman",
        "given": "David"
      },
      {
        "family": "Crane",
        "given": "Gregory"
      }
    ],
    "chapter-number": "5",
    "collection-title": "Theory and applications of natural language processing",
    "container-title": "Language technology for cultural heritage",
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "dropping-particle": "van den",
        "family": "Bosch",
        "given": "Antal"
      },
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      }
    ],
    "id": "Bamman2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, greek, latin",
    "page": "79-98",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "The Ancient Greek and Latin dependency treebanks",
    "type": "chapter"
  },
  {
    "DOI": "10.2307/1170560",
    "abstract": "Word processing in writing instruction may provide lasting educational benefits to users because it encourages a fluid conceptualization of text and frees the writer from mechanical concerns. This meta-analysis reviews 32 studies that compared two groups of students receiving identical writing instruction but allowed only one group to use word processing for writing assignments. Word processing groups, especially weaker writers, improved the quality of their writing. Word processing students wrote longer documents but did not have more positive attitudes toward writing. More effective uses of word processing as an instructional tool might include adapting instruction to software strengths and adding metacognitive prompts to the writing program.",
    "author": [
      {
        "family": "Bangert-Drowns",
        "given": "Robert L."
      }
    ],
    "container-title": "Review of Educational Research",
    "id": "Bangert-Drowns1993",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "interactive_editing, writing_research",
    "page": "69-93",
    "publisher": "American Educational Research Association",
    "title": "The word processor as an instructional tool: A meta-analysis of word processing in writing instruction",
    "title-short": "The word processor as an instructional tool",
    "type": "article-journal",
    "volume": "63"
  },
  {
    "abstract": "Taking advantage of user-friendly technology, audience response systems (ARS) facilitates greater interaction with participants engaged in a variety of group activities. Each participant has an input device that permits them to express a view in complete anonymity, and the composite view of the total group appears on a public screen. ARS can then be used to support summative and formative activities with groups ranging in size from as small as five through to large groups of several hundred. The data can be used to help the facilitator adjust the pace of teaching to match the requirements of the learners, gauge understanding, or trigger discussion and debate. Audience Response Systems in Higher Education: Applications and Cases reveals some of the history behind these systems, explores current theory and practice, and indicates where technology may move in the future. Cases are used to present the work of educators in a wide range of subject areas and with differing levels of experience with these systems.",
    "editor": [
      {
        "family": "Banks",
        "given": "David A."
      }
    ],
    "id": "Banks2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "e-learning",
    "note": "Via Google Books",
    "publisher": "Hardcover; Information Science Publishing",
    "publisher-place": "Hershey, PA",
    "title": "Audience response systems in higher education: Applications and cases",
    "title-short": "Audience response systems in higher education",
    "type": "book"
  },
  {
    "URL": "http://www.mt-archive.info/CompAutomation-1953-Bar-Hillel.pdf",
    "abstract": "More than a year ago, I wrote a paper entitled \"The Present State of Research on Mechanical Translation\", which was published in \"American Documentation\" (see reference 8 below). Few engineers perhaps are likely to consult the journal in which it was published, and additional advances have been made during 1952 and the first months of 1953, both in theory and in organization. Therefore it may be worthwhile to present here a summary of my earlier paper and to indicate some of these advances.",
    "author": [
      {
        "family": "Bar-Hillel",
        "given": "Yehoshua"
      }
    ],
    "container-title": "Computers and Automation",
    "id": "Bar-Hillel1953",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          1953
        ]
      ]
    },
    "keyword": "classic, machine_translation",
    "page": "1-6",
    "title": "Machine translation",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "URL": "http://www.mt-archive.info/AmSci-1954-Bar-Hillel.pdf",
    "abstract": "A good translation of a novel requires many qualities from the translator. He must be a native speaker or have at least native-like command of the language into which he translates, the target language; he must have a good knowledge of the language from which he translates, the source language, and preferably have lived among the people who habitually speak the source language; he must have some knowledge of the author’s biography and cultural background, and of the social situation prevailing at the time and place of the novel’s action; he must have imagination and some original creative ability; and finally he must also have everything that is included under experience. Even if the translator possesses all these qualities to the highest degree, he is still apt to be ac- cused of being a traitor to the author’s original intentions—every translator being by necessity a traitor: traduttore, traditore, as the Italian proverb goes.",
    "author": [
      {
        "family": "Bar-Hillel",
        "given": "Yehoshua"
      }
    ],
    "container-title": "American Scientist",
    "id": "Bar-Hillel1954",
    "issue": "42",
    "issued": {
      "date-parts": [
        [
          1954
        ]
      ]
    },
    "keyword": "classic, machine_translation",
    "page": "248-260",
    "title": "Can translation be mechanized?",
    "type": "article-journal"
  },
  {
    "ISBN": "3436014435",
    "author": [
      {
        "family": "Bar-Hillel",
        "given": "Yehoshua"
      }
    ],
    "container-title": "Informationen über Information: Probleme der Kybernetik",
    "editor": [
      {
        "dropping-particle": "von",
        "family": "Ditfurth",
        "given": "Hoimar"
      }
    ],
    "id": "Bar-Hillel1971",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "keyword": "cybernetics",
    "language": "de-DE",
    "page": "9-31",
    "publisher": "Fischer Taschenbuch",
    "publisher-place": "Frankfurt am Main",
    "title": "Wesen und Bedeutung der Informationstheorie",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/2245276.2245403",
    "ISBN": "978-1-4503-0857-1",
    "author": [
      {
        "family": "Barabucci",
        "given": "Gioele"
      },
      {
        "family": "Peroni",
        "given": "Silvio"
      },
      {
        "family": "Poggi",
        "given": "Francesco"
      },
      {
        "family": "Vitali",
        "given": "Fabio"
      }
    ],
    "collection-title": "SAC ’12",
    "container-title": "Proceedings of the 27th annual ACM symposium on applied computing",
    "id": "Barabucci2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "EARMARK, markup",
    "page": "658-663",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Embedding semantic annotations within texts: The FRETTA approach",
    "title-short": "Embedding semantic annotations within texts",
    "type": "paper-conference"
  },
  {
    "DOI": "10.16995/dscn.275",
    "author": [
      {
        "family": "Barber",
        "given": "John F."
      }
    ],
    "container-title": "Digital Studies/Le champ numérique",
    "id": "Barber2017",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "page": "1-15",
    "title": "Radio Nouspace: Sound, radio, digital humanities",
    "title-short": "Radio Nouspace",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "DOI": "10.1093/llc/fqy040",
    "ISSN": "2055-7671",
    "abstract": "In ’Christopher Marlowe: Hype and Hoax’(2018), Hartmut Ilsemann implies that his application of the Rolling Delta feature of R Stylo is sufficiently robust that a century and a half of traditional scholarship should be overturned, and Marlowe stripped of the majority of his canon, including Doctor Faustus and Edward II. The article concludes that ’Marlowe is totally overrated in his influence on modern English drama’ (p. 26), the natural consequence of stripping away 5/7ths of his canon. In this response, I demonstrate that the assumptions underlying this application of the Delta method, and the application itself, are fundamentally flawed, leading to predictably erroneous conclusions. Problems with the study include a poorly designed test environment, incorrect preparation of texts, assuming that ’Marlowe’s style’ can be determined by a single early play, selecting and constructing Shakespeare’s comparison texts in a manner likely to prejudice results, ignoring the effect upon style of a play’s date and genre, failing to consider the effect of different-length comparison texts, and dismissing external evidence of authorship that conflicts with the test outcomes. I argue that in the light of these issues, the results and conclusions must be dismissed. Further, the question is raised as to whether the current methods of computational stylistics, even when more rigorously applied, are equipped to challenge the attribution of the accepted Marlowe canon.",
    "author": [
      {
        "family": "Barber",
        "given": "Ros"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Barber2018",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "1-12",
    "title": "Marlowe and overreaching: A misuse of stylometry",
    "title-short": "Marlowe and overreaching",
    "type": "article-journal",
    "volume": "34"
  },
  {
    "ISBN": "9780804738712",
    "abstract": "Bootstrapping analyzes the genesis of personal computing from both technological and social perspectives, through a close study of the pathbreaking work of one researcher, Douglas Engelbart. In his lab at the Stanford Research Institute in the 1960s, Engelbart, along with a small team of researchers, developed some of the cornerstones of personal computing as we know it, including the mouse, the windowed user interface, and hypertext. Today, all these technologies are well known, even taken for granted, but the assumptions and motivations behind their invention are not. Bootstrapping establishes Douglas Engelbart’s contribution through a detailed history of both the material and the symbolic constitution of his system’s human-computer interface in the context of the computer research community in the United States in the 1960s and 1970s. Engelbart felt that the complexity of many of the world’s problems was becoming overwhelming, and the time for solving these problems was becoming shorter and shorter. What was needed, he determined, was a system that would augment human intelligence, co-transforming or co-evolving both humans and the machines they use. He sought a systematic way to think and organize this coevolution in an effort to discover a path on which a radical technological improvement could lead to a radical improvement in how to make people work effectively. What was involved in Engelbart’s project was not just the invention of a computerized system that would enable humans, acting together, to manage complexity, but the invention of a new kind of human, \"the user.\" What he ultimately envisioned was a \"bootstrapping\" process by which those who actually invented the hardware and software of this new system would simultaneously reinvent the human in a new form. The book also offers a careful narrative of the collapse of Engelbart’s laboratory at Stanford Research Institute, and the further translation of Engelbart’s vision. It shows that Engelbart’s ultimate goal of coevolution came to be translated in terms of technological progress and human adaptation to supposedly user-friendly technologies. At a time of the massive diffusion of the World Wide Web, Bootstrapping recalls the early experiments and original ideals that led to today’s \"information revolution.\"",
    "author": [
      {
        "family": "Bardini",
        "given": "Thierry"
      }
    ],
    "id": "Bardini2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "hci, interactive_editing",
    "language": "en-US",
    "publisher": "Stanford University Press",
    "publisher-place": "Stanford, CA, USA",
    "title": "Bootstrapping: Douglas Engelbart, coevolution, and the origins of personal computing",
    "title-short": "Bootstrapping",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Baron",
        "given": "Dennis"
      }
    ],
    "container-title": "Passions, pedagogies, and 21<sup>st</sup> century technologies",
    "editor": [
      {
        "family": "Hawisher",
        "given": "Gail E."
      },
      {
        "family": "Selfe",
        "given": "Cynthia L."
      }
    ],
    "id": "Baron1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "interactive_editing, writing_research",
    "page": "15-33",
    "publisher": "Uta State Press",
    "publisher-place": "Logan, UT, USA",
    "title": "From pencils to pixels: The stages of literacy technologies",
    "title-short": "From pencils to pixels",
    "type": "chapter"
  },
  {
    "abstract": "When applying corpus linguistic techniques to historical corpora, the corpus researcher should be cautious about the results obtained. Corpus annotation techniques such as part of speech tagging, trained for modern languages, are particularly vulnerable to inaccuracy due to vocabulary and grammatical shifts in language over time. Basic corpus retrieval techniques such as frequency profiling and concordancing will also be affected, in addition to the more sophisticated",
    "author": [
      {
        "family": "Baron",
        "given": "Alistair"
      },
      {
        "family": "Rayson",
        "given": "Paul"
      }
    ],
    "container-title": "Proceedings of the postgraduate conference in corpus linguistics",
    "id": "Baron2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "publisher": "Aston University",
    "publisher-place": "Birmingham, UK",
    "title": "VARD 2: A tool for dealing with spelling variation in historical corpora",
    "title-short": "VARD 2",
    "type": "paper-conference"
  },
  {
    "URL": "http://ucrel.lancs.ac.uk/publications/cl2009/314_FullPaper.pdf",
    "abstract": "Large quantities of spelling variation in corpora, such as that found in Early Modern English, can cause significant problems for corpus linguistic tools and methods. Having texts with standardised spelling is key to making such tools and methods accurate and meaningful in their analysis. Gaining access to such versions of texts can be problematic however, and manual standardisation of the texts is often too time-consuming to be feasible. Our solution is a piece of software named VARD 2 which can be used to manually and automatically standardise spelling variation in individual texts, or corpora of any size. This paper evaluates VARD 2’s performance on a corpus of Early Modern English letters and a corpus of children’s written English. The software’s ability to learn from manual standardisation is put under particular scrutiny as we examine what effect different levels of training have on its performance.",
    "author": [
      {
        "family": "Baron",
        "given": "Alistair"
      },
      {
        "family": "Rayson",
        "given": "Paul"
      }
    ],
    "container-title": "Proceedings of corpus linguistics 2009",
    "id": "Baron2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, english, spelling_normalization",
    "title": "Automatic standardization of texts containing spelling variation, how much training data do you need?",
    "type": "paper-conference"
  },
  {
    "abstract": "This paper examines the technique of key word analysis. This is one of the most widely-used methods for discovering significant words, and is achieved by comparing the frequencies of words in a corpus with frequencies of those words in a (usually larger) reference corpus. It should be noted that the vast majority of key words studies take place using corpora of modern language. However, in this paper, we look at the possible problems that may occur when applying the same technique to historical corpora, and in particular, corpora of Early Modern English, a variety for which there are significant volumes of text already available. In addition, there is a growing body of historical data from this period being scanned and transcribed in the large digitisation initiatives such as Early English Books Online, British Library Newspapers Digitisation Project etc.",
    "author": [
      {
        "family": "Baron",
        "given": "Alistair"
      },
      {
        "family": "Rayson",
        "given": "Paul"
      },
      {
        "family": "Archer",
        "given": "Dawn"
      }
    ],
    "container-title": "Anglistik",
    "id": "Baron2009b",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, english, orthography",
    "page": "41-67",
    "title": "Word frequency and key word statistics in corpus linguistics",
    "type": "article-journal",
    "volume": "20"
  },
  {
    "URL": "http://www.fabula.org/actualites/_68286.php",
    "abstract": "Cette prise de position, qui n’engage que l’auteur de ces lignes, est le fruit de réflexions menées au sein de plusieurs groupes de travail visant à créer des programmes d’enseignement de niveau Master portant sur les humanités digitales et les cultures numériques à l’Université de Lausanne.",
    "author": [
      {
        "family": "Baroni",
        "given": "Raphaël"
      }
    ],
    "id": "Baroni2015",
    "issued": {
      "date-parts": [
        [
          2015,
          4,
          27
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "fr-FR",
    "title": "Quelle place donner aux humanités digitales et à l’étude des cultures numériques à l’Université?",
    "type": ""
  },
  {
    "URL": "http://electronicportfolios.org/reflect/whitepaper.pdf",
    "abstract": "This paper provides the theoretical background for a study of student learning, engagement and collaboration through the development of electronic portfolios. After covering an overview of the limited research on portfolios in education, definitions, multiple purposes of portfolios, and conflicting theoretical paradigms are discussed. Principles of student motivation and engagement are covered, along with philosophical and assessment issues and the importance of reflection in learning. The relationship between storytelling and reflection is elaborated. Finally, the paper describes several technology tools that engage learners in reflecting, including blogging and digital storytelling.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Barrett",
        "given": "Helen C."
      }
    ],
    "genre": "White Paper",
    "id": "Barrett2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "e-learning, e-portfolios",
    "publisher": "REFLECT Initiative",
    "title": "Researching electronic portfolios and learner engagement",
    "type": "report"
  },
  {
    "DOI": "10.1145/1835449.1835687",
    "ISBN": "978-1-4503-0153-4",
    "abstract": "Plagiarism, the unacknowledged reuse of text, has increased in recent years due to the large amount of texts readily available. For instance, recent studies claim that nowadays a high rate of student reports include plagiarism, making manual plagiarism detection practically infeasible. Automatic plagiarism detection tools assist experts to analyse documents for plagiarism. Nevertheless, the lack of standard collections with cases of plagiarism has prevented accurate comparing models, making differences hard to appreciate. Seminal efforts on the detection of text reuse [2] have fostered the composition of standard resources for the accurate evaluation and comparison of methods. The aim of this PhD thesis is to address three of the main problems in the development of better models for automatic plagiarism detection: (i) the adequate identification of good potential sources for a given suspicious text; (ii) the detection of plagiarism despite modifications, such as words substitution and paraphrasing (special stress is given to cross-language plagiarism); and (iii) the generation of standard collections of cases of plagiarism and text reuse in order to provide a framework for accurate comparison of models. Regarding difficulties (i) and (ii) , we have carried out preliminary experiments over the METER corpus [2]. Given a suspicious document dq and a collection of potential source documents D, the process is divided in two steps. First, a small subset of potential source documents D* in D is retrieved. The documents d in D* are the most related to dq and, therefore, the most likely to include the source of the plagiarised fragments in it. We performed this stage on the basis of the Kullback-Leibler distance, over a subsample of document’s vocabularies. Afterwards, a detailed analysis is carried out comparing dq to every d in D* in order to identify potential cases of plagiarism and their source. This comparison was made on the basis of word n-grams, by considering n = 2, 3. These n-gram levels are flexible enough to properly retrieve plagiarised fragments and their sources despite modifications [1]. The result is offered to the user to take the final decision. Further experiments were done in both stages in order to compare other similarity measures, such as the cosine measure, the Jaccard coefficient and diverse fingerprinting and probabilistic models. One of the main weaknesses of currently available models is that they are unable to detect cross-language plagiarism. Approaching the detection of this kind of plagiarism is of high relevance, as the most of the information published is written in English, and authors in other languages may find it attractive to make use of direct translations. Our experiments, carried out over parallel and a comparable corpora, show that models of \"standard\" cross-language information retrieval are not enough. In fact, if the analysed source and target languages are related in some way (common linguistic ancestors or technical vocabulary), a simple comparison based on character n-grams seems to be the option. However, in those cases where the relation between the implied languages is weaker, other models, such as those based on statistical machine translation, are necessary [3]. We plan to perform further experiments, mainly to approach the detection of cross-language plagiarism. In order to do that, we will use the corpora developed under the framework of the PAN competition on plagiarism detection (cf. PAN@CLEF: http://pan.webis.de). Models that consider cross-language thesauri and comparison of cognates will also be applied.",
    "author": [
      {
        "family": "Barrón-Cedeño",
        "given": "Alberto"
      }
    ],
    "container-title": "Proceedings of the 33<sup>rd</sup> international ACM SIGIR conference on research and development in information retrieval",
    "id": "Barron-Cedeno2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "ir, plagiarism",
    "page": "914",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "On the mono- and cross-language detection of text reuse and plagiarism",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1162/coli_a_00153",
    "ISSN": "0891-2017",
    "abstract": "Although paraphrasing is the linguistic mechanism underlying many plagiarism cases, little attention has been paid to its analysis in the framework of automatic plagiarism detection. Therefore, state-of-the-art plagiarism detectors find it difficult to detect cases of paraphrase plagiarism. In this article, we analyze the relationship between paraphrasing and plagiarism, paying special attention to which paraphrase phenomena underlie acts of plagiarism and which of them are detected by plagiarism detection systems. With this aim in mind, we created the P4P corpus, a new resource that uses a paraphrase typology to annotate a subset of the PAN-PC-10 corpus for automatic plagiarism detection. The results of the Second International Competition on Plagiarism Detection were analyzed in the light of this annotation. The presented experiments show that i more complex paraphrase phenomena and a high density of paraphrase mechanisms make plagiarism detection more difficult, ii lexical substitutions are the paraphrase mechanisms used the most when plagiarizing, and iii paraphrase mechanisms tend to shorten the plagiarized text. For the first time, the paraphrase mechanisms behind plagiarism have been analyzed, providing critical insights for the improvement of automatic plagiarism detection systems.",
    "author": [
      {
        "family": "Barrón-Cedeño",
        "given": "Alberto"
      },
      {
        "family": "Vila",
        "given": "Marta"
      },
      {
        "family": "Martí",
        "given": "M."
      },
      {
        "family": "Rosso",
        "given": "Paolo"
      }
    ],
    "container-title": "Computational Linguistics",
    "id": "Barron-Cedeno2013",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "document_engineering, e-learning, intertextuality, plagiarism",
    "page": "917-947",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Plagiarism meets paraphrasing: Insights for the next generation in automatic plagiarism detection",
    "title-short": "Plagiarism meets paraphrasing",
    "type": "article-journal",
    "volume": "39"
  },
  {
    "DOI": "10.1016/j.knosys.2013.06.018",
    "ISSN": "0950-7051",
    "abstract": "Three reasons make plagiarism across languages to be on the rise: (i) speakers of under-resourced languages often consult documentation in a foreign language, (ii) people immersed in a foreign country can still consult material written in their native language, and (iii) people are often interested in writing in a language different to their native one. Most efforts for automatically detecting cross-language plagiarism depend on a preliminary translation, which is not always available. In this paper we propose a freely available architecture for plagiarism detection across languages covering the entire process: heuristic retrieval, detailed analysis, and post-processing. On top of this architecture we explore the suitability of three cross-language similarity estimation models: Cross-Language Alignment-based Similarity Analysis (CL-ASA), Cross-Language Character n-Grams (CL-CNG), and Translation plus Monolingual Analysis (T+MA); three inherently different models in nature and required resources. The three models are tested extensively under the same conditions on the different plagiarism detection sub-tasks-something never done before. The experiments show that T+MA produces the best results, closely followed by CL-ASA. Still CL-ASA obtains higher values of precision, an important factor in plagiarism detection when lesser user intervention is desired.",
    "author": [
      {
        "family": "Barrón-Cedeño",
        "given": "Alberto"
      },
      {
        "family": "Gupta",
        "given": "Parth"
      },
      {
        "family": "Rosso",
        "given": "Paolo"
      }
    ],
    "container-title": "Knowledge-Based Systems",
    "id": "Barron-Cedeno2013a",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "document_engineering, e-learning, intertextuality, plagiarism",
    "page": "211-217",
    "publisher": "Elsevier",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "Methods for cross-language plagiarism detection",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "abstract": "The Standard Generalised Markup Language (SGML) is a recently-adopted International Standard (ISO 8879), the first of a series of proposed Standards in the area of Information Processing — Text and Office Systems. The paper presents some background material on markup systems, gives a brief account of SGML, and attempts to clarify the precise nature and purpose of SGML, which are widely misunderstood. It then goes on to explore the reasons why SGML should (or should not) be used in preference to older-established systems.",
    "author": [
      {
        "family": "Barron",
        "given": "David"
      }
    ],
    "container-title": "Electronic Publishing",
    "id": "Barron1989",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "document_engineering, markup, sgml",
    "page": "3-24",
    "title": "Why use SGML",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "URL": "http://www.sepln.org/revistaSEPLN/revista/29/29-Pag247.pdf",
    "abstract": "This paper shows how the framework of Rhetorical Structure Theory (RST) for discourse modelling can be expressed through XML annotations and then used to implement a natural language generation (NLG) system for the web. The system applies simplified RST schemes to the elaboration of a master document in XML from which content segments are chosen to suit the user’s needs. The personalisation of the document is achieved through the application of a sequence of filtering levels of content selection based on the user aspects given as input.",
    "author": [
      {
        "family": "Barrutieta",
        "given": "Guillermo"
      },
      {
        "family": "Abaitua",
        "given": "Joseba"
      },
      {
        "family": "Díaz",
        "given": "JosuKa"
      }
    ],
    "container-title": "Procesamiento del Lenguaje Natural",
    "id": "Barrutieta2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "document_research, nlp, xml",
    "page": "247-253",
    "publisher": "g1",
    "title": "An XML/RST-based approach to multilingual document generation for the web",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "URL": "http://infolingu.univ-mlv.fr/Colloques/Bonifacio/proceedings/barthelemy.pdf",
    "abstract": "Dans cet article, nous établissons une analogie entre les systèmes d’écriture SMS et cunéiforme qui sont deux systèmes hétérogènes, mêlant un sous-système phonétique à d’autres sous-systèmes, notamment idéographiques. Nous partons de cette analogie pour proposer l’adaptation aux SMS de techniques de transcription développées pour l’écriture cunéiforme.",
    "author": [
      {
        "family": "Barthélemy",
        "given": "François"
      }
    ],
    "container-title": "Acte du 26<sup>e</sup> colloque international lexique grammaire",
    "id": "Barthelemy2007",
    "issued": {
      "date-parts": [
        [
          2007,
          1
        ]
      ]
    },
    "keyword": "french, microtext, orthography",
    "title": "Cunéiforme et SMS: analyse graphémique de systèmes d’écriture hétérogènes",
    "type": "paper-conference"
  },
  {
    "URL": "http://atala.org/IMG/pdf/TAL-2009-50-2-07-Barthelemy.pdf",
    "abstract": "This article is devoted to a grammar of the Akkadian verb using finite state technology. It is based on new techniques for which relationships between several representations of a form (four in the Akkadian grammar) are expressed using a tree structure. Feature structures compiled statically in finite transducers are also involved.",
    "author": [
      {
        "family": "Barthélemy",
        "given": "François"
      }
    ],
    "container-title": "Traitement Automatique des Langues",
    "id": "Barthelemy2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "accadian, cultural_heritage, morphology",
    "page": "173-199",
    "title": "Une description morphologique structurée en arbre du verbe akkadien qui utilise des structures de traits et des transducteurs multirubans",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "DOI": "10.1629/24160",
    "abstract": "The literary world is replete with examples of relationships between works such as Homer’s Odyssey and James Joyce’s Ulysses . A Linked Data approach offers the opportunity to categorize these relationships then make them openly available and easily discoverable. The most suitable framework for categorizing these relationships (which can be applied across all cultural output) is provided by Gérard Genette, who proposes five types of relationship. In this paper, we examine Genette’s framework, then apply it to Jane Eyre and relationships around that novel. By so doing, we have produced some RDF to model these relationships following Linked Data principles.This case study demonstrates the broader benefits of adopting Linked Data in this area of literary criticism – namely that scholars will be able to share discoveries and insights, laypeople will discover additional cultural artefacts of interest, and a clear and granular picture of cultural history will be openly available to everyone.",
    "author": [
      {
        "family": "Bartlett",
        "given": "Sarah"
      },
      {
        "family": "Hughes",
        "given": "Bill"
      }
    ],
    "container-title": "Serials",
    "id": "Bartlett2011",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "digital_humanities, rdf, semantic_web",
    "page": "160-165",
    "title": "Intertextuality and the Semantic Web: <i>Jane eyre</i> as a test case for modelling literary relationships with Linked Data",
    "title-short": "Intertextuality and the Semantic Web",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "DOI": "10.1023/A:1016905407491",
    "abstract": "Abstract  A study was conducted to explore what could and should be a reasonable response rate in academic studies. One hundred and forty-one papers which included 175 different studies were examined. They were published in the Academy of Management Journal, Human Relations, Journal of Applied Psychology, Organizational Behavior and Human Decision Processes , and Journal of International Business Studies in the years 1975, 1985, and 1995, covering about 200,000respondents. The average response rate was 55.6 with a standard deviation of 19.7. Variations among the journals such as the year of publication and other variables were discussed. Most notable is the decline through the years (average 48.4, standard deviation of20.1, in 1995), the lower level found in studies involving top management or organizational representatives (average 36.1, standard deviation of 13.3), and the predominance of North American studies. It is suggested that the average and standard deviation found in this study should be used as a norm for future studies, bearing in mind the specific reference group. It is also recommended that a distinction is made between surveys directed at individual participants and those targeting organizational representatives.",
    "author": [
      {
        "family": "Baruch",
        "given": "Yehuda"
      }
    ],
    "container-title": "Human Relations",
    "id": "Baruch1999",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1999,
          4
        ]
      ]
    },
    "keyword": "assessment, psychology",
    "page": "421-438",
    "title": "Response rate in academic studies – a comparative analysis",
    "type": "article-journal",
    "volume": "52"
  },
  {
    "DOI": "10.1007/s00450-016-0336-1",
    "abstract": "Investigating the emotional impact of historical music, e.g. music of the 19th century, is a complex challenge since the subjects that listened to this music and their emotions are forever gone. As a result, asking them for their experiences is not possible anymore and we need other means to gain insights into the expressive quality of music of this century. In this vision paper, we describe a pattern-based method called MUSE4Music to quantitatively find similarities in different pieces of music. The reconstruction of musical patterns will allow us to draw conclusions from erratic documents that go far beyond the single pieces they are referring to.",
    "author": [
      {
        "family": "Barzen",
        "given": "Johanna"
      },
      {
        "family": "Breitenbücher",
        "given": "Uwe"
      },
      {
        "family": "Eusterbrock",
        "given": "Linus"
      },
      {
        "family": "Falkenthal",
        "given": "Michael"
      },
      {
        "family": "Hentschel",
        "given": "Frank"
      },
      {
        "family": "Leymann",
        "given": "Frank"
      }
    ],
    "container-title": "Computer Science – Research and Development",
    "id": "Barzen2017",
    "issue": "3–4",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities, music",
    "page": "323-328",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "The vision for MUSE4Music",
    "type": "article-journal",
    "volume": "32"
  },
  {
    "DOI": "10.1162/089120101317066131",
    "URL": "http://unesdoc.unesco.org/ulis/cgi-bin/ulis.pl?catno=126230",
    "abstract": "Combining elements appropriately within a coherent page layout is a well-recognized and crucial aspect of sophisticated information presentation. The precise function and nature of layout has not, however, been sufficiently addressed within computational approaches; attention is often restricted to relatively local issues of typography and text-formatting, leaving broader issues of layout unaddressed. In this paper we focus on the selection and function of layout in pages that appropriately combine textual and graphical representation styles to yield coherent presentation designs. We demonstrate that layout offers a rich resource for achieving presentational coherence, alongside more traditional resources such as text-formatting and the text-internal marking of discourse connections. We also introduce an integrated approach to layout, text, and diagram generation. Our approach is developed on the basis of a preliminary empirical investigation of professionally produced layouts, followed by implementation within a prototype information system in the area of art history.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Bateman",
        "given": "John"
      },
      {
        "family": "Kleinz",
        "given": "Jörg"
      },
      {
        "family": "Kamps",
        "given": "Thomas"
      },
      {
        "family": "Reichenberger",
        "given": "Klaus"
      }
    ],
    "container-title": "Computational Linguistics",
    "id": "Bateman2001",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "document_research, layout",
    "page": "409-449",
    "publisher": "MIT Press",
    "publisher-place": "Paris",
    "title": "Towards constructive text, diagram, and layout generation for information presentation",
    "type": "article-journal",
    "volume": "27"
  },
  {
    "DOI": "10.1002/9780470773895",
    "ISBN": "9780470773895",
    "abstract": "Drawing on his work in Iceland, Ireland, Scotland, North America, Ghana, and Fiji, linguistic anthropologist and folklorist Richard Bauman presents a series of ethnographic case studies that offer a sparkling look at intertextuality as communicative practice.",
    "author": [
      {
        "family": "Bauman",
        "given": "Richard"
      }
    ],
    "id": "Bauman2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "intertextuality",
    "publisher": "Blackwell",
    "publisher-place": "Oxford, UK",
    "title": "A world of others’ words",
    "type": "book"
  },
  {
    "abstract": "Das Buch stellt die Ergebnisse einer umfassenden Evaluationsstudie im Auftrag des österreichischen Bundesministerium für Wissenschaft und Forschung – bzw. wie es heute heißt: Bundesministeriums für Unterricht, Kunst und Kultur (bm:ukk) – zusammen.",
    "author": [
      {
        "family": "Baumgartner",
        "given": "Peter"
      },
      {
        "family": "Häfele",
        "given": "Hartmut"
      },
      {
        "family": "Maier-Häfele",
        "given": "Kornelia"
      }
    ],
    "id": "Baumgartner2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "StudienVerlag",
    "publisher-place": "Innsbruck",
    "title": "E-Learning Praxishandbuch: Auswahl von Lernplattformen. Marktübersicht – Funktionen – Fachbegriffe.",
    "type": "book"
  },
  {
    "URL": "http://www.dlr.de/pt_nmb/Foerderung/Bekanntmachungen/Audit_Bericht_2003.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Baumgartner",
        "given": "Peter"
      }
    ],
    "id": "Baumgartner2003",
    "issued": {
      "date-parts": [
        [
          2003,
          12
        ]
      ]
    },
    "publisher": "DLR Projektträger Neue Medien in der Bildung und Fachinformation",
    "publisher-place": "St. Augustin",
    "title": "Förderprogramm Neue Medien in der Bildung: Audit-Bericht des Experten/innen-Teams unter Vorsitz von Prof. Dr. Peter Baumgartner",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Baumgartner",
        "given": "Peter"
      },
      {
        "family": "Häfele",
        "given": "Hartmut"
      },
      {
        "family": "Meier-Häfele",
        "given": "Kornelia"
      }
    ],
    "id": "Baumgartner2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "publisher": "StudienVerlag",
    "publisher-place": "Innsbruck",
    "title": "Content Management Systeme in e-Education",
    "type": "book"
  },
  {
    "URL": "https://persee.fr/doc/efr_0000-0000_1977_act_31_1_2252",
    "abstract": "L’informatique s’applique de façon en quelque sorte idéale - chacun des participants de ce colloque en est convaincu - aux documents répétitifs : l’intégralité du contenu d’un journal ou d’un compte de péage ou de port, d’un cadastre, d’un terrier, d’un censier, d’un rôle ou d’un compte d’imposition etc. peut passer sans problèmes majeurs dans la mémoire d’un ordinateur, et l’efficience de l’application des méthodes informatiques sera accrue si au lieu d’avoir un document isolé, on désire traiter une série homogène de documents de cette nature. On peut dire la même chose des séries importantes de documents dont la structure interne se prête à une formalisation relativement aisée, tels que les suppliques et une large part des lettres communes de la chancellerie pontificale, comme aussi de nombre de minutes notariales d’une époque déjà avancée, dont le contenu se laisse facilement appréhender et dont les énormes masses conservées empêchent pratiquement toute autre forme de mise à la disposition des chercheurs. La documentation automatique peut aussi être tenue pour essentielle, de façon plus générale, dans ses applications aux archives, à la fois comme méthode d’inventaire et pour signaler au public ce qui peut concerner ses recherches et qui se cache dans les instruments de travail existants.",
    "author": [
      {
        "family": "Bautier",
        "given": "Robert-Henri"
      }
    ],
    "collection-number": "31",
    "collection-title": "Publications de l’École française de Rome",
    "container-title": "Informatique et histoire médiévale. Actes du colloque de Rome (20–22 mai 1975)",
    "editor": [
      {
        "family": "Fossier",
        "given": "Lucie"
      },
      {
        "family": "Vauchez",
        "given": "André"
      },
      {
        "family": "Violante",
        "given": "Cinzio"
      }
    ],
    "id": "Bautier1977",
    "issued": {
      "date-parts": [
        [
          1977
        ]
      ]
    },
    "keyword": "classic, cultural_heritage",
    "language": "fr-FR",
    "page": "179-186",
    "publisher": "École Française de Rome",
    "publisher-place": "Rome, Italy",
    "title": "Les demandes des historiens à l’informatique. La forme diplomatique et le contenu juridique des actes",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s11528-006-0036-y",
    "ISSN": "8756-3894",
    "URL": "file:///home/mxp/.mozilla/firefox/8stezsh7.default/zotero/storage/12776/bg0613214k409500.html",
    "abstract": "Online teaching and learning has been in transition for its entire existence. The number of courses offered at a distance has grown rapidly. According to the National Center for Education Statistics (Waits & Lewis, 2003), in 2000-2001 more than 56% of four-year colleges and universities in the United States offered distance education degree programs. Many classroom based higher education programs, which meet in person regularly throughout a semester, use online technologies as well. In San Francisco State University (SFSU), 70% of all courses use online technologies; some of these courses are taught in a traditional face to face delivery mode. Approximately 90% of SFSU faculty who use online technologies use the Blackboard Learning Management System (LMS) and the rest are engaged in a scalability test of an alternative to Blackboard, using a local installation of the open source Moodle LMS which SFSU calls \"ilearn.\" This paper presents selections of the authors’ experiences using the Moodle LMS for the first time in a recent semester at SFSU. The purpose of this paper is to provide a glimpse into some of the factors that may be important considerations as more universities transition from commercial LMSs to open-source systems such as Moodle.",
    "author": [
      {
        "family": "Beatty",
        "given": "Brian"
      },
      {
        "family": "Ulasewicz",
        "given": "Connie"
      }
    ],
    "container-title": "TechTrends",
    "id": "Beatty2006",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2006,
          8,
          1
        ]
      ]
    },
    "keyword": "e-learning",
    "page": "36-45",
    "publisher": "Springer Boston",
    "title": "Faculty perspectives on moving from Blackboard to the Moodle learning management system",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "ISBN": "0335206271",
    "abstract": "How do academics perceive themselves and colleagues in their own disciplines, and how do they rate those in other subjects? How closely related are their intellectual tasks and their ways of organizing their professional lives? What are the interconnections between academic cultures and the nature of disciplines? Academic Tribes and Territories maps academic knowledge and explores the diverse characteristics of those who inhabit and cultivate it. This second edition provides a thorough update to Tony Becher’s classic text, first published in 1989, and incorporates research findings and new theoretical perspectives. Fundamental changes in the nature of higher education and in the academic’s role are reviewed and their significance for academic cultures is assessed. This edition moves beyond the first edition’s focus on elite universities and the research role to examine academic cultures in lower status institutions internationally and to place a new emphasis on issues of gender and ethnicity. This second edition successfully renews a classic in the field of higher education.",
    "author": [
      {
        "family": "Becher",
        "given": "Tony"
      },
      {
        "family": "Trowler",
        "given": "Paul R."
      }
    ],
    "edition": "2",
    "id": "Becher2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "philosophy_of_science",
    "language": "en-US",
    "publisher": "Open University Press",
    "publisher-place": "Buckingham, UK",
    "title": "Academic tribes and territories: Intellectual enquiry and the cultures of discipline",
    "title-short": "Academic tribes and territories",
    "type": "book"
  },
  {
    "DOI": "10.1016/j.future.2011.08.004",
    "ISSN": "0167739X",
    "abstract": "Scientific data represents a significant portion of the linked open data cloud and scientists stand to benefit from the data fusion capability this will afford. Publishing linked data into the cloud, however, does not ensure the required reusability. Publishing has requirements of provenance, quality, credit, attribution and methods to provide the reproducibility that enables validation of results. In this paper we make the case for a scientific data publication model on top of linked data and introduce the notion of Research Objects as first class citizens for sharing and publishing. – We identify and characterise different aspects of reuse and reproducibility. – We examine requirements for such reuse. – We propose a scientific data publication model that layers on top of linked data publishing.",
    "author": [
      {
        "family": "Bechhofer",
        "given": "Sean"
      },
      {
        "family": "Buchan",
        "given": "Iain"
      },
      {
        "family": "De Roure",
        "given": "David"
      },
      {
        "family": "Missier",
        "given": "Paolo"
      },
      {
        "family": "Ainsworth",
        "given": "John"
      },
      {
        "family": "Bhagat",
        "given": "Jiten"
      },
      {
        "family": "Couch",
        "given": "Philip"
      },
      {
        "family": "Cruickshank",
        "given": "Don"
      },
      {
        "family": "Delderfield",
        "given": "Mark"
      },
      {
        "family": "Dunlop",
        "given": "Ian"
      },
      {
        "family": "Gamble",
        "given": "Matthew"
      },
      {
        "family": "Michaelides",
        "given": "Danius"
      },
      {
        "family": "Owen",
        "given": "Stuart"
      },
      {
        "family": "Newman",
        "given": "David"
      },
      {
        "family": "Sufi",
        "given": "Shoaib"
      },
      {
        "family": "Goble",
        "given": "Carole"
      }
    ],
    "container-title": "Future Generation Computer Systems",
    "id": "Bechhofer2013",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "nanopublications, semantic_web",
    "page": "599-611",
    "title": "Why linked data is not enough for scientists",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "URL": "http://www.unicode.org/history/unicode88.pdf",
    "abstract": "This document is a draft proposal for the design of an international/multilingual text character encoding system, tentatively called Unicode.",
    "author": [
      {
        "family": "Becker",
        "given": "Joseph D."
      }
    ],
    "id": "Becker1988",
    "issued": {
      "date-parts": [
        [
          1988,
          8,
          29
        ]
      ]
    },
    "keyword": "classic, document_engineering, typesetting",
    "publisher": "Xerox Corp.",
    "publisher-place": "Palo Alto, CA, USA",
    "title": "Unicode 88",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Beierle",
        "given": "Christoph"
      },
      {
        "family": "Kulaš",
        "given": "Marija"
      },
      {
        "family": "Widera",
        "given": "Manfred"
      }
    ],
    "collection-title": "Lecture notes in informatics",
    "container-title": "DeLFI 2003. Proceedings der 1. E-learning fachtagung informatik",
    "editor": [
      {
        "family": "Bode",
        "given": "Arndt"
      },
      {
        "family": "Desel",
        "given": "Jörg"
      },
      {
        "family": "Ratmayer",
        "given": "Sabine"
      },
      {
        "family": "Wessner",
        "given": "Martin"
      }
    ],
    "id": "Beierle2003a",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "page": "144-153",
    "publisher": "GI-Verlag",
    "publisher-place": "Bonn",
    "title": "Automatic analysis of programming assignments",
    "type": "paper-conference",
    "volume": "P-37"
  },
  {
    "author": [
      {
        "family": "Beierle",
        "given": "C."
      },
      {
        "family": "Kulaš",
        "given": "M."
      },
      {
        "family": "Widera",
        "given": "M."
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Applications of declarative programming and knowledge management. 15<sup>th</sup> international conference on applications of declarative programming and knowledge management, INAP 2004, and 18<sup>th</sup> workshop on logic programming, WLP 2004, potsdam, germany, march 4-6, 2004, revised selected papers",
    "editor": [
      {
        "family": "Seipel",
        "given": "D."
      },
      {
        "family": "Hanus",
        "given": "M."
      },
      {
        "family": "Geske",
        "given": "U."
      },
      {
        "family": "Breitenstein",
        "given": "O."
      }
    ],
    "id": "Beierle2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "page": "294-308",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "A pragmatic approach to pre-testing Prolog programs",
    "type": "paper-conference",
    "volume": "3392"
  },
  {
    "DOI": "10.1145/1498759.1498835",
    "ISBN": "978-1-60558-390-7",
    "abstract": "With the overwhelming number of reports on similar events originating from different sources on the web, it is often hard, using existing web search paradigms, to find the original source of \"facts\", statements, rumors, and opinions, and to track their development. Several techniques have been previously proposed for detecting such text reuse between different sources, however these techniques have been tested against relatively small and homogeneous TREC collections. In this work, we test the feasibility of text reuse detection techniques in the setting of web search. In addition to text reuse detection, we develop a novel technique that addresses the unique challenges of finding original sources on the web, such as defining a timeline. We also explore the use of link analysis for identifying reliable and relevant reports. Our experimental results show that the proposed techniques can operate on the scale of the web, are significantly more accurate than standard web search for finding text reuse, and provide a richer representation for tracking the information flow.",
    "author": [
      {
        "family": "Bendersky",
        "given": "Michael"
      },
      {
        "family": "Croft",
        "given": "W. Bruce"
      }
    ],
    "collection-title": "WSDM ’09",
    "container-title": "Proceedings of the second ACM international conference on web search and data mining",
    "id": "Bendersky2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "ir, plagiarism",
    "page": "262-271",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Finding text reuse on the web",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Bennett",
        "given": "Randy Elliot"
      },
      {
        "family": "Ward",
        "given": "William C."
      },
      {
        "family": "Rock",
        "given": "Donald A."
      },
      {
        "family": "LaHart",
        "given": "Colleen"
      }
    ],
    "id": "Bennett1990",
    "issued": {
      "date-parts": [
        [
          1990,
          6
        ]
      ]
    },
    "publisher": "Educational Testing Service",
    "publisher-place": "Princeton, NJ, USA",
    "title": "Toward a framework for constructed-response items",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Bennett",
        "given": "Randy Elliot"
      }
    ],
    "id": "Bennett1994",
    "issued": {
      "date-parts": [
        [
          1994,
          12
        ]
      ]
    },
    "publisher": "Educational Testing Service",
    "publisher-place": "Princeton, NJ, USA",
    "title": "An electronic infrastructure for a future generation of tests",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Bennett",
        "given": "Randy Elliot"
      }
    ],
    "id": "Bennett1998",
    "issued": {
      "date-parts": [
        [
          1998,
          6
        ]
      ]
    },
    "publisher": "Educational Testing Service",
    "publisher-place": "Princeton, NJ, USA",
    "title": "Reinventing assessment: Speculations on the future of large-scale educational testing",
    "title-short": "Reinventing assessment",
    "type": "report"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2010/workshops/W4.pdf",
    "abstract": "We report on our experiences in annotating a historical corpus of German with structural and linguistic information, providing an example of the needs and challenges encountered by smaller humanities-based corpus projects. Our approach attempts to follow current standardisation efforts to allow for future comparative studies between projects and the potential extension of our annotation scheme. Structural information is encoded according to TEI (P5) guidelines, and the corpus is further being annotated with linguistic information in terms of word tokens, sentence boundaries, normalised word forms, lemmas, POS tags, and morphological tags. The major problem encountered to date has been how to merge the linguistic mark-up with the TEI-annotated version of the corpus. In the interest of interoperability and comparative studies between corpora we would welcome the development of clearer procedures whereby structural and linguistic annotations might be merged.",
    "author": [
      {
        "family": "Bennett",
        "given": "Paul"
      },
      {
        "family": "Durrell",
        "given": "Martin"
      },
      {
        "family": "Scheible",
        "given": "Silke"
      },
      {
        "family": "Whitt",
        "given": "Richard J."
      }
    ],
    "container-title": "Proceedings of the LREC 2010 workshop on language resource and language technology: Standards—state of the art, emerging needs, and future developments",
    "id": "Bennett2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "page": "64-68",
    "publisher": "ELRA",
    "publisher-place": "Paris",
    "title": "Annotating a historical corpus of German: A case study",
    "title-short": "Annotating a historical corpus of German",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.ieg-ego.eu/introduction-2010-de",
    "author": [
      {
        "family": "Berger",
        "given": "Joachim"
      },
      {
        "family": "Willenberg",
        "given": "Jennifer"
      },
      {
        "family": "Landes",
        "given": "Lisa"
      }
    ],
    "container-title": "Europäische Geschichte Online",
    "editor": [
      {
        "literal": "Leibniz-Institut für Europäische Geschichte"
      }
    ],
    "id": "Berger2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Leibniz-Institut für Europäische Geschichte",
    "publisher-place": "Mainz",
    "title": "EGO | Europäische Geschichte Online: Eine transkulturelle Geschichte Europas im Internet",
    "type": "chapter"
  },
  {
    "DOI": "10.1109/SPIRE.2000.878178",
    "ISBN": "0-7695-0746-8",
    "abstract": "The aim of this paper is to give a comprehensive comparison of well-known longest common subsequence algorithms (for two input strings) and study their behaviour in various application environments. The performance of the methods depends heavily on the properties of the problem instance as well as the supporting data structures used in the implementation. We want to make also a clear distinction between methods that determine the actual lcs and those calculating only its length, since the execution time and more importantly, the space demand depends crucially on the type of the task. To our knowledge, this is the first time this kind of survey has been done. Due to the page limits, the paper gives only a coarse overview of the performance of the algorithms; more detailed studies are reported elsewhere",
    "author": [
      {
        "family": "Bergroth",
        "given": "Lasse"
      },
      {
        "family": "Hakonen",
        "given": "Harri"
      },
      {
        "family": "Raita",
        "given": "Timo"
      }
    ],
    "container-title": "Proceedings of the seventh international symposium on string processing and information retrieval (SPIRE 2000)",
    "id": "Bergroth2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "approximate_matching",
    "page": "39-48",
    "publisher": "IEEE",
    "publisher-place": "New York, NY, USA",
    "title": "A survey of longest common subsequence algorithms",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-540-39984-1_22",
    "ISBN": "978-3-540-20177-9",
    "abstract": "Certain properties of the input strings have dominating influence on the running time of an algorithm selected to solve the longest common subsequence (lcs) problem of two input strings. It has turned out to be difficult – as well theoretically as practically – to develop an lcs algorithm which would be superior for all problem instances. Furthermore, implementing the most evolved lcs algorithms presented recently is laborious. This paper shows that it is still beneficial to refine the traditional lcs algorithms to get new algorithm variants that are in practice competitive to the modern lcs methods in certain problem instances. We present and analyse a general-purpose algorithm NKY-MODIF, which has a moderate time and space efficiency and can easily be implemented correctly. The algorithm bases on the so-called diagonal-wise method of Nakatsu, Kambayashi and Yajima (NKY). The NKY algorithm was selected for our further consideration due to its algorithmic independence of the size of the input alphabet and its light pre-processing phase. The NKY-MODIF algorithm refines the NKY method essentially in three ways: by reducing unnecessary scanning over the input sequences, storing the intermediate results more locally, and utilizing lower and upper bound knowledge about the lcs. In order to demonstrate that the some of the presented ideas are not specific for the NKY only, we apply lower bound information on two lcs algorithms having a different processing approach than the NKY has. This introduces a new way to solve the lcs problem. The lcs problem has two variants: calculating only the length of the lcs, and determining also the symbols belonging to one instance of the lcs. We verify the presented ideas for both of these problem types by extensive test runs.",
    "author": [
      {
        "family": "Bergroth",
        "given": "Lasse"
      },
      {
        "family": "Hakonen",
        "given": "Harri"
      },
      {
        "family": "Väisänen",
        "given": "Juri"
      }
    ],
    "chapter-number": "22",
    "collection-title": "Lecture notes in computer science",
    "container-title": "String processing and information retrieval (SPIRE 2003)",
    "editor": [
      {
        "family": "Nascimento",
        "given": "Mario A."
      },
      {
        "family": "Moura",
        "given": "Edleno S."
      },
      {
        "family": "Oliveira",
        "given": "Arlindo L."
      }
    ],
    "id": "Bergroth2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "approximate_matching",
    "page": "287-303",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "New refinement techniques for longest common subsequence algorithms",
    "type": "chapter",
    "volume": "2857"
  },
  {
    "DOI": "10.1109/ICALT.2003.1215047",
    "abstract": "Content management systems nowadays are used to manage complex publications far more often than some years ago. The basic principles are the separation of structure, content and presentation, an exactly defined workflow management and the management of content in the form of small units, so called assets. This leads to improved quality, better reusability and reduced costs. We focus on similarities of CMS-systems and e-learning systems and the possibility to transfer gained experiences from the field of CMS to e-learning systems. This leads to a set of demands that can be made on e-learning systems. We conclude with the thesis that transferring the principles of content management systems to the world of e-learning will result in better systems with the improved functionality we already know from current CMS.",
    "author": [
      {
        "family": "Bergstedt",
        "given": "Stefan"
      },
      {
        "family": "Wiegreffe",
        "given": "Stefan"
      },
      {
        "family": "Wittmann",
        "given": "Jochen"
      },
      {
        "family": "Möller",
        "given": "Dietmar"
      }
    ],
    "container-title": "Proceedings of the 3rd IEEE international conference on advanced learning technologies (ICALT’03)",
    "id": "Bergstedt2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "e-learning",
    "page": "155-159",
    "publisher": "IEEE",
    "title": "Content management systems and e-learning systems – a symbiosis?",
    "type": "paper-conference"
  },
  {
    "URL": "http://wackybook.sslmit.unibo.it/pdfs/bernardini.pdf",
    "author": [
      {
        "family": "Bernardini",
        "given": "Silvia"
      },
      {
        "family": "Baroni",
        "given": "Marco"
      },
      {
        "family": "Evert",
        "given": "Stefan"
      }
    ],
    "container-title": "Wacky! Working papers on the web as corpus",
    "editor": [
      {
        "family": "Baroni",
        "given": "Marco"
      },
      {
        "family": "Bernardini",
        "given": "Silvia"
      }
    ],
    "id": "Bernadini2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "computational_linguistics, corpus_linguistics",
    "page": "9-40",
    "publisher": "GEDIT",
    "publisher-place": "Bologna",
    "title": "A WaCky introduction",
    "type": "chapter"
  },
  {
    "URL": "http://ietf.org/rfc/rfc1866.txt",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Berners-Lee",
        "given": "Tim"
      },
      {
        "family": "Connolly",
        "given": "Dan"
      }
    ],
    "genre": "RFC",
    "id": "Berners-Lee1995",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "number": "1866",
    "publisher": "Internet Engineering Task Force",
    "title": "Hypertext markup language – 2.0",
    "type": "report"
  },
  {
    "abstract": "This book is written to address the questions most people ask - From \"What were you thinking when you invented it?\" through \"So what do you think of it now?\" to \"Where is this all going to take us?\", this is the story. It is not a technical book. (If you want the technical details, check out the W3C web site!). It does mention a little about how technologies you may have heard of - like XML - fit in to the past, present and future, but only in the course of charting the course for the Web from the initial dream - still largely unfulfilled - to the next technical and social revolution. -TimBL",
    "author": [
      {
        "family": "Berners-Lee",
        "given": "Tim"
      }
    ],
    "id": "Berners-Lee1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "semantic_web",
    "language": "en-US",
    "publisher": "Harper San Francisco",
    "publisher-place": "San Francisco, CA, USA",
    "title": "Weaving the web: The original design and ultimate destiny of the world wide web by its inventor",
    "title-short": "Weaving the web",
    "type": "book"
  },
  {
    "DOI": "10.1038/scientificamerican0501-34",
    "abstract": "A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities.",
    "author": [
      {
        "family": "Berners-Lee",
        "given": "Tim"
      },
      {
        "family": "Hendler",
        "given": "James"
      },
      {
        "family": "Lassila",
        "given": "Ora"
      }
    ],
    "container-title": "Scientific American",
    "id": "Berners-Lee2001",
    "issued": {
      "date-parts": [
        [
          2001,
          5
        ]
      ]
    },
    "keyword": "semantic_web",
    "page": "29-37",
    "title": "The Semantic Web",
    "type": "article-journal",
    "volume": "284"
  },
  {
    "ISSN": "0011-1600",
    "URL": "https://cairn.info/revue-critique-2015-8-page-613.htm",
    "author": [
      {
        "family": "Berra",
        "given": "Aurélien"
      }
    ],
    "container-title": "Critique",
    "id": "Berra2015",
    "issue": "8",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "fr-FR",
    "page": "613-626",
    "title": "Pour une histoire des humanités numériques",
    "type": "article-journal",
    "volume": "2015"
  },
  {
    "ISSN": "1465-4121",
    "URL": "https://culturemachine.net/wp-content/uploads/2019/01/10-Computational-Turn-440-893-1-PB.pdf",
    "author": [
      {
        "family": "Berry",
        "given": "David M."
      }
    ],
    "container-title": "Culture Machine",
    "id": "Berry2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "digital_humanities",
    "page": "1-22",
    "title": "The computational turn: Thinking about the digital humanities",
    "title-short": "The computational turn",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "DOI": "10.1057/9780230371934",
    "ISBN": "978-0-230-29265-9",
    "abstract": "Confronting the digital revolution in academia, this book examines the application of new computational techniques and visualisation technologies in the Arts & Humanities. Uniting differing perspectives, leading and emerging scholars discuss the theoretical and practical challenges that computation raises for these disciplines.",
    "editor": [
      {
        "family": "Berry",
        "given": "David M."
      }
    ],
    "id": "Berry2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "publisher": "Palgrave Macmillan",
    "publisher-place": "Basingstoke",
    "title": "Understanding digital humanities",
    "type": "book"
  },
  {
    "DOI": "10.1057/9780230371934_2",
    "ISBN": "978-0-230-29265-9",
    "abstract": "Confronting the digital revolution in academia, this book examines the application of new computational techniques and visualisation technologies in the Arts & Humanities. Uniting differing perspectives, leading and emerging scholars discuss the theoretical and practical challenges that computation raises for these disciplines.",
    "author": [
      {
        "family": "Evans",
        "given": "Leighton"
      },
      {
        "family": "Rees",
        "given": "Sian"
      }
    ],
    "chapter-number": "2",
    "container-title": "Understanding digital humanities",
    "editor": [
      {
        "family": "Berry",
        "given": "David M."
      }
    ],
    "id": "Evans2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "21-41",
    "publisher": "Palgrave Macmillan",
    "publisher-place": "Basingstoke",
    "title": "An interpretation of digital humanities",
    "type": "chapter"
  },
  {
    "DOI": "10.1057/9780230371934_4",
    "ISBN": "978-0-230-29265-9",
    "abstract": "Confronting the digital revolution in academia, this book examines the application of new computational techniques and visualisation technologies in the Arts & Humanities. Uniting differing perspectives, leading and emerging scholars discuss the theoretical and practical challenges that computation raises for these disciplines.",
    "author": [
      {
        "family": "Rieder",
        "given": "Bernhard"
      },
      {
        "family": "Röhle",
        "given": "Theo"
      }
    ],
    "chapter-number": "4",
    "container-title": "Understanding digital humanities",
    "editor": [
      {
        "family": "Berry",
        "given": "David M."
      }
    ],
    "id": "Rieder2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "67--84",
    "publisher": "Palgrave Macmillan",
    "publisher-place": "Basingstoke",
    "title": "Digital methods: Five challenges",
    "title-short": "Digital methods",
    "type": "chapter"
  },
  {
    "DOI": "10.1057/9780230371934_11",
    "ISBN": "978-0-230-29265-9",
    "abstract": "Confronting the digital revolution in academia, this book examines the application of new computational techniques and visualisation technologies in the Arts & Humanities. Uniting differing perspectives, leading and emerging scholars discuss the theoretical and practical challenges that computation raises for these disciplines.",
    "author": [
      {
        "family": "Dixon",
        "given": "Dan"
      }
    ],
    "chapter-number": "11",
    "container-title": "Understanding digital humanities",
    "editor": [
      {
        "family": "Berry",
        "given": "David M."
      }
    ],
    "id": "Dixon2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "191--209",
    "publisher": "Palgrave Macmillan",
    "publisher-place": "Basingstoke",
    "title": "Analysis tool or research methodology: Is there an epistemology for patterns?",
    "title-short": "Analysis tool or research methodology",
    "type": "chapter"
  },
  {
    "DOI": "10.1057/9780230371934_15",
    "ISBN": "978-0-230-29265-9",
    "abstract": "Confronting the digital revolution in academia, this book examines the application of new computational techniques and visualisation technologies in the Arts & Humanities. Uniting differing perspectives, leading and emerging scholars discuss the theoretical and practical challenges that computation raises for these disciplines.",
    "author": [
      {
        "dropping-particle": "van",
        "family": "Zundert",
        "given": "Joris"
      },
      {
        "family": "Antonijevic",
        "given": "Smiljana"
      },
      {
        "family": "Beaulieu",
        "given": "Anne"
      },
      {
        "dropping-particle": "van",
        "family": "Dalen-Oskam",
        "given": "Karina"
      },
      {
        "family": "Zeldenrust",
        "given": "Douwe"
      },
      {
        "family": "Andrews",
        "given": "Tara L."
      }
    ],
    "chapter-number": "15",
    "container-title": "Understanding digital humanities",
    "editor": [
      {
        "family": "Berry",
        "given": "David M."
      }
    ],
    "id": "Zundert2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "publisher": "Palgrave Macmillan",
    "publisher-place": "Basingstoke",
    "title": "Cultures of formalisation: Towards an encounter between humanities and computing",
    "title-short": "Cultures of formalisation",
    "type": "chapter"
  },
  {
    "ISBN": "9780745697666",
    "abstract": "As the twenty-first century unfolds, computers challenge the way in which we think about culture, society and what it is to be human: areas traditionally explored by the humanities. In a world of automation, Big Data, algorithms, Google searches, digital archives, real-time streams and social networks, our use of culture has been changing dramatically. The digital humanities give us powerful theories, methods and tools for exploring new ways of being in a digital age. Berry and Fagerjord provide a compelling guide, exploring the history, intellectual work, key arguments and ideas of this emerging discipline. They also offer an important critique, suggesting ways in which the humanities can be enriched through computing, but also how cultural critique can transform the digital humanities. Digital Humanities will be an essential book for students and researchers in this new field but also related areas, such as media and communications, digital media, sociology, informatics, and the humanities more broadly.",
    "author": [
      {
        "family": "Berry",
        "given": "David M."
      },
      {
        "family": "Fagerjord",
        "given": "Anders"
      }
    ],
    "id": "Berry2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "publisher": "Polity",
    "publisher-place": "Cambridge",
    "title": "Digital humanities",
    "type": "book"
  },
  {
    "DOI": "10.1145/1600193.1600223",
    "ISBN": "978-1-60558-575-8",
    "abstract": "In this paper we propose a technique of limiarization (also known as thresholding or binarization) tailored to improve the readability of degraded historical documents. Limiarization is a simple image processing technique, which is employed in many complex tasks like image compression, object segmentation and character recognition. The technique also finds applications on itself: since it results in a high-contrast image, in which the foreground is clearly separated from the background, it can greatly improve the readability of a document, provided that other attributes (like character shape) do not suffer. Our technique exploits statistical characteristics of textual documents and applies both global and local thresholding. Under visual inspection on experiments made in a collection of severely degraded historical documents, it compares favorably with the state of the art.",
    "author": [
      {
        "family": "Bertholdo",
        "given": "Flávio"
      },
      {
        "family": "Valle",
        "given": "Eduardo"
      },
      {
        "dropping-particle": "de",
        "family": "A. Araújo",
        "given": "Arnaldo"
      }
    ],
    "collection-title": "DocEng ’09",
    "container-title": "Proceedings of the 9th ACM symposium on document engineering",
    "id": "Bertholdo2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "page": "131-134",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Layout-aware limiarization for readability enhancement of degraded historical documents",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.tlg.uci.edu/encoding/BCM2011.pdf",
    "abstract": "Beta Code is a character and formatting encoding convention developed specifically to enable the accurate digital representation of ancient Greek texts (and other archaic languages) on an ASCII-based system. It was developed by David W. Packard in the late 70’s and adopted by the TLG® in 1981. Beta has become the standard for encoding polytonic Greek and has also been used by a number of other projects such as Perseus, the Packard Humanities Institute, the Duke collection of Documentary Papyri and the Greek Epigraphy Project at Cornell and the Ohio State University. The Beta Code Manual has undergone a number of revisions. Earlier versions have been disseminated in hard copy and posted online. This edition (hereafter called The 2011 TLG® Beta Code Manual) supersedes any previous versions.",
    "author": [
      {
        "literal": "Thesaurus Linguae Graecae"
      }
    ],
    "id": "BetaCode2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, document_engineering, greek",
    "publisher": "Thesaurus Linguae Graecae",
    "publisher-place": "Irvine, CA, USA",
    "title": "The TLG beta code manual 2011",
    "type": "book"
  },
  {
    "URL": "http://www.spiegel.de/spiegel/print/d-84430237.html",
    "abstract": "Der Google-Konzern unterstützt Gelehrte bei der Auswertung seiner 15 Millionen Bücher umfassenden Digitalbibliothek. Geisteswissenschaftler sind begeistert.",
    "author": [
      {
        "family": "Bethge",
        "given": "Philip"
      }
    ],
    "container-title": "Der Spiegel",
    "id": "Bethge2012",
    "issue": "12",
    "issued": {
      "date-parts": [
        [
          2012,
          3
        ]
      ]
    },
    "keyword": "digital_humanities, in_the_media",
    "language": "de-DE",
    "title": "Schatztruhe des Wissens",
    "type": "article-journal"
  },
  {
    "URL": "http://home.arcor.de/bjoern-beutel/malaga/malaga.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Beutel",
        "given": "Björn"
      }
    ],
    "id": "Beutel2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "language": "en-US",
    "title": "Malaga 7.12. User’s and programmer’s manual",
    "type": "book"
  },
  {
    "DOI": "10.1080/02626667.2015.1031761",
    "ISSN": "0262-6667",
    "abstract": "This paper presents a discussion of some of the issues associated with the multiple sources of uncertainty and non-stationarity in the analysis and modelling of hydrological systems. Different forms of aleatory, epistemic, semantic, and ontological uncertainty are defined. The potential for epistemic uncertainties to induce disinformation in calibration data and arbitrary non-stationarities in model error characteristics, and surprises in predicting the future, are discussed in the context of other forms of non-stationarity. It is suggested that a condition tree is used to be explicit about the assumptions that underlie any assessment of uncertainty. This also provides an audit trail for providing evidence to decision makers.",
    "author": [
      {
        "family": "Beven",
        "given": "Keith"
      }
    ],
    "container-title": "Hydrological Sciences Journal",
    "id": "Beven2016",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "philosophy_of_science, uncertainty",
    "page": "1652-1665",
    "title": "Facets of uncertainty: Epistemic uncertainty, non-stationarity, likelihood, hypothesis testing, and communication",
    "title-short": "Facets of uncertainty",
    "type": "article-journal",
    "volume": "61"
  },
  {
    "DOI": "10.1093/llc/fql015",
    "ISSN": "0268-1145",
    "abstract": "This article is based on a session given by the authors at the ACH/ALLC conference at the University of Victoria in June 2005. It discusses the prospects for partnership between the humanities and computing from the alternative perspective afforded by Empirical Modelling (EM). Perceived dualities that separate the two cultures of science and art are identified as the primary impediment to this partnership. A vision for ’human computing’ that promises to dissolve these dualities is outlined. The key characteristics and potential for EM for the humanities are illustrated with reference to a modelling exercise on the theme of Schubert’s Erlkönig. This highlights how each of the six varieties of modelling identified by McCarty can be represented within an EM model. The implications of EM are discussed with reference to McCarty’s account of the key role for modelling in the humanities, in relation to James’s “philosophic attitude” of Radical Empiricism and to ideas from phenomenological sources.",
    "author": [
      {
        "family": "Beynon",
        "given": "Meurig"
      },
      {
        "family": "Russ",
        "given": "Steve"
      },
      {
        "family": "McCarty",
        "given": "Willard"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Beynon2006",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "en-US",
    "page": "141-157",
    "title": "Human computing—modelling with meaning",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "URL": "https://aclanthology.org/N10-1102",
    "abstract": "The task of identifying the language of text or utterances has a number of applications in natural language processing. Language identification has traditionally been approached with character-level language models. However, the language model approach crucially depends on the length of the text in question. In this paper, we consider the problem of language identification of names. We show that an approach based on SVMs with n-gram counts as features performs much better than language models. We also experiment with applying the method to pre-process transliteration data for the training of separate models.",
    "author": [
      {
        "family": "Bhargava",
        "given": "Aditya"
      },
      {
        "family": "Kondrak",
        "given": "Grzegorz"
      }
    ],
    "container-title": "Human language technologies: The 2010 annual conference of the north american chapter of the association for computational linguistics",
    "id": "Bhargava2010",
    "issued": {
      "date-parts": [
        [
          2010,
          6
        ]
      ]
    },
    "keyword": "language_identification",
    "page": "693-696",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Language identification of names with SVMs",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1016/j.ipm.2006.09.003",
    "ISSN": "03064573",
    "abstract": "This paper examines the meaning of context in relation to ontology based query expansion and contains a review of query expansion approaches. The various query expansion approaches include relevance feedback, corpus dependent knowledge models and corpus independent knowledge models. Case studies detailing query expansion using domain-specific and domain-independent ontologies are also included. The penultimate section attempts to synthesise the information obtained from the review and provide success factors in using an ontology for query expansion. Finally the area of further research in applying context from an ontology to query expansion within a newswire domain is described.",
    "author": [
      {
        "family": "Bhogal",
        "given": "Jagdev"
      },
      {
        "family": "MacFarlane",
        "given": "Andy"
      },
      {
        "family": "Smith",
        "given": "Peter"
      }
    ],
    "container-title": "Information Processing & Management",
    "id": "Bhogal2007",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "ir",
    "page": "866-886",
    "title": "A review of ontology based query expansion",
    "type": "article-journal",
    "volume": "43"
  },
  {
    "DOI": "10.1093/llc/5.4.257",
    "ISSN": "0268-1145",
    "abstract": "Although corpus-based analyses of linguistic variation have provided fresh insights into previously intractable issues, several methodological criticisms have been raised about the overall design of text corpora and the validity of text ’genres’ as a basis for analyses of variation. Unfortunately, most of these criticisms have been based on intuitive judgements rather than empirical investigation. The present study begins to correct this lack of evidence concerning these issues It focuses on four particular methodological issues. (1) how long texts should be in order to reliably represent the distribution of linguistic features in particular text categories; (2) how many texts within each text category are required in order to reliablity represent the linguistic characteristics of that category, and related questions concerning the validity of ’genre’categories, (3) how many texts are needed in a corpus to accurately identify the salient parameters of linguistic variation among texts; and (4) how much of a cross-section is required to identify and analyze the salient parameters of variation among texts These issues are addressed through statistical investigation of the distribution of linguistic features across various sub-samples of the LOB and London-Lond corpora, in comparison to their distribution across the full corpora. The results indicate that existing corpora are adequate for many analyses of linguistic variation, In conclusion, the paper welcomes the future availablity of larger and more representative corpora, but it also urges researches to fully exploit existing corpora for ongoing investigations of linguistic variation.",
    "author": [
      {
        "family": "Biber",
        "given": "Douglas"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Biber1990",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "corpus_linguistics",
    "language": "en-US",
    "page": "257-269",
    "title": "Methodological issues regarding corpus-based analyses of linguistic variation",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.1093/llc/8.4.243",
    "ISSN": "0268-1145",
    "abstract": "The present paper addresses a number of issues related to achieving representativeness’ in linguistic corpus design, including: discussion of what it means to represent’ a language, definition of the target population, stratified versus proportional sampling of a language, sampling within texts, and issues relating to the required sample size (number of texts) of a corpus. The paper distinguishes among various ways that linguistic features can be distributed within and across texts; it analyses the distributions of several particular features, and it discusses the implications of these distributions for corpus design. The paper argues that theoretical research should be prior in corpus design, to identify the situational parameters that distinguish among texts in a speech community, and to identify the types of linguistic features that will be analysed in the corpus. These theoretical considerations should be complemented by empirical investigations of linguistic variation in a pilot corpus of texts, as a basis for specific sampling decisions. The actual construction of a corpus would then proceed in cycles: the original design based on theoretical and pilot-study analyses, followed by collection of texts, followed by further empirical investigations of linguistic variation and revision of the design.",
    "author": [
      {
        "family": "Biber",
        "given": "Douglas"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Biber1993",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "corpus_linguistics",
    "language": "en-US",
    "page": "243-257",
    "title": "Representativeness in corpus design",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "DOI": "10.18452/19146",
    "abstract": "The paper discusses conceptual matters and implementation practices for authority information for musical works in the German authority file (GND). Options of re-use of the authority data in libraries as well as in musicological contexts are introduced. Challenges of theoretical concepts as well as practical realization are shown and potentials of information included in authority files are demonstrated.",
    "author": [
      {
        "family": "Bicher",
        "given": "Katrin"
      },
      {
        "family": "Wiermann",
        "given": "Barbara"
      }
    ],
    "container-title": "BIBLIOTHEK – Forschung und Praxis",
    "id": "Bicher2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "keyword": "music, ontologies",
    "title": "Normdaten zu  und ihr Potenzial für die digitale Musikwissenschaft",
    "type": "article-journal"
  },
  {
    "DOI": "10.1007/b105772",
    "abstract": "This work presents an unsupervised solution to language identification. The method sorts multilingual text corpora on the basis of sentences into the different languages that are contained and makes no assumptions on the number or size of the monolingual fractions. Evaluation on 7-lingual corpora and bilingual corpora show that the quality of classification is comparable to supervised approaches and works almost error-free from 100 sentences per language on.",
    "author": [
      {
        "family": "Biemann",
        "given": "Chris"
      },
      {
        "family": "Teresniak",
        "given": "Sven"
      }
    ],
    "container-title": "Computational linguistics and intelligent text processing. 6<sup>th</sup> international conference, CICLing 2005, mexico city, mexico, february 13–19, 2005. proceedings",
    "id": "Biemann2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "page": "773-784",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Disentangling from Babylonian confusion – unsupervised language identification",
    "type": "paper-conference"
  },
  {
    "URL": "http://dblp.uni-trier.de/rec/bibtex/journals/ldvf/Biemann05",
    "abstract": "After the vision of the Semantic Web was broadcasted at the turn of the millennium, ontology became a synonym for the solution to many problems concerning the fact that computers do not understand human language: if there were an ontology and every document were marked up with it and we had agents that would understand the mark- up, then computers would finally be able to process our queries in a really sophisticated way. Some years later, the success of Google shows us that the vision has not come true, being hampered by the incredible amount of extra work required for the intellectual encoding of semantic mark-up – as compared to simply uploading an HTML page. To alleviate this acquisition bottleneck, the field of ontology learning has since emerged as an important sub-field of ontology engineering.",
    "author": [
      {
        "family": "Biemann",
        "given": "Chris"
      }
    ],
    "container-title": "LDV Forum",
    "id": "Biemann2005a",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "ontologies, semantic_web",
    "page": "75-93",
    "title": "Ontology learning from text: A survey of methods",
    "title-short": "Ontology learning from text",
    "type": "article-journal",
    "volume": "20"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/summaries/447.html",
    "author": [
      {
        "family": "Biemann",
        "given": "Chris"
      },
      {
        "family": "Quasthoff",
        "given": "Uwe"
      },
      {
        "family": "Heyer",
        "given": "Gerhard"
      },
      {
        "family": "Holz",
        "given": "Florian"
      }
    ],
    "container-title": "Proceedings of the sixth international conference on language resources and evaluation (LREC’08)",
    "editor": [
      {
        "family": "Calzolari",
        "given": "Nicoletta"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Maegaard",
        "given": "Bente"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Odijk",
        "given": "Jan"
      },
      {
        "family": "Piperidis",
        "given": "Stelios"
      },
      {
        "family": "Tapias",
        "given": "Daniel"
      }
    ],
    "id": "Biemann2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "morphology, nlp, pos_tagging",
    "page": "1760-1767",
    "publisher": "European Language Resources Association (ELRA)",
    "publisher-place": "Paris",
    "title": "ASV toolbox: A modular collection of language exploration tools",
    "title-short": "ASV toolbox",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s11168-010-9067-9",
    "ISSN": "1570-7075",
    "abstract": "Syntactic preprocessing is a step that is widely used in NLP applications. Traditionally, rule-based or statistical Part-of-Speech (POS) taggers are employed that either need considerable rule development times or a sufficient amount of manually labeled data. To alleviate this acquisition bottleneck and to enable preprocessing for minority languages and specialized domains, a method is presented that constructs a statistical syntactic tagger model from a large amount of unlabeled text data. The method presented here is called unsupervised POS-tagging, as its application results in corpus annotation in a comparable way to what POS-taggers provide. Nevertheless, its application results in slightly different categories as opposed to what is assumed by a linguistically motivated POS-tagger. These differences hamper evaluation procedures that compare the output of the unsupervised POS-tagger to a tagging with a supervised tagger. To measure the extent to which unsupervised POS-tagging can contribute in application-based settings, the system is evaluated in supervised POS-tagging, word sense disambiguation, named entity recognition and chunking. Unsupervised POS-tagging has been explored since the beginning of the 1990s. Unlike in previous approaches, the kind and number of different tags is here generated by the method itself. Another difference to other methods is that not all words above a certain frequency rank get assigned a tag, but the method is allowed to exclude words from the clustering, if their distribution does not match closely enough with other words. The lexicon size is considerably larger than in previous approaches, resulting in a lower out-of-vocabulary (OOV) rate and in a more consistent tagging. The system presented here is available for download as open-source software along with tagger models for several languages, so the contributions of this work can be easily incorporated into other applications.",
    "author": [
      {
        "family": "Biemann",
        "given": "Chris"
      }
    ],
    "container-title": "Research on Language & Computation",
    "id": "Biemann2010",
    "issue": "2–4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "pos_tagging",
    "page": "101-135",
    "publisher": "Springer Netherlands",
    "title": "Unsupervised part-of-speech tagging in the large",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "DOI": "10.4230/DAGREP.4.7.80",
    "collection-number": "14301",
    "collection-title": "Dagstuhl seminar",
    "editor": [
      {
        "family": "Biemann",
        "given": "Chris"
      },
      {
        "family": "Crane",
        "given": "Gregory R."
      },
      {
        "family": "Fellbaum",
        "given": "Christiane D."
      },
      {
        "family": "Mehler",
        "given": "Alexander"
      }
    ],
    "id": "Biemann2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Schloss Dagstuhl – Leibniz-Zentrum für Informatik",
    "title": "Computational humanities: Bridging the gap between computer science and digital humanities",
    "title-short": "Computational humanities",
    "type": "book"
  },
  {
    "URL": "http://jstor.org/stable/27903534",
    "abstract": "This article is an account of an electronic discussion which took place between November 1995 and June 1996. A number of specialists in relevant research areas had been invited to take part in the discussion from the very beginning, and some were added later. Altogether 30 individuals subscribed to the list. The \"target paper,\" written by Allen Renear, was sent to the list on November 27, 1995. The target paper alone comprised 8,500 words. The ensuing discussion, to which 9 of the members on the list made one or more individual contributions, comprises 30 entries of altogether 37,000 words. It is the function of this printed article to provide a brief summary and a key to the full electronic discussion which accompanies it.",
    "author": [
      {
        "family": "Biggs",
        "given": "Michael"
      },
      {
        "family": "Huitfeldt",
        "given": "Claus"
      },
      {
        "family": "Bringsjord",
        "given": "Selmer"
      },
      {
        "family": "Bohan-Broderick",
        "given": "Paul"
      },
      {
        "family": "Ore",
        "given": "Espen S."
      },
      {
        "family": "Pichler",
        "given": "Alois"
      },
      {
        "family": "Raymond",
        "given": "Darrell"
      },
      {
        "family": "Renear",
        "given": "Allen H."
      },
      {
        "family": "Sperberg-McQueen",
        "given": "C. M."
      }
    ],
    "container-title": "The Monist",
    "id": "Biggs1997",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "document_research, markup, sgml",
    "language": "en-US",
    "page": "348-367",
    "title": "Philosophy and electronic publishing: Theory and metatheory in the development of text encoding",
    "title-short": "Philosophy and electronic publishing",
    "type": "article-journal",
    "volume": "80"
  },
  {
    "URL": "http://dharchive.org/paper/DH2014/Paper-874.xml",
    "abstract": "The GeoBib project is constructing a georeferenced online bibliography of early Holocaust and camp literature published between 1933 and 1949 (Entrup et al. 2013a). Our immediate objectives include identifying the texts of interest in the first place, composing abstracts for them, researching their history, and annotating relevant places and times. Relations between persons, texts, and places will be visualized using digital maps and GIS software as an integral part of the resulting GeoBib information portal.",
    "author": [
      {
        "family": "Binder",
        "given": "Frank"
      },
      {
        "family": "Entrup",
        "given": "Bastian"
      },
      {
        "family": "Schiller",
        "given": "Ines"
      },
      {
        "family": "Lobin",
        "given": "Henning"
      }
    ],
    "container-title": "Proceedings of Digital Humanities 2014",
    "id": "Binder2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities, uncertainty",
    "language": "en-US",
    "page": "95-98",
    "title": "Uncertain about uncertainty: Different ways of processing fuzziness in digital humanities data",
    "title-short": "Uncertain about uncertainty",
    "type": "paper-conference"
  },
  {
    "URL": "https://web.archive.org/web/20190714202557/http://www.drdobbs.com/architecture-and-design/interview-with-alan-kay/240003442",
    "accessed": {
      "date-parts": [
        [
          2019,
          9,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Binstock",
        "given": "Andrew"
      }
    ],
    "container-title": "Dr. Dobb’s Journal",
    "id": "Binstock2012",
    "issued": {
      "date-parts": [
        [
          2012,
          7,
          10
        ]
      ]
    },
    "language": "en-US",
    "title": "Interview with alan kay",
    "type": "article-journal"
  },
  {
    "DOI": "10.1007/978-3-319-90008-7",
    "author": [
      {
        "family": "Binstock",
        "given": "Andrew"
      }
    ],
    "container-author": [
      {
        "family": "Aiello",
        "given": "Marco"
      }
    ],
    "container-title": "The web was done by amateurs",
    "id": "Binstock2012-2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "page": "145-152",
    "publisher": "Springer",
    "title": "Dr. Dobb’s interview with alan kay",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Bird",
        "given": "Richard"
      }
    ],
    "id": "Bird1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "publisher": "Europe",
    "publisher-place": "Prentice Hall",
    "title": "Introduction to functional programming using Haskell",
    "type": "book"
  },
  {
    "DOI": "10.1109/TE.1961.4322215",
    "abstract": "PLATO—a teaching machine developed during the past nine months at the Coordinated Science Laboratory of the University of Illinois—is a device for teaching a number of students individually by means of a single, central, high-speed general-purpose digital computer. Each student is provided with his own keyset and television display. The keyset enables the student to control the sequence of materials presented to him by the machine, as well as to transmit to the computer answers to its questions. The computer communicates to each student by closed circuit television. It selects slides and writes or erases sentences and diagrams on a storage tube. These two outputs are superimposed and displayed on the student’s television screen. Not only are textual materials presented to each student at a rate determined by that student, but the computer frequently poses questions. The student’s answerswhich may take the form of numerals, algebraic expressions, or words and phrases are judged by the computer without revealing the correct answer to the question. Supplementary material is presented by the machine upon request for any question which the student finds difficult. The computer keeps detailed records of each student’s progress through the material. Though a two-student version of PLATO is now in operation, the paper describes in detail an earlier one-student system. The system has been used to present a variety of subject matters, ranging from mathematics to topics in French grammar.",
    "author": [
      {
        "family": "Bitzer",
        "given": "Donald L."
      },
      {
        "family": "Braunfeld",
        "given": "Peter G."
      },
      {
        "family": "Lichtenberger",
        "given": "W. W."
      }
    ],
    "container-title": "IRE Transactions on Education",
    "id": "Bitzer1961",
    "issued": {
      "date-parts": [
        [
          1961
        ]
      ]
    },
    "page": "157-161",
    "title": "PLATO: An automatic teaching device",
    "title-short": "PLATO",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "author": [
      {
        "family": "Bitzer",
        "given": "Donald L."
      },
      {
        "family": "Slottow",
        "given": "H. Gene"
      },
      {
        "family": "Willson",
        "given": "Robert H."
      }
    ],
    "id": "Bitzer1971",
    "issued": {
      "date-parts": [
        [
          1971,
          1
        ]
      ]
    },
    "note": "Filing Date: December 22, 1966",
    "publisher": "United States Patent 3 559 190",
    "title": "A gaseous discharge display and memory mechanism",
    "type": ""
  },
  {
    "DOI": "10.1109/aict.2007.29",
    "abstract": "The growing choice of e-learning platform technologies leads to new difficulties. In particular, it becomes hard to manage or to compare available platforms and to re-use existing course material from different kinds of platforms because of technological discrepancy. This paper proposes a tentative solution of this difficulty - a common platform-independent framework that can be used to specify and classify existing or future Learning management systems (LMS) and possibility of their cooperation. The proposed framework is based on OMG’s Model Driven Architecture.",
    "author": [
      {
        "family": "Bizoňová",
        "given": "Zuzana"
      },
      {
        "family": "Ranc",
        "given": "Daniel"
      }
    ],
    "container-title": "The third advanced international conference on telecommunications, 2007 (AICT’07)",
    "id": "Bizonova2007a",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "e-learning",
    "page": "25-29",
    "publisher": "IEEE Computer Society",
    "title": "Model driven LMS platform integration",
    "type": "paper-conference"
  },
  {
    "URL": "http://ftp.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-288/p02.pdf",
    "abstract": "The success of the e-learning paradigm observed in recent times created a growing demand for e-learning systems in universities and other educational institutions, that itself led to the development of a number of either commercial or open source learning management systems (LMS). While the usage of these systems gains recognition and acceptance amongst institutions,there are new problems arising that need to be solved. Because of multiplicity of platforms and approaches used for various systems implementation, it becomes increasingly difficult to manage or compare them. Their variety and growing number is also a true barrier for re-use of existing learning materials that is a clear economical concern for the future of these technologies. Applications and their data become isolated. As the result of platform diversity, future vision of interconnecting LMS of different educational institutions is demanding, too. The present study ambitions to overcome the aforementioned difficulties by using the Model Driven Architecture (MDA) approach of the Object Management Group (OMG). The goal is to provide a generalized architectural framework enabling an integrated specification of platform architectures. This platform-independent framework can then be used to specify and classify existing or future Learning management systems and to simplify migration of data between different kinds of e-learning systems.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Bizoňová",
        "given": "Zuzana"
      },
      {
        "family": "Ranc",
        "given": "Daniel"
      },
      {
        "family": "Drozdová",
        "given": "Matilda"
      }
    ],
    "collection-title": "CEUR workshop proceedings",
    "container-title": "2<sup>nd</sup> european conference on technology enhanced learning EC-TEL PROLEARN 2007 doctoral consortium",
    "editor": [
      {
        "family": "Maillet",
        "given": "Katherine"
      },
      {
        "family": "Klobucar",
        "given": "Tomaz"
      },
      {
        "family": "Gillet",
        "given": "Denis"
      },
      {
        "family": "Klamma",
        "given": "Ralf"
      }
    ],
    "id": "Bizonova2007d",
    "issued": {
      "date-parts": [
        [
          2007,
          9
        ]
      ]
    },
    "keyword": "e-learning",
    "page": "8-14",
    "publisher": "CEUR-WS.org",
    "title": "Model driven e-learning platform integration",
    "type": "paper-conference",
    "volume": "288"
  },
  {
    "abstract": "Chapter 11, by Jim Blandy, explains how a set of very simple components and an extension language can turn the humble text editor into an operating system, the Swiss army knife of a programmer’s toolchest.",
    "author": [
      {
        "family": "Blandy",
        "given": "Jim"
      }
    ],
    "chapter-number": "11",
    "container-title": "Beautiful architecture",
    "editor": [
      {
        "family": "Spinellis",
        "given": "Diomidis"
      },
      {
        "family": "Gousios",
        "given": "Georgios"
      }
    ],
    "id": "Blandy2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "emacs, interactive_editing, software_components",
    "page": "263-277",
    "publisher": "O’Reilly",
    "publisher-place": "Sebastopol, CA, USA",
    "title": "GNU Emacs: Creeping featurism is a strength",
    "title-short": "GNU Emacs",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/2133806.2133826",
    "ISSN": "0001-0782",
    "abstract": "Surveying a suite of algorithms that offer a solution to managing large document archives.",
    "author": [
      {
        "family": "Blei",
        "given": "David M."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Blei2012",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "formal_models, topic_modeling",
    "language": "en-US",
    "page": "77-84",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Probabilistic topic models",
    "type": "article-journal",
    "volume": "55"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2014/pdf/1009_Paper.pdf",
    "abstract": "We present a web-based application which is called TEA (Textual Emigration Analysis) as a showcase that applies textual analysis for the humanities. The TEA tool is used to transform raw text input into a graphical display of emigration source and target countries (under a global or an individual perspective). It provides emigration-related frequency information, and gives access to individual textual sources, which can be downloaded by the user. Our application is built on top of the CLARIN infrastructure which targets researchers of the humanities. In our scenario, we focus on historians, literary scientists, and other social scientists that are interested in the semantic interpretation of text. Our application processes a large set of documents to extract information about people who emigrated. The current implementation integrates two data sets: A data set from the Global Migrant Origin Database, which does not need additional processing, and a data set which was extracted from the German Wikipedia edition. The TEA tool can be accessed by using the following URL: http://clarin01.ims.uni-stuttgart.de/geovis/showcase.html",
    "author": [
      {
        "family": "Blessing",
        "given": "André"
      },
      {
        "family": "Kuhn",
        "given": "Jonas"
      }
    ],
    "container-title": "Proceedings of the ninth international conference on language resources and evaluation (LREC’14)",
    "editor": [
      {
        "family": "Calzolari",
        "given": "Nicoletta"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Declerck",
        "given": "Thierry"
      },
      {
        "family": "Loftsson",
        "given": "Hrafn"
      },
      {
        "family": "Maegaard",
        "given": "Bente"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Moreno",
        "given": "Asuncion"
      },
      {
        "family": "Odijk",
        "given": "Jan"
      },
      {
        "family": "Piperidis",
        "given": "Stelios"
      }
    ],
    "id": "Blessing2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities, nlp, wikipedia",
    "language": "en-US",
    "page": "2089-2093",
    "publisher": "European Language Resources Association (ELRA)",
    "publisher-place": "Paris",
    "title": "Textual emigration analysis",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Blickle",
        "given": "Peter"
      }
    ],
    "container-title": "Historische Zeitschrift",
    "id": "Blickle1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "rechtsquellen",
    "page": "121-136",
    "title": "Ordnung schaffen. Alteuropäische Rechtskultur in der Schweiz. Eine monumentale Edition",
    "type": "article-journal",
    "volume": "268"
  },
  {
    "URL": "http://www.jisc.ac.uk/uploaded_documents/Altilab04-infrastructureV2.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Blinco",
        "given": "Kerry"
      },
      {
        "family": "Mason",
        "given": "Jon"
      },
      {
        "family": "McLean",
        "given": "Neil"
      },
      {
        "family": "Wilson",
        "given": "Scott"
      }
    ],
    "genre": "A White Paper for alt-i-lab 2004",
    "id": "Blinco2004",
    "issued": {
      "date-parts": [
        [
          2004,
          7
        ]
      ]
    },
    "publisher": "prepared on behalf of DEST (Australia) and JISC-CETIS (UK)",
    "title": "Trends and issues in e-learning infrastructure development",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Bloch",
        "given": "Marc"
      }
    ],
    "id": "Bloch1949",
    "issued": {
      "date-parts": [
        [
          1949
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Armand Colin",
    "publisher-place": "Paris",
    "title": "Apologie pour l’histoire ou Métier d’historien",
    "type": "book"
  },
  {
    "editor": [
      {
        "family": "Bloom",
        "given": "Benjamin S."
      }
    ],
    "id": "Bloom1956",
    "issued": {
      "date-parts": [
        [
          1956
        ]
      ]
    },
    "publisher": "Longman",
    "publisher-place": "White Plains, NY, USA",
    "title": "Taxonomy of educational objectives, handbook 1: Cognitive domain",
    "title-short": "Taxonomy of educational objectives, handbook 1",
    "type": "book"
  },
  {
    "edition": "5",
    "editor": [
      {
        "family": "Bloom",
        "given": "Benjamin S."
      }
    ],
    "id": "Bloom1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "publisher": "Beltz",
    "publisher-place": "Weinheim",
    "title": "Taxonomie von Lernzielen im kognitiven Bereich",
    "type": "book"
  },
  {
    "DOI": "10.1016/j.cor.2009.02.005",
    "ISSN": "0305-0548",
    "abstract": "The longest common subsequence problem is a classical string problem that concerns finding the common part of a set of strings. It has several important applications, for example, pattern recognition or computational biology. Most research efforts up to now have focused on solving this problem optimally. In comparison, only few works exist dealing with heuristic approaches. In this work we present a deterministic beam search algorithm. The results show that our algorithm outperforms the current state-of-the-art approaches not only in solution quality but often also in computation time.",
    "author": [
      {
        "family": "Blum",
        "given": "Christian"
      },
      {
        "family": "Blesa",
        "given": "Maria J."
      },
      {
        "family": "Ibáñez",
        "given": "Manuel L."
      }
    ],
    "container-title": "Computers & Operations Research",
    "id": "Blum2009",
    "issue": "12",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "approximate_matching",
    "page": "3178-3186",
    "publisher": "Elsevier",
    "publisher-place": "Oxford, UK",
    "title": "Beam search for the longest common subsequence problem",
    "type": "article-journal",
    "volume": "36"
  },
  {
    "ISBN": "0198758391",
    "author": [
      {
        "family": "Bod",
        "given": "Rens"
      }
    ],
    "id": "Bod2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Oxford University Press",
    "publisher-place": "Oxford",
    "title": "A new history of the humanities: The search for principles and patterns from antiquity to the present",
    "title-short": "A new history of the humanities",
    "type": "book"
  },
  {
    "DOI": "doi:10.1215/00267929-3699787",
    "abstract": "The approaches to data-rich literary history that dominate academic and public debate—Franco Moretti’s “distant reading” and Matthew Jockers’s “macroanalysis”—model literary systems in limited, abstract, and often ahistorical ways. This problem arises from neglect of the activities and insights of textual scholarship and is inherited from, rather than opposed to, the New Criticism and its core method of “close reading.” Literary history requires not new or integrated methods but a new scholarly object capable of managing the documentary record’s complexity, especially as manifested in emerging digital knowledge infrastructure. Building on significant, though uneven and unacknowledged, departures from Moretti’s and Jockers’s work in data-rich literary history, this essay describes such an object, modeled on the foundational technology of textual scholarship: the scholarly edition.",
    "author": [
      {
        "family": "Bode",
        "given": "Katherine"
      }
    ],
    "container-title": "Modern Language Quarterly",
    "id": "Bode2017",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities, literature",
    "page": "77-106",
    "title": "The equivalence of “close” and “distant” reading; or, towards a new object for data-rich literary history",
    "type": "article-journal",
    "volume": "78"
  },
  {
    "DOI": "10.1007/s10579-007-9018-8",
    "ISSN": "1574-020X",
    "abstract": "TimeBank is the only reference corpus for TimeML, an expressive language for annotating complex temporal information. It is a rich resource for a broad range of research into various aspects of the expression of time and temporally related events. This paper traces the development of TimeBank from its initial—and somewhat noisy—version (1.1) to a substantially revised release (1.2), now available via the Linguistic Data Consortium. The development path is motivated by the encouraging empirical results of TimeML-compliant annotators developed on the basis of TimeBank 1.1, and is informed by a detailed study of the characteristics of that initial release, which guides a clean-up process turning TimeBank 1.2 into a consistent and robust community resource.",
    "author": [
      {
        "family": "Boguraev",
        "given": "Branimir"
      },
      {
        "family": "Pustejovsky",
        "given": "James"
      },
      {
        "family": "Ando",
        "given": "Rie"
      },
      {
        "family": "Verhagen",
        "given": "Marc"
      }
    ],
    "container-title": "Language Resources and Evaluation",
    "id": "Boguraev2010",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2007,
          2,
          1
        ]
      ]
    },
    "keyword": "spatio-temporal_annotation",
    "page": "91-115",
    "publisher": "Springer Netherlands",
    "title": "TimeBank evolution as a community resource for TimeML parsing",
    "type": "article-journal",
    "volume": "41"
  },
  {
    "URL": "http://aclweb.org/anthology/W11-4106",
    "abstract": "This paper deals with normalization of language data from Early New High German. We describe an unsupervised, rule-based approach which maps historical wordforms to modern wordforms. Rules are specified in the form of context-aware rewrite rules that apply to sequences of characters. They are derived from two aligned versions of the Luther bible and weighted according to their frequency. The evaluation shows that our approach (83%­–91% exact matches) clearly outperforms the baseline (65%).",
    "author": [
      {
        "family": "Bollmann",
        "given": "Marcel"
      },
      {
        "family": "Petran",
        "given": "Florian"
      },
      {
        "family": "Dipper",
        "given": "Stefanie"
      }
    ],
    "container-title": "Proceedings of the RANLP 2011 workshop on language technologies for digital humanities and cultural heritage",
    "id": "Bollmann2011a",
    "issued": {
      "date-parts": [
        [
          2011,
          9
        ]
      ]
    },
    "keyword": "cultural_heritage, german, spelling_normalization",
    "page": "34-42",
    "publisher-place": "Hissar, Bulgaria",
    "title": "Rule-Based normalization of historical texts",
    "type": "paper-conference"
  },
  {
    "abstract": "This paper deals with normalization of language data from Early New High German. We describe an unsupervised, rule-based approach which maps historical wordforms to modern wordforms. Rules are specified in the form of context-aware rewrite rules that apply to sequences of characters. They are derived from two aligned versions of the Luther bible and weighted according to their frequency. Applying the normalization rules to texts by Luther results in 91% exact matches, clearly outperforming the baseline (65%). Matches can be improved to 93% by combining the approach with a word substitution list. If applied to more diverse language data from roughly the same period, performance goes down to 42% exact matches (baseline: 32%), but is higher than using a wordlist. The results show that rules derived from a highly different type of text can support normalization to a certain extent.",
    "author": [
      {
        "family": "Bollmann",
        "given": "Marcel"
      },
      {
        "family": "Petran",
        "given": "Florian"
      },
      {
        "family": "Dipper",
        "given": "Stefanie"
      }
    ],
    "container-title": "Proceedings of the 5<sup>th</sup> language & technology conference",
    "id": "Bollmann2011b",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, german, spelling_normalization",
    "title": "Applying rule-based normalization to different types of historical texts—an evaluation",
    "type": "paper-conference"
  },
  {
    "ISBN": "9782880749682",
    "editor": [
      {
        "family": "Bolognini",
        "given": "Yves"
      },
      {
        "family": "Stamm",
        "given": "Marielle"
      }
    ],
    "id": "Bolognini2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Presses polytechniques et universitaires romandes",
    "title": "Disparition programmée: Le Musée Bolo mène l’enquête. (= Programmed Disappearance. The Bolo Museum Investigates)",
    "title-short": "Disparition programmée",
    "type": "book"
  },
  {
    "DOI": "10.1016/j.jsg.2015.03.003",
    "ISSN": "01918141",
    "abstract": "Uncertainty in the interpretation of geological data is an inherent element of geology. Datasets from different sources: remotely sensed seismic imagery, field data and borehole data, are often combined and interpreted to create a geological model of the sub-surface. The data have limited resolution and spatial distribution that results in uncertainty in the interpretation of the data and in the subsequent geological model(s) created. Methods to determine the extent of interpretational uncertainty of a dataset, how to capture and express that uncertainty, and consideration of uncertainties in terms of risk have been investigated. Here I review the work that has taken place and discuss best practice in accounting for uncertainties in structural interpretation workflows. Barriers to best practice are reflected on, including the use of software packages for interpretation. Experimental evidence suggests that minimising interpretation error through the use of geological reasoning and rules can help decrease interpretation uncertainty; through identification of inadmissible interpretations and in highlighting areas of uncertainty. Understanding expert thought processes and reasoning, including the use of visuospatial skills, during interpretation may aid in the identification of uncertainties, and in the education of new geoscientists.",
    "author": [
      {
        "family": "Bond",
        "given": "Clare E."
      }
    ],
    "container-title": "Journal of Structural Geology",
    "id": "Bond2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "philosophy_of_science, uncertainty",
    "page": "185-200",
    "title": "Uncertainty in structural interpretation: Lessons to be learnt",
    "title-short": "Uncertainty in structural interpretation",
    "type": "article-journal",
    "volume": "74"
  },
  {
    "URL": "https://www.chronicle.com/article/Digital-Is-Not-the/241634",
    "author": [
      {
        "family": "Bond",
        "given": "Sarah E."
      },
      {
        "family": "Long",
        "given": "Hoyt"
      },
      {
        "family": "Underwood",
        "given": "Ted"
      }
    ],
    "container-title": "The Chronicle of Higher Education",
    "id": "Bond2017",
    "issued": {
      "date-parts": [
        [
          2017,
          11,
          1
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "title": "’Digital’ is not the opposite of ’humanities’",
    "type": "webpage"
  },
  {
    "DOI": "10.1080/01973760903331742",
    "ISSN": "0197-3762",
    "abstract": "The Wesleyan‐Brown Monastic Archaeology project (MonArch) integrates research results from standing remains, excavated material culture, and texts from the Augustinian abbey of Saint‐Jean‐des‐Vignes in northern France. The digital dimension of the MonArch project re‐presents the site through three‐dimensional reconstructions of its architecture, inventories of its material culture, and searchable encoded texts. The site employs a variety of strategies to engage the viewer/user in critiques of our knowledge representations. In this paper, we explore the ethical and analytic aspects of archaeological recording and present preliminary results of our work on representing time, human movement, and uncertainty.",
    "author": [
      {
        "family": "Bonde",
        "given": "Sheila"
      },
      {
        "family": "Maines",
        "given": "Clark"
      },
      {
        "family": "Mylonas",
        "given": "Elli"
      },
      {
        "family": "Flanders",
        "given": "Julia"
      }
    ],
    "container-title": "Visual Resources",
    "id": "Bonde2009",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "digital_humanities, uncertainty",
    "language": "en-US",
    "page": "363-377",
    "title": "The virtual monastery: Re-presenting time, human movement, and uncertainty at Saint-Jean-des-Vignes, Soissons",
    "title-short": "The virtual monastery",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "DOI": "10.1016/s0020-7373(85)80001-8",
    "ISSN": "00207373",
    "author": [
      {
        "family": "Bonissone",
        "given": "Piero P."
      },
      {
        "family": "Tong",
        "given": "Richard M."
      }
    ],
    "container-title": "International Journal of Man-Machine Studies",
    "id": "Bonissone1985",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "keyword": "ai, uncertainty",
    "language": "en-US",
    "page": "241-250",
    "title": "Editorial: Reasoning with uncertainty in expert systems",
    "title-short": "Editorial",
    "type": "article-journal",
    "volume": "22"
  },
  {
    "DOI": "10.1093/llc/fqy073",
    "ISSN": "2055-7671",
    "abstract": "While the challenge of historical reconstruction of past musical performances is not a fully solved problem, not all the elements are equally unknown, or of equal magnitude. Despite some uncertainty about the interpretation of individual performers on specific dates, scholarship can still inform other factors of greater perceptual importance, leading to a good approximation of historical performances. In addition to performance style and period instruments, computer simulations make it possible to also account for the acoustics of the period performance space. In addition, the most accurate reconstruction should simulate the room’s acoustics in real time for the performers, thus retaining the feedback mechanisms of room response on performance practice.",
    "author": [
      {
        "family": "Boren",
        "given": "Braxton B."
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Boren2018",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "title": "Computational acoustic musicology",
    "type": "article-journal",
    "volume": "34"
  },
  {
    "DOI": "10.1145/57167.57175",
    "ISBN": "0-201-14237-6",
    "abstract": "The introduction of automated information retrieval (IR) systems was met with great enthusiasm and predictions that manual literature searching soon would be replaced. Three decades later, IR systems have not progressed to the stage where any but the dedicated few can operate them without a highly skilled human intermediary acting as interface between user and system. In the interim, we have learned that the retrieval process is extremely complex both in terms of understanding people and their communication and in terms of understanding scientific information and technical vocabulary. Experiments with new techniques suggest to many the possibility of eliminating the human intermediary, either in large part or altogether; others would argue that the retrieval problems are too complex to be resolved for more than highly restricted domains. The possibility of eliminating the human intermediary is of current research interest to the several disciplines that are represented on this panel.The discussion will revolve around one central question: Is our inability to get end-users to search primarily a problem of understanding people and their communication, or is it primarily a problem of understanding scientific information and technical vocabulary? Among the constituent issues the panel will address are these: Which are the most fundamental problems in constructing retrieval interfaces for the information seeker?How successful have been the attempts to build end-user oriented interfaces?Is a single, elegant solution to the problems of language handling, query negotiation, user modeling, and information retrieval possible?What techniques hold the most promise for automating the functions of the intermediary?What approaches are likely to be fruitless?Under what conditions is it possible and appropriate to automate some or all of the intermediary’s skills and function?",
    "author": [
      {
        "family": "Borgman",
        "given": "Christine L."
      },
      {
        "family": "Belkin",
        "given": "Nicholas J."
      },
      {
        "family": "Croft",
        "given": "W. Bruce"
      },
      {
        "family": "Lesk",
        "given": "Michael E."
      },
      {
        "family": "Landauer",
        "given": "Thomas K."
      }
    ],
    "collection-title": "CHI ’88",
    "container-title": "Proceedings of the SIGCHI conference on human factors in computing systems",
    "id": "Borgman1988",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "classic, ir",
    "page": "51-53",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Retrieval systems for the information seeker: Can the role of the intermediary be automated?",
    "title-short": "Retrieval systems for the information seeker",
    "type": "paper-conference"
  },
  {
    "ISBN": "2870092024",
    "abstract": "Recueil d’essais qui sont autant de jalons d’une réflexion sur les relations complexes qui se sont établies depuis une vingtaine d’années entre l’informatique et les sciences de l’homme. Point de vue opératoire: impact sur les conditions matérielles de la recherche. Point de vue conceptuel: facteur d’évolution des structures du raisonnement.",
    "author": [
      {
        "family": "Borillo",
        "given": "Mario"
      }
    ],
    "id": "Borillo1985",
    "issued": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "keyword": "classic, digital_humanities",
    "language": "fr-FR",
    "publisher": "Mardaga",
    "publisher-place": "Bruxelles",
    "title": "Informatique pour les sciences de l’homme: Limites de la formalisation du raisonnement",
    "title-short": "Informatique pour les sciences de l’homme",
    "type": "book"
  },
  {
    "URL": "https://aclanthology.org/W/W07/W07-0901",
    "abstract": "This paper provides a description and evaluation of a generic named-entity recognition (NER) system for Swedish applied to electronic versions of Swedish literary classics from the 19th century. We discuss the challenges posed by these texts and the necessary adaptations introduced into the NER system in order to achieve accurate results, useful both for metadata generation, but also for the enhancement of the searching and browsing capabilities of Litteraturbanken, the Swedish Literature Bank, an ongoing cultural heritage project which aims to digitize significant works of Swedish literature.",
    "author": [
      {
        "family": "Borin",
        "given": "Lars"
      },
      {
        "family": "Kokkinakis",
        "given": "Dimitrios"
      },
      {
        "family": "Olsson",
        "given": "Leif J."
      }
    ],
    "container-title": "Proceedings of the ACL 2007 workshop on language technology for cultural heritage data (LaTeCH 2007)",
    "id": "Borin2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage, proper_names",
    "page": "1-8",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Prague, Czech Republic",
    "title": "Naming the past: Named entity and animacy recognition in 19th century Swedish literature",
    "title-short": "Naming the past",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/workshops/W22_Proceedings.pdf",
    "abstract": "We present a computational morphological description of Old Swedish implemented in Functional Morphology. The objective of the work is concrete ­ connecting word forms in real text to entries in electronic dictionaries, for use in an online reading aid for students learning Old Swedish. The challenge we face is to find an appropriate model of Old Swedish able to deal with the rich linguistic variation in real texts, so that word forms appearing in real texts can be connected to the idealized citation forms of the dictionaries.",
    "author": [
      {
        "family": "Borin",
        "given": "Lars"
      },
      {
        "family": "Forsberg",
        "given": "Markus"
      }
    ],
    "container-title": "Proceedings of the LREC 2008 workshop on language technology for cultural heritage data (LaTeCH 2008)",
    "editor": [
      {
        "family": "Ribarov",
        "given": "Kiril"
      },
      {
        "family": "Sporleder",
        "given": "Caroline"
      }
    ],
    "id": "Borin2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, morphology, swedish",
    "page": "9-16",
    "title": "Something old, something new: A computational morphological description of Old Swedish",
    "title-short": "Something old, something new",
    "type": "paper-conference"
  },
  {
    "URL": "http://ucrel.lancs.ac.uk/publications/cl2007/paper/150_Paper.pdf",
    "abstract": "The principal corpora currently available in classical literature, while quite thorough, are based on authoritative editions without critical apparatuses. However, philologists need to deal with textual variants attested by manuscripts and conjectures suggested by scholars through the centuries. This paper will explore some methods for information extraction applied to digitised apparatuses of critical editions and digital repertories of conjectures.",
    "author": [
      {
        "family": "Boschetti",
        "given": "Federico"
      }
    ],
    "container-title": "Proceedings of the corpus linguistics conference CL2007",
    "editor": [
      {
        "family": "Davies",
        "given": "Matthew"
      },
      {
        "family": "Rayson",
        "given": "Paul"
      },
      {
        "family": "Hunston",
        "given": "Susan"
      },
      {
        "family": "Danielsson",
        "given": "Pernilla"
      }
    ],
    "id": "Boschetti2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, digital_edition, greek, latin",
    "publisher": "University of Birmingham",
    "title": "Methods to extend Greek and Latin corpora with variants and conjectures: Mapping critical apparatuses onto reference text",
    "title-short": "Methods to extend Greek and Latin corpora with variants and conjectures",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-04346-8_17",
    "ISBN": "3-642-04345-3, 978-3-642-04345-1",
    "abstract": "This paper describes a work-flow designed to populate a digital library of ancient Greek critical editions with highly accurate OCR scanned text. While the most recently available OCR engines are now able after suitable training to deal with the polytonic Greek fonts used in 19th and 20th century editions, further improvements can also be achieved with postprocessing. In particular, the progressive multiple alignment method applied to different OCR outputs based on the same images is discussed in this paper.",
    "author": [
      {
        "family": "Boschetti",
        "given": "Federico"
      },
      {
        "family": "Romanello",
        "given": "Matteo"
      },
      {
        "family": "Babeu",
        "given": "Alison"
      },
      {
        "family": "Bamman",
        "given": "David"
      },
      {
        "family": "Crane",
        "given": "Gregory"
      }
    ],
    "container-title": "ECDL’09: Proceedings of the 13th european conference on research and advanced technology for digital libraries",
    "id": "Boschetti2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "page": "156-167",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Improving OCR accuracy for classical critical editions",
    "type": "paper-conference"
  },
  {
    "URL": "http://eprints-phd.biblio.unitn.it/185/",
    "abstract": "The aim of this work is the application of techniques developed in the domain of corpus linguistics to a collection of ancient Greek texts, taking into account not only the canonical text established by modern editors, but also the variant readings recorded in the critical apparatus or in the repertories of conjectures. The dissertation is divided in three connected parts: construction, mapping and analysis of the corpus. The first part is devoted to corpus construction and it is focused on the techniques to improve the OCR accuracy on classical critical editions. This task is challenging because critical editions are multilingual, the set of characters to recognize is wide and the quality of last centuries paper is variable. Three OCR engines are applied to the same texts and a Bayesian classifier, joint to a specific spell-checker, evaluates the most probable output. It is demonstrated that the improvement is significative and, in the best cases, it is more than 3%. The second part is devoted to the alignment of the contents extracted from critical apparatus and repertories of conjectures to the reference text. A parser has been developed to classify the chunks of information (verse number, Greek word sequences, textual operation, scholar that suggested the conjecture). Alignment algorithms used to find the precise position of the conjecture in its context are illustrated in detail. The third part is devoted to the study of the semantic spaces of ancient Greek. The chapter is focused on the specificity of the corpus, that is morphologically complex, literary (both poetry and prose) and diachronical (from VIII century B.C. to XV century A.D.). The word senses in documents belonging to different genres are explored, and the diachronical change of meaning is observed. Finally, a couple of meaningful conjectures extracted in the first part is analysed, evaluating the most interesting reciprocal relations in the semantic space.",
    "author": [
      {
        "family": "Boschetti",
        "given": "Federico"
      }
    ],
    "genre": "PhD thesis",
    "id": "Boschetti2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_edition, digital_humanities, greek, latin, ocr",
    "publisher": "University of Trento",
    "publisher-place": "Trento, Italy",
    "title": "A corpus-based approach to philological issues",
    "type": "thesis"
  },
  {
    "DOI": "10.1007/s10844-010-0136-1",
    "ISSN": "0925-9902",
    "abstract": "The evolution of multimedia document production and diffusion technologies has lead to a significant spread of knowledge in form of pictures and recordings. However, scholarly reading tasks are still principally performed on textual contents. We argue that this is due to a lack of critical and structured tools: (1) to handle the wide spectrum of interpretive operations involved by the polymorphous scholarly reading process; (2) to perform these operations on a heterogeneous multimedia corpus. This firstly calls for identifying fundamental document requirements for such reading practices. Then, we present a flexible model and a software environment which enable the reader to structure, annotate, link, fragment, compare, freely organise and spatially lay out documents, and to prepare the writing of their critical comment. We eventually discuss experiments with humanities scholars, and explore new academic reading practices which take advantage of document engineering principles such as multimedia document structuring, publication or sharing.",
    "author": [
      {
        "family": "Bottini",
        "given": "Thomas"
      },
      {
        "family": "Morizet-Mahoudeaux",
        "given": "Pierre"
      },
      {
        "family": "Bachimont",
        "given": "Bruno"
      }
    ],
    "container-title": "Journal of Intelligent Information Systems",
    "id": "Bottini2011",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "document_engineering",
    "page": "39-63",
    "publisher": "Springer Netherlands",
    "title": "A model and environment for improving multimedia scholarly reading practices",
    "type": "article-journal",
    "volume": "37"
  },
  {
    "DOI": "10.1080/713695728",
    "abstract": "Assessment practices in higher education institutions tend not to equip students well for the processes of effective learning in a learning society. The purposes of assessment should be extended to include the preparation of students for sustainable assessment. Sustainable assessment encompasses the abilities required to undertake those activities that necessarily accompany learning throughout life in formal and informal settings. Characteristics of effective formative assessment identified by recent research are used to illustrate features of sustainable assessment. Assessment acts need both to meet the specific and immediate goals of a course as well as establishing a basis for students to undertake their own assessment activities in the future. To draw attention to the importance of this, the idea that assessment always has to do double duty is introduced.",
    "author": [
      {
        "family": "Boud",
        "given": "David"
      }
    ],
    "container-title": "Studies in Continuing Education",
    "id": "Boud2000",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "assessment, pedagogy",
    "page": "151-167",
    "publisher": "Routledge",
    "title": "Sustainable assessment: Rethinking assessment for the learning society",
    "title-short": "Sustainable assessment",
    "type": "article-journal",
    "volume": "22"
  },
  {
    "ISBN": "9782759221998",
    "URL": "https://www.cairn.info/la-modelisation-critique--9782759221998.htm",
    "abstract": "Aujourd’hui, le travail de l’ingénieur ou du chercheur se fait principalement à l’aide des modèles. Des sciences de la matière à la sociologie en passant par l’environnement et l’économie, la modélisation est devenue, grâce à l’informatique, le mode d’expression le mieux adapté à la préparation des projets et à la décision collective sous toutes ses formes, parce qu’elle met en oeuvre naturellement l’interdisciplinarité indispensable aux problèmes complexes. Faut-il prendre ces modèles pour la vérité ? C’est ce que prétendent souvent leurs auteurs. Pourtant, on voit bien à travers des cas concrets qu’on aurait pu prendre les choses autrement, qu’une autre approche aurait fait apparaître d’autres aspects, d’autres idées, d’autres risques. Éclairé par diverses théories de la connaissance (W. Quine, T. Kuhn et U. Beck), ce livre montre que critiquer efficacement une modélisation nécessite de construire d’autres modèles du cas étudié. La contre-modélisation est la pratique indispensable aujourd’hui pour que la connaissance ne se cantonne pas à l’ésotérisme technocratique des boîtes noires ; elle doit être enseignée comme une dissertation scientifique, en suscitant la pensée critique. Un public large trouvera dans ce livre matière à réflexion : les chercheurs et les doctorants d’abord, en situation de construire des modèles ; les membres actifs des associations qui s’engagent sur le terrain et sur internet, et sont amenés à participer à des modélisations ; enfin les enseignants qui, utilisant à juste titre la modélisation à des fins pédagogiques, sont souvent mal à l’aise avec le parti pris inhérent à une démarche de représentation.",
    "author": [
      {
        "family": "Bouleau",
        "given": "Nicolas"
      }
    ],
    "id": "Bouleau2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "formal_models",
    "language": "fr-FR",
    "publisher": "Quæ",
    "publisher-place": "Versailles",
    "title": "La modélisation critique",
    "type": "book"
  },
  {
    "URL": "https://cairn.info/revue-les-cahiers-du-numerique-2017-3-page-19.htm",
    "abstract": "The profusion of digital, both in the private and public spheres, affects information practices and uses. Digital humanities constitute a field that can be related to the acceleration of technological innovations at the service of human actors in a context of knowledge production, management and transmission. Using various points of view, we want to understand better how digital humanities are related to digital uses and practices. We will rely on a prospective approach in order to address: community creation and collective intelligence as well as interaction with the environment. We are also interested in the issues and challenges of digital humanities.",
    "author": [
      {
        "family": "Bouzidi",
        "given": "Laïd"
      },
      {
        "family": "Boulesnane",
        "given": "Sabrina"
      }
    ],
    "container-title": "Les Cahiers du numérique",
    "id": "Bouzidi2017",
    "issue": "3–4",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "fr-FR",
    "page": "19-38",
    "title": "Les humanités numériques: L’évolution des usages et des pratiques",
    "title-short": "Les humanités numériques",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "editor": [
      {
        "family": "Bowles",
        "given": "Edmund A."
      }
    ],
    "id": "Bowles1967",
    "issued": {
      "date-parts": [
        [
          1967
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Prentice-Hall",
    "publisher-place": "Englewood Cliffs, NJ, USA",
    "title": "Computers in humanistic research: Readings and perspectives",
    "title-short": "Computers in humanistic research",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Blitzer",
        "given": "Charles"
      }
    ],
    "chapter-number": "21",
    "container-title": "Computers in humanistic research: Readings and perspectives",
    "editor": [
      {
        "family": "Bowles",
        "given": "Edmund A."
      }
    ],
    "id": "Blitzer1967",
    "issued": {
      "date-parts": [
        [
          1967
        ]
      ]
    },
    "language": "en-US",
    "page": "221-230",
    "publisher": "Prentice-Hall",
    "publisher-place": "Englewood Cliffs, NJ, USA",
    "title": "“This wonderful machine:” Some thoughts on computers and the humanities: Readings and perspectives",
    "title-short": "“This wonderful machine:” Some thoughts on computers and the humanities",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/bf00186486",
    "ISSN": "0010-4817",
    "abstract": "This article describes a second aspect of the Project for Latin Lexicography (see previous article). We here concentrate on two aspects of the project. First, we describe the morphological analyzer, which comprises a base dictionary, a table of suffixes, a table of endings and a table of postfixes. Second, we describe the lemmatization module, which operates by reference to a series of grammatical codes or information given for the base, and reference codes.",
    "author": [
      {
        "family": "Bozzi",
        "given": "Andrea"
      },
      {
        "family": "Giuseppe",
        "given": "Cappelli"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Bozzi1990",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "cultural_heritage, latin, morphology",
    "page": "421-426",
    "publisher": "Springer Netherlands",
    "title": "A project for Latin lexicography: 2. A Latin morphological analyzer",
    "title-short": "A project for Latin lexicography",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "DOI": "10.1016/j.is.2008.01.006",
    "abstract": "XML is successful as a machine processable data interchange format, but it is often too verbose for human use. For this reason, many XML languages permit an alternative more legible non-XML syntax. XSLT stylesheets are often used to convert from the XML syntax to the alternative syntax; however, such transformations are not reversible since no general tool exists to automatically parse the alternative syntax back into XML. We present XSugar, which makes it possible to manage dual syntax for XML languages. An XSugar specification is built around a context-free grammar that unifies the two syntaxes of a language. Given such a specification, the XSugar tool can translate from alternative syntax to XML and vice versa. Moreover, the tool statically checks that the transformations are reversible and that all XML documents generated from the alternative syntax are valid according to a given XML schema.",
    "author": [
      {
        "family": "Brabrand",
        "given": "Claus"
      },
      {
        "family": "Møller",
        "given": "Anders"
      },
      {
        "family": "Schwartzbach",
        "given": "Michael I."
      }
    ],
    "container-title": "Information Systems",
    "id": "Brabrand2008",
    "issue": "4–5",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "markup, xml",
    "page": "385-406",
    "title": "Dual syntax for XML languages",
    "type": "article-journal",
    "volume": "33"
  },
  {
    "ISBN": "9781558609327",
    "abstract": "Knowledge representation is at the very core of a radical idea for understanding intelligence. Instead of trying to understand or build brains from the bottom up, its goal is to understand and build intelligent behavior from the top down, putting the focus on what an agent needs to know in order to behave intelligently, how this knowledge can be represented symbolically, and how automated reasoning procedures can make this knowledge available as needed. This landmark text takes the central concepts of knowledge representation developed over the last 50 years and illustrates them in a lucid and compelling way. Each of the various styles of representation is presented in a simple and intuitive form, and the basics of reasoning with that representation are explained in detail. This approach gives readers a solid foundation for understanding the more advanced work found in the research literature. The presentation is clear enough to be accessible to a broad audience, including researchers and practitioners in database management, information retrieval, and object-oriented systems as well as artificial intelligence. This book provides the foundation in knowledge representation and reasoning that every AI practitioner needs.",
    "author": [
      {
        "family": "Brachman",
        "given": "Ronald J."
      },
      {
        "family": "Levesque",
        "given": "Hector J."
      }
    ],
    "id": "Brachman2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "knowledge_representation",
    "language": "en-US",
    "publisher": "Morgan Kaufmann",
    "publisher-place": "San Francisco, CA, USA",
    "title": "Knowledge representation and reasoning",
    "type": "book"
  },
  {
    "URL": "http://jstor.org/stable/10.7722/j.ctt31nk19.15",
    "abstract": "Computing offers something of a paradox when it comes to historical studies. On one hand, one suspects that almost all academic historians in at least Western Europe and North America have a computer both in their office and at home and use it daily for email, word processing, and for surfing the World Wide Web. However, in spite of their daily contact with the machine, they view it as having little or nothing to do with the essence of their research. Now, the fact that historians use the computer every day as a part of their research activities, but both hardly notice it and probably do not often think that it actually affects what they do, turns out to be an interesting phenomenon that is, of course, not restricted to the field of history. Indeed, the ability of tools such as word processing, email, and the WWW to fit into the normal way of doing things so that they are almost invisible reveals an aspect to computing that is significant in its own right. However, this chapter presents an example of a more prominent role for the computer in the doing of history. We focus on one of the ways in which computing obviously significantly impacts on the research: representing the product of historical research as highly structured materials in databases, and use the Paradox of Medieval Scotland project as the prime example.",
    "author": [
      {
        "family": "Bradley",
        "given": "John"
      },
      {
        "family": "Pasin",
        "given": "Michele"
      }
    ],
    "container-title": "New perspectives on medieval Scotland, 1093–1286",
    "id": "Bradley2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models, history",
    "language": "en-US",
    "page": "203-214",
    "publisher": "Boydell and Brewer",
    "publisher-place": "Woodbridge, England",
    "title": "Structuring that which cannot be structured: A role for formal models in representing aspects of medieval Scotland",
    "title-short": "Structuring that which cannot be structured",
    "type": "chapter",
    "volume": "32"
  },
  {
    "ISSN": "1757-0522",
    "URL": "http://thereasoner.org/files/2015/01/TheReasoner-124.pdf",
    "author": [
      {
        "family": "Bradley",
        "given": "Seamus"
      }
    ],
    "container-title": "The Reasoner",
    "id": "Bradley2018",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "31-32",
    "title": "Uncertain reasoning",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "DOI": "10.1093/llc/fqy033",
    "ISSN": "2055-7671",
    "abstract": "Man is a Tool-making Animal(Attributed to Benjamin Franklin by Samuel Johnson)Although digital tools built by those outside the Digital Humanities (DH) community (particularly tools for big data analysis or social network analysis) currently take a significant amount of DH attention these days, there has been toolmaking inside the DH for about 70 years. This article’s author has acted as a tool developer from time to time over many years, and this work has been part of his personal experience of DH from the 1970s to the present day (some history of his involvement in DH can be found in Bradley and Nylan, 2016). Text Analysis Computing Tools (TACT) (in the late 1980s) was one of the tools created by the author that fit within the then text-oriented DH mainstream, and at the time, it influenced our community about what an interactive text analysis tool might be like. Most recently, this author built Pliny (Pliny, 2015) which proposed a type of tool aimed at a broad range of tasks involved in humanities scholarship.",
    "author": [
      {
        "family": "Bradley",
        "given": "John"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "BradleyJ2018",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "page": "13-20",
    "title": "Digital tools in the humanities: Some fundamental provocations?",
    "title-short": "Digital tools in the humanities",
    "type": "article-journal",
    "volume": "34"
  },
  {
    "URL": "http://ietf.org/html/rfc2026",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Bradner",
        "given": "Scott"
      }
    ],
    "genre": "RFC",
    "id": "Bradner1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "number": "2026 (BCP 9)",
    "publisher": "Internet Engineering Task Force",
    "title": "The Internet standards process – revision 3",
    "type": "report"
  },
  {
    "DOI": "10.1145/1600193.1600202",
    "ISBN": "978-1-60558-575-8",
    "abstract": "For much of England and Wales marriage registers began to be kept in 1537. The marriage details were recorded locally, and in longhand, until 1st July 1837, when central records began. All registers were kept in the local parish church. In the period from 1896 to 1922 an attempt was made, by the Phillimore company of London, using volunteer help, to transcribe marriage registers for as many English parishes as possible and to have them printed. This paper describes an experiment in the automated re-typesetting of Volume 2 of the 15-volume Phillimore series relating to the county of Derbyshire. The source material was plain text derived from running Optical Character Recognition (OCR) on a set of page scans taken from the original printed volume. The aim of the experiment was to avoid any idea of labour-intensive page-by-page rebuilding with tools such as Acrobat Capture. Instead, it proved possible to capitalise on the regular, tabular, structure of the Register pages as a means of automating the re-typesetting process, using UNIX troff software and its tbl preprocessor. A series of simple software tools helped to bring about the OCR-to-troff transformation. However, the re-typesetting of the text was not just an end in itself but, additionally, a step on the way to content enhancement and content repurposing. This included the indexing of the marriage entries and their potential transformation into XML and GEDCOM notations. The experiment has shown, for highly regular material, that the efforts of one programmer, with suitable low-level tools, can be far more effective than attempting to recreate the printed material using WYSIWYG software.",
    "author": [
      {
        "family": "Brailsford",
        "given": "David F."
      }
    ],
    "container-title": "Proceedings of the 9th ACM symposium on document engineering (DocEng ’09)",
    "id": "Brailsford2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "document_analysis, document_engineering, troff, typesetting, typography",
    "page": "29-38",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Automated re-typesetting, indexing and content enhancement for scanned marriage registers",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/A00-1031",
    "abstract": "Trigrams’n’Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora.",
    "author": [
      {
        "family": "Brants",
        "given": "Thorsten"
      }
    ],
    "container-title": "Proceedings of the sixth applied natural language processing conference (ANLP-2000)",
    "id": "Brants2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "pos_tagging",
    "title": "TnT – a statistical part-of-speech tagger",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2000/pdf/333.pdf",
    "abstract": "This paper presents the results of an investigation on inter-annotator agreement for the NEGRA corpus, consisting of German newspaper texts. The corpus is syntactically annotated with part-of-speech and structural information. Agreement for part-of-speech is 98.6%, the labeled F-score for structures is 92.4%. The two annotations are used to create a common final version by discussing differences and by several iterations of cleaning. Initial and final versions are compared. We identify categories causing large numbers of differences and categories that are handled inconsistently.",
    "author": [
      {
        "family": "Brants",
        "given": "Thorsten"
      }
    ],
    "container-title": "Second international conference on language resources and evaluation (LREC-2000)",
    "id": "Brants2000a",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "corpus_linguistics, german, nlp",
    "publisher": "European Language Resources Association (ELRA)",
    "publisher-place": "Paris",
    "title": "Inter-annotator agreement for a German newspaper corpus",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1023/B:INRT.0000011208.60754.a1",
    "abstract": "Information retrieval systems operating on free text face difficulties when word forms used in the query and documents do not match. The usual solution is the use of a “stemming component” that reduces related word forms to a common stem. Extensive studies of such components exist for English, but considerably less is known for other languages. Previously, it has been claimed that stemming is essential for highly declensional languages. We report on our experiments on stemming for German, where an additional issue is the handling of compounds, which are formed by concatenating several words. The major contribution of our work that goes beyond its focus on German lies in the investigation of a complete spectrum of approaches, ranging from language-independent to elaborate linguistic methods. The main findings are that stemming is beneficial even when using a simple approach, and that carefully designed decompounding, the splitting of compound words, remarkably boosts performance. All findings are based on a thorough analysis using a large reliable test collection.",
    "author": [
      {
        "family": "Braschler",
        "given": "Martin"
      },
      {
        "family": "Ripplinger",
        "given": "Bärbel"
      }
    ],
    "container-title": "Information Retrieval",
    "id": "Braschler2004",
    "issue": "3–4",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "ir, nlp",
    "page": "291-316",
    "title": "How effective is stemming and decompounding for German text retrieval?",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "URL": "http://aclweb.org/anthology/C10-2010",
    "abstract": "We address the problem of unsupervised and language-pair independent alignment of symmetrical and asymmetrical parallel corpora. Asymmetrical parallel corpora contain a large proportion of 1-to-0/0-to-1 and 1-to-many/many-to-1 sentence correspondences. We have developed a novel approach which is fast and allows us to achieve high accuracy in terms of F1 for the alignment of both asymmetrical and symmetrical parallel corpora. The source code of our aligner and the test sets are freely available.",
    "author": [
      {
        "family": "Braune",
        "given": "Fabienne"
      },
      {
        "family": "Fraser",
        "given": "Alexander"
      }
    ],
    "container-title": "Proceedings of the 23<sup>rd</sup> international conference on computational linguistics (coling 2010): posters",
    "id": "Braune2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "machine_translation, nlp",
    "page": "81-89",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Improved unsupervised sentence alignment for symmetrical and asymmetrical parallel corpora",
    "type": "paper-conference"
  },
  {
    "abstract": "After a decade of investment and hype, what has the field accomplished? Not much.",
    "author": [
      {
        "family": "Brennan",
        "given": "Timothy"
      }
    ],
    "container-title": "The Chronicle of Higher Education",
    "id": "Brennan2017",
    "issue": "8",
    "issued": {
      "date-parts": [
        [
          2017,
          10
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "title": "The digital-humanities bust",
    "type": "article-journal",
    "volume": "64"
  },
  {
    "DOI": "10.1117/12.783598",
    "abstract": "OCRopus is a new, open source OCR system emphasizing modularity, easy extensibility, and reuse, aimed at both the research community and large scale commercial document conversions. This paper describes the current status of the system, its general architecture, as well as the major algorithms currently being used for layout analysis and text line recognition.",
    "author": [
      {
        "family": "Breuel",
        "given": "Thomas M."
      }
    ],
    "container-title": "Proceedings of the ISTSPIE 20<sup>th</sup> annual symposium",
    "editor": [
      {
        "family": "Yanikoglu",
        "given": "Berrin A."
      },
      {
        "family": "Berkner",
        "given": "Kathrin"
      }
    ],
    "id": "Breuel2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ocr",
    "title": "The OCRopus open source OCR system",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1577802.1577805",
    "ISBN": "978-1-60558-698-4",
    "abstract": "The OCRopus system is an open source OCR system developed for book capture and digital library applications. It is designed to be a multilingual system in which all components are easily pluggable and replaceable. In this paper, I describe recent progress, on-going work, and preliminary results in the development of the OCRopus system, including the new component model, a new line recognizer, a new set of decoders, and language modeling tools.",
    "author": [
      {
        "family": "Breuel",
        "given": "Thomas"
      }
    ],
    "container-title": "Proceedings of the international workshop on multilingual OCR (MOCR ’09)",
    "id": "Breuel2009a",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "ocr",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Recent progress on the OCRopus OCR system",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-00155-0_21",
    "ISBN": "978-3-642-00154-3",
    "abstract": "OCRopus is an open source OCR system currently being developed, intended to be omni-lingual and omni-script. In addition to modern digital library applications, applications of the system include capturing and recognizing classical literature, as well as the large body of research literature about classics. OCRopus advances the state of the art in a number of ways, including the ability easily to plug in new text recognition and layout analysis modules, the use of adaptive and user extensible character recognition, and statistical and trainable layout analysis. Of particular interest for computational linguistics applications is the consistent use of probability estimates throughout the system and the use of weighted finite state transducers to represent both alternative recognition hypotheses and statistical language models. In this paper, I first give an overview of these technologies and their relevance to digital library applications in the humanities, and then focus on the use of statistical language models and their use for the integration of OCR output with subsequent computational linguistic and information extraction modules.",
    "author": [
      {
        "family": "Breuel",
        "given": "Thomas M."
      }
    ],
    "container-title": "Sanskrit computational linguistics",
    "editor": [
      {
        "family": "Huet",
        "given": "Gérard"
      },
      {
        "family": "Kulkarni",
        "given": "Amba"
      },
      {
        "family": "Scharf",
        "given": "Peter"
      }
    ],
    "id": "Breuel2009b",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "page": "391-402",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Applying the OCRopus OCR system to scholarly Sanskrit literature",
    "type": "chapter"
  },
  {
    "DOI": "10.1109/icdar.2013.140",
    "ISBN": "978-0-7695-4999-6",
    "ISSN": "1520-5363",
    "abstract": "Long Short-Term Memory (LSTM) networks have yielded excellent results on handwriting recognition. This paper describes an application of bidirectional LSTM networks to the problem of machine-printed Latin and Fraktur recognition. Latin and Fraktur recognition differs significantly from handwriting recognition in both the statistical properties of the data, as well as in the required, much higher levels of accuracy. Applications of LSTM networks to handwriting recognition use two-dimensional recurrent networks, since the exact position and baseline of handwritten characters is variable. In contrast, for printed OCR, we used a one-dimensional recurrent network combined with a novel algorithm for baseline and x-height normalization. A number of databases were used for training and testing, including the UW3 database, artificially generated and degraded Fraktur text and scanned pages from a book digitization project. The LSTM architecture achieved 0.6% character-level test-set error on English text. When the artificially degraded Fraktur data set is divided into training and test sets, the system achieves an error rate of 1.64%. On specific books printed in Fraktur (not part of the training set), the system achieves error rates of 0.15% (Fontane) and 1.47% (Ersch-Gruber). These recognition accuracies were found without using any language modelling or any other post-processing techniques.",
    "author": [
      {
        "family": "Breuel",
        "given": "Thomas M."
      },
      {
        "family": "Ul-Hasan",
        "given": "Adnan"
      },
      {
        "family": "Al-Azawi",
        "given": "Mayce A."
      },
      {
        "family": "Shafait",
        "given": "Faisal"
      }
    ],
    "container-title": "2013 12th international conference on document analysis and recognition",
    "id": "Breuel2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "ocr",
    "language": "en-US",
    "page": "683-687",
    "publisher": "Tech. Univ. of Kaiserslautern, Kaiserslautern, Germany; IEEE",
    "title": "High-Performance OCR for printed english and fraktur using LSTM networks",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/3-540-45735-6_9",
    "ISBN": "978-3-540-44158-8",
    "abstract": "In this paper we describe a stemming algorithm for Galician language, which supports, at the same time, the four current orthographic regulations for Galician. The algorithm has already been implemented, and we have started to use it for its improvement. But this stemming algorithm cannot be applied over documents previous to the appearance of the first Galician orthographic regulation in 1977; therefore we have adopted an exhaustive approach, consisting in defining a huge collection of wordsets for allowing systematic word comparisons, to stem documents written before that date. We also describe here a tool to build the wordsets needed in this approach.",
    "author": [
      {
        "family": "Brisaboa",
        "given": "Nieves"
      },
      {
        "family": "Callón",
        "given": "Carlos"
      },
      {
        "family": "López",
        "given": "Juan-Ramón"
      },
      {
        "family": "Places",
        "given": "Ángeles"
      },
      {
        "family": "Sanmartín",
        "given": "Goretti"
      }
    ],
    "chapter-number": "9",
    "collection-title": "Lecture notes in computer science",
    "container-title": "String processing and information retrieval. 9<sup>th</sup> international symposium, SPIRE 2002 lisbon, portugal, september 11–13, 2002 proceedings",
    "editor": [
      {
        "family": "Laender",
        "given": "Alberto"
      },
      {
        "family": "Oliveira",
        "given": "Arlindo"
      }
    ],
    "id": "Brisaboa2002",
    "issued": {
      "date-parts": [
        [
          2002,
          9
        ]
      ]
    },
    "keyword": "cultural_heritage, galician, ir",
    "page": "201-206",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Stemming Galician texts",
    "type": "chapter",
    "volume": "2476"
  },
  {
    "DOI": "10.1007/3-540-36456-0_43",
    "ISBN": "978-3-540-00532-2",
    "abstract": "In this work we present a manually marked up corpus of Old Galician language (460 documents, 5,601,290 running words) and a diachronic dictionary extracted from it, as well as its potential applications, whose implementation is a topic of future work.",
    "author": [
      {
        "family": "Brisaboa",
        "given": "Nieves"
      },
      {
        "family": "López",
        "given": "Juan-Ramón"
      },
      {
        "family": "Penabad",
        "given": "Miguel"
      },
      {
        "family": "Places",
        "given": "Ángeles"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Computational linguistics and intelligent text processing. 4th international conference, CICLing 2003",
    "editor": [
      {
        "family": "Gelbukh",
        "given": "Alexander"
      }
    ],
    "id": "Brisaboa2003",
    "issued": {
      "date-parts": [
        [
          2003,
          4
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, galician",
    "page": "39-65",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Diachronic stemmed corpus and dictionary of Galician language",
    "type": "chapter",
    "volume": "2588"
  },
  {
    "abstract": "Au principe de toute activité scientifique, la confrontation systématique de la logique et de l’expérience passe de plus en plus par l’élaboration de structures mathématisables qui confèrent en définitive aux modèles leur cohérence, et aussi une partie de leur pertinence. Sur ce thème interdisciplinaire de la modélisation, trente mathématicien, biologistes, économistes et sociologues, français ou étrangers, présentent ici leur contribution à une réflexion collective conduite depuis quelques années à l’initiative de plusieurs centres de recherche lyonnais.",
    "editor": [
      {
        "family": "Brissaud",
        "given": "Marcel"
      },
      {
        "family": "Forsé",
        "given": "Michel"
      },
      {
        "family": "Zighed",
        "given": "Abdelkader"
      }
    ],
    "id": "Brissaud1990",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "formal_models",
    "language": "fr-FR",
    "publisher": "CNRS",
    "publisher-place": "Paris",
    "title": "La modélisation: confluent des sciences",
    "title-short": "La modélisation",
    "type": "book"
  },
  {
    "abstract": "In this treatise on the central role of science, John Brockman contends that science is becoming the predominant culture and scientists are taking the place of traditional intellectuals in answering the important questions facing humankind. Structured in interview format, _The Third Culture_ consists of 23 noted scientists discussing their theories, the nature of scientific inquiry, and their common desire to be recognized as today’s intellectual leaders.",
    "author": [
      {
        "family": "Brockman",
        "given": "John"
      }
    ],
    "id": "Brockman1995",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "publisher": "Touchstone",
    "publisher-place": "New York, NY, USA",
    "title": "The third culture: Beyond the scientific revolution",
    "title-short": "The third culture",
    "type": "book"
  },
  {
    "DOI": "10.1007/3-540-48005-6_7",
    "ISBN": "3-540-43760-6",
    "URL": "http://portal.acm.org/citation.cfm?id=711426",
    "abstract": "RDF and RDF Schema are two W3C standards aimed at enriching the Web with machine-processable semantic data.We have developed Sesame, an architecture for efficient storage and expressive querying of large quantities of metadata in RDF and RDF Schema. Sesame’s design and implementation are independent from any specific storage device. Thus, Sesame can be deployed on top of a variety of storage devices, such as relational databases, triple stores, or object-oriented databases, without having to change the query engine or other functional modules. Sesame offers support for concurrency control, independent export of RDF and RDFS information and a query engine for RQL, a query language for RDF that offers native support for RDF Schema semantics. We present an overview of Sesame as a generic architecture, as well as its implementation and our first experiences with this implementation.",
    "author": [
      {
        "family": "Broekstra",
        "given": "Jeen"
      },
      {
        "family": "Kampman",
        "given": "Arjohn"
      },
      {
        "dropping-particle": "van",
        "family": "Harmelen",
        "given": "Frank"
      }
    ],
    "container-title": "Proceedings of the first international semantic web conference on the semantic web (ISWC ’02)",
    "id": "Broekstra2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "rdf, semantic_web",
    "page": "54-68",
    "publisher": "Springer",
    "publisher-place": "London, UK",
    "title": "Sesame: A generic architecture for storing and querying RDF and RDF Schema",
    "title-short": "Sesame",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqv072",
    "ISSN": "2055-768X",
    "abstract": "Modernist authors such as Virginia Woolf and James Joyce greatly expanded the use of ’free indirect discourse’, a form of third-person narration that is strongly influenced by the language of a viewpoint character. Unlike traditional approaches to analyzing characterization using common words, such as those based on Burrows (1987), the nature of free indirect discourse and the sparseness of our data require that we understand the stylistic connotations of rarer words and expressions which cannot be gleaned directly from our target texts. To this end, we apply methods introduced in our recent work to derive information with regards to six stylistic aspects from a large corpus of texts from Project Gutenberg. We thus build high-coverage, finely grained lexicons that include common multiword collocations. Using this information along with student annotations of two modernist texts, Woolf’s To The Lighthouse and Joyce’s The Dead, we confirm that free indirect discourse does, at a stylistic level, reflect a mixture of narration and direct speech, and we investigate the extent to which social attributes of the various characters (in particular age, class, and gender) are reflected in their lexical stylistic profile.",
    "author": [
      {
        "family": "Brooke",
        "given": "Julian"
      },
      {
        "family": "Hammond",
        "given": "Adam"
      },
      {
        "family": "Hirst",
        "given": "Graeme"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Brooke2016",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities, literature",
    "page": "234-250",
    "publisher": "Oxford University Press",
    "title": "Using models of lexical style to quantify free indirect discourse in modernist fiction",
    "type": "article-journal",
    "volume": "32"
  },
  {
    "DOI": "10.1109/MC.1987.1663532",
    "author": [
      {
        "family": "Brooks",
        "given": "Frederick P."
      }
    ],
    "container-title": "Computer",
    "id": "Brooks1986",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1986,
          4
        ]
      ]
    },
    "keyword": "classic, software_engineering",
    "page": "10-19",
    "title": "No silver bullet: Essence and accidents of software engineering",
    "title-short": "No silver bullet",
    "type": "article-journal",
    "volume": "20"
  },
  {
    "author": [
      {
        "family": "Brooks",
        "given": "Frederick P.",
        "suffix": "Jr."
      }
    ],
    "edition": "2<sup>nd</sup>",
    "id": "Brooks1995",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "publisher": "Addison-Wesley",
    "publisher-place": "Harlow, UK",
    "title": "The mythical man-month",
    "type": "book"
  },
  {
    "DOI": "10.1002/(SICI)1097-4571(199806)49:8<731::AID-ASI7>3.0.CO;2-O",
    "abstract": "Orthography is the linguistic study of written language: Elements of text such as letters, punctuation marks, and spelling. Information retrieval systems operate in the orthographic realm matching some text strings (i.e., index entries) from documents with other text strings (i.e., query terms) from patrons. During the early history of information retrieval, it has been convenient to assume the rationality and uniformity of orthography in order to concentrate effort building information retrieval systems. Fundamental orthographic problems have persisted into modern information retrieval systems, however, where white-space normalization and the arbitrary treatment of punctuation have exacerbated the orthographic impediment to information retrieval.",
    "author": [
      {
        "family": "Brooks",
        "given": "Terrence A."
      }
    ],
    "container-title": "Journal of the American Society for Information Science",
    "id": "Brooks1998",
    "issue": "8",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "ir, orthography",
    "page": "731-741",
    "title": "Orthography as a fundamental impediment to online information retrieval",
    "type": "article-journal",
    "volume": "49"
  },
  {
    "DOI": "10.1017/s1472669610000289",
    "abstract": "Vanda Broughton describes the methodology of constructing a thesaurus from a faceted classification for law (Bliss Bibliographic Classification 2nd ed. Class S). The structure of the classification is described, and the way in which the thesaural relationships are derived from this is demonstrated. Recent work to automate the process, using the encoded classification data used for the schedule display and alphabetical index, is explained. Examples of the way in which this functions are included, and an analysis of the output shows where editorial work will help to improve the resulting thesaurus.",
    "author": [
      {
        "family": "Broughton",
        "given": "Vanda"
      }
    ],
    "container-title": "Legal Information Management",
    "id": "Broughton2010",
    "issue": "01",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "terminology",
    "page": "35-42",
    "title": "The use and construction of thesauri for legal documentation",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "URL": "http://www.ml1.org.uk/pdf/ml1-macro-processor.pdf",
    "author": [
      {
        "family": "Brown",
        "given": "Peter J."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Brown1967",
    "issue": "10",
    "issued": {
      "date-parts": [
        [
          1967,
          10
        ]
      ]
    },
    "page": "618-623",
    "title": "The ML/I macro processor",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "URL": "http://jstor.org/stable/30204934",
    "abstract": "This paper explores theoretical and practical aspects of intertextuality, in relation to the highly interpretative <intertextuality> tag within the SGML tagset developed by the Orlando Project for its history of women’s writing in the British Isles. Arguing that the concept of intertextuality is both crucial to and poses particular challenges to the creation of an encoding scheme for literary historical text, it outlines the ways in which the project’s tags address broader issues of intertextuality. The paper then describes the specific <intertextuality> tag in detail, and argues on the basis of provisional results drawn from the Orlando Project’s textbase that despite the impossibility of tracking intertextuality exhaustively or devising a tagset that completely disambiguates the concept, this tag provides useful pathways through the textbase and valuable departure points for further inquiry. Finally, the paper argues that the challenges to notions of rigour posed by the concept of intertextuality can help us fruitfully to examine some of the suppositions (gendered and other) that we bring to electronic text markup.",
    "author": [
      {
        "family": "Brown",
        "given": "Susan"
      },
      {
        "family": "Grundy",
        "given": "Isobel"
      },
      {
        "family": "Clements",
        "given": "Patricia"
      },
      {
        "family": "Elio",
        "given": "Renée"
      },
      {
        "family": "Balazs",
        "given": "Sharon"
      },
      {
        "family": "Cameron",
        "given": "Rebecca"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Brown2004",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "digital_edition, intertextuality",
    "page": "191-206",
    "publisher": "Springer",
    "title": "Intertextual encoding in the writing of women’s literary history",
    "type": "article-journal",
    "volume": "38"
  },
  {
    "DOI": "10.1145/1460007.1460013",
    "ISBN": "978-1-60558-253-5",
    "abstract": "In this paper, we explore the spatial distribution of the referents of ambiguous toponyms and compare it to the distribution of randomly selected unambiguous toponym pairs. We show that for a number of gazetteers, ambiguous toponyms are spatially autocorrelated and that typical autocorrelations are similar to the size of document scopes for a newspaper corpus.",
    "author": [
      {
        "family": "Brunner",
        "given": "Tobias J."
      },
      {
        "family": "Purves",
        "given": "Ross S."
      }
    ],
    "container-title": "GIR ’08: Proceeding of the 2nd international workshop on geographic information retrieval",
    "id": "Brunner2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "geocoding, toponym_resolution",
    "page": "25-26",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Spatial autocorrelation and toponym ambiguity",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Brunsmann",
        "given": "Jörg"
      },
      {
        "family": "Homrighausen",
        "given": "Andreas"
      },
      {
        "family": "Six",
        "given": "Hans-Werner"
      },
      {
        "family": "Voss",
        "given": "Josef"
      }
    ],
    "container-title": "Proc. 19<sup>th</sup> world conference on open learning and distance education, vienna, austria",
    "id": "Brunsmann1999a",
    "issued": {
      "date-parts": [
        [
          1999,
          6
        ]
      ]
    },
    "title": "Assignments in a virtual university – the WebAssign-system",
    "type": "paper-conference"
  },
  {
    "ISBN": "978-90-420-2831-9",
    "collection-title": "Poznań studies in the philosophy of the sciences and the humanities",
    "editor": [
      {
        "family": "Brzechczyn",
        "given": "Krzysztof"
      }
    ],
    "id": "Brzechczyn2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Rodopi",
    "publisher-place": "Amsterdam",
    "title": "Idealization XIII: Modeling in history",
    "title-short": "Idealization XIII",
    "type": "book",
    "volume": "97"
  },
  {
    "URL": "http://people.dbmi.columbia.edu/~ehs7001/Buchanan-Shortliffe-1984/MYCIN%20Book.htm",
    "abstract": "Artificial intelligence, or AI, is largely an experimental science—at least as much progress has been made by building and analyzing programs as by examining theoretical questions. MYCIN is one of several well-known programs that embody some intelligence and provide data on the extent to which intelligent behavior can be programmed. As with other AI programs, its development was slow and not always in a forward direction. But we feel we learned some useful lessons in the course of nearly a decade of work on MYCIN and related programs. In this book we share the results of many experiments performed in that time, and we try to paint a coherent picture of the work. The book is intended to be a critical analysis of several pieces of related research, performed by a large number of scientists. We believe that the whole field of AI will benefit from such attempts to take a detailed retrospective look at experiments, for in this way the scientific foundations of the field will gradually be defined. It is for all these reasons that we have prepared this analysis of the MYCIN experiments.",
    "editor": [
      {
        "family": "Buchanan",
        "given": "Bruce G."
      },
      {
        "family": "Shortliffe",
        "given": "Edward H."
      }
    ],
    "id": "Buchanan1984",
    "issued": {
      "date-parts": [
        [
          1984
        ]
      ]
    },
    "keyword": "ai, classic, uncertainty",
    "language": "en-US",
    "publisher": "Addison-Wesley",
    "publisher-place": "Reading, MA, USA",
    "title": "Rule-based expert systems: The MYCIN experiments of the Stanford Heuristic Programming Project",
    "title-short": "Rule-based expert systems",
    "type": "book"
  },
  {
    "DOI": "10.3166/jancl.21.35-60",
    "ISSN": "1166-3081",
    "abstract": "Justification logics are epistemic logics that explicitly include justifications for the agents’ knowledge. We develop a multi-agent justification logic with evidence terms for individual agents as well as for common knowledge. We define a Kripke-style semantics that is similar to Fitting’s semantics for the Logic of Proofs LP. We show the soundness, completeness, and finite model property of our multi-agent justification logic with respect to this Kripke-style semantics. We demonstrate that our logic is a conservative extension of Yavorskaya’s minimal bimodal explicit evidence logic, which is a two-agent version of LP. We discuss the relationship of our logic to the multi-agent modal logic S4 with common knowledge. Finally, we give a brief analysis of the coordinated attack problem in the newly developed language of our logic.",
    "author": [
      {
        "family": "Bucheli",
        "given": "Samuel"
      },
      {
        "family": "Kuznets",
        "given": "Roman"
      },
      {
        "family": "Studer",
        "given": "Thomas"
      }
    ],
    "container-title": "Journal of Applied Non-Classical Logics",
    "id": "Bucheli2012",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "logic, uncertainty",
    "language": "en-US",
    "page": "35-60",
    "title": "Justifications for common knowledge",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "URL": "https://letterpress.uchicago.edu/index.php/jdhcs/article/view/60",
    "abstract": "This paper describes both a syntactic and a semantic approach to the detection of citations. After introducing these two approaches, some results are presented that focus on ancient Greek texts and the adaptability of these approaches on two different literary genres. Furthermore, visualisations for ancient Greek philologists (micro view) as well as for historians (macro view) are provided to highlight the combination of text mining results with Visual Analytics.",
    "author": [
      {
        "family": "Büchler",
        "given": "Marco"
      },
      {
        "family": "Geßner",
        "given": "Annette"
      },
      {
        "family": "Eckart",
        "given": "Thomas"
      },
      {
        "family": "Heyer",
        "given": "Gerhard"
      }
    ],
    "container-title": "Journal of the Chicago Colloquium on Digital Humanities and Computer Science",
    "id": "Buechler2010",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_humanities, greek, plagiarism",
    "page": "1-17",
    "title": "Unsupervised detection and visualisation of textual reuse on Ancient Greek texts",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.1007/978-3-642-33290-6_11",
    "ISBN": "978-3-642-33289-0",
    "abstract": "High precision text re-use detection allows humanists to discover where and how particular authors are quoted (e.g., the different sections of Plato’s work that come in and out of vogue). This paper reports on on-going work to provide the high recall text re-use detection that humanists often demand. Using an edition of one Greek work that marked quotations and paraphrases from the Homeric epics as our testbed, we were able to achieve a recall of at least 94% while maintaining a precision of 73%. This particular study is part of a larger effort to detect text re-use across 15 million words of Greek and 10 million words of Latin available or under development as openly licensed TEI XML.",
    "author": [
      {
        "family": "Büchler",
        "given": "Marco"
      },
      {
        "family": "Crane",
        "given": "Gregory"
      },
      {
        "family": "Moritz",
        "given": "Maria"
      },
      {
        "family": "Babeu",
        "given": "Alison"
      }
    ],
    "chapter-number": "11",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Proceedings of the second international conference on theory and practice of digital libraries (TPDL’12)",
    "editor": [
      {
        "family": "Zaphiris",
        "given": "Panayiotis"
      },
      {
        "family": "Buchanan",
        "given": "George"
      },
      {
        "family": "Rasmussen",
        "given": "Edie"
      },
      {
        "family": "Loizides",
        "given": "Fernando"
      }
    ],
    "id": "Buechler2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "intertextuality, plagiarism",
    "page": "95-100",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Increasing recall for text re-use in historical documents to support research in the humanities",
    "type": "paper-conference",
    "volume": "7489"
  },
  {
    "URL": "http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-108515",
    "abstract": "Was ist Text Re-use? Text Re-use beschreibt die mit unterschiedlichen Absichten mündliche und schriftliche Wiedergabe von Textinhalten. Diese können im Sinne einer Definition das Anerkennen einer Autorität aber auch das Wiedergeben einer besonders interessanten Information sein. Während der Fokus dieser Arbeit auf dem Erstellen eines Hypertextes durch eine Text Re-use Analysis liegt, sind die PageRanking-Technik oder auch bibliometrische Analysen weiterführende Anwendungen. Im Kontext derartiger Einsatzmöglichkeiten kann auf historischen Dokumenten, die dieser Arbeit zugrunde liegen, durch eine automatische Analyse eine noch nie zuvor erstellte Breite von Zitierabhängigkeiten erstellt werden, welche heutzutage Aufschluss darüber geben, was in früheren Zeiten als wichtig erachtet worden ist, auch wenn es in der Gegenwart für Sprachen, wie dem Altgriechischen oder dem Latein, keine Muttersprachler mehr gibt.",
    "author": [
      {
        "family": "Büchler",
        "given": "Marco"
      }
    ],
    "genre": "PhD thesis",
    "id": "Buechler2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_humanities, intertextuality, plagiarism",
    "language": "de-DE",
    "publisher": "Universität Leipzig",
    "publisher-place": "Leipzig, Germany",
    "title": "Informationstechnische Aspekte des Historical Text Re-use",
    "type": "thesis"
  },
  {
    "DOI": "10.1007/978-3-319-12655-5_11",
    "abstract": "Text re-use describes the spoken and written repetition of information. Historical text re-use, with its longer time span, embraces a larger set of morphological, linguistic, syntactic, semantic and copying variations, thus adding a complication to text-reuse detection. Furthermore, it increases the chances of redundancy in a Digital Library. In Natural Language Processing it is crucial to remove these redundancies before applying any kind of machine learning techniques to the text. In Humanities, these redundancies foreground textual criticism and allow scholars to identify lines of transmission. This chapter investigates two aspects of the historical text re-use detection process, based on seven English editions of the Holy Bible. First, we measure the performance of several techniques. For this purpose, when considering a verse—such as book Genesis, Chapter 1, Verse 1—that is present in two editions, one verse is always understood as a paraphrase of the other. It is worth noting that paraphrasing is considered a hyponym of text re-use. Depending on the intention with which the new version was created, verses tend to differ significantly in the wording, but not in the meaning. Secondly, this chapter explains and evaluates a way of extracting paradigmatic relations. However, as regards historical languages, there is a lack of language resources (for example, WordNet) that makes non-literal text re-use and paraphrases much more difficult to identify. These differences are present in the form of replacements, corrections, varying writing styles, etc. For this reason, we introduce both the aforementioned and other correlated steps as a method to identify text re-use, including language acquisition to detect changes that we call paradigmatic relations. The chapter concludes with the recommendation to move from a ”single run” detection to an iterative process by using the acquired relations to run a new task.",
    "author": [
      {
        "family": "Büchler",
        "given": "Marco"
      },
      {
        "family": "Burns",
        "given": "Philip R."
      },
      {
        "family": "Müller",
        "given": "Martin"
      },
      {
        "family": "Franzini",
        "given": "Emily"
      },
      {
        "family": "Franzini",
        "given": "Greta"
      }
    ],
    "collection-title": "Theory and applications of natural language processing",
    "container-title": "Text mining",
    "editor": [
      {
        "family": "Biemann",
        "given": "Chris"
      },
      {
        "family": "Mehler",
        "given": "Alexander"
      }
    ],
    "id": "Buechler2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities, intertextuality",
    "page": "221-238",
    "publisher": "Springer International Publishing",
    "title": "Towards a historical text re-use detection",
    "type": "chapter"
  },
  {
    "DOI": "10.1061/(asce)1084-0680(2008)13:1(24)",
    "ISSN": "1084-0680",
    "abstract": "Structural engineering design is replete with uncertainties, some of which are obvious and some of which many engineers may never have considered. This paper is an examination of the uncertainties facing structural engineers and the ways that engineers have developed to handle those uncertainties. Uncertainty can be separated into two categories: Aleatory, related to luck or chance, and epistemic, related to knowledge. This breakdown has an impact on how we handle the various types of uncertainty. Uncertainty has a range of sources; we consider five broad sources: time, statistical limits, model limits, randomness, and human error. These five sources will be examined using examples from structural engineering, particularly with respect to the allowable stress design and LRFD code formats. Some uncertainties are explicitly dealt with in design codes, some are dealt with through quality control measures, and some are dealt with in implicit ways that we often do not think much about, e.g., heuristics. Design codes can deal with uncertainties caused by randomness, statistical limits, some aspects of time, and modeling. Other uncertainties such as human error must be dealt with using quality control methods, such as peer reviews and construction inspection.",
    "author": [
      {
        "family": "Bulleit",
        "given": "William M."
      }
    ],
    "container-title": "Practice Periodical on Structural Design and Construction",
    "id": "Bulleit2008",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2008,
          2
        ]
      ]
    },
    "keyword": "uncertainty",
    "page": "24-30",
    "title": "Uncertainty in structural engineering",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "ISBN": "9780262528863",
    "abstract": "Digital_Humanities is a compact, game-changing report on the state of contemporary knowledge production. Answering the question “What is digital humanities?,” it provides an in-depth examination of an emerging field. This collaboratively authored and visually compelling volume explores methodologies and techniques unfamiliar to traditional modes of humanistic inquiry—including geospatial analysis, data mining, corpus linguistics, visualization, and simulation—to show their relevance for contemporary culture. Written by five leading practitioner-theorists whose varied backgrounds embody the intellectual and creative diversity of the field, Digital_Humanities is a vision statement for the future, an invitation to engage, and a critical tool for understanding the shape of new scholarship.",
    "author": [
      {
        "family": "Burdick",
        "given": "Anne"
      },
      {
        "family": "Drucker",
        "given": "Johanna"
      },
      {
        "family": "Lunenfeld",
        "given": "Peter"
      },
      {
        "family": "Presner",
        "given": "Todd"
      },
      {
        "family": "Schnapp",
        "given": "Jeffrey"
      }
    ],
    "id": "Burdick2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Digital_Humanities",
    "type": "book"
  },
  {
    "ISBN": "0873529715",
    "abstract": "The long history of textual editing and scholarship has been intimately involved with the physique of the book, which set limits on the presentation and study of text. Increasingly, since the 1980s, the written word has taken on a digital form, and the shift from codex to computer, from print to electronic media, creates new opportunities–and new difficulties. This volume offers an emerging consensus about the fundamental issues of electronic textual editing. It provides practical advice and faces theoretical questions. Its twenty-four essays deal with markup coding and procedures, electronic archive administration, use of standards (such as Unicode), rights and permissions, and the changing and challenging environment of the Internet. Some of the specific texts discussed are Greek and Latin inscriptions, the Gospel of John, the Canterbury Tales, William Blake’s poems and art, Percy Bysshe Shelley’s The Devil’s Walk, Stijn Streuvels’s De teleurgang van den Waterhoek, Ludwig Wittgenstein’s Nachlass, and the papers of Thomas Edison. The guidelines of the MLA’s Committee on Scholarly Editions, recently revised to address electronic editions, are included in full.",
    "editor": [
      {
        "family": "Burnard",
        "given": "Lou"
      },
      {
        "family": "O’Brien O’Keeffe",
        "given": "Katherine"
      },
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "id": "Burnard2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_edition, digital_humanities, tei",
    "publisher": "Modern Language Association of America",
    "publisher-place": "New York, NY, USA",
    "title": "Electronic textual editing",
    "type": "book"
  },
  {
    "URL": "http://www.ets.org/research/dload/iaai03bursteinj.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Burstein",
        "given": "Jill"
      },
      {
        "family": "Chodorow",
        "given": "Martin"
      },
      {
        "family": "Leacock",
        "given": "Claudia"
      }
    ],
    "container-title": "Proceedings of the fifteenth annual conference on innovative applications of artificial intelligence",
    "id": "Burstein2003",
    "issued": {
      "date-parts": [
        [
          2003,
          8
        ]
      ]
    },
    "publisher-place": "Acapulco, Mexico",
    "title": "Criterion: Online essay evaluation: An application for automated evaluation of student essays",
    "title-short": "Criterion",
    "type": "paper-conference"
  },
  {
    "URL": "https://www.calico.org/a-411-Towards\\%20Better\\%20Tutorial\\%20CALL\\%20A\\%20Matter\\%20of\\%20Intelligent\\%20Control.html",
    "abstract": "Although drill and practice exercises continue to account for a substantial proportion of CALL production, most available foreign language programs suffer from expedient programming and poor pedagogy. At the heart of the problem is the failure of language teaching specialists to involve themselves in CALL research and development. In the absence of AI-derived natural language processors, programs based on anticipated student responses remain the only viable means of creating sophisticated CALL lessons. Special purpose authoring languages offer the most accessible approach to such programming, but are seriously hampered by their lack of flexible feedback capabilities. In particular, much greater author control is needed over pattern matching devices to accommodate the processing of non-differential variations in predicted answers. Provided that basic command structures incorporate variable parameters with pre-set, but alterable, default values, increased author management of response handling need not place unreasonable programming den7ands upon the courseware designer. In any event, variable lesson parameters are required to enable students to n2ore intelligently exploit the computer-aided learning environment. To demonstrate the feasibility of implementing flexible response handling control, the essential design features of an Australian developed authoring language, EMU, are examined in some detail.",
    "author": [
      {
        "family": "Burston",
        "given": "Jack"
      }
    ],
    "container-title": "CALICO Journal",
    "id": "Burston1989",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1989,
          6
        ]
      ]
    },
    "keyword": "call, e-learning, plato_system",
    "page": "75-89",
    "title": "Towards better tutorial CALL: A matter of intelligent control",
    "title-short": "Towards better tutorial CALL",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "URL": "https://jstor.org/stable/30207304",
    "author": [
      {
        "family": "Busa",
        "given": "Roberto"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Busa1980",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "keyword": "classic, digital_humanities, latin",
    "language": "en-US",
    "page": "83-90",
    "title": "The annals of humanities computing: The Index Thomisticus",
    "title-short": "The annals of humanities computing",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "DOI": "10.1002/9780470999875.fmatter",
    "author": [
      {
        "family": "Busa",
        "given": "Roberto A."
      }
    ],
    "container-title": "A companion to digital humanities",
    "editor": [
      {
        "family": "Schreibman",
        "given": "Susan"
      },
      {
        "family": "Siemens",
        "given": "Ray"
      },
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "id": "Busa2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "language": "en-US",
    "page": "xvi-xxi",
    "publisher": "Blackwell",
    "title": "Foreword: Perspectives on the digital humanities",
    "title-short": "Foreword",
    "type": "chapter"
  },
  {
    "URL": "http://caacentre.lboro.ac.uk/dldocs/BUSHMARK.PDF",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Bush",
        "given": "Martin"
      }
    ],
    "container-title": "Proceedings of the 7<sup>th</sup> annual conference on the teaching of computing",
    "id": "Bush1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "publisher-place": "Belfast",
    "title": "Alternative marking schemes for online multiple-choice tests",
    "type": "paper-conference"
  },
  {
    "URL": "https://eduforge.org/docman/view.php/176/1111/",
    "abstract": "As part of the Tertiary Education Commission’s e-Learning Collaboration Development Fund, Massey University, the Auckland University of Technology, the Open Polytechnic of New Zealand and Victoria University of Wellington have been contracted to develop an electronic portfolio application for the New Zealand tertiary sector. To provide a context for the development of the electronic portfolio application and guidelines for its use, a review of the literature on electronic portfolios and more traditional paper-based portfolios has been conducted.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Butler",
        "given": "Philippa"
      }
    ],
    "genre": "Project report",
    "id": "Butler2006",
    "issued": {
      "date-parts": [
        [
          2006,
          10
        ]
      ]
    },
    "keyword": "e-learning, e-portfolios",
    "publisher": "eCDF ePortfolio Project",
    "title": "A review of the literature on portfolios and electronic portfolios",
    "type": "report"
  },
  {
    "DOI": "10.1109/icsc.2007.107",
    "ISBN": "978-0-7695-2997-4",
    "abstract": "This paper describes work on Named Entity Recognition (NER), in preparation for Relation Extraction (RE), on data from a historical archive organisation. As is often the case in the cultural heritage domain, the source text includes a high percentage of specialist terminology, and is of very variable quality in terms of grammaticality and completeness. The NER and RE tasks were carried out using a specially annotated corpus, and are themselves preliminary steps in a larger project whose aim is to transform discovered relations into a graph structure that can be queried using standard tools. Experimental results from the NER task are described, with emphasis on dealing with nested entities using a multi-word token method. The overall objective is to improve access by non-specialist users to a valuable cultural resource.",
    "author": [
      {
        "family": "Byrne",
        "given": "Kate"
      }
    ],
    "container-title": "International conference on semantic computing, 2007 (ICSC 2007)",
    "id": "Byrne2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage, proper_names",
    "page": "589-596",
    "publisher": "IEEE",
    "publisher-place": "New York, NY, USA",
    "title": "Nested named entity recognition in historical archive text",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Bösch",
        "given": "Frank"
      }
    ],
    "container-title": "NZZ Geschichte",
    "id": "Bösch2019",
    "issue": "24",
    "issued": {
      "date-parts": [
        [
          2019,
          10
        ]
      ]
    },
    "language": "de-DE",
    "page": "26-42",
    "title": "Computer unter uns",
    "type": "article-journal"
  },
  {
    "URL": "http://cidoc-crm.org/sites/default/files/cidoc_crm_version_5.0.4.pdf",
    "editor": [
      {
        "family": "Crofts",
        "given": "Nick"
      },
      {
        "family": "Doerr",
        "given": "Martin"
      },
      {
        "family": "Gill",
        "given": "Tony"
      },
      {
        "family": "Stead",
        "given": "Stephen"
      },
      {
        "family": "Stiff",
        "given": "Matthew"
      }
    ],
    "id": "CIDOC-CRM504",
    "issued": {
      "date-parts": [
        [
          2011,
          11
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "number": "Version 5.0.4",
    "publisher": "The International Committee for Documentation of the International Council of Museums (ICOM–CIDOC)",
    "publisher-place": "Paris",
    "title": "Definition of the CIDOC Conceptual Reference Model",
    "type": "report"
  },
  {
    "URL": "http://www.clomedia.com/newsletters/2007/May/1814/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Anonymous"
      }
    ],
    "container-title": "Chief Learning Officer magazine",
    "id": "CLO2007",
    "issued": {
      "date-parts": [
        [
          2007,
          5
        ]
      ]
    },
    "title": "Europe’s e-learning industry group revamped, renamed",
    "type": "article-journal"
  },
  {
    "URL": "http://www.w3.org/TR/CSS21/",
    "author": [
      {
        "family": "Bos",
        "given": "Bert"
      },
      {
        "family": "elik",
        "given": "Tantek"
      },
      {
        "family": "Hickson",
        "given": "Ian"
      },
      {
        "family": "Lie",
        "given": "Håkon Wium"
      }
    ],
    "genre": "{W3C Candidate Recommendation}",
    "id": "CSS",
    "issued": {
      "date-parts": [
        [
          2007,
          7
        ]
      ]
    },
    "publisher": "World Wide Web Consortium",
    "title": "Cascading Style Sheets Level 2 Revision 1 (CSS 2.1) Specification",
    "type": "report"
  },
  {
    "DOI": "10.1001/jama.2017.7797",
    "ISSN": "0098-7484",
    "abstract": "Over the past decade, machine learning techniques have made substantial advances in many domains. In health care, global interest in the potential of machine learning has increased; for example, a deep learning algorithm has shown high accuracy in detecting diabetic retinopathy. There have been suggestions that machine learning will drive changes in health care within a few years, specifically in medical disciplines that require more accurate prognostic models (eg, oncology) and those based on pattern recognition (eg, radiology and pathology). However, comparative studies on the effectiveness of machine learning–based decision support systems (ML-DSS) in medicine are lacking, especially regarding the effects on health outcomes. Moreover, the introduction of new technologies in health care has not always been straightforward or without unintended and adverse effects. In this Viewpoint we consider the potential unintended consequences that may result from the application of ML-DSS in clinical practice.",
    "author": [
      {
        "family": "Cabitza",
        "given": "Federico"
      },
      {
        "family": "Rasoini",
        "given": "Raffaele"
      },
      {
        "family": "Gensini",
        "given": "Gian F."
      }
    ],
    "container-title": "JAMA",
    "id": "Cabitza2017",
    "issued": {
      "date-parts": [
        [
          2017,
          7,
          20
        ]
      ]
    },
    "keyword": "formal_models, machine_learning",
    "language": "en-US",
    "title": "Unintended consequences of machine learning in medicine",
    "type": "article-journal"
  },
  {
    "DOI": "10.1007/978-3-642-25456-7",
    "ISBN": "978-3-642-25455-0",
    "abstract": "Knowledge Representation plays an essential role in Semantic Web, in particular in automated information processing and communications among software agents. This book, entitled \"Fuzzy Computational Ontologies in Contexts: Formal Models of Knowledge Representation with Membership Degree and Typicality, and Their Applications\", discusses knowledge representation in Semantic Web. It introduces the relevant background knowledge, models of fuzzy ontologies, importance and priority of properties in concepts, and object typicality in fuzzy ontologies and context-aware ontologies.",
    "author": [
      {
        "family": "Cai",
        "given": "Yi"
      },
      {
        "family": "Au Yeung",
        "given": "Ching-man"
      },
      {
        "family": "Leung",
        "given": "Ho-fung"
      }
    ],
    "id": "Cai2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Fuzzy computational ontologies in contexts",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-642-25456-7_4",
    "abstract": "The classical view in cognitive psychology holds that an object is either an instance of a concept or it is not. In terms of mathematics, every concept is a crisp set. However, as we have discussed above, many concepts do not have clear boundaries or definitions. Different objects have different degrees of membership or typicality with respect to a certain concept. In this section, we give a review of studies that investigate how graded membership, vagueness and uncertainty are modeled. Several extensions to existing ontology models or description logics involves fuzzy sets, therefore we will start by briefly reviewing the basic notions of fuzzy set theory.",
    "author": [
      {
        "family": "Cai",
        "given": "Yi"
      },
      {
        "family": "Au Yeung",
        "given": "Ching-man"
      },
      {
        "family": "Leung",
        "given": "Ho-fung"
      }
    ],
    "chapter-number": "4",
    "container-title": "Fuzzy computational ontologies in contexts",
    "id": "Cai2012-Ch4",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "knowledge_representation, rdf, uncertainty",
    "language": "en-US",
    "page": "37-47",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Modeling uncertainty in knowledge representation",
    "type": "chapter"
  },
  {
    "URL": "http://www.digitalhumanities.org/dhq/vol/14/1/000450/000450.html",
    "abstract": "In this paper we run a topic model on over 300 article-length pieces from the extended bibliography of Melissa Terras, Juliette Nyhan, and Edward Vanhoutte’s edited collection Defining Digital Humanities. We use this topic model as a way to think through entry into the digital humanities as a negotiation between warm invitation and gatekeeping, the “pull” and “push” of digital humanities. We then analyze the metadata we collected about these pieces to explore how the push and pull manifest themselves unevenly across different demographics.",
    "author": [
      {
        "family": "Callaway",
        "given": "Elizabeth"
      },
      {
        "family": "Turner",
        "given": "Jeffrey"
      },
      {
        "family": "Stone",
        "given": "Heather"
      },
      {
        "family": "Halstrom",
        "given": "Adam"
      }
    ],
    "container-title": "Digital Humanities Quarterly",
    "id": "Callaway2020",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "title": "The push and pull of digital humanities: Topic modeling the “what is digital humanities?” genre",
    "title-short": "The push and pull of digital humanities",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "DOI": "10.1145/1809980.1810064",
    "ISBN": "978-85-7669-199-0",
    "abstract": "Dictionaries are important sources of study and knowledge about languages. Historical Dictionaries allow to study language evolution, and also to support historical documents understanding. This study presents a dictionary writing system called Procorph, developed to support the creation of Historical Portuguese dictionaries entries. This tool also can be adapted to work with contemporaries dictionaries. The system stores several information about the entries, such as definitions, text samples, spelling variations, sub-entries, and others.",
    "author": [
      {
        "family": "Candido Jr.",
        "given": "Arnaldo"
      },
      {
        "family": "Aluísio",
        "given": "Sandra Maria"
      }
    ],
    "container-title": "Companion Proceedings of the XIV Brazilian Symposium on Multimedia and the Web (WebMedia ’08)",
    "id": "Candido2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, portuguese",
    "language": "portugese",
    "page": "347-352",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Procorph: um sistema de apoio à criação de dicionários históricos",
    "title-short": "Procorph",
    "type": "paper-conference"
  },
  {
    "URL": "http://atala.org/IMG/pdf/TAL-2009-50-2-03-Candido.pdf",
    "abstract": "Historical corpora are important resources for different areas. Philology, Human Language Technology, Literary Studies, History, and Lexicography are some that benefit from them. However, compiling historical corpora is different from compiling contemporary corpora. Corpus designers have to deal with several characteristics inherent in historical texts, such as: absence of a spelling standard, pervasive use of abbreviations plus their spelling variations, lack of space between words, irregular use of hyphenation, non-standard typographical symbols. This paper addresses the challenges posed in processing the corpus designed for the Historical Dictionary of Brazilian Portuguese (HDBP) project, which is composed of texts from the sixteenth through the beginning of the nineteenth century, and the solutions found to support the compilation of a Historical Portuguese dictionary based on this corpus.",
    "author": [
      {
        "family": "Candido Jr.",
        "given": "Arnaldo"
      },
      {
        "family": "Aluísio",
        "given": "Sandra Maria"
      }
    ],
    "container-title": "Traitement Automatique des Langues",
    "id": "Candido2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, portugese, corpus_linguistics",
    "language": "en-US",
    "page": "73-102",
    "title": "Building a corpus-based historical Portuguese dictionary: Challenges and opportunities",
    "title-short": "Building a corpus-based historical Portuguese dictionary",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "URL": "http://www.ifi.uzh.ch/teaching/studiengaenge/allg_infos/diplomarbeiten/abstracts_2007/cantaluppi_viviane/",
    "abstract": "This paper analyses the impact of online assessment on the learning performance of students. A positive correlation between exercise and exam performance is shown. Further investigations state the non existing influence of endless repetition of online assessments to learning performance. An influence of question-pools is shown. They are included into online assessments, for presenting different questions to each student, for them not to simply copy their answers. With question-pools, students show lower performance during exercises but an improved performance during exams can be observed. Students show higher achievements with online assessments than traditional written ones. A trend can be observed for students becoming “lazy” towards the end of the course, when it comes to the preparation of the exercises.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Cantaluppi",
        "given": "Viviane"
      }
    ],
    "genre": "Diplomarbeit",
    "id": "Cantaluppi2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "publisher": "Universität Zürich",
    "title": "Analyse des Lernerfolges bei Online-Übungen",
    "type": "thesis"
  },
  {
    "DOI": "10.1007/s10796-006-9014-3",
    "abstract": "The dissemination of knowledge is currently being improved by e-learning, which consists of a combination of teaching methodologies and computer-based tools. Recently e-learning environments have started to exploit web technology to provide a simple, flexible, distributed and open platform. In this paper we propose a model for an e-learning system, aiming at sharing both course contents and teaching materials, in order to provide students with a single and uniform set of concepts to be learned, and promoting active learning by allowing the construction of courses which are personalized in terms of both contents and teaching materials, selected according to each student’s needs and capabilities. A first, open-source prototype based on the proposed model has been implemented to validate the model.",
    "author": [
      {
        "family": "Carchiolo",
        "given": "Vincenza"
      },
      {
        "family": "Longheu",
        "given": "Alessandro"
      },
      {
        "family": "Malgeri",
        "given": "Michele"
      },
      {
        "family": "Mangioni",
        "given": "Giuseppe"
      }
    ],
    "container-title": "Information Systems Frontiers",
    "id": "Carchiolo2007",
    "issue": "2–3",
    "issued": {
      "date-parts": [
        [
          2007,
          7
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "267-282",
    "publisher": "Springer",
    "title": "A model for a web-based learning system",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "author": [
      {
        "family": "Carnap",
        "given": "Rudolf"
      }
    ],
    "id": "Carnap1950",
    "issued": {
      "date-parts": [
        [
          1950
        ]
      ]
    },
    "keyword": "philosophy_of_science, uncertainty",
    "language": "en-US",
    "publisher": "University of Chicago Press",
    "publisher-place": "Chicago, IL, USA",
    "title": "Logical foundations of probability",
    "type": "book"
  },
  {
    "URL": "http://www.cogs.susx.ac.uk/lab/nlp/carroll/papers/lre98.pdf",
    "accessed": {
      "date-parts": [
        [
          2009,
          1,
          25
        ]
      ]
    },
    "author": [
      {
        "family": "Carroll",
        "given": "John"
      },
      {
        "family": "Briscoe",
        "given": "Ted"
      },
      {
        "family": "Sanfilippo",
        "given": "Antonio"
      }
    ],
    "container-title": "Proceedings of the 1st international conference on language resources and evaluation",
    "id": "Carroll1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "page": "447-454",
    "publisher-place": "Granada, Spain",
    "title": "Parser evaluation: A survey and a new proposal",
    "title-short": "Parser evaluation",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/11431053_8",
    "ISBN": "978-3-540-26124-7",
    "abstract": "RDF uses the RFC3066 standard for language tags for literals in natural languages. The revision RFC3066bis includes productive use of language, country and script codes. These form an implicit ontology of natural languages for marking-up texts. Relating each language tag with classes of appropriately tagged literals allows this implicit ontology to be made explicit as an ontology in OWL in which every class in the ontology is a datarange. The treatment extends to XML Literals, which may have multiple embedded language tags. Further features of RFC3066bis such as the relationship with deprecated codes, language ranges and language tag fallback can be expressed in OWL. A small change to the RDF model theory is suggested to permit access to the language tag in the formal semantics, giving this ontology a precise formal meaning. Illustrative use cases refer to use of English, Japanese, Chinese and Klingon texts.",
    "author": [
      {
        "family": "Carroll",
        "given": "Jeremy J."
      },
      {
        "family": "Phillips",
        "given": "Addison"
      }
    ],
    "chapter-number": "8",
    "collection-title": "Lecture notes in computer science",
    "container-title": "The semantic web: Research and applications, 6th european semantic web conference, ESWC 2009, heraklion, crete, greece, may 31-june 4, 2009, proceedings",
    "editor": [
      {
        "family": "Gómez-Pérez",
        "given": "Asunción"
      },
      {
        "family": "Euzenat",
        "given": "Jérôme"
      }
    ],
    "id": "Carroll2005a",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "document_engineering, rdf",
    "language": "en-US",
    "page": "15-19",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Multilingual RDF and OWL",
    "type": "chapter",
    "volume": "3532"
  },
  {
    "DOI": "10.1145/1060745.1060835",
    "ISBN": "1-59593-046-9",
    "abstract": "The Semantic Web consists of many RDF graphs nameable by URIs. This paper extends the syntax and semantics of RDF to cover such Named Graphs. This enables RDF statements that describe graphs, which is beneficial in many Semantic Web application areas. As a case study, we explore the application area of Semantic Web publishing: Named Graphs allow publishers to communicate assertional intent, and to sign their graphs; information consumers can evaluate specific graphs using task-specific trust policies, and act on information from those Named Graphs that they accept. Graphs are trusted depending on: their content; information about the graph; and the task the user is performing. The extension of RDF to Named Graphs provides a formally defined framework to be a foundation for the Semantic Web trust layer.",
    "author": [
      {
        "family": "Carroll",
        "given": "Jeremy J."
      },
      {
        "family": "Bizer",
        "given": "Christian"
      },
      {
        "family": "Hayes",
        "given": "Pat"
      },
      {
        "family": "Stickler",
        "given": "Patrick"
      }
    ],
    "container-title": "Proceedings of the 14<sup>th</sup> international conference on world wide web (WWW ’05)",
    "id": "Carroll2005b",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "rdf",
    "language": "en-US",
    "page": "613-622",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Named graphs, provenance and trust",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-8274-2224-8_1",
    "abstract": "Die Computerlinguistik ist das Fachgebiet, das sich mit der maschinellen Verarbeitung natürlicher Sprache beschäftigt. Sie ist im Überschneidungsbereich von Informatik und Linguistik angesiedelt, aber die Wurzeln der Computerlinguistik reichen bis in die fünfziger Jahre zurück. In diesem halben Jahrhundert seit ihrem Entstehen hat sie sich mittlerweile national und international erfolgreich etabliert, so dass auf dem Wissen aus der Informatik und der Linguistik aufbauend neue und eigenständige Methoden für die maschinelle Verarbeitung gesprochener und geschriebener Sprache entwickelt wurden.",
    "author": [
      {
        "family": "Carstensen",
        "given": "Kai-Uwe"
      },
      {
        "family": "Jekat",
        "given": "Susanne J."
      },
      {
        "family": "Klabunde",
        "given": "Ralf"
      }
    ],
    "container-title": "Computerlinguistik und Sprachtechnologie",
    "editor": [
      {
        "family": "Carstensen",
        "given": "Kai-Uwe"
      },
      {
        "family": "Ebert",
        "given": "Christian"
      },
      {
        "family": "Ebert",
        "given": "Cornelia"
      },
      {
        "family": "Jekat",
        "given": "Susanne J."
      },
      {
        "family": "Klabunde",
        "given": "Ralf"
      },
      {
        "family": "Langer",
        "given": "Hagen"
      }
    ],
    "id": "Carstensen2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "computational_linguistics, nlp",
    "language": "de-DE",
    "page": "1-25",
    "publisher": "Spektrum",
    "title": "Computerlinguistik – Was ist das?",
    "type": "chapter"
  },
  {
    "DOI": "10.1016/j.ijar.2017.08.011",
    "ISSN": "0888613X",
    "abstract": "Recent years have witnessed an increasingly mature body of research on the Semantic Web (SW), with new standards being developed and more complex problems being addressed. As complexity increases in SW applications, so does the need to cope with uncertainty. Several approaches to uncertainty representation and reasoning in the SW have emerged. Among these is Probabilistic Web Ontology Language (PR-OWL), which provides a means of representing uncertainty in ontologies expressed in Web Ontology Language (OWL). PR-OWL allows values of random variables to range over OWL datatypes, following an approach suggested by Poole et al. to formalizing the association between random variables from probabilistic theories with the individuals, classes and properties from ontological languages such as OWL.",
    "author": [
      {
        "family": "Carvalho",
        "given": "Rommel N."
      },
      {
        "family": "Laskey",
        "given": "Kathryn B."
      },
      {
        "family": "Costa",
        "given": "Paulo C. G."
      }
    ],
    "container-title": "International Journal of Approximate Reasoning",
    "id": "Carvalho2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "56-79",
    "title": "PR-OWL – a language for defining probabilistic ontologies",
    "type": "article-journal",
    "volume": "91"
  },
  {
    "author": [
      {
        "family": "Carver",
        "given": "Richard"
      }
    ],
    "container-title": "Electronic proceedings for FIE 1996 conference",
    "id": "Carver1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "publisher": "IEEE",
    "title": "Computer-Assisted instruction for a first course in computer science",
    "type": "paper-conference"
  },
  {
    "URL": "https://www.chronicle.com/article/The-Job-Market-Moment-of/238944",
    "accessed": {
      "date-parts": [
        [
          2020,
          2,
          28
        ]
      ]
    },
    "author": [
      {
        "family": "Cassuto",
        "given": "Leonard"
      }
    ],
    "container-title": "The Chronicle of Higher Education",
    "id": "Cassuto2017",
    "issued": {
      "date-parts": [
        [
          2017,
          1,
          22
        ]
      ]
    },
    "title": "The job-market moment of digital humanities",
    "type": "webpage"
  },
  {
    "URL": "https://eduforge.org/docman/view.php/7/18/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "Catalyst IT Ltd."
      }
    ],
    "id": "Catalyst2004",
    "issued": {
      "date-parts": [
        [
          2004,
          5
        ]
      ]
    },
    "publisher": "The Open Polytechnic of New Zealand",
    "publisher-place": "Wellington, New Zealand",
    "title": "Technical evaluation of selected learning management systems",
    "type": "report"
  },
  {
    "DOI": "10.1162/109966201753537123",
    "ISSN": "1099-6621",
    "abstract": "Every text realizes a communicative act, but most contemporary text encoding ignores this. Betraying its unwitting subjection to a persuasive formalist ideology, current descriptive markup offers a static, document-oriented view of texts that occludes their temporal and performative nature.",
    "author": [
      {
        "family": "Caton",
        "given": "Paul"
      }
    ],
    "container-title": "Markup Languages: Theory and Practice",
    "id": "Caton2001",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "document_research, markup",
    "language": "en-US",
    "page": "1-13",
    "title": "Markup’s current imbalance",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "ISBN": "978-1-58603-975-2",
    "URL": "http://www.cavar.me/damir/CroMo/",
    "abstract": "CroMo is a finite state transducer that combines three major functionalities into one high-performance monolithic machine for morphological segmentation, annotation, and lemmatization. It is designed to be flexible, extensible, and applicable to any language that allows for purely morphotactic modeling on the lexical level of morphological structure. While it minimizes the development cycle of the morphology, its annotation schema maximizes interoperability by using a direct mapping from the GOLD ontology of linguistic concepts and features. The use of standardized ontology based annototains provides advanced possibilities for a DL-based post-processing of the annotated output.",
    "author": [
      {
        "family": "Ćavar",
        "given": "Damir"
      },
      {
        "family": "Jazbec",
        "given": "Ivo P."
      },
      {
        "family": "Stojanov",
        "given": "Tomislav"
      }
    ],
    "container-title": "Proceeding of the 2009 conference on finite-state methods and natural language processing: Post-proceedings of the 7th international workshop FSMNLP 2008",
    "id": "Cavar2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "croatian, cultural_heritage, morphology",
    "language": "en-US",
    "page": "183-190",
    "publisher": "IOS Press",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "CroMo—morphological analysis for standard Croatian and its synchronic and diachronic dialects and variants",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1353/esc.2010.0031",
    "ISSN": "1913-4835",
    "abstract": "One of the constants of McLuhan criticism and biography is that his training as a scholar of Renaissance English literature and its debt to the rhetorical tradition, coupled with his lifelong interest in English literary modernism, especially Joyce and Eliot, morphed into his foundational theories of mediation. McLuhan’s literary studies were undoubtedly crucial to his theories of how media shape messages. But what was to be the afterlife of these insights for the study of literature and for the humanities at large? What was to be their fate in the postliterate era, an era into which we have fully entered with Wikipedia, Google, the digital library, and e-books?",
    "author": [
      {
        "family": "Cavell",
        "given": "Richard"
      }
    ],
    "container-title": "ESC: English Studies in Canada",
    "id": "Cavell2010",
    "issue": "2–3",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "digital_humanities, media_studies",
    "language": "en-US",
    "page": "14-18",
    "title": "McLuhan and the humanities",
    "type": "article-journal",
    "volume": "36"
  },
  {
    "abstract": "Text categorization is a fundamental task in document processing, allowing the automated handling of enormous streams of documents in electronic form. One difficulty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through OCR. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems. We describe here an N-gram-based approach to text categorization that is tolerant of textual errors. The system is small, fast and robust. This system worked very well for language classification, achieving in one test a 99.8% correct classification rate on Usenet newsgroup articles written in different languages. The system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80% correct classification rate. There are also several obvious directions for improving the system’s classification performance in those cases where it did not do as well. The system is based on calculating and comparing profiles of N-gram frequencies. First, we use the system to compute profiles on training set data that represent the various categories, e.g.,language samples or newsgroup content samples. Then the system computes a profile for a particular document that is to be classified. Finally, the system computes a distance measure between the document’s profile and each of the category profiles. The system selects the category whose profile has the smallest distance to the document’s profile. The profiles involved are quite small, typically 10K bytes for a category training set, and less than 4K bytes for an individual document. Using N-gram frequency profiles provides a simple and reliable way to categorize documents in a wide range of classification tasks.",
    "author": [
      {
        "family": "Cavnar",
        "given": "William B."
      },
      {
        "family": "Trenkle",
        "given": "John M."
      }
    ],
    "container-title": "Proceedings of SDAIR-94,  annual symposium on document analysis and information retrieval",
    "id": "Cavnar1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "language": "en-US",
    "page": "161-175",
    "title": "N-gram-based text categorization",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1529282.1529669",
    "ISBN": "978-1-60558-166-8",
    "abstract": "We consider the following spelling variants clustering problem: Given a list of distinct words, called lexicon, compute (possibly overlapping) clusters of words which are spelling variants of each other. This problem naturally arises in the context of error-tolerant full-text search of the following kind: For a given query, return not only documents matching the query words exactly but also those matching their spelling variants. This is the inverse of the well-known \"Did you mean: … ?\" web search engine feature, where the error tolerance is on the side of the query, and not on the side of the documents. We combine various ideas from the large body of literature on approximate string searching and spelling correction techniques to a new algorithm for the spelling variants clustering problem that is both accurate and very efficient in time and space. Our largest lexicon, containing roughly 10 million words, can be processed in about 16 minutes on a standard PC using 10 MB of additional space. This beats the previously best scheme by a factor of two in running time and by a factor of more than ten in space usage. We have integrated our algorithms into the CompleteSearch engine in a way that achieves error-tolerant search without significant blowup in neither index size nor query processing time.",
    "author": [
      {
        "family": "Celikik",
        "given": "Marjan"
      },
      {
        "family": "Bast",
        "given": "Holger"
      }
    ],
    "collection-title": "SAC ’09",
    "container-title": "Proceedings of the 2009 ACM symposium on applied computing",
    "id": "Celikik2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "ir, spelling_correction, spelling_normalization",
    "language": "en-US",
    "page": "1724-1731",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Fast error-tolerant search on very large texts",
    "type": "paper-conference"
  },
  {
    "ISBN": "978-1-932432-46-6",
    "URL": "http://aclweb.org/anthology/P09-1120",
    "abstract": "We consider the language identification problem for search engine queries. First, we propose a method to automatically generate a data set, which uses click-through logs of the Yahoo! Search Engine to derive the language of a query indirectly from the language of the documents clicked by the users. Next, we use this data set to train two decision tree classifiers; one that only uses linguistic features and is aimed for textual language identification, and one that additionally uses a non-linguistic feature, and is geared towards the identification of the language intended by the users of the search engine. Our results show that our method produces a highly reliable data set very efficiently, and our decision tree classifier outperforms some of the best methods that have been proposed for the task of written language identification on the domain of search engine queries.",
    "author": [
      {
        "family": "Ceylan",
        "given": "Hakan"
      },
      {
        "family": "Kim",
        "given": "Yookyung"
      }
    ],
    "container-title": "ACL-IJCNLP ’09: Proceedings of the joint conference of the 47th annual meeting of the ACL and the 4th international joint conference on natural language processing of the AFNLP",
    "id": "Ceylan2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "ir, language_identification, nlp",
    "page": "1066-1074",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Morristown, NJ, USA",
    "title": "Language identification of search engine queries",
    "type": "paper-conference",
    "volume": "2"
  },
  {
    "ISBN": "9782881240126",
    "author": [
      {
        "family": "Chandler",
        "given": "William J."
      }
    ],
    "collection-number": "7",
    "collection-title": "Studies in cybernetics",
    "id": "Chandler1984",
    "issued": {
      "date-parts": [
        [
          1984
        ]
      ]
    },
    "keyword": "cybernetics, formal_models, history",
    "language": "en-US",
    "publisher": "Gordon and Breach Science",
    "title": "The science of history: A cybernetic approach",
    "title-short": "The science of history",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-1-4757-1388-6",
    "ISBN": "978-1-4757-1390-9",
    "abstract": "Two central ideas in the movement toward advanced automation systems are the office-of-the-future (or office automation system), and the factory­ of-the-future (or factory automation system). An office automation system is an integrated system with diversified office equipment, communication devices, intelligent terminals, intelligent copiers, etc., for providing information management and control in a dis­ tributed office environment. A factory automation system is also an inte­ grated system with programmable machine tools, robots, and other pro­ cess equipment such as new \"peripherals,\" for providing manufacturing information management and control. Such advanced automation systems can be regarded as the response to the demand for greater variety, greater flexibility, customized designs, rapid response, and \"Just-in-time\" delivery of office services or manufac­tured goods. The economy of scope, which allows the production of a vari­ ety of similar products in random order, gradually replaces the economy of scale derived from overall volume of operations. In other words, we are gradually switching from the production of large volumes of standard products to systems for the production of a wide variety of similar products in small batches. This is the phenomenon of \"demassification\" of the marketplace, as described by Alvin Toffler in The Third Wave.",
    "editor": [
      {
        "family": "Chang",
        "given": "Shi-Kuo"
      }
    ],
    "id": "Chang1985",
    "issued": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "keyword": "programming",
    "language": "en-US",
    "publisher": "Plenum",
    "publisher-place": "New York, NY, USA",
    "title": "Languages for automation",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Chanod",
        "given": "Jean-Pierre"
      }
    ],
    "collection-title": "Text, speech and language technology",
    "container-title": "Robustness in language and speech technology",
    "editor": [
      {
        "family": "Junqua",
        "given": "Jean-Claude"
      },
      {
        "dropping-particle": "van",
        "family": "Noord",
        "given": "Gertjan"
      }
    ],
    "id": "Chanod2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "page": "187-204",
    "publisher": "Kluwer",
    "publisher-place": "Amsterdam",
    "title": "Robust parsing and beyond",
    "type": "chapter",
    "volume": "17"
  },
  {
    "DOI": "10.1093/llc/fqy070",
    "ISSN": "2055-7671",
    "abstract": "Identifying the stylistic signatures characteristic of different genres is of central importance to literary theory and criticism. In this article we report a large-scale computational analysis of Latin prose and verse using a combination of quantitative stylistics and supervised machine learning. We train a set of classifiers to differentiate prose and poetry with high accuracy (>97%) based on a set of twenty-six text-based, primarily syntactic features and rank the relative importance of these features to identify a low-dimensional set still sufficient to achieve excellent classifier performance. This analysis demonstrates that Latin prose and verse can be classified effectively using just three top features. From examination of the highly ranked features, we observe that measures of the hypotactic style favored in Latin prose (i.e. subordinating constructions in complex sentences, such as relative clauses) are especially useful for classification.",
    "author": [
      {
        "family": "Chaudhuri",
        "given": "Pramit"
      },
      {
        "family": "Dasgupta",
        "given": "Tathagata"
      },
      {
        "family": "Dexter",
        "given": "Joseph P."
      },
      {
        "family": "Iyer",
        "given": "Krithika"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Chaudhuri2018",
    "issued": {
      "date-parts": [
        [
          2018,
          12,
          24
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "title": "A small set of stylometric features differentiates Latin prose and verse",
    "type": "article-journal"
  },
  {
    "DOI": "10.1145/3148011.3148038",
    "ISBN": "9781450355537",
    "abstract": "Digital music scores are a way to present music notation and lack of semantic information useful for musicology purposes in order to manipulate music concepts. We propose a general approach to extend score encodings with semantic annotations. It relies on an ontology of music notation designed to integrate semantic music elements either extracted or produced by a knowledge process. We illustrate the whole mechanism by extracting RDF facts based on the identification of dissonances in Renaissance counterpoint.",
    "author": [
      {
        "family": "Cherfi",
        "given": "Samira S."
      },
      {
        "family": "Guillotel",
        "given": "Christophe"
      },
      {
        "family": "Hamdi",
        "given": "Fayçal"
      },
      {
        "family": "Rigaux",
        "given": "Philippe"
      },
      {
        "family": "Travers",
        "given": "Nicolas"
      }
    ],
    "container-title": "Proceedings of the knowledge capture conference (k-CAP 2017)",
    "id": "Cherfi2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "music, ontologies",
    "language": "en-US",
    "page": "1-4",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Ontology-Based annotation of music scores",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1159890.806455",
    "author": [
      {
        "family": "Cherry",
        "given": "Lorinda"
      }
    ],
    "container-title": "ACM SIGOA Newsletter",
    "id": "Cherry1981",
    "issue": "1–2",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "page": "61-67",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Computer aids for writers",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.1145/2166896.2166898",
    "ISBN": "978-1-4503-1076-5",
    "abstract": "A major challenge of linked data is resolving the many different identifiers representing the same object Interconnecting the data requires mappings between the vocabularies and identifiers used in different data sets. To help with this issue, we have developed a service, which we use specifically for nanopublication provenance, that provides the conversion between two types of identifiers; the PubMed Identifier (PMID) which is a unique number assigned to PubMed citations of life science journal articles and the Digital Object Identifier™ (DOI) which is used for identifying digital content. DOI’s are used to provide current information, including where the content (or information about the content) can be found on the Internet. DOI’s are a very useful identifier as they often give a direct link back to the full text scientific article. We provide SOAP and REST web services the conversion data. In addition, there is a SPARQL endpoint for querying the mappings. http://www.pmid2doi.org/",
    "author": [
      {
        "family": "Chichester",
        "given": "Christine"
      },
      {
        "family": "Burger",
        "given": "Kees"
      },
      {
        "family": "Mei",
        "given": "Hailiang"
      },
      {
        "family": "Mons",
        "given": "Barend"
      }
    ],
    "collection-title": "SWAT4LS ’11",
    "container-title": "Proceedings of the 4th international workshop on semantic web applications and tools for the life sciences (SWAT4LS ’11)",
    "id": "Chichester2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "nanopublications, rdf, semantic_web",
    "language": "en-US",
    "page": "3-4",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Supporting nanopublication provenance: PMID2DOI converter",
    "title-short": "Supporting nanopublication provenance",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Chickering",
        "given": "Arthur W."
      },
      {
        "family": "Gamson",
        "given": "Zelda F."
      }
    ],
    "container-title": "AAHE Bulletin",
    "id": "Chickering1987",
    "issue": "7",
    "issued": {
      "date-parts": [
        [
          1987,
          3
        ]
      ]
    },
    "page": "3-7",
    "title": "Seven principles for good practice in undergraduate education",
    "type": "article-journal",
    "volume": "39"
  },
  {
    "DOI": "10.1145/958220.958257",
    "abstract": "Since the XML format became a de facto standard for structured documents, the IT research and industry have developed a number of XML editors to help users produce structured documents in XML format. However, the manual generation of structured documents in XML format remains a tedious and time-consuming process because of the excessive verbosity and length of XML code. In this paper, we design a structural adviser for the XML document authoring. The adviser intervenes at any step of the authoring process to suggest one tag or entire tree-like pattern the user is most likely to use next. Adviser suggestions are based on finding analogies between the currently edited fragment and sample data being either previously generated documents in the collection or the history of the current document authoring. The adviser is beneficial in cases when no schema is provided for XML documents, or schema associated with the document is too general and sample data contain specific patterns not captured in the schema. We design the adviser architecture and develop a method for efficient indexing and retrieval of optimal suggestions at any step of the document authoring.",
    "author": [
      {
        "family": "Chidlovskii",
        "given": "Boris"
      }
    ],
    "container-title": "DocEng ’03: Proceedings of the 2003 ACM symposium on document engineering",
    "id": "Chidlovskii2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "interactive_editing, xml",
    "language": "en-US",
    "page": "203-211",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A structural adviser for the XML document authoring",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1067445.1067468",
    "author": [
      {
        "family": "Chinn",
        "given": "Donald"
      }
    ],
    "container-title": "ITiCSE ’05: Proceedings of the 10<sup>th</sup> annual SIGCSE conference on innovation and technology in computer science education",
    "id": "Chinn2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "page": "69-73",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Peer assessment in the algorithms course",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s10032-007-0054-0",
    "ISSN": "1433-2833",
    "abstract": "Language usage over computer mediated discourses, such as chats, emails and SMS texts, significantly differs from the standard form of the language and is referred to as texting language (TL). The presence of intentional misspellings significantly decrease the accuracy of existing spell checking techniques for TL words. In this work, we formally investigate the nature and type of compressions used in SMS texts, and develop a Hidden Markov Model based word-model for TL. The model parameters have been estimated through standard machine learning techniques from a word-aligned SMS and standard English parallel corpus. The accuracy of the model in correcting TL words is 57.7%, which is almost a threefold improvement over the performance of Aspell. The use of simple bigram language model results in a 35% reduction of the relative word level error rates.",
    "author": [
      {
        "family": "Choudhury",
        "given": "Monojit"
      },
      {
        "family": "Saraf",
        "given": "Rahul"
      },
      {
        "family": "Jain",
        "given": "Vijit"
      },
      {
        "family": "Mukherjee",
        "given": "Animesh"
      },
      {
        "family": "Sarkar",
        "given": "Sudeshna"
      },
      {
        "family": "Basu",
        "given": "Anupam"
      }
    ],
    "container-title": "International Journal on Document Analysis and Recognition",
    "id": "Choudhury2007",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "microtext, spelling_correction, spelling_normalization",
    "language": "en-US",
    "page": "157-174",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Investigation and modeling of the structure of texting language",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "DOI": "10.1016/j.jbi.2008.04.010",
    "ISSN": "15320464",
    "PMID": "18583197",
    "abstract": "Developing cures for highly complex diseases, such as neurodegenerative disorders, requires extensive interdisciplinary collaboration and exchange of biomedical information in context. Our ability to exchange such information across sub-specialties today is limited by the current scientific knowledge ecosystem’s inability to properly contextualize and integrate data and discourse in machine-interpretable form. This inherently limits the productivity of research and the progress toward cures for devastating diseases such as Alzheimer’s and Parkinson’s. SWAN (Semantic Web Applications in Neuromedicine) is an interdisciplinary project to develop a practical, common, semantically structured, framework for biomedical discourse initially applied, but not limited, to significant problems in Alzheimer Disease (AD) research. The SWAN ontology has been developed in the context of building a series of applications for biomedical researchers, as well as in extensive discussions and collaborations with the larger bio-ontologies community. In this paper, we present and discuss the SWAN ontology of biomedical discourse. We ground its development theoretically, present its design approach, explain its main classes and their application, and show its relationship to other ongoing activities in biomedicine and bio-ontologies.",
    "author": [
      {
        "family": "Ciccarese",
        "given": "Paolo N."
      },
      {
        "family": "Wu",
        "given": "Elizabeth"
      },
      {
        "family": "Wong",
        "given": "Gwen"
      },
      {
        "family": "Ocana",
        "given": "Marco"
      },
      {
        "family": "Kinoshita",
        "given": "June"
      },
      {
        "family": "Ruttenberg",
        "given": "Alan"
      },
      {
        "family": "Clark",
        "given": "Tim"
      }
    ],
    "container-title": "Journal of Biomedical Informatics",
    "id": "Ciccarese2007",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "rdf, semantic_web",
    "language": "en-US",
    "page": "739-751",
    "title": "The SWAN biomedical discourse ontology",
    "type": "article-journal",
    "volume": "41"
  },
  {
    "URL": "http://www.semantic-web-journal.net/",
    "abstract": "Most literature searching in biomedicine is now conducted via PubMed, Google Scholar or other web-based bibliographic search mechanisms. Yet until now a public, open, interoperable and complete web-adapted information schema for bibliographic citations, bibliographic references and scientific discourse has not been available. Such a schema, expressed in the form of a description logic compatible with current web semantics approaches, would provide the ability to treat bibliographic references and citations, and rhetorical discourse in scientific publications, as semantic metadata on the web, with all the benefits that implies for organization, search and mash-up of web-based scientific information. In this paper we present CiTO + SWAN, a set of fully harmonized ontology modules resulting from the harmonization of CiTO (the Citation Typing Ontology) with SWAN (Semantic Web Applications in Neuromedicine), which we have developed by jointly adapting and evolving version 1.6 of CiTO, the Citation Typing Ontology, and version 1.2 of the SWAN Scientific Discourse Ontology (v1.2). The CiTO + SWAN model is specified in OWL 2 DL, is fully modular, and inherently supports agent-based searching and mash-ups. Through the harmonization activity presented here, and previous work that harmonized SWAN with the SIOC (Semantically-Interlinked Online Communities) Ontology for describing blogs, wikis and discussion groups, we have construct the basis of a powerful new web framework for scientific communications.",
    "author": [
      {
        "family": "Ciccarese",
        "given": "Paolo N."
      },
      {
        "family": "Shotton",
        "given": "David"
      },
      {
        "family": "Peroni",
        "given": "Silvio"
      },
      {
        "family": "Clark",
        "given": "Tim"
      }
    ],
    "container-title": "Semantic Web",
    "id": "Ciccarese2012",
    "issue": "4",
    "issued": {
      "date-parts": []
    },
    "keyword": "intertextuality, ontologies, semantic_web",
    "language": "en-US",
    "title": "CiTO+SWAN: The web semantics of bibliographic references, citations, evidence and discourse relationships",
    "title-short": "CiTO+SWAN",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=1661688",
    "abstract": "The field of information retrieval and text manipulation (classification, clustering) still strives for models allowing semantic information to be folded in to improve performance with respect to standard bag-of-word based models. Many approaches aim at a concept-based retrieval, but differ in the nature of the concepts, which range from linguistic concepts as defined in lexical resources such as WordNet, latent topics derived from the data itself - as in Latent Semantic Indexing (LSI) or (Latent Dirichlet Allocation (LDA) - to Wikipedia articles as proxies for concepts, as in the recently proposed Explicit Semantic Analysis (ESA) model. A crucial question which has not been answered so far is whether models based on explicitly given concepts (as in the ESA model for instance) perform inherently better than retrieval models based on \"latent\" concepts (as in LSI and/or LDA). In this paper we investigate this question closer in the context of a cross-language setting, which inherently requires concept-based retrieval bridging between different languages. In particular, we compare the recently proposed ESA model with two latent models (LSI and LDA) showing that the former is clearly superior to the both. From a general perspective, our results contribute to clarifying the role of explicit vs. implicitly derived or latent concepts in (cross-language) information retrieval research.",
    "author": [
      {
        "family": "Cimiano",
        "given": "Philipp"
      },
      {
        "family": "Schultz",
        "given": "Antje"
      },
      {
        "family": "Sizov",
        "given": "Sergej"
      },
      {
        "family": "Sorg",
        "given": "Philipp"
      },
      {
        "family": "Staab",
        "given": "Steffen"
      }
    ],
    "container-title": "Proceedings of the 21<sup>st</sup> international joint conference for artificial intelligence (IJCAI’09)",
    "id": "Cimiano2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "ir, wikipedia",
    "language": "en-US",
    "page": "1513-1518",
    "publisher": "Morgan Kaufmann",
    "publisher-place": "San Francisco, CA, USA",
    "title": "Explicit versus latent concept models for cross-language information retrieval",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqn018",
    "ISSN": "1477-4615",
    "abstract": "This article focuses on the use of technologies traditionally associated with knowledge representation to express complex associations between entities in historical texts that have been marked up in XML, according to the Text Encoding Initiative guidelines. In particular, we describe our exploration of the potential role of an ontology in facilitating the interpretation of implicit and hidden associations in the sources of interest, examining its use, and limits in a digital humanities project in connection with editing tools and delivery issues. We demonstrate our findings based on the Henry III Fine Rolls project, where an ontology—built using the RDF (Resource Description Framework)/OWL (Web Ontology Language) technologies—is being developed to make explicit information about person, place, and subject entities marked up as instances in the core texts themselves. For any historian, there is a natural tension between primary sources (as documentary records) and the analysis that produces a context for interpretation. We will argue that the combination of core mark-up (encoded in TEI) and an ontology (in RDF/OWL) provides a powerful model for representing the complexity of this tension and facilitates the necessarily dynamic process of scholarly interpretation.",
    "author": [
      {
        "family": "Ciula",
        "given": "Arianna"
      },
      {
        "family": "Spence",
        "given": "Paul"
      },
      {
        "family": "Vieira",
        "given": "José M."
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Ciula2008",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "digital_edition, digital_humanities, rdf, semantic_web, tei",
    "language": "en-US",
    "page": "311-325",
    "publisher": "Oxford University Press",
    "title": "Expressing complex associations in medieval historical documents: The Henry III Fine Rolls Project",
    "title-short": "Expressing complex associations in medieval historical documents",
    "type": "article-journal",
    "volume": "23"
  },
  {
    "DOI": "10.1145/2595188.2595207",
    "ISBN": "9781450325882",
    "abstract": "Computer based modelling in cultural heritage has focused on database development, generalised as data standards and, since the 1990s, also formal ontologies. Modelling in digital humanities has had its core in textual scholarship, including close reading and text encoding of literary and historical sources as well as models of text corpora, usually relying on statistical methods. Integration between the two modelling paradigms has been undertaken at the practical level. This paper goes beyond pragmatic concerns by focusing on comparing the two modelling traditions at a more abstract level. To this end, one core standard development undertaken in each domain is selected: CIDOC’s Conceptual Reference Model (CRM) for modelling in cultural heritage—narrowed down to museum documentation—and Text Encoding Initiative (TEI) for modelling in digital humanities—narrowed down to textual scholarship. This does not imply that these two standards are only used in the two areas mentioned above, rather that their main focus has been in those areas. We will use the two standards to investigate what is meant by modelling in the two communities, thus, clarify the differences and the similarities between the concepts of modelling and models as they are used in each community. Partly this will be done based on a survey of previous literature, and partly by an investigation into modelling practices within these two standardisation initiatives. Minutes and reports, descriptions of the standards themselves and their developments, mailing lists threads, and the participants’ own experience in the development of the respective standards have informed this study.",
    "author": [
      {
        "family": "Ciula",
        "given": "Arianna"
      },
      {
        "family": "Eide",
        "given": "Øyvind"
      }
    ],
    "container-title": "Proceedings of the first international conference on digital access to textual cultural heritage – DATeCH ’14",
    "id": "Ciula2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models, knowledge_representation, ontologies",
    "language": "en-US",
    "page": "35-41",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Reflections on cultural heritage and digital humanities",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqw045",
    "ISSN": "2055-7671",
    "abstract": "In this paper we focus on modelling as a creative process to gain new knowledge about material and immaterial objects by generating and manipulating external representations of them. We aim at enriching the current theoretical understanding by contextualising digital humanities practices within a semiotic conceptualisation of modelling. A semiotic approach enables us to contextualise modelling in a scholarly framework well suited to humanistic enquiries, forcing us to investigate how models function as signs within specific contexts of production and use. Kralemann and Lattmann’s semiotic model of modelling complemented by Elleström’s theories on iconicity are some of the tools we use to inform this semiotic perspective on modelling. We contextualise Kralemann and Lattmann’s theory within modelling practices in digital humanities by using three examples of models representing components and structure of historical artefacts. We show how their model of models can be used to understand and contextualise the models we study and how their classification of model types clarify important aspects of digital humanities modelling practice.",
    "author": [
      {
        "family": "Ciula",
        "given": "Arianna"
      },
      {
        "family": "Eide",
        "given": "Øyvind"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Ciula2016",
    "issue": "Supplement 1",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "en-US",
    "page": "i33-i46",
    "title": "Modelling in digital humanities: Signs in context",
    "title-short": "Modelling in digital humanities",
    "type": "article-journal",
    "volume": "32"
  },
  {
    "DOI": "10.12759/HSR.SUPPL.31.2018.7-29",
    "author": [
      {
        "family": "Ciula",
        "given": "Arianna"
      },
      {
        "family": "Eide",
        "given": "Øyvind"
      },
      {
        "family": "Marras",
        "given": "Cristina"
      },
      {
        "family": "Sahle",
        "given": "Patrick"
      }
    ],
    "container-title": "Historical Social Research Supplement",
    "id": "Ciula2018a",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "page": "7-29",
    "title": "Modelling: Thinking in practice: An introduction",
    "title-short": "Modelling",
    "type": "article-journal",
    "volume": "31"
  },
  {
    "DOI": "10.12759/HSR.43.2018.4.343-361",
    "author": [
      {
        "family": "Ciula",
        "given": "Arianna"
      },
      {
        "family": "Eide",
        "given": "Øyvind"
      },
      {
        "family": "Marras",
        "given": "Cristina"
      },
      {
        "family": "Sahle",
        "given": "Patrick"
      }
    ],
    "container-title": "Historical Social Research",
    "id": "Ciula2018b",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "page": "343-361",
    "title": "Models and modelling between digital and humanities: Remarks from a multidisciplinary perspective",
    "title-short": "Models and modelling between digital and humanities",
    "type": "article-journal",
    "volume": "43"
  },
  {
    "DOI": "10.1145/351240.351266",
    "author": [
      {
        "family": "Claessen",
        "given": "Koen"
      },
      {
        "family": "Hughes",
        "given": "John"
      }
    ],
    "container-title": "ICFP ’00: Proceedings of the fifth ACM SIGPLAN international conference on functional programming",
    "id": "Claessen2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "page": "268-279",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "QuickCheck: A lightweight tool for random testing of haskell programs",
    "title-short": "QuickCheck",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Claessen",
        "given": "Koen"
      },
      {
        "family": "Hughes",
        "given": "John"
      }
    ],
    "collection-title": "Cornerstones of computing",
    "container-title": "The fun of programming",
    "editor": [
      {
        "family": "Gibbons",
        "given": "Jeremy"
      },
      {
        "dropping-particle": "de",
        "family": "Moor",
        "given": "Oege"
      }
    ],
    "id": "Claessen2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "page": "17-40",
    "publisher": "Palgrave",
    "publisher-place": "Houndsmills, England",
    "title": "Specification based testing with QuickCheck",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Clark",
        "given": "David"
      }
    ],
    "container-title": "Informatics in Education",
    "id": "Clark2004",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "161-178",
    "title": "Testing programming skills with multiple choice questions",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "URL": "http://arxiv.org/abs/1305.3506",
    "abstract": "The Micropublications semantic model for scientific claims, evidence, argumentation and annotation in biomedical publications, is designed to support several key requirements for exchange and value-addition of semantic metadata across the biomedical publications ecosystem. This model is intended to complement the more narrowly focused \"nanopublication\" model. Its theoretical basis comes from argumentation theory, with some fundamental concepts adapted from third-generation activity theory. Its practical basis is a set of experiences working through use cases and unmet needs with users of our Domeo web annotation toolkit and the SWAN biomedical discourse ontology. Use cases supported by this model include directly citable assertions (claims); direct citation of content in scientific data repositories, methods repositories and materials repositories in support of citable claims; citation of images as evidence; digital abstracting with semantic structure; citable annotation and discussion; claim network construction; and bipolar argumentation.",
    "author": [
      {
        "family": "Clark",
        "given": "Tim"
      },
      {
        "family": "Ciccarese",
        "given": "Paolo N."
      },
      {
        "family": "Goble",
        "given": "Carole A."
      }
    ],
    "id": "Clark2013",
    "issued": {
      "date-parts": [
        [
          2014,
          2
        ]
      ]
    },
    "keyword": "nanopublications, rdf, semantic_web",
    "language": "en-US",
    "note": "Submitted to Journal of Biomedical Semantics",
    "title": "Micropublications: A semantic model for claims, evidence, arguments and annotations in biomedical communications",
    "title-short": "Micropublications",
    "type": ""
  },
  {
    "DOI": "10.1186/2041-1480-5-28",
    "ISSN": "2041-1480",
    "abstract": "BACKGROUND:Scientific publications are documentary representations of defeasible arguments, supported by data and repeatable methods. They are the essential mediating artifacts in the ecosystem of scientific communications. The institutional \"goal\" of science is publishing results. The linear document publication format, dating from 1665, has survived transition to the Web.Intractable publication volumes; the difficulty of verifying evidence; and observed problems in evidence and citation chains suggest a need for a web-friendly and machine-tractable model of scientific publications. This model should support: digital summarization, evidence examination, challenge, verification and remix, and incremental adoption. Such a model must be capable of expressing a broad spectrum of representational complexity, ranging from minimal to maximal forms.RESULTS:The micropublications semantic model of scientific argument and evidence provides these features. Micropublications support natural language statements; data; methods and materials specifications; discussion and commentary; challenge and disagreement; as well as allowing many kinds of statement formalization.The minimal form of a micropublication is a statement with its attribution. The maximal form is a statement with its complete supporting argument, consisting of all relevant evidence, interpretations, discussion and challenges brought forward in support of or opposition to it. Micropublications may be formalized and serialized in multiple ways, including in RDF. They may be added to publications as stand-off metadata.An OWL 2 vocabulary for micropublications is available at http://purl.org/mp. A discussion of this vocabulary along with RDF examples from the case studies, appears as OWL Vocabulary and RDF Examples in Additional file 2.CONCLUSION:Micropublications, because they model evidence and allow qualified, nuanced assertions, can play essential roles in the scientific communications ecosystem in places where simpler, formalized and purely statement-based models, such as the nanopublications model, will not be sufficient. At the same time they will add significant value to, and are intentionally compatible with, statement-based formalizations.We suggest that micropublications, generated by useful software tools supporting such activities as writing, editing, reviewing, and discussion, will be of great value in improving the quality and tractability of biomedical communications.",
    "author": [
      {
        "family": "Clark",
        "given": "Tim"
      },
      {
        "family": "Ciccarese",
        "given": "Paolo N."
      },
      {
        "family": "Goble",
        "given": "Carole A."
      }
    ],
    "container-title": "Journal of Biomedical Semantics",
    "id": "Clark2014",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "nanopublications, rdf, semantic_web",
    "language": "en-US",
    "page": "28+",
    "title": "Micropublications: A semantic model for claims, evidence, arguments and annotations in biomedical communications",
    "title-short": "Micropublications",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.1007/978-3-642-20169-1_4",
    "ISBN": "978-3-642-20168-4",
    "abstract": "This article presents the central algorithm of an open system for grammar checking, based on deep parsing. The grammatical specification is a context-free grammar with flat feature structures. After a shared-forest analysis where feature agreement constraints are relaxed, error detection globally minimizes the number of corrections and alternative correct sentences are automatically proposed.",
    "author": [
      {
        "family": "Clément",
        "given": "Lionel"
      },
      {
        "family": "Gerdes",
        "given": "Kim"
      },
      {
        "family": "Marlet",
        "given": "Renaud"
      }
    ],
    "chapter-number": "4",
    "collection-title": "Lecture notes in computer science",
    "editor": [
      {
        "family": "Groote",
        "given": "Philippe"
      },
      {
        "family": "Egg",
        "given": "Markus"
      },
      {
        "family": "Kallmeyer",
        "given": "Laura"
      }
    ],
    "id": "Clement2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "french, grammar_checking, interactive_editing, interactive_parsing",
    "language": "en-US",
    "page": "47-63",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "A grammar correction algorithm: Deep parsing and minimal corrections for a grammar checker",
    "title-short": "A grammar correction algorithm",
    "type": "chapter",
    "volume": "5591"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/pdf/908_paper.pdf",
    "accessed": {
      "date-parts": [
        [
          2009,
          1,
          25
        ]
      ]
    },
    "author": [
      {
        "dropping-particle": "de la",
        "family": "Clergerie",
        "given": "Éric Villemonte"
      },
      {
        "family": "Hamon",
        "given": "Olivier"
      },
      {
        "family": "Mostefa",
        "given": "Djamel"
      },
      {
        "family": "Ayache",
        "given": "Christelle"
      },
      {
        "family": "Paroubek",
        "given": "Patrick"
      },
      {
        "family": "Vilnat",
        "given": "Anne"
      }
    ],
    "container-title": "Proceedings of the 6<sup>th</sup> international conference on language resources and evaluation (LREC 2008)",
    "id": "Clergerie2008",
    "issued": {
      "date-parts": [
        [
          2008,
          5
        ]
      ]
    },
    "language": "en-US",
    "publisher": "European Language Resources Association (ELRA)",
    "publisher-place": "Marrakech, Morocco",
    "title": "PASSAGE: From French parser evaluation to large sized treebank",
    "title-short": "PASSAGE",
    "type": "paper-conference"
  },
  {
    "DOI": "10.3115/1073083.1073110",
    "abstract": "In this paper we present results from the METER (MEasuring TExt Reuse) project whose aim is to explore issues pertaining to text reuse and derivation, especially in the context of newspapers using newswire sources. Although the reuse of text by journalists has been studied in linguistics, we are not aware of any investigation using existing computational methods for this particular task. We investigate the classification of newspaper articles according to their degree of dependence upon, or derivation from, a newswire source using a simple 3-level scheme designed by journalists. Three approaches to measuring text similarity are considered: n-gram overlap, Greedy String Tiling, and sentence alignment. Measured against a manually annotated corpus of source and derived news text, we show that a combined classifier with features automatically selected performs best overall for the ternary classification achieving an average F1-measure score of 0.664 across all three categories.",
    "author": [
      {
        "family": "Clough",
        "given": "Paul"
      },
      {
        "family": "Gaizauskas",
        "given": "Robert"
      },
      {
        "family": "Piao",
        "given": "Scott S. L."
      },
      {
        "family": "Wilks",
        "given": "Yorick"
      }
    ],
    "container-title": "Proceedings of the 40<sup>th</sup> annual meeting of the association for computational linguistics (ACL ’02)",
    "id": "Clough2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "ir, plagiarism",
    "language": "en-US",
    "page": "152-159",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "METER: MEasuring TExt reuse",
    "title-short": "METER",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqs033",
    "ISSN": "1477-4615",
    "abstract": "Tesserae is a web-based tool for automatically detecting allusions in Latin poetry. Although still in the start-up phase, it already is capable of identifying significant numbers of known allusions, as well as similar numbers of allusions previously unnoticed by scholars. In this article, we use the tool to examine allusions to Vergil’s Aeneid in the first book of Lucan’s Civil War. Approximately 3,000 linguistic parallels returned by the program were compared with a list of known allusions drawn from commentaries. Each was examined individually and graded for its literary significance, in order to benchmark the program’s performance. All allusions from the program and commentaries were then pooled in order to examine broad patterns in Lucan’s allusive techniques which were largely unapproachable without digital methods. Although Lucan draws relatively constantly from Vergil’s generic language in order to maintain the epic idiom, this baseline is punctuated by clusters of pointed allusions, in which Lucan frequently subverts Vergil’s original meaning. These clusters not only attend the most significant characters and events but also play a role in structuring scene transitions. Work is under way to incorporate the ability to match on word meaning, phrase context, as well as metrical and phonological features into future versions of the program.",
    "author": [
      {
        "family": "Coffee",
        "given": "Neil"
      },
      {
        "family": "Koenig",
        "given": "Jean-Pierre"
      },
      {
        "family": "Poornima",
        "given": "Shakthi"
      },
      {
        "family": "Forstall",
        "given": "Christopher W."
      },
      {
        "family": "Ossewaarde",
        "given": "Roelant"
      },
      {
        "family": "Jacobson",
        "given": "Sarah L."
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Coffee2013",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2013,
          6,
          1
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_humanities, intertextuality, plagiarism",
    "language": "en-US",
    "page": "221-228",
    "publisher": "Oxford University Press",
    "title": "The Tesserae project: Intertextual analysis of Latin poetry",
    "title-short": "The Tesserae project",
    "type": "article-journal",
    "volume": "28"
  },
  {
    "author": [
      {
        "family": "Cohen",
        "given": "Michael D."
      },
      {
        "family": "March",
        "given": "James G."
      },
      {
        "family": "Olsen",
        "given": "Johan P."
      }
    ],
    "container-title": "Administrative Science Quarterly",
    "id": "Cohen1972",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1972,
          3
        ]
      ]
    },
    "page": "1-25",
    "title": "A garbage can model of organizational choice",
    "type": "article-journal",
    "volume": "17"
  },
  {
    "DOI": "10.1007/978-3-319-15168-7_31",
    "abstract": "Early modern printed gazettes relied on a system of news exchange and text reuse largely based on handwritten sources. The reconstruction of this information exchange system is possible by detecting reused texts. We present a method to individuate text borrowings within noisy OCRed texts from printed gazettes based on string kernels and local text alignment. We apply our methods on a corpus of Italian gazettes for the year 1648. Beside unveiling substantial overlaps in news sources, we are able to assess the editorial policy of different gazettes and account for a multi-faceted system of text reuse.",
    "author": [
      {
        "family": "Colavizza",
        "given": "Giovanni"
      },
      {
        "family": "Infelise",
        "given": "Mario"
      },
      {
        "family": "Kaplan",
        "given": "Frédéric"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Social informatics",
    "editor": [
      {
        "family": "Aiello",
        "given": "Luca M."
      },
      {
        "family": "McFarland",
        "given": "Daniel"
      }
    ],
    "id": "Colavizza2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "cultural_heritage, intertextuality",
    "language": "en-US",
    "page": "244-253",
    "publisher": "Springer",
    "publisher-place": "Cham, Switzerland",
    "title": "Mapping the early modern news flow: An enquiry by robust text reuse detection",
    "title-short": "Mapping the early modern news flow",
    "type": "paper-conference",
    "volume": "8852"
  },
  {
    "DOI": "10.22148/001c.11828",
    "author": [
      {
        "family": "Colavizza",
        "given": "Giovanni"
      }
    ],
    "container-title": "Journal of Cultural Analytics",
    "id": "Colavizza2019a",
    "issued": {
      "date-parts": [
        [
          2019,
          9,
          17
        ]
      ]
    },
    "language": "en-US",
    "title": "Are we breaking the social contract?",
    "type": "article-journal"
  },
  {
    "ISBN": "0521592771",
    "abstract": "Languages, in all their forms, are the more efficient and natural means for people to communicate. Enormous quantities of information are produced, distributed and consumed using languages. Human language technology’s main purpose is to allow the use of automatic systems and tools to assist humans in producing and accessing information, to improve communication between humans, and to assist humans in communicating with machines. This book, sponsored by the Directorate General XIII of the European Union and the Information Science and Engineering Directorate of the National Science Foundation, USA, offers the first comprehensive overview of the human language technology field.",
    "id": "Cole1997",
    "issued": {
      "date-parts": [
        [
          1998,
          3,
          13
        ]
      ]
    },
    "keyword": "interactive_editing, nlp",
    "language": "en-US",
    "publisher": "Hardcover; Cambridge University Press",
    "title": "Survey of the state of the art in human language technology (studies in natural language processing)",
    "type": "book"
  },
  {
    "DOI": "10.1148/rg.262055145",
    "PMID": "16549616",
    "abstract": "The multiple-choice question (MCQ) is the most commonly used type of test item in radiologic graduate medical and continuing medical education examinations. Now that radiologists are participating in the maintenance of certification process, there is an increased need for self-assessment modules that include MCQs and persons with test item-writing skills to develop such modules. Although principles of effective test item writing have been documented, violations of these principles are common in medical education. Guidelines for test construction are related to development of educational objectives, defining levels of learning for each objective, and writing effective MCQs that test that learning. Educational objectives should be written in observable, behavioral terms that allow for an accurate assessment of whether the learner has achieved the objectives. Learning occurs at many levels, from simple recall to problem solving. The educational objectives and the MCQs that accompany them should target all levels of learning appropriate for the given content. Characteristics of effective MCQs can be described in terms of the overall item, the stem, and the options. Flawed MCQs interfere with accurate and meaningful interpretation of test scores and negatively affect student pass rates. Therefore, to develop reliable and valid tests, items must be constructed that are free of such flaws. The article provides an overview of established guidelines for writing effective MCQs, a discussion of writing appropriate educational objectives and MCQs that match those objectives, and a brief review of item analysis. (C) RSNA, 2006 10.1148/rg.262055145",
    "author": [
      {
        "family": "Collins",
        "given": "Jannette"
      }
    ],
    "container-title": "Radiographics",
    "id": "Collins2006",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2006,
          3,
          1
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "543-551",
    "title": "Education techniques for lifelong learning: Writing Multiple-Choice questions for continuing medical education activities and Self-Assessment modules",
    "title-short": "Education techniques for lifelong learning",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "ISSN": "0276-3869",
    "PMID": "17210552",
    "URL": "http://view.ncbi.nlm.nih.gov/pubmed/17210552",
    "abstract": "Audience response system (ARS) technology offers considerable promise for bringing more interactivity and engagement to the classroom. Since gaining momentum in the late 1990s, ARS use has been promoted for its ability to provide immediate feedback, focus student attention, identify gaps in knowledge, and enhance student involvement. This inexpensive technology is potentially of great value for improving the educational experience of students and instructors alike. doi:10.1300/J115v26n01_08.",
    "author": [
      {
        "family": "Collins",
        "given": "L. J."
      }
    ],
    "container-title": "Medical reference services quarterly",
    "id": "Collins2007",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "81-88",
    "publisher-place": "Health Sciences Library, University of North Carolina, Chapel Hill 27599-7585, USA. Linda_Collins@med.unc.edu",
    "title": "Livening up the classroom: Using audience response systems to promote active learning.",
    "title-short": "Livening up the classroom",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "DOI": "10.1177/1474022211427421",
    "ISSN": "1741-265X",
    "abstract": "In recent years, many studies have highlighted the changing nature of scholarly research, reflecting the new digital tools and techniques that have been developed. But researcher uptake of these tools is strongly influenced by existing information behaviour, itself affected by a number of factors, particularly discipline. This article outlines findings from a recent study which used six case studies to look at the information behaviours of researchers working in different disciplinary fields or academic departments, or using specific tools. The study suggested that researchers’ uses of, and attitudes towards, digital technologies are affected by existing disciplinary habits and preconceptions. Furthermore, it found that the computational and collaborative complexity of the tools that researchers used was linked to their disciplinary backgrounds.",
    "author": [
      {
        "family": "Collins",
        "given": "Ellen"
      },
      {
        "family": "Bulger",
        "given": "Monica E."
      },
      {
        "family": "Meyer",
        "given": "Eric T."
      }
    ],
    "container-title": "Arts and Humanities in Higher Education",
    "id": "Collins2012",
    "issue": "1-2",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "76-92",
    "publisher": "SAGE",
    "title": "Discipline matters",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "abstract": "Technological development has resulted in widespread change in the way students can learn. The scale of this change cannot be overemphasized, with over 10 million students now using distance education as their main access to learning. Every university in the world now relies on digital systems to a greater or lesser extent. Students routinely use multimedia and Internet resources. Against this background and the fact that technology is now a staple of education, this book answers the questions \"What lessons have been learned from decades of research and practical experience with educational technology?\" and \"What are the implications of these lessons for HE now, and in the future?\"",
    "author": [
      {
        "family": "Collis",
        "given": "Betty"
      },
      {
        "family": "Moonen",
        "given": "Jef"
      }
    ],
    "collection-title": "Open & distance learning series",
    "id": "Collis2001",
    "issued": {
      "date-parts": [
        [
          2001,
          4
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "Paperback; Kogan Page",
    "publisher-place": "London, UK",
    "title": "Flexible learning in a digital world: Experiences and expectations",
    "title-short": "Flexible learning in a digital world",
    "type": "book"
  },
  {
    "URL": "http://purl.org/utwente/44610",
    "abstract": "The aim of this study is to investigate which scenarios are emerging with respect to the use of ICT in higher education and how future developments can be predicted and strategic choices can be based on that.",
    "editor": [
      {
        "family": "Collis",
        "given": "Betty"
      },
      {
        "dropping-particle": "van der",
        "family": "Wende",
        "given": "Marijk"
      }
    ],
    "id": "Collis2002",
    "issued": {
      "date-parts": [
        [
          2002,
          12
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "Center for Higher Education Policy Studies, University of Twente",
    "publisher-place": "Enschede, The Netherlands",
    "title": "Models of technology and change in higher education",
    "type": "report"
  },
  {
    "DOI": "10.1130/0016-7606(1996)108\\%3C1508:grgaai\\%3E2.3.co;2",
    "ISSN": "00167606",
    "abstract": "Frodeman (1995) argued that geology is a historical and interpretive science, that is, essentially a subjective discipline with its own set of logical procedures. This perspective is in general agreement with my belief that geology (along with its attendant subdiscipline of geochemistry) is a descriptive (as opposed to purely experimental) science that uses its own terminology and set of symbols. Frodeman (1995) used the philosophical framework laid down by Heidegger (1962). Heidegger, one of the most important of the philosophers of the existentialist school, can be credited with having formalized all human understanding into a narrative, interpretive framework. At this point some clear definitions of objectivity and subjectivity are helpful.",
    "author": [
      {
        "family": "Comet",
        "given": "Paul A."
      }
    ],
    "container-title": "Geological Society of America Bulletin",
    "id": "Comet1996",
    "issue": "11",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "philosophy_of_science, uncertainty",
    "language": "en-US",
    "page": "1508-1510",
    "title": "Geological reasoning: Geology as an interpretive and historical science: discussion",
    "title-short": "Geological reasoning",
    "type": "article-journal",
    "volume": "108"
  },
  {
    "DOI": "10.1117/12.143623",
    "abstract": "The accuracies of OCR systems have increased in recent years due to improvements in pre- processing methods and recognition algorithms. It has been suggested that even higher accuracy can be attained by integrating the results of two or more OCR systems with uncorrelated errors. There are several methods for integrating outputs, some of which had already been published. However, prior to integration, the individual characters or symbols from the various OCR outputs have to be synchronously tracked in order to compare them. Whereas it is simple to determine character correspondence among strings containing only substitution errors, the matching of strings of unequal length, which result when an OCR system generates insertion and deletion errors, is more complicated. Detecting loss of synchronization is made more difficult when consecutive errors occur. The length of the error burst must be determined or upper bounded before the error can be classified or synchronicity restored. This paper focuses on the tracking problem, and uses a dynamic programming search in n dimensions, where n is the number of OCR systems. The algorithm models the error generation process at each of the OCR systems and looks for the most probable combination of synchronized OCR outputs from the beginning to the end of all strings. The final output of the process (which can be the input to an integrator) is a series of n-tuples, each one containing exactly one output character (including nulls or deletions) from an OCR system.",
    "author": [
      {
        "family": "Concepcion",
        "given": "Vicente P."
      },
      {
        "family": "D’Amato",
        "given": "Donald P."
      }
    ],
    "container-title": "Proceedings of SPIE",
    "id": "Concepcion1993",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "ocr",
    "language": "en-US",
    "page": "218-228",
    "publisher": "The International Society for Optical Engineering",
    "title": "Synchronous tracking of outputs from multiple OCR systems",
    "type": "paper-conference",
    "volume": "1906"
  },
  {
    "URL": "http://wwwp.oakland.edu/Assets/Oakland/ais/files-and-documents/Issues-in-Interdisciplinary-Studies/volume-34/THE\\%20INTERDISCIPLINARY\\%20TURN\\%20IN\\%20THE\\%20ARTS\\%20AND\\%20HUMANITIES.pdf",
    "abstract": "This article suggests that the arts and humanities are in the midst of an interdisciplinary turn. This turn is a reaction to two problems: the transformation of universities in the twenty-first century and the challenges posed by postmodernism. The interdisciplinary turn, as identified here, is toward critical thinking in teaching and learning and toward critical interdisciplinarity in humanistic inquiry. Exposing the problems and highlighting the opportunities can help scholars, artists, and educators consciously apply these approaches and intentionally plan for interdisciplinarity, with the goals of fostering student learning and advancing scholarship. By understanding what is happening in this turn, scholars and educators in the arts and humanities can better lead interdisciplinarity into the future.",
    "author": [
      {
        "family": "Condee",
        "given": "William"
      }
    ],
    "container-title": "Issues in Interdisciplinary Studies",
    "id": "Condee2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "philosophy_of_science",
    "language": "en-US",
    "page": "12-29",
    "title": "The interdisciplinary turn in the arts and humanities",
    "type": "article-journal",
    "volume": "34"
  },
  {
    "DOI": "10.1093/comjnl/bxs068",
    "ISSN": "0010-4620",
    "abstract": "Computation can be seen as symbol manipulation. Indeed, a computation is a sequence of simple, welldefined steps that can lead to the solution of a problem. The problem itself must be defined exactly and unambiguously, and each step in the computation that solves the problem must be described in very specific terms.",
    "author": [
      {
        "family": "Conery",
        "given": "John S."
      }
    ],
    "container-title": "The Computer Journal",
    "id": "Conery2012",
    "issue": "7",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "formal_models",
    "language": "en-US",
    "page": "814-816",
    "title": "Computation is symbol manipulation",
    "type": "article-journal",
    "volume": "55"
  },
  {
    "abstract": "E-learning is at an exciting point in its development; its potential in terms of research is great and its impact on institutional practices is fully recognised. This book aims to define e-learning as a field of research, highlighting the complex issues, activities and tensions that characterize the area. Written by a team of experienced researchers and commented upon by internationally recognised experts, this book engages researchers and practitioners in critical discussion and debate of findings emerging from the field and the associated impact on practice. Key topics examined include: access and inclusion; the social-cultural contexts of e-learning; organisational structures, processes and identities; technical aspects of learning research - using tools and resources; approaches to learning and teaching practices and associated learning theories; designing for e-learning and the management of educational resources; professional roles and identities; the evolution of e-assessment; and, collaboration, motivation and educational evaluation.  provides a synthesis of research, giving a grounding in contemporary e-learning scholarship whilst identifying the debates that make it such a lively and fast-moving area. A landmark text in an evolving field, this book will prove invaluable for all researchers, practitioners, policy makers and students engaging with e-learning.",
    "collection-title": "Open and flexible learning series",
    "editor": [
      {
        "family": "Conole",
        "given": "Gráinne"
      },
      {
        "family": "Oliver",
        "given": "Martin"
      }
    ],
    "id": "Conole2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "e-learning, pedagogy",
    "publisher": "Routledge",
    "publisher-place": "Abingdon",
    "title": "Contemporary perspectives in e-learning research: Themes, methods and impact on practice",
    "title-short": "Contemporary perspectives in e-learning research",
    "type": "book"
  },
  {
    "DOI": "10.3233/sw-150177",
    "ISSN": "22104968",
    "abstract": "The availability in machine-readable form of descriptions of the structure of documents, as well as of the document discourse (e.g. the scientific discourse within scholarly articles), is crucial for facilitating semantic publishing and the overall comprehension of documents by both users and machines. In this paper we introduce DoCO, the Document Components Ontology, an OWL 2 DL ontology that provides a general-purpose structured vocabulary of document elements to describe both structural and rhetorical document components in RDF. In addition to giving a formal description of the ontology, this paper showcases its utility in practice in a variety of our own applications and other activities of the Semantic Publishing community that rely on DoCO to annotate and retrieve document components of scholarly articles.",
    "author": [
      {
        "family": "Constantin",
        "given": "Alexandru"
      },
      {
        "family": "Peroni",
        "given": "Silvio"
      },
      {
        "family": "Pettifer",
        "given": "Steve"
      },
      {
        "family": "Shotton",
        "given": "David"
      },
      {
        "family": "Vitali",
        "given": "Fabio"
      }
    ],
    "container-title": "Semantic Web",
    "id": "Constantin2016",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "document_engineering, scientific_publishing, semantic_web",
    "language": "en-US",
    "page": "167-181",
    "title": "The document components ontology (DoCO)",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "DOI": "10.1002/spe.422",
    "abstract": "Incremental parsing has long been recognized as a technique of great utility in the construction of language-based editors, and correspondingly, the area currently enjoys a mature theory. Unfortunately, many practical considerations have been largely overlooked in previously published algorithms. Many user requirements for an editing system necessarily impact on the design of its incremental parser, but most approaches focus only on one: response time. This paper details an incremental parser based on LR parsing techniques and designed for use in a modeless syntax recognition editor. The nature of this editor places significant demands on the structure and quality of the document representation it uses, and hence, on the parser. The strategy presented here is novel in that both the parser and the representation it constructs are tolerant of the inevitable and frequent syntax errors that arise during editing. This is achieved by a method that differs from conventional error repair techniques, and that is more appropriate for use in an interactive context. Furthermore, the parser aims to minimize disturbance to this representation, not only to ensure other system components can operate incrementally, but also to avoid unfortunate consequences for certain user-oriented services. The algorithm is augmented with a limited form of predictive tree-building, and a technique is presented for the determination of valid symbols for menu-based insertion. Copyright © 2001 John Wiley & Sons, Ltd.",
    "author": [
      {
        "family": "Cook",
        "given": "Phil"
      },
      {
        "family": "Welsh",
        "given": "Jim"
      }
    ],
    "container-title": "Software: Practice and Experience",
    "id": "Cook2001",
    "issue": "15",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "interactive_editing, interactive_parsing",
    "language": "en-US",
    "page": "1461-1486",
    "publisher-place": "Software Verification Research Centre, School of Computer Science and Electrical Engineering, The University of Queensland, Brisbane, QLD 4072, Australia",
    "title": "Incremental parsing in language-based editors: User needs and how to meet them",
    "title-short": "Incremental parsing in language-based editors",
    "type": "article-journal",
    "volume": "31"
  },
  {
    "DOI": "10.3917/lang.171.0095",
    "ISSN": "0458-726X",
    "abstract": "Depuis ses origines, le traitement automatique des langues (TAL) est soumis à des tensions, à des oppositions de diverses sortes (voir Cori & Léon 2002). Parmi ces oppositions, il y a l’opposition entre objectifs théoriques et visées pratiques, ainsi que l’opposition entre méthodes symboliques et méthodes quantitatives ou numériques. Depuis le début des années 1990, les visées pratiques ont pris un poids supérieur. Il en est résulté qu’une importance accrue a été accordée aux méthodes numériques, c’est-à-dire aux méthodes statistiques et probabilistes (regroupées parfois sous le terme de méthodes stochastiques). En prolongement de ces travaux, qui ont donné lieu à des outils pratiques dont il serait difficile de nier l’intérêt, certains chercheurs ont voulu tirer des conséquences sur les fondements et les méthodes de la linguistique. L’objet de cet article est de s’interroger sur le lien entre ces conceptions (nouvelles ?) apparues en linguistique et les méthodes numériques en traitement automatique. Dans une première partie, on met en évidence l’opposition entre deux grands types de méthodes en TAL, méthodes que l’on peut regrouper sous les étiquettes de « TAL robuste » et de « TAL théorique ». Dans une deuxième partie, on examine de plus près les méthodes stochastiques en essayant d’en dégager les présupposés théoriques sous-jacents. Dans la troisième et dernière partie, on montre comment un glissement a pu être opéré entre méthodes de traitement automatique (pratiques) et affirmations de conceptions théoriques sur le langage. Il en découle une interrogation sur le statut épistémologique du TAL stochastique, et plus globalement du TAL, de sa place par rapport à la linguistique.",
    "author": [
      {
        "family": "Cori",
        "given": "Marcel"
      }
    ],
    "container-title": "Langages",
    "id": "Cori2008",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "computational_linguistics",
    "language": "fr-FR",
    "page": "95+",
    "title": "Des méthodes de traitement automatique aux linguistiques fondées sur les corpus",
    "type": "article-journal",
    "volume": "171"
  },
  {
    "DOI": "10.1023/a:1023860521975",
    "abstract": "In this paper we develop novel algorithmic ideas for building a natural language parser grounded upon the hypothesis of incrementality. Although widely accepted and experimentally supported under a cognitive perspective as a model of the human parser, the incrementality assumption has never been exploited for building automatic parsers of unconstrained real texts. The essentials of the hypothesis are that words are processed in a left-to-right fashion, and the syntactic structure is kept totally connected at each step. Our proposal relies on a machine learning technique for predicting the correctness of partial syntactic structures that are built during the parsing process. A recursive neural network architecture is employed for computing predictions after a training phase on examples drawn from a corpus of parsed sentences, the Penn Treebank. Our results indicate the viability of the approach and lay out the premises for a novel generation of algorithms for natural language processing which more closely model human parsing. These algorithms may prove very useful in the development of efficient parsers.",
    "author": [
      {
        "family": "Costa",
        "given": "Fabrizio"
      },
      {
        "family": "Frasconi",
        "given": "Paolo"
      },
      {
        "family": "Lombardo",
        "given": "Vincenzo"
      },
      {
        "family": "Soda",
        "given": "Giovanni"
      }
    ],
    "id": "Costa2003",
    "issue": "1-2",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "interactive_parsing",
    "language": "en-US",
    "page": "9-25",
    "publisher": "Kluwer Academic Publishers",
    "title": "Towards incremental parsing of natural language using recursive neural networks",
    "type": "article-journal",
    "volume": "19"
  },
  {
    "ISBN": "978-1-932432-29-9",
    "URL": "http://portal.acm.org/citation.cfm?id=1596407",
    "abstract": "This paper presents a method for automatic topic identification using an encyclopedic graph derived from Wikipedia. The system is found to exceed the performance of previously proposed machine learning algorithms for topic identification, with an annotation consistency comparable to human annotations.",
    "author": [
      {
        "family": "Coursey",
        "given": "Kino"
      },
      {
        "family": "Mihalcea",
        "given": "Rada"
      },
      {
        "family": "Moen",
        "given": "William"
      }
    ],
    "container-title": "Proceedings of the thirteenth conference on computational natural language learning (CoNLL ’09)",
    "id": "Coursey2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "topic_modeling, wikipedia",
    "language": "en-US",
    "page": "210-218",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Using encyclopedic knowledge for automatic topic identification",
    "type": "paper-conference"
  },
  {
    "ISSN": "0891-2017",
    "URL": "http://acl.ldc.upenn.edu/J/J96/J96-4002.pdf",
    "abstract": "The first step in applying the comparative method to a pair of words suspected of being cognate is to align the segments of each word that appear to correspond. Finding the right alignment may require searching. For example, Latin dō ’I give’ lines up with the middle dō in Greek didōmi, not the initial di.This paper presents an algorithm for finding probably correct alignments on the basis of phonetic similarity. The algorithm consists of an evaluation metric and a guided search procedure. The search algorithm can be extended to implement special handling of metathesis, assimilation, or other phenomena that require looking ahead in the string, and can return any number of alignments that meet some criterion of goodness, not just the one best. It can serve as a front end to computer implementations of the comparative method.",
    "author": [
      {
        "family": "Covington",
        "given": "Michael A."
      }
    ],
    "container-title": "Computational Linguistics",
    "id": "Covington1996",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "cultural_heritage, nlp",
    "language": "en-US",
    "page": "481-496",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "An algorithm to align words for historical comparison",
    "type": "article-journal",
    "volume": "22"
  },
  {
    "DOI": "10.3115/980451.980890",
    "URL": "http://www.aclweb.org/anthology-new/C/C98/C98-1043.pdf",
    "abstract": "An essential step in comparative reconstruction is to align corresponding phonological segments in the words being compared. To do this, one must search among huge numbers of potential alignments to find those that give a good phonetic fit. This is a hard computational problem, and it becomes exponentially more difficult when more than two strings are being aligned. In this paper I extend the guided-search alignment algorithm of Covington (Computational Linguistics, 1996) to handle more than two strings. The resulting algorithm has been implemented in Prolog and gives reasonable results when tested on data from several languages.",
    "author": [
      {
        "family": "Covington",
        "given": "Michael A."
      }
    ],
    "container-title": "COLING-ACL ’98: Proceedings of the 36<sup>th</sup> annual meeting of the association for computational linguistics and 17<sup>th</sup> international conference on computational linguistics",
    "id": "Covington1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "cultural_heritage, nlp",
    "language": "en-US",
    "page": "275-279",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Morristown, NJ, USA",
    "title": "Alignment of multiple languages for historical comparison",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/b97670",
    "ISBN": "0-387-98767-3",
    "abstract": "Probabilistic expert systems are graphical networks that support the modelling of uncertainty and decisions in large complex domains, while retaining ease of calculation. Building on original research by the authors over a number of years, this book gives a thorough and rigorous mathematical treatment of the underlying ideas, structures, and algorithms, emphasizing those cases in which exact answers are obtainable. It covers both the updating of probabilistic uncertainty in the light of new evidence, and statistical inference, about unknown probabilities or unknown model structure, in the light of new data. The careful attention to detail will make this work an important reference source for all those involved in the theory and applications of probabilistic expert systems. This book was awarded the first DeGroot Prize by the International Society for Bayesian Analysis for a book making an important, timely, thorough, and notably original contribution to the statistics literature.",
    "author": [
      {
        "family": "Cowell",
        "given": "Robert G."
      },
      {
        "family": "Dawid",
        "given": "A. Philip"
      },
      {
        "family": "Lauritzen",
        "given": "Steffen L."
      },
      {
        "family": "Spiegelhalter",
        "given": "David J."
      }
    ],
    "collection-title": "Information science and statistics",
    "id": "Cowell1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "knowledge_representation, uncertainty",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "New York",
    "title": "Probabilistic networks and expert systems",
    "type": "book"
  },
  {
    "DOI": "10.1147/sj.234.0326",
    "ISSN": "0018-8670",
    "abstract": "One way of classifying computer languages is by two classes: languages needing skilled programmers, and personal languages used by an expanding population of general users. REstructured eXtended eXecutor (REXX) is a flexible personal language designed with particular attention to feedback from its users. It has proved to be effective and easy to use, yet it is sufficiently general and powerful to fulfill the needs of many demanding professional applications. REXX is system and hardware independent, so that it has been possible to integrate it experimentally into several operating systems. Here REXX is used for such purposes as command and macro programming, prototyping, education, and personal programming. This paper introduces REXX and describes the basic design principles that were followed in developing it.",
    "author": [
      {
        "family": "Cowlishaw",
        "given": "Michael F."
      }
    ],
    "container-title": "IBM Systems Journal",
    "id": "Cowlishaw1984",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1984
        ]
      ]
    },
    "keyword": "programming",
    "language": "en-US",
    "page": "326-335",
    "title": "The design of the REXX language",
    "type": "article-journal",
    "volume": "23"
  },
  {
    "author": [
      {
        "family": "Cox",
        "given": "Kevin"
      },
      {
        "family": "Clark",
        "given": "David"
      }
    ],
    "container-title": "Computers & Education",
    "id": "Cox1998",
    "issue": "3/4",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "page": "157-167",
    "title": "The use of formative quizzes for deep learning",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "DOI": "10.1093/llc/fqp033",
    "abstract": "The authors have worked over several years on a software tool to make word counts from an archive of old-spelling early modern English plays and poems. In this article we present the outcome, a computational model for dealing automatically with variant spelling, implemented in an application which we call an “Intelligent Archive”. We also reflect on the perspective on Early Modern English, and on the probabilistic aspect of language in general, gained from working through the practical problems which arose in establishing the model.",
    "author": [
      {
        "family": "Craig",
        "given": "Hugh"
      },
      {
        "family": "Whipp",
        "given": "R."
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Craig2010",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2010,
          4
        ]
      ]
    },
    "keyword": "cultural_heritage, english, spelling_correction",
    "language": "en-US",
    "page": "37-52",
    "title": "Old spellings, new methods: Automated procedures for indeterminate linguistic data",
    "title-short": "Old spellings, new methods",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "author": [
      {
        "family": "Crane",
        "given": "Gregory"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Crane1991",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "language": "en-US",
    "page": "243-245",
    "title": "Generating and parsing classical Greek",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "DOI": "10.1007/3-540-45747-X_47",
    "ISBN": "978-3-540-44178-6",
    "abstract": "This paper describes preliminary conclusions from a long-term study of cultural heritage digital collections. First, those features most important to cultural heritage digital libraries are described. Second, we list those components that have proven most useful in boot-strapping new collections.",
    "author": [
      {
        "family": "Crane",
        "given": "Gregory"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Research and advanced technology for digital libraries",
    "editor": [
      {
        "family": "Agosti",
        "given": "Maristella"
      },
      {
        "family": "Thanos",
        "given": "Costantino"
      }
    ],
    "id": "Crane2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "cultural_heritage, ir, nlp",
    "language": "en-US",
    "page": "51-60",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Cultural heritage digital libraries: Needs and components",
    "title-short": "Cultural heritage digital libraries",
    "type": "chapter",
    "volume": "2458"
  },
  {
    "ISBN": "0-7695-1939-3",
    "URL": "http://portal.acm.org/citation.cfm?id=827140.827150",
    "abstract": "This paper surveys research areas relevant to cultural heritage digital libraries. The emerging National Science Digital Library promises to establish the foundation on which those of us beyond the scientific and engineering community will likely build. This paper thus articulates the particular issues that we have encountered in developing cultural heritage collections. We provide a broad overview of audiences, collections, and services.",
    "author": [
      {
        "family": "Crane",
        "given": "Gregory"
      },
      {
        "family": "Wulfman",
        "given": "Clifford"
      }
    ],
    "container-title": "JCDL ’03: Proceedings of the 3rd ACM/IEEE-CS joint conference on digital libraries",
    "id": "Crane2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_library",
    "language": "en-US",
    "page": "75-86",
    "publisher": "IEEE Computer Society",
    "publisher-place": "Washington, DC, USA",
    "title": "Towards a cultural heritage digital library",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s00799-007-0031-8",
    "ISSN": "1432-5012",
    "abstract": "Humanists face problems that are comparable to their colleagues in the sciences. Like scientists, humanists have electronic sources and datasets that are too large for traditional labor intensive analysis. They also need to work with materials that presuppose more background knowledge than any one researcher can master: no one can, for example, know all the languages needed for subjects that cross multiple disciplines. Unlike their colleagues in the sciences, however, humanists have relatively few resources with which to develop this new infrastructure. They must therefore systematically cultivate alliances with better funded disciplines, learning how to build on emerging infrastructure from other disciplines and, where possible, contributing to the design of a cyberinfrastructure that serves all of academia, including the humanities.",
    "author": [
      {
        "family": "Crane",
        "given": "Gregory"
      },
      {
        "family": "Babeu",
        "given": "Alison"
      },
      {
        "family": "Bamman",
        "given": "David"
      }
    ],
    "container-title": "International Journal on Digital Libraries",
    "id": "Crane2007",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2007,
          10
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_library",
    "language": "en-US",
    "page": "117-122",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "eScience and the humanities",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "abstract": "This book fills the gap in the book literature on algorithms on words, and brings together the many results presently dispersed in the masses of journal articles.",
    "author": [
      {
        "family": "Crochemore",
        "given": "Maxime"
      },
      {
        "family": "Rytter",
        "given": "Wojciech"
      }
    ],
    "id": "Crochemore2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "programming, text_processing",
    "language": "en-US",
    "publisher": "World Scientific",
    "publisher-place": "Hackensack, NJ, USA",
    "title": "Jewels of stringology: Text algorithms",
    "title-short": "Jewels of stringology",
    "type": "book"
  },
  {
    "ISSN": "0169-2968",
    "abstract": "Two algorithms are presented that solve the problem of recovering the longest common subsequence of two strings. The first algorithm is an improvement of Hirschberg’s divide-and-conquer algorithm. The second algorithm is an improvement of Hunt-Szymanski algorithm based on an efficient computation of all dominant match points. These two algorithms use bit-vector operations and are shown to work very efficiently in practice.",
    "author": [
      {
        "family": "Crochemore",
        "given": "Maxime"
      },
      {
        "family": "Ilipoulos",
        "given": "Costas S."
      },
      {
        "family": "Pinzon",
        "given": "Yoan J."
      }
    ],
    "container-title": "Fundamenta Informaticae",
    "id": "Crochemore2002b",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "approximate_matching",
    "language": "en-US",
    "page": "89-103",
    "publisher": "IOS Press",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "Speeding-up Hirschberg and Hunt-Szymanski LCS algorithms",
    "type": "article-journal",
    "volume": "56"
  },
  {
    "DOI": "10.1007/11671299_45",
    "URL": "http://www.cs.unt.edu/\\~{}rada/papers/csomai.cicling06.pdf",
    "abstract": "The automatic generation of back-of-the book indexes seems to be out of sight of the Information Retrieval and Natural Language Processing communities, although the increasingly large number of books available in electronic format, as well as recent advances in keyphrase extraction, should motivate an increased interest in this topic. In this paper, we describe the background relevant to the process of creating back-of-the-book indexes, namely (1) a short overview of the origin and structure of back-of-the-book indexes, and (2) the correspondence that can be established between techniques for automatic index construction and keyphrase extraction. Since the development of any automatic system requires in the first place an evaluation testbed, we describe our work in building a gold standard collection of books and indexes, and we present several metrics that can be used for the evaluation of automatically generated indexes against the gold standard. Finally, we investigate the properties of the gold standard index, such as index size, length of index entries, and upper bounds on coverage as indicated by the presence of index entries in the document.",
    "author": [
      {
        "family": "Csomai",
        "given": "Andras"
      },
      {
        "family": "Mihalcea",
        "given": "Rada"
      }
    ],
    "container-title": "Computational Linguistics and Intelligent Text Processing",
    "id": "Csomai2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage, indices, nlp",
    "language": "en-US",
    "page": "429-440",
    "title": "Creating a testbed for the evaluation of automatically generated Back-of-the-Book indexes",
    "type": "chapter"
  },
  {
    "URL": "http://aclweb.org/anthology/P08-1106",
    "abstract": "In this paper we present a supervised method for back-of-the-book index construction. We introduce a novel set of features that goes beyond the typical frequency-based analysis, including features based on discourse comprehension, syntactic patterns, and information drawn from an online encyclopedia. In experiments carried out on a book collection, the method was found to lead to an improvement of roughly 140% as compared to an existing state-of-the-art supervised method.",
    "author": [
      {
        "family": "Csomai",
        "given": "Andras"
      },
      {
        "family": "Mihalcea",
        "given": "Rada"
      }
    ],
    "container-title": "Proceedings of ACL-08: HLT",
    "id": "Csomai2008",
    "issued": {
      "date-parts": [
        [
          2008,
          6
        ]
      ]
    },
    "keyword": "indices, nlp, wikipedia",
    "language": "en-US",
    "page": "932-940",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Columbus, Ohio",
    "title": "Linguistically motivated features for enhanced Back-of-the-Book indexing",
    "type": "paper-conference"
  },
  {
    "abstract": "Translation shifts can be informative in various ways. Amongst other things, they can point to typological differences between languages or be indicators of properties of translated text like e.g. explicitation or normalisation. Detecting translation shifts in parallel corpora is thus a major task from the viewpoint of translation studies. This paper presents an analysis of translation shifts in a parallel corpus (English-German). It offers an operationalisation of queries which can exploit multi-layer annotation and alignment in order to detect various kinds of translation shifts across category boundary lines and empty alignment links. The paper furthermore discusses the shifts and links them to certain translation properties.",
    "author": [
      {
        "family": "Čulo",
        "given": "Oliver"
      },
      {
        "family": "Hansen-Schirra",
        "given": "Silvia"
      },
      {
        "family": "Maksymski",
        "given": "Karin"
      },
      {
        "family": "Neumann",
        "given": "Stella"
      }
    ],
    "container-title": "Translation: Computation, Corpora, Cognition",
    "id": "Culo2011",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "corpus_linguistics, xml, xquery",
    "language": "en-US",
    "page": "75-104",
    "title": "Empty links and crossing lines: Querying multi-layer annotation and alignment in parallel corpora",
    "title-short": "Empty links and crossing lines",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.1016/j.compcom.2006.08.002",
    "abstract": "This article explores the connection between computer programming (coding) and traditional composition. It first looks at how a discussion about adopting the open source community’s copyleft publishing model suggests a deeper parallel between coding and composition than has been previously acknowledged. By repositioning the rhetorical triangle as a coding triangle, the article argues that the act of writing programs for a machine informs the process of constructing an audience, in traditional composition. To better inform the act of how a traditional writer invokes an audience, the article summarizes how Walter J. Ong has characterized this process. The article then examines how Claudia Herbst’s portrayal of the cultural power of computer code raises questions as to how computer users similarly invoke an author. It also briefly considers how computer programming texts have characterized coding as an act of writing. Next the parallels between coding and composition and their treatment thus far in composition literature and new media theory are considered. The article considers Alfred Kern’s work in employing BASIC programming to teach grammar and composition and then offers suggestions for thinking of new ways that a knowledge of coding can inform the teaching of writing. This article concludes with guidelines for writing teachers who wish to incorporate computer coding into their curriculum.",
    "author": [
      {
        "family": "Cummings",
        "given": "Robert E."
      }
    ],
    "container-title": "Computers and Composition",
    "id": "Cummings2006",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "programming, wrabii, writing_research",
    "language": "en-US",
    "page": "430-443",
    "title": "Coding with power: Toward a rhetoric of computer coding and composition",
    "title-short": "Coding with power",
    "type": "article-journal",
    "volume": "23"
  },
  {
    "DOI": "10.1093/llc/fqp019",
    "abstract": "This article reports on the details behind a poster presented a the Text Encoding Initiative (TEI) Members’ Meeting at the University of Maryland, College Park, in November 2007. It looks at the creation of of af scholarly electronic edition of a late-medieval play, The Conversion of Saint Paul from Bodleian MS Digby 133 using TEI P5 XML. In addition to exploring various new features available in the TEI P5 Guidelines, it also examines the methodology used to create the text, up-scaling from purely presentation markup to descriptive markup, and how this might simplify the creation of such editions. In an attempt to create an interoperable, flexible, and agile edition, it stores anything not directly related to the transcription of the text in separate files in a stand-off manner. In an attempt to experiment with creating a resource which leverages the advantages of networked editions, it documents the attempt to interoperate with the Middle English Dictionary. Although this first appears to be a failure, it highlights some of the inherent problems in attempting to build editions that are dependent on the resources of others. The article concludes with an urge to text encoders to make more of an effort to share examples of, both good and bad, community practice.",
    "author": [
      {
        "family": "Cummings",
        "given": "James"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Cummings2009",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, tei, xml",
    "language": "en-US",
    "page": "307-317",
    "title": "Converting Saint Paul: A new TEI P5 edition of The Conversion of Saint Paul using stand-off methodology",
    "title-short": "Converting Saint Paul",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "author": [
      {
        "family": "Cunningham",
        "given": "Hamish"
      },
      {
        "family": "Maynard",
        "given": "Diana"
      },
      {
        "family": "Bontcheva",
        "given": "Kalina"
      },
      {
        "family": "Tablan",
        "given": "Valentin"
      }
    ],
    "container-title": "Proceedings of the 40<sup>th</sup> anniversary meeting of the association for computational linguistics",
    "id": "Cunningham2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "title": "GATE: A framework and graphical development environment for robust NLP tools and applications",
    "title-short": "GATE",
    "type": "paper-conference"
  },
  {
    "DOI": "10.3115/1067807.1067821",
    "ISBN": "1-333-56789-0",
    "abstract": "This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar.",
    "author": [
      {
        "family": "Curran",
        "given": "James R."
      },
      {
        "family": "Clark",
        "given": "Stephen"
      }
    ],
    "container-title": "Proceedings of the 10<sup>th</sup> conference of the european chapter of the association for computational linguistics (EACL 2003)",
    "id": "Curran2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "english, pos_tagging",
    "language": "en-US",
    "page": "91-98",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Investigating GIS and smoothing for maximum entropy taggers",
    "type": "paper-conference",
    "volume": "1"
  },
  {
    "DOI": "10.1145/1860559.1860589",
    "ISBN": "978-1-4503-0231-9",
    "abstract": "High quality conversions of scanned documents into PDF usually either rely on full OCR or token compression. This paper describes an approach intermediate between those two: it is based on token clustering, but additionally groups tokens into candidate fonts. Our approach has the potential of yielding OCR-like PDFs when the inputs are high quality and degrading to token based compression when the font analysis fails, while preserving full visual fidelity. Our approach is based on an unsupervised algorithm for grouping tokens into candidate fonts. The algorithm constructs a graph based on token proximity and derives token groups by partitioning this graph. In initial experiments on scanned 300 dpi pages containing multiple fonts, this technique reconstructs candidate fonts with 100% accuracy.",
    "author": [
      {
        "family": "Cutter",
        "given": "Michael P."
      },
      {
        "dropping-particle": "van",
        "family": "Beusekom",
        "given": "Joost"
      },
      {
        "family": "Shafait",
        "given": "Faisal"
      },
      {
        "family": "Breuel",
        "given": "Thomas M."
      }
    ],
    "collection-title": "DocEng ’10",
    "container-title": "Proceedings of the 10th ACM symposium on document engineering",
    "id": "Cutter2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "language": "en-US",
    "page": "143-150",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Unsupervised font reconstruction based on token co-occurrence",
    "type": "paper-conference"
  },
  {
    "ISBN": "978-0-9565793-0-0",
    "URL": "http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/",
    "id": "DBLP:conf/dihu/2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "publisher": "ADHO",
    "title": "Digital humanities 2010 conference abstracts, king’s college london, london, england, UK, july 7-10, 2010",
    "type": "book"
  },
  {
    "URL": "https://dh2014.org/program/abstracts/",
    "id": "DBLP:conf/dihu/2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "publisher": "Alliance of Digital Humanities Organizations (ADHO)",
    "title": "Digital humanities 2014, DH 2014, conference abstracts, EPFL - UNIL, lausanne, switzerland, 8-12 july 2014",
    "type": "book"
  },
  {
    "DOI": "10.1086/702594",
    "author": [
      {
        "family": "Da",
        "given": "Nan Z."
      }
    ],
    "container-title": "Critical Inquiry",
    "id": "Da2019",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "601-639",
    "title": "The computational case against computational literary studies",
    "type": "article-journal",
    "volume": "45"
  },
  {
    "URL": "https://hal.archives-ouvertes.fr/hal-01228945",
    "abstract": "Réalisée par Marin Dacos et Pierre Mounier, animateurs du Centre pour l’édition électronique ouverte (acteur majeur en France dans ce domaine à travers le portail Open Edition, palette d’instruments bien connue des chercheurs), Humanités numériques – État des lieux et positionnement de la recherche française dans le contexte international trace les contours d’un champ en pleine expansion. Qu’il s’agisse de publier en ligne de vastes fonds d’archives, d’analyser en direct l’impact des réseaux sociaux sur les mobilisations militantes ou de mettre au travail une communauté d’internautes sur des pans entiers du patrimoine culturel, le tournant numérique concerne aujourd’hui tous les secteurs de la recherche sur la société, les œuvres et la culture : l’étude ici publiée éclaire de manière précise et accessible cette transformation globale.",
    "author": [
      {
        "family": "Dacos",
        "given": "Marin"
      },
      {
        "family": "Mounier",
        "given": "Pierre"
      }
    ],
    "id": "Dacos2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "fr-FR",
    "publisher": "Institut français",
    "publisher-place": "Paris",
    "title": "Humanités numériques: État des lieux et positionnement de la recherche française dans le contexte international",
    "title-short": "Humanités numériques",
    "type": "report"
  },
  {
    "URL": "http://aclweb.org/anthology/W96-0102",
    "abstract": "We introduce a memory-based approach to part of speech tagging. Memory-based learning is a form of supervised learning based on similarity-based reasoning. The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory. Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger. Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably. Memory-based tagging shares this advantage with other statistical or machine learning approaches. Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging. In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using IGTree, a tree-based formalism for indexing and searching huge case bases. The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed.",
    "author": [
      {
        "family": "Daelemans",
        "given": "Walter"
      },
      {
        "family": "Zavrel",
        "given": "Jakub"
      },
      {
        "family": "Berck",
        "given": "Peter"
      },
      {
        "family": "Gillis",
        "given": "Steven"
      }
    ],
    "container-title": "Proceedings of the fourth workshop on very large corpora",
    "editor": [
      {
        "family": "Ejerhed",
        "given": "Eva"
      },
      {
        "family": "Dagan",
        "given": "Ido"
      }
    ],
    "id": "Daelemans1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "pos_tagging",
    "language": "en-US",
    "page": "14-27",
    "title": "MBT: A memory-based part of speech tagger-generator",
    "title-short": "MBT",
    "type": "paper-conference"
  },
  {
    "abstract": "Memory-based language processing–a machine learning and problem solving method for language technology–is based on the idea that the direct re-use of examples using analogical reasoning is more suited for solving language processing problems than the application of rules extracted from those examples. This book discusses the theory and practice of memory-based language processing, showing its comparative strengths over alternative methods of language modelling. Language is complex, with few generalizations, many sub-regularities and exceptions, and the advantage of memory-based language processing is that it does not abstract away from this valuable low-frequency information.",
    "author": [
      {
        "family": "Daelemans",
        "given": "Walter"
      },
      {
        "dropping-particle": "van den",
        "family": "Bosch",
        "given": "Antal"
      }
    ],
    "collection-title": "Studies in natural language processing",
    "id": "Daelemans2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "interactive_parsing, nlp",
    "language": "en-US",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge, UK",
    "title": "Memory-based language processing",
    "type": "book"
  },
  {
    "DOI": "10.1109/MIC.2007.70",
    "abstract": "The learning management system (LMS) has dominated Internet-based education for the past two decades. However, the traditional LMS is failing to keep pace with advances in Internet technologies and social interactions online. To support technological diversity, current frameworks such as the E-Learning Framework, the IMS Abstract Framework, and the Open Knowledge Initiative have defined the initial steps toward service-oriented e-learning platforms. Here, the authors discuss LMS evolution and present core challenges that must be addressed to achieve information interoperability in next-generation e-learning platforms.",
    "author": [
      {
        "family": "Dagger",
        "given": "Declan"
      },
      {
        "family": "O’Connor",
        "given": "Alexander"
      },
      {
        "family": "Lawless",
        "given": "Séamus"
      },
      {
        "family": "Walsh",
        "given": "Eddie"
      },
      {
        "family": "Wade",
        "given": "Vincent P."
      }
    ],
    "container-title": "IEEE Internet Computing",
    "id": "Dagger2007",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "28-35",
    "title": "Service-oriented e-learning platforms: From monolithic systems to flexible services",
    "title-short": "Service-oriented e-learning platforms",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "author": [
      {
        "family": "Dale",
        "given": "Robert"
      }
    ],
    "chapter-number": "2",
    "container-title": "Recent developments and applications of natural language processing",
    "editor": [
      {
        "family": "Peckham",
        "given": "Jeremy"
      }
    ],
    "id": "Dale1989",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "interactive_editing, authoring",
    "page": "8-22",
    "publisher": "Kogan Page",
    "publisher-place": "London",
    "title": "Computer-based editorial aids",
    "type": "chapter"
  },
  {
    "URL": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=190756",
    "abstract": "The problem of errors in text is particularly severe in the publishing world, where high standards are required. The editor’s job often involves extensive rewriting and high-level reorganisation of a text, but the most time consuming tasks are copy editing and proofreading. The paper describes a system whose purpose is to assist a human writer or editor in massaging a text to deal with these kinds of errors. Rules of grammar, style, punctuation and usage can be maintained as rules in a knowledge base, distinct from the mechanism that applies those rules against a text. The research described in the paper is concerned with assisting an editor in ensuring the correctness of low-level matters such as the use of punctuation, the format of numbers and numerical values, and the use of abbreviations. The Editor’s Assistant operates by interactively detecting and, where possible, offering corrections for those aspects of a text which do not conform to the rules of style embodied in the knowledge base in use",
    "author": [
      {
        "family": "Dale",
        "given": "Robert"
      }
    ],
    "container-title": "Hypertext, IEE colloquium on",
    "id": "Dale1990",
    "issued": {
      "date-parts": [
        [
          1990,
          11,
          2
        ]
      ]
    },
    "keyword": "authoring, interactive_editing",
    "language": "en-US",
    "page": "3/1-3/3",
    "title": "Automating editorial assistance",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Dale",
        "given": "Robert"
      },
      {
        "family": "Douglas",
        "given": "Shona"
      }
    ],
    "chapter-number": "8",
    "container-title": "The new writing environment: Writers at work in a world of technology",
    "editor": [
      {
        "family": "Sharples",
        "given": "Mike"
      },
      {
        "dropping-particle": "van der",
        "family": "Geest",
        "given": "Thea"
      }
    ],
    "id": "Dale1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "editing, grammar_checking, style_checking, authoring",
    "page": "123-145",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Two investigations into intelligent text processing",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/971375",
    "author": [
      {
        "family": "Daly",
        "given": "Charlie"
      },
      {
        "family": "Waldron",
        "given": "John"
      }
    ],
    "container-title": "Proceedings of the 35<sup>th</sup> SIGCSE technical symposium on computer science education, SIGCSE 2004, norfolk, virginia, USA, march 3-7, 2004",
    "editor": [
      {
        "family": "Joyce",
        "given": "Dan"
      },
      {
        "family": "Knox",
        "given": "Deborah"
      },
      {
        "family": "Dann",
        "given": "Wanda"
      },
      {
        "family": "Naps",
        "given": "Thomas L."
      }
    ],
    "id": "Daly2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "210-213",
    "publisher": "ACM",
    "title": "Assessing the assessment of programming ability",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1047344.1047473",
    "author": [
      {
        "family": "Daly",
        "given": "Charlie"
      },
      {
        "family": "Horgan",
        "given": "Jane"
      }
    ],
    "container-title": "SIGCSE ’05: Proceedings of the 36th SIGCSE technical symposium on computer science education",
    "id": "Daly2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "page": "383-387",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Patterns of plagiarism",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Dalziel",
        "given": "James"
      }
    ],
    "container-title": "Proceedings of international computer assisted assessment conference",
    "id": "Dalziel2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "language": "en-US",
    "title": "Enhancing web-based learning with computer assisted assessment: Pedagogical and technical considerations",
    "title-short": "Enhancing web-based learning with computer assisted assessment",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/363958.363994",
    "ISSN": "0001-0782",
    "abstract": "The method described assumes that a word which cannot be found in a dictionary has at most one error, which might be a wrong, missing or extra letter or a single transposition. The unidentified input word is compared to the dictionary again, testing each time to see if the words match—assuming one of these errors occurred. During a test run on garbled text, correct identifications were made for over 95 percent of these error types.",
    "author": [
      {
        "family": "Damerau",
        "given": "Fred J."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Damerau1964",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1964
        ]
      ]
    },
    "keyword": "classic, spelling_correction",
    "language": "en-US",
    "page": "171-176",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A technique for computer detection and correction of spelling errors",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "ISBN": "0195079930",
    "abstract": "The World’s Writing Systems meets the need for a definitive volume on the major historical and modern writing systems of the world. Comprising more than eighty articles contributed by expert scholars in the field, the work is organized in twelve units, each dealing with a particular group of writing systems defined historically, geographically, or conceptually. Each unit begins with an introductory article providing the social and cultural context in which the group of writing systems was created and developed. Articles on individual scripts detail the historical origin of the writing system in question, its structure (with tables showing the forms of the written symbols), and its relationship to the phonology of the corresponding spoken language. Each writing system is illustrated by a passage of text, accompanied by a romanized version, a phonetic transcription, and a modern English translation. Each article concludes with a bibliography. Units are arranged according to the chronological development of writing systems and their historical relationship within geographical areas. First, there is a discussion of the earliest scripts of the ancient Near East. Subsequent units focus on the scripts of East Asia, the writing systems of Europe, Asia, and Africa that have descended from ancient West Semitic (\"Phoenician\"), and the scripts of South and Southeast Asia. Other units deal with the recent and ongoing process of decipherment of ancient writing systems; the adaptation of traditional scripts to new languages; new scripts invented in modern times; and graphic systems for numerical, music, and movement notation. The result is a comprehensive resource of all of the major writing systems of the world.",
    "editor": [
      {
        "family": "Daniels",
        "given": "Peter T."
      },
      {
        "family": "Bright",
        "given": "William"
      }
    ],
    "id": "Daniels1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "orthography, writing_systems",
    "language": "en-US",
    "publisher": "Oxford University Press",
    "publisher-place": "New York, NY, USA",
    "title": "The world’s writing systems",
    "type": "book"
  },
  {
    "DOI": "10.1016/j.websem.2011.08.005",
    "ISSN": "15708268",
    "PMCID": "PMC3293483",
    "PMID": "22408576",
    "abstract": "One of the key promises of the Semantic Web is its potential to enable and facilitate data interoperability. The ability of data providers and application developers to share and reuse ontologies is a critical component of this data interoperability: if different applications and data sources use the same set of well defined terms for describing their domain and data, it will be much easier for them to “talk” to one another. Ontology libraries are the systems that collect ontologies from different sources and facilitate the tasks of finding, exploring, and using these ontologies. Thus ontology libraries can serve as a link in enabling diverse users and applications to discover, evaluate, use, and publish ontologies. In this paper, we provide a survey of the growing—and surprisingly diverse—landscape of ontology libraries. We highlight how the varying scope and intended use of the libraries affects their features, content, and potential exploitation in applications. From reviewing 11 ontology libraries, we identify a core set of questions that ontology practitioners and users should consider in choosing an ontology library for finding ontologies or publishing their own. We also discuss the research challenges that emerge from this survey, for the developers of ontology libraries to address.",
    "author": [
      {
        "family": "Aquin",
        "given": "Mathieu",
        "non-dropping-particle": "d’"
      },
      {
        "family": "Noy",
        "given": "Natalya F."
      }
    ],
    "container-title": "Web Semantics: Science, Services and Agents on the World Wide Web",
    "id": "Daquin2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "semantic_web, vocabularies",
    "language": "en-US",
    "page": "96-111",
    "title": "Where to publish and find ontologies? A survey of ontology libraries",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "author": [
      {
        "family": "Davidson",
        "given": "Cathy N."
      }
    ],
    "chapter-number": "10",
    "container-title": "Between humanities and the digital",
    "editor": [
      {
        "family": "Svensson",
        "given": "Patrik"
      },
      {
        "family": "Goldberg",
        "given": "David T."
      }
    ],
    "id": "Davidson2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "en-US",
    "page": "131-143",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Why yack needs hack (and vice versa): From digital humanities to digital literacy",
    "title-short": "Why yack needs hack (and vice versa)",
    "type": "chapter"
  },
  {
    "URL": "http://www.bcs.org/upload/pdf/ewic_el05_s1paper4.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          24
        ]
      ]
    },
    "author": [
      {
        "family": "Davies",
        "given": "Will M."
      },
      {
        "family": "Davis",
        "given": "Hugh C."
      }
    ],
    "container-title": "1st international ELeGI conference on advanced technology for enhanced learning",
    "editor": [
      {
        "family": "Albano",
        "given": "Giovanna"
      },
      {
        "family": "Ritrovato",
        "given": "Pierluigi"
      },
      {
        "family": "Salerno",
        "given": "Saverio"
      }
    ],
    "id": "Davies2005",
    "issued": {
      "date-parts": [
        [
          2005,
          3
        ]
      ]
    },
    "publisher-place": "Vico Equense, Italy",
    "title": "Designing assessment tools in a service oriented architecture",
    "type": "paper-conference"
  },
  {
    "URL": "http://groups.csail.mit.edu/medg/ftp/psz/k-rep.html",
    "abstract": "Although knowledge representation is one of the central and in some ways most familiar concepts in AI, the most fundamental question about it–What is it?–has rarely been answered directly. Numerous papers have lobbied for one or another variety of representation, other papers have argued for various properties a representation should have, while still others have focused on properties that are important to the notion of representation in general. In this paper we go back to basics to address the question directly. We believe that the answer can best be understood in terms of five important and distinctly different roles that a representation plays, each of which places different and at times conflicting demands on the properties a representation should have. We argue that keeping in mind all five of these roles provides a usefully broad perspective that sheds light on some longstanding disputes and can invigorate both research and practice in the field.",
    "author": [
      {
        "family": "Davis",
        "given": "Randall"
      },
      {
        "family": "Shrobe",
        "given": "Howard"
      },
      {
        "family": "Szolovits",
        "given": "Peter"
      }
    ],
    "container-title": "AI Magazine",
    "id": "Davis1993",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "ai, knowledge_representation",
    "language": "en-US",
    "page": "17-33",
    "title": "What is a knowledge representation?",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "abstract": "_Engaging Minds: Learning and Teaching in a Complex World_ involves readers in a stimulating, informative, comprehensive exploration of teaching and learning. It prompts examinations of the complexities of learning, pedagogy, and schooling while refusing simplistic notions or unresolvable tensions that sometimes infuse popular debates. A variety of sophisticated, interactive pedagogical features and graphic displays draw readers into new ways of thinking about and responding to the ideas and information presented. Topics include: * the biological and social roots of perception; * historical and contemporary perspectives on learning; * emergent understandings of intelligence, creativity, and diversity; * complexities and contingencies of self concept; and * technology, its impact on cognition, and its place in schooling. In addition to conceptual reviews of these topics, the text provides elaborated descriptions of many specific teaching events, in different subject areas and at all age levels, followed by interpretations which include suggestions for teachers. Written by authors with over 60 years collective experience as teachers at all levels of formal education, _Engaging Minds_ offers fresh and insightful perspectives on topics such as lesson planning, classroom management, assessment and evaluation, learner diversity, inclusivity, and technology. It will be of interest to undergraduate students in teacher education, experienced teachers, and graduate students. This is an ideal text for courses in curriculum and instruction, curriculum and learning theory, social foundations of education, human development, or an excellent companion volume for any subject-specific teaching methods course.",
    "author": [
      {
        "family": "Davis",
        "given": "Brent"
      },
      {
        "family": "Sumara",
        "given": "Dennis"
      },
      {
        "family": "Luce-Kapler",
        "given": "Rebecca"
      }
    ],
    "id": "Davis2000",
    "issued": {
      "date-parts": [
        [
          2000,
          6
        ]
      ]
    },
    "keyword": "assessment, e-learning",
    "language": "en-US",
    "publisher": "Lawrence Erlbaum Associates",
    "publisher-place": "Mahwah, NJ, USA",
    "title": "Engaging minds",
    "type": "book"
  },
  {
    "DOI": "10.1080/01621459.1977.10479968",
    "ISSN": "0162-1459",
    "abstract": "If the reported data of an experiment have been subject to selection, then inference from such data should be modified accordingly. We investigate the modification required to the face-value likelihood. In particular, we derive conditions under which no modification is necessary and the data can be taken at face value.",
    "author": [
      {
        "family": "Dawid",
        "given": "A. P."
      },
      {
        "family": "Dickey",
        "given": "James M."
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "id": "Dawid1977",
    "issue": "360a",
    "issued": {
      "date-parts": [
        [
          1977
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "845-850",
    "title": "Likelihood and Bayesian inference from selectively reported data",
    "type": "article-journal",
    "volume": "72"
  },
  {
    "URL": "http://muse.jhu.edu/journals/social_science_history/v035/35.4.debats01.html",
    "author": [
      {
        "family": "DeBats",
        "given": "Donald A."
      },
      {
        "family": "Gregory",
        "given": "Ian N."
      }
    ],
    "container-title": "Social Science History",
    "id": "DeBats2011",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "digital_humanities, gis",
    "language": "en-US",
    "page": "455-463",
    "title": "Introduction to historical GIS and the study of urban history",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "DOI": "10.1007/bf02941632",
    "abstract": "The way in which text is represented on a computer affects the kinds of uses to which it can be put by its creator and by subsequent users. The electronic document model currently in use is impoverished and restrictive. The authors argue that text is best represented as an ordered hierarchy of content object (OHCO), because that is what text really is. This model conforms with emerging standards such as SGML and contains within it advantages for the writer, publisher, and researcher. The authors then describe how the hierarchical model can allow future use and reuse of the document as a database, hypertext, or network.",
    "author": [
      {
        "family": "DeRose",
        "given": "Steven J."
      },
      {
        "family": "Durand",
        "given": "David G."
      },
      {
        "family": "Mylonas",
        "given": "Elli"
      },
      {
        "family": "Renear",
        "given": "Allen H."
      }
    ],
    "container-title": "Journal of Computing in Higher Education",
    "id": "DeRose1990",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "document_research, formal_models, markup, markup_overlap",
    "language": "en-US",
    "page": "3-26",
    "title": "What is text, really?",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.1145/264842.264843",
    "ISSN": "0731-1001",
    "author": [
      {
        "family": "DeRose",
        "given": "Steven J."
      },
      {
        "family": "Durand",
        "given": "David G."
      },
      {
        "family": "Mylonas",
        "given": "Elli"
      },
      {
        "family": "Renear",
        "given": "Allen H."
      }
    ],
    "container-title": "SIGDOC Asterisk J. Comput. Doc.",
    "id": "DeRose1997a",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1997,
          8
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "page": "1-24",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "What is text, really?",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "DOI": "10.1145/264842.264847",
    "ISSN": "0731-1001",
    "author": [
      {
        "family": "DeRose",
        "given": "Steven J."
      }
    ],
    "container-title": "SIGDOC Asterisk J. Comput. Doc.",
    "id": "DeRose1997b",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1997,
          8
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "page": "40-44",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Further context for “what is text, really?”",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "URL": "https://jstor.org/stable/30200484",
    "abstract": "Electronic texts are claimed to exhibit features distinct from their more tangible cousins. The Snapshot project aims to observe and capture language usage in an electronic medium by creating an open corpus of World Wide Web documents. These documents are re-encoded using the TEI guidelines to create a flexible, persistent and portable data repository. This report gives an overview of the decisions made with respect to the re-encoding of HTML documents, and with the structuring the overall corpus.",
    "author": [
      {
        "family": "DeRose",
        "given": "Steven J."
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "DeRose1999",
    "issue": "1–2",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "markup, sgml, tei, xml",
    "language": "en-US",
    "page": "11-30",
    "title": "XML and the TEI",
    "type": "article-journal",
    "volume": "33"
  },
  {
    "ISBN": "9781101871553",
    "abstract": "At a time when Steve Jobs was only a teenager and Mark Zuckerberg wasn’t even born, a group of visionary engineers and designers—some of them only high school students—in the late 1960s and 1970s created a computer system called PLATO, which was light-years ahead in experimenting with how people would learn, engage, communicate, and play through connected computers. Not only did PLATO engineers make significant hardware breakthroughs with plasma displays and touch screens but PLATO programmers also came up with a long list of software innovations: chat rooms, instant messaging, message boards, screen savers, multiplayer games, online newspapers, interactive fiction, and emoticons. Together, the PLATO community pioneered what we now collectively engage in as cyberculture. They were among the first to identify and also realize the potential and scope of the social interconnectivity of computers, well before the creation of the internet. PLATO was the foundational model for every online community that was to follow in its footsteps. The Friendly Orange Glow is the first history to recount in fascinating detail the remarkable accomplishments and inspiring personal stories of the PLATO community. The addictive nature of PLATO both ruined many a college career and launched pathbreaking multimillion-dollar software products. Its development, impact, and eventual disappearance provides an instructive case study of technological innovation and disruption, project management, and missed opportunities. Above all, The Friendly Orange Glow at last reveals new perspectives on the origins of social computing and our internet-infatuated world.",
    "author": [
      {
        "family": "Dear",
        "given": "Brian"
      }
    ],
    "id": "Dear2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "language": "en-US",
    "publisher": "Pantheon",
    "publisher-place": "New York, NY, USA",
    "title": "The friendly orange glow: The untold story of the PLATO system and the dawn of cyberculture",
    "title-short": "The friendly orange glow",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-16925-5",
    "author": [
      {
        "family": "Dechow",
        "given": "Douglas R."
      },
      {
        "family": "Struppa",
        "given": "Daniele C."
      }
    ],
    "collection-title": "History of computing",
    "id": "Dechow2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "hypertext",
    "language": "en-US",
    "publisher": "Springer",
    "title": "Intertwingled: The work and influence of Ted Nelson",
    "title-short": "Intertwingled",
    "type": "book"
  },
  {
    "URL": "http://www.degruyter.de/cont/fb/sk/detailEn.cfm?id=IS-9783484522121-1",
    "author": [
      {
        "family": "Dees",
        "given": "Anthonij"
      }
    ],
    "collection-title": "Beihefte zur zeitschrift für romanische philologie",
    "id": "Dees1987",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "keyword": "cultural_heritage, french",
    "language": "en-US",
    "publisher": "De Gruyter",
    "publisher-place": "Berlin, Germany",
    "title": "Atlas des formes linguistiques des textes littéraires de l’ancien français",
    "type": "book",
    "volume": "212"
  },
  {
    "DOI": "10.1214/aoms/1177698950",
    "ISSN": "0003-4851",
    "abstract": "A multivalued mapping from a space X to a space S carries a probability measure defined over subsets of X into a system of upper and lower probabilities over subsets of S. Some basic properties of such systems are explored in Sections 1 and 2. Other approaches to upper and lower probabilities are possible and some of these are related to the present approach in Section 3. A distinctive feature of the present approach is a rule for conditioning, or more generally, a rule for combining sources of information, as discussed in Sections 4 and 5. Finally, the context in statistical inference from which the present theory arose is sketched briefly in Section 6.",
    "author": [
      {
        "family": "Dempster",
        "given": "Arthur P."
      }
    ],
    "container-title": "The Annals of Mathematical Statistics",
    "id": "Dempster1967",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1967
        ]
      ]
    },
    "keyword": "classic, uncertainty",
    "language": "en-US",
    "page": "325-339",
    "title": "Upper and lower probabilities induced by a multivalued mapping",
    "type": "article-journal",
    "volume": "38"
  },
  {
    "DOI": "10.1145/3041047",
    "ISSN": "00010782",
    "abstract": "Common misconceptions about computer science hinder professional growth and harm the identity of computing.",
    "author": [
      {
        "family": "Denning",
        "given": "Peter J."
      },
      {
        "family": "Tedre",
        "given": "Matti"
      },
      {
        "family": "Yongpradit",
        "given": "Pat"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Denning2017",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "computational_thinking",
    "language": "en-US",
    "page": "31-33",
    "title": "Misconceptions about computer science",
    "type": "article-journal",
    "volume": "60"
  },
  {
    "URL": "http://promethee.philo.ulg.ac.be/LASLApdf/lordinateuretlelatin.pdf",
    "author": [
      {
        "family": "Denooz",
        "given": "Joseph"
      }
    ],
    "container-title": "Revue de l’organisation internationale pour l’étude des langues anciennes par ordinateur",
    "id": "Denooz1976",
    "issued": {
      "date-parts": [
        [
          1978
        ]
      ]
    },
    "keyword": "cultural_heritage, latin",
    "language": "fr-FR",
    "page": "1-36",
    "title": "L’ordinateur et le latin, Techniques et méthodes",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "URL": "http://hdl.handle.net/2268/357",
    "author": [
      {
        "family": "Denooz",
        "given": "Joseph"
      }
    ],
    "container-title": "Euphrosyne",
    "id": "Denooz2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "cultural_heritage, latin",
    "language": "fr-FR",
    "page": "79-88",
    "title": "Opera latina : une base de données sur internet",
    "title-short": "Opera latina ",
    "type": "article-journal",
    "volume": "32"
  },
  {
    "author": [
      {
        "family": "Denooz",
        "given": "Joseph"
      },
      {
        "family": "Rosmorduc",
        "given": "Serge"
      }
    ],
    "container-title": "Traitement Automatique des Langues",
    "id": "Denooz2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "page": "13-16",
    "title": "Preface to the special issue on “NLP and Ancient Languages”",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "author": [
      {
        "family": "Dervaric",
        "given": "Christian"
      }
    ],
    "genre": "Diplomarbeit",
    "id": "Dervaric2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Fakultät für Informatik, Otto-von-Guericke-Universität",
    "publisher-place": "Magdeburg",
    "title": "Erkennung und Behandlung von Plagiaten bei Lösungen zu Übungsaufgaben",
    "type": "thesis"
  },
  {
    "DOI": "10.1145/1104973.1104974",
    "ISBN": "1-59593-111-2",
    "abstract": "Wikis are simple to use, asynchronous, Web-based collaborative hypertext authoring systems which are quickly gaining in popularity. In spite of much anecdotal evidence to the effect that wikis are usable by non technical experts, this has never been studied formally. In this paper, we studied the usability of a wiki through observation and problem-solving interaction with several children who used the tool to collaboratively author hypertext stories over several sessions. The children received a minimal amount of instruction, but were able to ask for help during their work sessions. Despite minimal instruction, 5 out of 6 teams were able to complete their story. Our data indicate that the major usability problems were related to hyperlink management. We report on this and other usability issues, and provide suggestions for improving the usability of wikis. Our analysis and conclusions also apply to hypertext authoring with non wiki-based tools.",
    "author": [
      {
        "family": "Désilets",
        "given": "Alain"
      },
      {
        "family": "Paquet",
        "given": "Sébastien"
      },
      {
        "family": "Vinson",
        "given": "Norman G."
      }
    ],
    "container-title": "WikiSym ’05: Proceedings of the 2005 international symposium on wikis",
    "id": "Desilets2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "interactive_editing, wiki",
    "language": "en-US",
    "page": "3-15",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Are wikis usable?",
    "type": "paper-conference"
  },
  {
    "URL": "http://publications.teachernet.gov.uk/eOrderingDownload/2102-2005.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "United Kingdom. Department for Education and Skills"
      }
    ],
    "id": "DfES2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "note": "Ref. 2102-2005DBW-EN",
    "title": "Learning platforms (secondary)",
    "type": "book"
  },
  {
    "URL": "http://www2.tisip.no/quis/public_files/wp4-analysis-of-e-systems.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Di Domenico",
        "given": "F."
      },
      {
        "family": "Panizzi",
        "given": "Emanuele"
      },
      {
        "family": "Sterbini",
        "given": "Andrea"
      },
      {
        "family": "Temperini",
        "given": "Marco"
      }
    ],
    "id": "DiDomenico2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "publisher": "TISIP Research Foundation",
    "publisher-place": "Trondheim, Norway",
    "title": "Analysis of commercial and experimental e-learning sytems",
    "type": "report"
  },
  {
    "URL": "http://ceur-ws.org/Vol-1486/paper_72.pdf",
    "abstract": "This paper introduces the RASH Framework, i.e., a set of specifications and tools for writing academic articles in RASH, a simplified version of HTML. RASH focuses strictly on writing the content of the paper leaving all the issues about its validation, visualisation, conversion, and data extraction to the tools developed within the framework.",
    "author": [
      {
        "family": "Di Iorio",
        "given": "Angelo"
      },
      {
        "family": "Nuzzolese",
        "given": "Andrea G."
      },
      {
        "family": "Osborne",
        "given": "Francesco"
      },
      {
        "family": "Peroni",
        "given": "Silvio"
      },
      {
        "family": "Poggi",
        "given": "Francesco"
      },
      {
        "family": "Smith",
        "given": "Michael"
      },
      {
        "family": "Vitali",
        "given": "Fabio"
      },
      {
        "family": "Zhao",
        "given": "Jun"
      }
    ],
    "container-title": "Proceedings of the ISWC 2015 posters & demonstrations track",
    "editor": [
      {
        "family": "Villata",
        "given": "Serena"
      },
      {
        "family": "Pan",
        "given": "Jeff Z."
      },
      {
        "family": "Dragoni",
        "given": "Mauro"
      }
    ],
    "id": "DiIorio2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "document_engineering, markup, scientific_publishing",
    "language": "en-US",
    "page": "4",
    "publisher": "CEUR",
    "publisher-place": "Aachen",
    "title": "The RASH framework: Enabling HTML+RDF submissions in scholarly venues",
    "title-short": "The RASH framework",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.voies.uottawa.ca/Using_POS_Annot.pdf",
    "abstract": "There is a large research effort for building a Semantic Web for complementing the current text-based web with machine understandable semantics. This paper introduces a new and complex algorithm for automatic Natural Language text exploration while exploring a corpus. The algorithm aims at searching and retrieving information from a corpus. At the same time the new algorithms can be used to store semantic based web repositories. The corpus is first explored by a morphologic and syntactic parser processing. An intermediate text corpus is structured according the Enhanced Monad dot Feature (EMdF) model. For the parsed text above an EMdF model is directly mapped and stored in an RDF database. Information retrieval on the initial text corpus is performed using queries on the RDF database. These queries resolve co- occurrences, verbs identification, and false positive elimination such that the results obtained are wellformed relations. The algorithms introduced are illustrated on examples of Semantic Web information retrieval on a medieval French corpus based on ” Mémoires” of Philippe de Commynes.",
    "author": [
      {
        "family": "Diaconescu",
        "given": "Rodica"
      },
      {
        "family": "Hirschbühler",
        "given": "Paul"
      },
      {
        "family": "Ionescu",
        "given": "Dan"
      },
      {
        "family": "Martineau",
        "given": "France"
      },
      {
        "family": "Trifan",
        "given": "Mircea"
      }
    ],
    "container-title": "I2TS 2006 – 5th international information and telecommunication technologies symposium",
    "id": "Diaconescu2006",
    "keyword": "corpus_linguistics, cultural_heritage, french, nlp",
    "language": "en-US",
    "title": "Using POS annotations and EMdF text modeling for semantic web search",
    "type": "paper-conference"
  },
  {
    "abstract": "Learner corpora are used to investigate computerised learner language so as to gain insights into foreign language learning. One of the methodologies that can be applied to this type of research is computer-aided error analysis (CEA), which, in general terms, consists in the study of learner errors as contained in a learner corpus. Surveys of current learner corpora and of issues of learner corpus research have been published in the last few years (Granger 1998, 2002, 2004a; Meunier 1998; Pravec 2002; Tono 2003; Nesselhauf 2004; Myles 2005), where information on CEA research can be found, although usually limited. This article is centred on CEA research and is intended as a review of error tagging systems, including error categorizations, dimensions and levels of description.",
    "author": [
      {
        "family": "Díaz-Negrillo",
        "given": "Ana"
      },
      {
        "family": "Fernández-Domínguez",
        "given": "Jesús"
      }
    ],
    "container-title": "Revista Española de Lingüística Aplicada",
    "id": "Diaz-Negrillo2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "learner_corpora, writing_research",
    "language": "en-US",
    "page": "83-102",
    "title": "Error tagging systems for learner corpora",
    "type": "article-journal",
    "volume": "19"
  },
  {
    "URL": "http://www.medienpaed.com/00-2/dichanz_ernst1.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Dichanz",
        "given": "Horst"
      },
      {
        "family": "Ernst",
        "given": "Annette"
      }
    ],
    "container-title": "MedienPädagogik",
    "id": "Dichanz2001",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "title": "E-Learning: Begriffliche, psychologische und didaktische Überlegungen zum ",
    "type": "article-journal",
    "volume": "0"
  },
  {
    "DOI": "10.2307/1162068",
    "author": [
      {
        "family": "Dick",
        "given": "Walter"
      }
    ],
    "container-title": "American Educational Research Journal",
    "id": "Dick1965",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1965
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "language": "en-US",
    "page": "41-54",
    "publisher": "American Educational Research Association",
    "title": "The development and current status of computer-based instruction",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "URL": "http://www.korpora.org/Fnhd/ikpab-nf02.pdf",
    "author": [
      {
        "family": "Diel",
        "given": "Marcel"
      },
      {
        "family": "Fisseni",
        "given": "Bernhard"
      },
      {
        "family": "Lenders",
        "given": "Winfried"
      },
      {
        "family": "Schmitz",
        "given": "Hans-Christian"
      }
    ],
    "collection-title": "IKP-Arbeitsbericht",
    "id": "Diel2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, german, markup, xml",
    "language": "de-DE",
    "publisher": "Universität Bonn",
    "publisher-place": "Bonn, Germany",
    "title": "XML-Kodierung des Bonner Frühneuhochdeutschkorpus",
    "type": "report",
    "volume": "NF 02"
  },
  {
    "author": [
      {
        "family": "Dilthey",
        "given": "Wilhelm"
      }
    ],
    "container-title": "Philosophische Abhandlungen: Christoph Sigwart zu seinem 70. Geburtstage gewidmet",
    "id": "Dilthey1900",
    "issued": {
      "date-parts": [
        [
          1900
        ]
      ]
    },
    "keyword": "hermeneutics, classic",
    "language": "de-DE",
    "page": "185-202",
    "publisher": "J. C. B. Mohr (Paul Siebeck)",
    "publisher-place": "Tübingen",
    "title": "Die Entstehung der Hermeneutik",
    "type": "chapter"
  },
  {
    "DOI": "10.1515/9783110211818.1.39",
    "abstract": "In this paper, we explore several ways of computing similarity between medieval text variants from German. In comparing these texts, we apply methods from word and sentence alignment and compute cosine similarity based on character and part-of-speech ngrams. The resulting similarity (or distance) scores are visualized by phylogenetic trees; the methods correctly reproduce the well-known distinction between Middle and Upper German (\"Mitteldeutsch\" vs. \"Oberdeutsch\").",
    "author": [
      {
        "family": "Dipper",
        "given": "Stefanie"
      },
      {
        "family": "Schrader",
        "given": "Bettina"
      }
    ],
    "collection-title": "Text, translation, computational processing",
    "container-title": "Text resources and lexical knowledge",
    "editor": [
      {
        "family": "Storrer",
        "given": "Angelika"
      },
      {
        "family": "Geyken",
        "given": "Alexander"
      },
      {
        "family": "Siebert",
        "given": "Alexander"
      },
      {
        "family": "Würzner",
        "given": "Kay-Michael"
      }
    ],
    "id": "Dipper2008",
    "issued": {
      "date-parts": [
        [
          2008,
          9,
          16
        ]
      ]
    },
    "keyword": "cultural_heritage, language_identification, nlp",
    "language": "en-US",
    "page": "39-51",
    "publisher": "Mouton de Gruyter",
    "publisher-place": "Berlin/New York",
    "title": "Computing distance and relatedness of medieval text variants from German",
    "type": "chapter",
    "volume": "8"
  },
  {
    "DOI": "10.1515/9783110211429.1.68",
    "author": [
      {
        "family": "Dipper",
        "given": "Stefanie"
      }
    ],
    "chapter-number": "5",
    "collection-title": "Handbooks of linguistics and communication science",
    "editor": [
      {
        "family": "Lüdeling",
        "given": "Anke"
      },
      {
        "family": "Kytö",
        "given": "Merja"
      }
    ],
    "id": "Dipper2008a",
    "issued": {
      "date-parts": [
        [
          2
        ]
      ]
    },
    "keyword": "computational_linguistics, corpus_linguistics",
    "language": "en-US",
    "page": "68-96+",
    "publisher": "Mouton de Gruyter",
    "publisher-place": "Berlin/New York",
    "title": "Theory-driven and corpus-driven computational linguistics, and the use of corpora",
    "type": "article-journal",
    "volume": "Part 1"
  },
  {
    "URL": "http://universaar.uni-saarland.de/monographien/volltexte/2010/12/",
    "abstract": "This paper deals with part-of-speech tagging applied to manuscripts written in Middle High German. We present the results of a set of experiments that involve different levels of token normalization and dialect-specific subcorpora. As expected, tagging with “normalized”, quasi-standardized tokens performs best (accuracy > 91%). Training on slightly simplified word forms or on larger corpora of heterogeneous texts does not result in considerable improvement.",
    "author": [
      {
        "family": "Dipper",
        "given": "Stefanie"
      }
    ],
    "container-title": "Semantic approaches in natural language processing: Proceedings of the conference on natural language processing 2010 (KONVENS)",
    "editor": [
      {
        "family": "Pinkal",
        "given": "Manfred"
      },
      {
        "family": "Rehbein",
        "given": "Ines"
      },
      {
        "dropping-particle": "Schulte im",
        "family": "Walde",
        "given": "Sabine"
      },
      {
        "family": "Storrer",
        "given": "Angelika"
      }
    ],
    "id": "Dipper2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, german, pos_tagging",
    "language": "en-US",
    "page": "117-121",
    "publisher": "Universaar",
    "publisher-place": "Saarbrücken, Germany",
    "title": "POS-tagging of historical language data: First experiments",
    "title-short": "POS-tagging of historical language data",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/W10-1828",
    "abstract": "This paper presents OTTO, a transcription tool designed for diplomatic transcription of historical language data. The tool supports easy and fast typing and instant rendering of transcription in order to gain a look as close to the original manuscript as possible. In addition, the tool provides support for the management of transcription projects which involve distributed, collaborative working of multiple parties on collections of documents.",
    "author": [
      {
        "family": "Dipper",
        "given": "Stefanie"
      },
      {
        "family": "Kresse",
        "given": "Lara"
      },
      {
        "family": "Schnurrenberger",
        "given": "Martin"
      },
      {
        "family": "Cho",
        "given": "Seong E."
      }
    ],
    "container-title": "Proceedings of the fourth linguistic annotation workshop",
    "id": "Dipper2010LAW",
    "issued": {
      "date-parts": [
        [
          2010,
          7
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "page": "182-185",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Uppsala, Sweden",
    "title": "OTTO: A transcription and management tool for historical texts",
    "title-short": "OTTO",
    "type": "paper-conference"
  },
  {
    "ISBN": "9783631553770",
    "abstract": "Modellbildung ist eine herausragende geistige Tätigkeit des Menschen. In den Wissenschaften kommt Modellen eine fundamentale Stellung zu, was sie zum Gegenstand intensiver Forschung macht. Die Autoren leisten Beiträge zur Aufklärung von Arten, Funktionen und Zwecken von Modellen, zum Verständnis ihres Wirklichkeitsbezugs und Wechselspiels mit Theorien, zur Erklärung ihrer handlungsleitenden, orientierenden und erkenntnisfördernden Kraft, zur Analyse des konstruktionalen Prozesses der Modellierung und zur Entwicklung eines differenzierten Modellbegriffs. Untersucht werden Rolle und Status von Modellbildungen in der Geschichte der Wissenschaften, der Einsatz von Modellen in den Technikwissenschaften sowie ausgewählte philosophische Modelle. Theoretische Modelle der Komplexitätsforschung und Kosmologie und ihr Zusammenhang mit unserem Weltbild werden ebenso erörtert wie kulturelle Wurzeln und Entwicklungen von Modellen in Mathematik, Chemie und Lebenswissenschaften.",
    "editor": [
      {
        "family": "Dirks",
        "given": "Ulrich"
      },
      {
        "family": "Knobloch",
        "given": "Eberhard"
      }
    ],
    "id": "Dirks2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "formal_models, models_in_general",
    "language": "de-DE",
    "publisher": "Peter Lang",
    "publisher-place": "Frankfurt am Main",
    "title": "Modelle",
    "type": "book"
  },
  {
    "ISBN": "3436014435",
    "editor": [
      {
        "dropping-particle": "von",
        "family": "Ditfurth",
        "given": "Hoimar"
      }
    ],
    "id": "Ditfurth1971",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "keyword": "cybernetics",
    "language": "de-DE",
    "publisher": "Fischer Taschenbuch",
    "publisher-place": "Frankfurt am Main",
    "title": "Informationen über Information: Probleme der Kybernetik",
    "title-short": "Informationen über Information",
    "type": "book"
  },
  {
    "URL": "https://www.theatlantic.com/technology/archive/2017/03/aristotle-computer/518697/",
    "author": [
      {
        "family": "Dixon",
        "given": "Chris"
      }
    ],
    "container-title": "The Atlantic",
    "id": "Dixon2017",
    "issued": {
      "date-parts": [
        [
          2017,
          3,
          20
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "title": "How Aristotle created the computer",
    "type": "webpage"
  },
  {
    "URL": "https://www.theatlantic.com/magazine/archive/2015/10/how-david-hume-helped-me-solve-my-midlife-crisis/403195/",
    "accessed": {
      "date-parts": [
        [
          2020,
          7,
          5
        ]
      ]
    },
    "author": [
      {
        "family": "Gopnik",
        "given": "Alison"
      }
    ],
    "container-title": "The Atlantic",
    "id": "Gopnik2015",
    "issued": {
      "date-parts": [
        [
          2015,
          10
        ]
      ]
    },
    "language": "en-US",
    "title": "How an 18<sup>th</sup>-century philosopher helped solve my midlife crisis: David Hume, the Buddha, and a search for the Eastern roots of the Western Enlightenment",
    "title-short": "How an 18<sup>th</sup>-century philosopher helped solve my midlife crisis",
    "type": "article-journal"
  },
  {
    "URL": "http://www.jot.fm/issues/issue_2002_11/column3",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Dodani",
        "given": "Mahesh H."
      }
    ],
    "container-title": "Journal of Object Technology",
    "id": "Dodani2002",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "language": "en-US",
    "page": "37-42",
    "title": "The dark side of object learning: Learning objects",
    "title-short": "The dark side of object learning",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.1145/1921614.1921615",
    "ISSN": "15564673",
    "abstract": "Modeling human argumentation should shed light on how knowledge described in information systems could be better accessed, structured, and used for real life research purposes. Current argumentation models are either not analytical enough or restricted to formal logic. For that purpose, we seek a model of human argumentation in which reasoning may not only consist of falsification or verification but more generally of strengthening or weakening hypotheses, and a way to connect this model to an ontology of the domain of discourse. We have studied examples of factual argumentation in empirical research in archaeology. Based on this and other empirical material, we propose an innovative integrated model of factual argumentation that includes evolution, composition, and revision of arguments. It makes explicit both the processes of argument-making and the states of belief at a particular point in time in a composite inference, and connects explicitly to a domain ontology, free of tacit background knowledge. We have implemented the model in a more restricted form and tested it with published archaeological examples. Future work may generalize the model to other kinds of argumentation.",
    "author": [
      {
        "family": "Doerr",
        "given": "Martin"
      },
      {
        "family": "Kritsotaki",
        "given": "Athina"
      },
      {
        "family": "Boutsika",
        "given": "Katerina"
      }
    ],
    "container-title": "Journal on Computing and Cultural Heritage",
    "id": "Doerr2011",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "argumentation, cultural_heritage, uncertainty",
    "language": "en-US",
    "page": "8:1-8:34",
    "title": "Factual argumentation—a core model for assertions making",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=800253.807669",
    "abstract": "The Programmer’s Workbench (PWB) is a specialized computing facility dedicated to satisfying the needs of developers of computer programs. The PWB might well be called a \"human-end\" computer; like “front-end” and “back-end” computers, it improves productivity by efficient specialization. It provides a convenient working environment and a uniform set of programming tools to a diverse group of programming projects. These projects produce software for various \"target\" computers, including IBM System/370 and UNIVAC 1100 systems of much greater size than the PWB machines. The projects range in size from several people up to several hundred. The first PWB machine was installed in October, 1973; usage, acceptance, and interest have grown rapidly since that time. The PWB currently supports about 110 time-sharing terminals, utilizing a network of four DEC PDP-11 computers, all running the UNIX Time-Sharing System. The PWB adds tools to UNIX to support large projects. This paper gives an overview of the PWB and its development; further details appear in the five following companion papers [BIA76A, DOL76B, KNU76A, MAS76A, MAS76B].",
    "author": [
      {
        "family": "Dolotta",
        "given": "Ted A."
      },
      {
        "family": "Mashey",
        "given": "John R."
      }
    ],
    "collection-title": "ICSE ’76",
    "container-title": "Proceedings of the 2<sup>nd</sup> international conference on software engineering",
    "id": "Dolotta1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "language": "en-US",
    "page": "164-168",
    "publisher": "IEEE",
    "publisher-place": "Los Alamitos, CA, USA",
    "title": "An introduction to the Programmer’s Workbench",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqu026",
    "ISSN": "1477-4615",
    "abstract": "This paper charts the origins, trajectory, development, challenges, and conclusion of Project Bamboo, a humanities cyberinfrastructure initiative funded by the Andrew W. Mellon Foundation between 2008 and 2012. Bamboo aimed to enhance arts and humanities research through the development of infrastructure and support for shared technology services. Its planning phase brought together scholars, librarians, and IT staff from a wide range of institutions, in order to gain insight into the scholarly practices Bamboo would support, and to build a community of future developers and users for Bamboo’s technical deliverables. From its inception, Bamboo struggled to define itself clearly and in a way that resonated with scholars, librarians, and IT staff alike. The early emphasis on a service-oriented architecture approach to supporting humanities research failed to connect with scholars, and the scope of Bamboo’s ambitions expanded to include scholarly networking, sharing ideas and solutions, and demonstrating how digital tools and methodologies can be applied to research questions. Funding constraints for Bamboo’s implementation phase led to the near-elimination of these community-oriented aspects of the project, but the lack of a shared vision that could supersede the individual interests of partner institutions resulted in a scope around which it was difficult to articulate a clear narrative. When Project Bamboo ended in 2012, it had failed to realize its most ambitious goals; this article explores the reasons for this, including technical approaches, communication difficulties, and challenges common to projects that bring together teams from different professional communities.",
    "author": [
      {
        "family": "Dombrowski",
        "given": "Quinn"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Dombrowski2014",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "326-339",
    "title": "What ever happened to Project Bamboo?",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "DOI": "10.1484/j.bpm.3.8",
    "author": [
      {
        "family": "Dondaine",
        "given": "Antoine"
      }
    ],
    "container-title": "Bulletin de Philosophie Médiévale",
    "id": "Dondaine1960",
    "issued": {
      "date-parts": [
        [
          1960
        ]
      ]
    },
    "keyword": "digital_edition",
    "language": "en-US",
    "page": "142-149",
    "title": "Abréviations latines et signes recommandés pour l’apparat critique des éditions de textes médiévaux",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.1145/1963405.1963487",
    "ISBN": "9781450306324",
    "abstract": "K-Nearest Neighbor Graph (K-NNG) construction is an important operation with many web related applications, including collaborative filtering, similarity search, and many others in data mining and machine learning. Existing methods for K-NNG construction either do not scale, or are specific to certain similarity measures. We present NN-Descent, a simple yet efficient algorithm for approximate K-NNG construction with arbitrary similarity measures. Our method is based on local search, has minimal space overhead and does not rely on any shared global index. Hence, it is especially suitable for large-scale applications where data structures need to be distributed over the network. We have shown with a variety of datasets and similarity measures that the proposed method typically converges to above 90% recall with each point comparing only to several percent of the whole dataset on average.",
    "author": [
      {
        "family": "Dong",
        "given": "Wei"
      },
      {
        "family": "Moses",
        "given": "Charikar"
      },
      {
        "family": "Li",
        "given": "Kai"
      }
    ],
    "container-title": "Proceedings of the 20th international conference on world wide web (WWW ’11)",
    "id": "Dong2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "approximate_matching",
    "language": "en-US",
    "page": "577-586",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Efficient k-nearest neighbor graph construction for generic similarity measures",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1163405.1163409",
    "author": [
      {
        "family": "Douce",
        "given": "Christopher"
      },
      {
        "family": "Livingstone",
        "given": "David"
      },
      {
        "family": "Orwell",
        "given": "James"
      }
    ],
    "container-title": "Journal on Educational Resources in Computing",
    "id": "Douce2005",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "page": "4",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Automatic test-based assessment of programming: A review",
    "title-short": "Automatic test-based assessment of programming",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "ISBN": "9782707328960",
    "URL": "https://cairn.info/revue-critique-2015-8-9-page-704.htm",
    "abstract": "Le tournant numérique n’affecte pas seulement les modes de diffusion des savoirs, mais leur nature même. Il pose des problèmes aux sciences humaines, particulièrement aux disciplines littéraires, plus habituées à s’intéresser à la singularité des œuvres qu’à l’analyse quantitative et la lecture à distance. Il propose aussi de nouveaux objets d’étude, des réseaux sociaux à la littérature numérique, et de nouvelles interrogations critiques, de l’utopie d’un « transhumanisme » en devenir à la dystopie d’une menaçante « gouvernance algorithmique ». Dans ce numéro dirigé par Alexandre Gefen, Critique donne la parole à ceux qui, travaillant dans ce nouveau champ transdisciplinaire, peuvent nous aider à en mieux comprendre les acquis et les écueils, les limites et les promesses.",
    "author": [
      {
        "family": "Doueihi",
        "given": "Milad"
      }
    ],
    "container-title": "Critique",
    "id": "Doueihi2015",
    "issue": "8-9",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "fr-FR",
    "page": "704-711",
    "publisher": "Minuit",
    "title": "Quelles humanités numériques?",
    "type": "article-journal",
    "volume": "819-820"
  },
  {
    "URL": "http://jeremydouglass.com/dissertation.html",
    "abstract": "The Interactive Fiction (IF) genre describes text-based narrative experiences in which a person interacts with a computer simulation by typing text phrases (usually commands in the imperative mood) and reading software-generated text responses (usually statements in the second person present tense). Re-examining historical and contemporary IF illuminates the larger fields of electronic literature and game studies. Intertwined aesthetic and technical developments in IF from 1977 to the present are analyzed in terms of language (person, tense, and mood), narrative theory (Iser’s gaps, the fabula / sjuzet distinction), game studies / ludology (player apprehension of rules, evaluation of strategic advancement), and filmic representation (subjective POV, time-loops). Two general methodological concepts for digital humanities analyses are developed in relation to IF: implied code, which facilitates studying the interactor’s mental model of an interactive work; and frustration aesthetics, which facilitates analysis of the constraints that structure interactive experiences. IF works interpreted in extended \"close interactions\" include Plotkin’s Shade (1999), Barlow’s Aisle (2000), Pontious’s Rematch (2000), Foster and Ravipinto’s Slouching Towards Bedlam (2003), and others. Experiences of these works are mediated by implications, frustrations, and the limiting figures of their protagonists.",
    "author": [
      {
        "family": "Douglass",
        "given": "Jeremy"
      }
    ],
    "genre": "PhD thesis",
    "id": "Douglass2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "interactive_fiction",
    "language": "en-US",
    "publisher": "University of California, Santa Barbara",
    "publisher-place": "Santa Barbara, CA, USA",
    "title": "Command lines: Aesthetics and technique in interactive fiction and new media",
    "title-short": "Command lines",
    "type": "thesis"
  },
  {
    "URL": "http://www-jime.open.ac.uk/2004/5/",
    "abstract": "The idea of a resource profile is that it is a multi-faceted, wide ranging description of a resource. A resource profile conforms to no particular XML schema, nor is it authored by any particular author. Additionally, unlike traditional resource descriptions, which are presumed to be instantiated as a single digital file and located in a particular place, a resource profile may be distributed, in pieces, across a large number of locations. And there is no single canonical or authoritative resource profile for a given resource. This paper describes the need for resource profiles, outlines their major conceptual properties, describes different types of constituent metadata, and examines the use of resource profiles in practice.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Downes",
        "given": "Stephen"
      }
    ],
    "container-title": "Journal of Interactive Media in Education",
    "id": "Downes2004",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2004,
          5
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "language": "en-US",
    "title": "Resource profiles",
    "type": "article-journal",
    "volume": "2004"
  },
  {
    "URL": "http://www.tei-c.org/Activities/ETE/Preview/driscoll.xml",
    "author": [
      {
        "family": "Driscoll",
        "given": "Matthew J."
      }
    ],
    "container-title": "Electronic textual editing",
    "editor": [
      {
        "family": "Burnard",
        "given": "Lou"
      },
      {
        "family": "O’Brien O’Keeffe",
        "given": "Katherine"
      },
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "id": "Driscoll2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_edition, digital_humanities, tei",
    "language": "en-US",
    "page": "254-261",
    "publisher": "Modern Language Association of America",
    "publisher-place": "New York, NY, USA",
    "title": "Levels of transcription",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Droysen",
        "given": "Johann G."
      }
    ],
    "edition": "3",
    "id": "Droysen1882",
    "issued": {
      "date-parts": [
        [
          1882
        ]
      ]
    },
    "keyword": "classic, history",
    "language": "de-DE",
    "publisher": "Veit",
    "publisher-place": "Leipzig",
    "title": "Grundriss der Historik",
    "type": "book"
  },
  {
    "ISBN": "9780816677948",
    "URL": "http://dhdebates.gc.cuny.edu/debates/text/34",
    "abstract": "After several decades of digital work, the question remains whether humanists are actually doing anything different or just extending the activities that have always been their core concerns, enabled by advantages of networked digital technology (easier access to primary materials, speed of comparison, searching, etc.). Whatever the answer, the role of humanities scholars is crucial in the production and interpretation of cultural materials. It may turn out that data mining or large corpus processing and distant reading are substantially different from close reading and textual analysis and may bring new insights and techniques into the humanities (Moretti). But my second question frames a very different agenda: Have the humanities had any impact on the digital environment? Can we create graphical interfaces and digital platforms from humanistic methods?",
    "author": [
      {
        "family": "Drucker",
        "given": "Johanna"
      }
    ],
    "chapter-number": "6",
    "container-title": "Debates in the digital humanities",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      }
    ],
    "id": "Drucker2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "85-95",
    "publisher": "University of Minnesota Press",
    "publisher-place": "Minneapolis, MN, USA",
    "title": "Humanistic theory and digital scholarship",
    "type": "chapter"
  },
  {
    "URL": "http://www.idealliance.org/proceedings/xml04/abstracts/paper32.html",
    "author": [
      {
        "family": "DuCharme",
        "given": "Bob"
      }
    ],
    "container-title": "Proceedings of XML2004",
    "id": "DuCharme2004",
    "issued": {
      "date-parts": [
        [
          2004,
          11
        ]
      ]
    },
    "publisher": "IDEAlliance",
    "title": "Documents vs. Data, schemas vs. schemas",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/3194133.3194138",
    "ISBN": "9781450357159",
    "abstract": "Many adaptive systems react to variations in their environment by changing their configuration. Often, they make the adaptation decisions based on some knowledge about how the reconfiguration actions impact the key performance indicators. However, the outcome of these actions is typically affected by uncertainty. Adaptation actions have non-deterministic impacts, potentially leading to multiple outcomes. When this uncertainty is not captured explicitly in the models that guide adaptation, decisions may turn out ineffective or even harmful to the system. Also critical is the need for these models to be interpretable to the human operators that are accountable for the system. However, accurate impact models for actions that result in non-deterministic outcomes are very difficult to obtain and existing techniques that support the automatic generation of these models, mainly based on machine learning, are limited in the way they learn non-determinism. In this paper, we propose a method to learn human-readable models that capture non-deterministic impacts explicitly. Additionally, we discuss how to exploit expert’s knowledge to bootstrap the adaptation process as well as how to use the learned impacts to revise models defined offline. We motivate our work on the adaptation of applications in the cloud, typically affected by hardware heterogeneity and resource contention. To validate our approach we use a prototype based on the RUBiS auction application.",
    "author": [
      {
        "family": "Duarte",
        "given": "Francisco"
      },
      {
        "family": "Gil",
        "given": "Richard"
      },
      {
        "family": "Romano",
        "given": "Paolo"
      },
      {
        "family": "Lopes",
        "given": "Antónia"
      },
      {
        "family": "Rodrigues",
        "given": "Luís"
      }
    ],
    "container-title": "Proceedings of the 13<sup>th</sup> international conference on software engineering for adaptive and self-managing systems (SEAMS ’18)",
    "id": "Duarte2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "keyword": "machine_learning, uncertainty",
    "language": "en-US",
    "page": "196-205",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Learning non-deterministic impact models for adaptation",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/18.1.39",
    "author": [
      {
        "family": "Dubin",
        "given": "David"
      },
      {
        "family": "Renear",
        "given": "Allen"
      },
      {
        "family": "Sperberg‐McQueen",
        "given": "C. M."
      },
      {
        "family": "Huitfeldt",
        "given": "Claus"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Dubin2003a",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "page": "39-47",
    "title": "A logic programming environment for document semantics and inference",
    "type": "article-journal",
    "volume": "18"
  },
  {
    "URL": "http://conferences.idealliance.org/extreme/html/2003/Dubin01/EML2003Dubin01.html",
    "author": [
      {
        "family": "Dubin",
        "given": "David"
      }
    ],
    "container-title": "Proceedings of extreme markup languages 2003",
    "id": "Dubin2003b",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "title": "Object mapping for markup semantics",
    "type": "paper-conference"
  },
  {
    "ISBN": "978-1-4244-5828-8",
    "URL": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5461810",
    "abstract": "The Quran is a significant religious text, followed by the 1.5 billion believers of the Islamic faith worldwide. The text dates to 610-632 CE and is written in Quranic Arabic, the direct ancestor language of modern standard Arabic in use today. This paper presents the Quranic Arabic Dependency Treebank (QADT) and reports on the approaches and solutions used to apply Natural Language Processing to the unique and challenging language of the Quran. This project differs from other Arabic treebanks by providing a deep computational linguistic model based on historical traditional Arabic grammar. The treebank is part of the Quranic Arabic Corpus (http://corpus.quran.com), a popular free Arabic resource developed at the University of Leeds. Motivated by the importance of the Quran as a central religious text, we also report on how online collaborative annotation was used to bring together Quranic scholars and Arabic language experts to ensure a high level of accuracy for grammatical analysis of the entire Quran.",
    "author": [
      {
        "family": "Dukes",
        "given": "Kais"
      },
      {
        "family": "Buckwalter",
        "given": "Tim"
      }
    ],
    "container-title": "The 7th international conference on informatics and systems (INFOS 2010)",
    "id": "Dukes2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "arabic, cultural_heritage",
    "language": "en-US",
    "page": "1-7",
    "publisher": "IEEE",
    "publisher-place": "New York, NY, USA",
    "title": "A dependency treebank of the Quran using traditional Arabic grammar",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s10579-011-9167-7",
    "abstract": "The Quranic Arabic Corpus (http://corpus.quran.com) is a collaboratively constructed linguistic resource with multiple layers of annotation, including morphological segmentation, part-of-speech tagging, and syntactic analysis using dependency grammar. The motivation behind this work is to produce a resource that enables further analysis of the Quran, the 1,400 year old central religious text of Islam. This project contrasts with other Arabic treebanks by providing a deep linguistic model based the historical traditional grammar known as i’rab (إعراب). By adapting this well-known canon of Quranic grammar into a familiar tagset, it is possible to attract a wide variety of expert Arabic linguists and Quranic scholars. This paper presents a new approach to linguistic annotation of an Arabic corpus: online supervised collaboration using a multi-stage approach. The different stages include automatic rule-based tagging, initial manual verification, and online collaborative annotation. A popular website attracting thousands of visitors per day, the Quranic Arabic Corpus has many hundreds of volunteer annotators each suggesting corrections to existing linguistic tagging. To ensure a high-quality resource, a restricted number of expert annotators are given a “supervisor” status, allowing them to review or veto suggestions made by other annotators. The Quran also benefits from a large body of existing historical grammatical analysis, which may be leveraged during this review. In this paper we evaluate and report on the effectiveness of the chosen annotation methodology. We also discuss the unique challenges of annotating Quranic Arabic and describe the custom linguistic software used to aid online collaborative annotation.",
    "author": [
      {
        "family": "Dukes",
        "given": "Kais"
      },
      {
        "family": "Atwell",
        "given": "Eric"
      },
      {
        "family": "Habash",
        "given": "Nizar"
      }
    ],
    "container-title": "Language Resources and Evaluation Journal",
    "id": "Dukes2011b",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "arabic, cultural_heritage",
    "language": "en-US",
    "title": "Supervised collaboration for syntactic annotation of Quranic Arabic",
    "type": "article-journal"
  },
  {
    "URL": "http://www.ibm.com/developerworks/xml/library/x-syntax.html",
    "abstract": "XML’s syntax has brought many benefits due to its interoperability, yet it can be tiresome to author XML documents. Edd Dumbill examines a range of alternative syntaxes for XML, and discusses their benefits and drawbacks.",
    "author": [
      {
        "family": "Dumbill",
        "given": "Edd"
      }
    ],
    "container-title": "developerWorks",
    "id": "Dumbill2002",
    "issued": {
      "date-parts": [
        [
          2002,
          10,
          1
        ]
      ]
    },
    "keyword": "markup, xml",
    "language": "en-US",
    "title": "XML Watch: Exploring alternative syntaxes for XML",
    "title-short": "XML Watch",
    "type": "article-journal"
  },
  {
    "URL": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.1958",
    "abstract": "A statistically based program has been written which learns to distinguish between languages. The amount of training text that such a program needs is surprisingly small, and the amount of text needed to make an identification is also quite small. The program incorporates no linguistic presuppositions other than the assumption that text can be encoded as a string of bytes. Such a program can be used to determine which language small bits of text are in. It also shows a potential for what might...",
    "author": [
      {
        "family": "Dunning",
        "given": "Ted"
      }
    ],
    "id": "Dunning1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "language": "en-US",
    "number": "MCCS-94-273",
    "publisher": "Computing Research Lab (CRL), New Mexico State University",
    "title": "Statistical identification of language",
    "type": "report"
  },
  {
    "DOI": "10.1145/374308.374346",
    "author": [
      {
        "family": "Duval",
        "given": "Erik"
      },
      {
        "family": "Forte",
        "given": "Eddy"
      },
      {
        "family": "Cardinaels",
        "given": "Kris"
      },
      {
        "family": "Verhoeven",
        "given": "Bart"
      },
      {
        "family": "Van Durm",
        "given": "Rafael"
      },
      {
        "family": "Hendrikx",
        "given": "Koen"
      },
      {
        "family": "Forte",
        "given": "Maria W."
      },
      {
        "family": "Ebel",
        "given": "Norbert"
      },
      {
        "family": "Macowicz",
        "given": "Maciej"
      },
      {
        "family": "Warkentyne",
        "given": "Ken"
      },
      {
        "family": "Haenni",
        "given": "Florence"
      }
    ],
    "container-title": "Commun. ACM",
    "id": "Duval2001",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2001,
          5
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "language": "en-US",
    "page": "72-78",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The ARIADNE knowledge pool system",
    "type": "article-journal",
    "volume": "44"
  },
  {
    "DOI": "10.3115/990820.990856",
    "ISBN": "1-55860-717-X",
    "abstract": "Typical approaches to XML authoring view a XML document as a mixture of structure (the tags) and surface (text between the tags). We advocate a radical approach where the surface disappears from the XML document altogether to be handled exclusively by rendering mechanisms. This move is based on the view that the author’s choices when authoring XML documents are best seen as language-neutral semantic decisions, that the structure can then be viewed as interlingual content, and that the textual output should be derived from this content by language-specific realization mechanisms, thus assimilating XML authoring to Multilingual Document Authoring. However, standard XML tools have important limitations when used for such a purpose: (1) they are weak at propagating semantic dependencies between different parts of the structure, and, (2) current XML rendering tools are ill-suited for handling the grammatical combination of textual units. We present two related proposals for overcoming these limitations: one (GF) originating in the tradition of mathematical proof editors and constructive type theory, the other (IG), a specialization of Definite Clause Grammars strongly inspired by GF.",
    "author": [
      {
        "family": "Dymetman",
        "given": "Marc"
      },
      {
        "family": "Lux",
        "given": "Veronika"
      },
      {
        "family": "Ranta",
        "given": "Aarne"
      }
    ],
    "container-title": "Proceedings of the 18th conference on computational linguistics - volume 1",
    "id": "Dymetman2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "authoring, markup, xml",
    "language": "en-US",
    "page": "243-249",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "XML and multilingual document authoring: Convergent trends",
    "title-short": "XML and multilingual document authoring",
    "type": "paper-conference"
  },
  {
    "URL": "http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=COM:2001:0172:FIN:EN:PDF",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "dropping-particle": "of the",
        "family": "European Communities",
        "given": "Commission"
      }
    ],
    "genre": "Communication from the Commission to the Council and\n                  the European Parliament",
    "id": "EC2001",
    "issued": {
      "date-parts": [
        [
          2001,
          3
        ]
      ]
    },
    "number": "COM(2001)172 final",
    "publisher": "Commission of the European Communities",
    "publisher-place": "Brussels",
    "title": "The eLearning action plan: Designing tomorrow’s education",
    "title-short": "The eLearning action plan",
    "type": "report"
  },
  {
    "URL": "http://www.ecma-international.org/publications/standards/Ecma-262.htm",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "Ecma International"
      }
    ],
    "id": "ECMAScript",
    "issued": {
      "date-parts": [
        [
          1999,
          12
        ]
      ]
    },
    "title": "Standard ECMA-262: ECMAScript Language Specification, 3<sup>rd</sup> edition",
    "type": ""
  },
  {
    "URL": "http://www.zora.uzh.ch/50451/",
    "abstract": "In this paper we introduce a module that combines person name recognition and reference resolution for German. Our data consisted of a corpus of Alpine texts. This text type poses special challenges because of a multitude of toponyms, some of which interfere with person names. Our reference resolution algorithm outputs person entities based on their last names and first names along with their associated features (jobs, addresses, academic titles).",
    "author": [
      {
        "family": "Ebling",
        "given": "Sarah"
      },
      {
        "family": "Sennrich",
        "given": "Rico"
      },
      {
        "family": "Klaper",
        "given": "David"
      },
      {
        "family": "Volk",
        "given": "Martin"
      }
    ],
    "container-title": "Proceedings of the 5<sup>th</sup> language & technology conference (LTC 2011)",
    "id": "Ebling2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "title": "Digging for names in the mountains: Combined person name recognition and reference resolution for German alpine texts",
    "title-short": "Digging for names in the mountains",
    "type": "paper-conference"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=1273073.1273097",
    "abstract": "In this paper we discuss the current methods in the representation of corpora annotated at multiple levels of linguistic organization (so-called multi-level or multi-layer corpora). Taking five approaches which are representative of the current practice in this area, we discuss the commonalities and differences between them focusing on the underlying data models. The goal of the paper is to identify the common concerns in multi-layer corpus representation and processing so as to lay a foundation for a unifying, modular data model.",
    "author": [
      {
        "family": "Eckart",
        "given": "Richard"
      }
    ],
    "collection-title": "COLING-ACL ’06",
    "container-title": "Proceedings of the COLING/ACL on main conference poster sessions",
    "id": "Eckart2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "corpus_linguistics, nlp, xml",
    "language": "en-US",
    "page": "183-190",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Towards a modular data model for multi-layer annotated corpora",
    "type": "paper-conference"
  },
  {
    "URL": "http://dcpapers.dublincore.org/pubs/article/viewFile/3669/1892",
    "abstract": "Provenance tracking for Linked Data requires the identification of Linked Data resources. Annotating Linked Data on the level of single statements requires the identification of these statements. The concept of a Provenance Context is introduced as the basis for a consistent data model for Linked Data that incorporates current best-practices and creates identity for every published Linked Dataset. A comparison of this model with the Dublin Core Abstract Model is provided to gain further understanding, how Linked Data affects the traditional view on metadata and to what extent our approach could help to mediate. Finally, a linking mechanism based on RDF reification is developed to annotate single statements within a Provenance Context.",
    "author": [
      {
        "family": "Eckert",
        "given": "Kai"
      }
    ],
    "container-title": "2013 proceedings of the international conference on dublin core and metadata applications (DC-2013)",
    "editor": [
      {
        "family": "Foulonneau",
        "given": "Muriel"
      },
      {
        "family": "Eckert",
        "given": "Kai"
      }
    ],
    "id": "Eckert2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_humanities, linked_data",
    "language": "en-US",
    "page": "9-18",
    "publisher": "DCMI",
    "title": "Provenance and annotations for linked data",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqv061",
    "author": [
      {
        "family": "Eder",
        "given": "Maciej"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Eder2017",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "page": "50-64",
    "title": "Visualization in stylometry: Cluster analysis using networks",
    "title-short": "Visualization in stylometry",
    "type": "article-journal",
    "volume": "32"
  },
  {
    "DOI": "10.3390/informatics6030036",
    "author": [
      {
        "family": "Edmond",
        "given": "Jennifer"
      }
    ],
    "container-title": "Informatics",
    "id": "Edmond2019",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "title": "Strategies and recommendations for the management of uncertainty in research tools and environments for digital history",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "DOI": "10.11647/obp.0192",
    "editor": [
      {
        "family": "Edmond",
        "given": "Jennifer"
      }
    ],
    "id": "Edmond2020",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Open Book Publishers",
    "title": "Digital technology and the practices of humanities research",
    "type": "book"
  },
  {
    "DOI": "10.11647/obp.0192.01",
    "author": [
      {
        "family": "Edmond",
        "given": "Jennifer"
      }
    ],
    "chapter-number": "1",
    "container-title": "Digital technology and the practices of humanities research",
    "editor": [
      {
        "family": "Edmond",
        "given": "Jennifer"
      }
    ],
    "id": "Edmond2020-ch01",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "1-20",
    "publisher": "Open Book Publishers",
    "title": "Introduction: Power, practices, and the gatekeepers of humanistic research in the digital age",
    "title-short": "Introduction",
    "type": "chapter"
  },
  {
    "DOI": "10.11647/obp.0192.02",
    "author": [
      {
        "dropping-particle": "van der",
        "family": "Weel",
        "given": "Adriaan"
      },
      {
        "family": "Praal",
        "given": "Fleur"
      }
    ],
    "chapter-number": "2",
    "container-title": "Digital technology and the practices of humanities research",
    "editor": [
      {
        "family": "Edmond",
        "given": "Jennifer"
      }
    ],
    "id": "Weel2020",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "21-48",
    "publisher": "Open Book Publishers",
    "title": "Publishing in the digital humanities: The treacle of the academic tradition",
    "title-short": "Publishing in the digital humanities",
    "type": "chapter"
  },
  {
    "DOI": "10.11647/obp.0192.06",
    "author": [
      {
        "dropping-particle": "van",
        "family": "Zundert",
        "given": "Joris J."
      },
      {
        "family": "Antonijević",
        "given": "Smiljana"
      },
      {
        "family": "Andrews",
        "given": "Tara L."
      }
    ],
    "chapter-number": "6",
    "container-title": "Digital technology and the practices of humanities research",
    "editor": [
      {
        "family": "Edmond",
        "given": "Jennifer"
      }
    ],
    "id": "Zundert2020",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "123-162",
    "publisher": "Open Book Publishers",
    "title": "“Black boxes” and true colour – a rhetoric of scholarly code",
    "type": "chapter"
  },
  {
    "DOI": "10.11647/obp.0192.10",
    "author": [
      {
        "family": "Tóth-Czifra",
        "given": "Erzsébet"
      }
    ],
    "chapter-number": "10",
    "container-title": "Digital technology and the practices of humanities research",
    "editor": [
      {
        "family": "Edmond",
        "given": "Jennifer"
      }
    ],
    "id": "Toth-Czifra2020",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "235-266",
    "publisher": "Open Book Publishers",
    "title": "The risk of losing the thick description: Data management challenges faced by the arts and humanities in the evolving FAIR data ecosystem",
    "title-short": "The risk of losing the thick description",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/949344.949431",
    "author": [
      {
        "family": "Edwards",
        "given": "Stephen H."
      }
    ],
    "container-title": "OOPSLA ’03: Companion of the 18<sup>th</sup> annual ACM SIGPLAN conference on object-oriented programming, systems, languages, and applications",
    "id": "Edwards2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "page": "318-319",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Teaching software testing: Automatic grading meets test-first coding",
    "title-short": "Teaching software testing",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/971300.971312",
    "author": [
      {
        "family": "Edwards",
        "given": "Stephen H."
      }
    ],
    "container-title": "SIGCSE ’04: Proceedings of the 35<sup>th</sup> SIGCSE technical symposium on computer science education",
    "id": "Edwards2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "26-30",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Using software testing to move students from trial-and-error to reflection-in-action",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1094811.1094851",
    "ISBN": "1-59593-031-0",
    "abstract": "Representing programs as text strings makes programming harder then it has to be. The source text of a program is far removed from its behavior. Bridging this conceptual gulf is what makes programming so inhumanly difficult – we are not compilers. Subtext is a new medium in which the representation of a program is the same thing as its execution. Like a spreadsheet, a program is visible and alive, constantly executing even as it is edited. Program edits are coherent semantic transformations.The essence of this new medium is copying. Programs are constructed by copying and executed by copy flow: the projection of changes through copies. The simple idea of copying develops into a rich theory of higher-order continual copying of trees. Notably absent are symbolic names, the workhorse of textual notation, replaced by immediately-bound explicit relationships. Subtext unifies traditionally distinct programming tools and concepts, and enables some novel ones. Ancestral structures are a new primitive data type that combines the features of lists and records, along with unproblematic multiple inheritance. Adaptive conditionals use first-class program edits to dynamically adapt behavior.A prototype implementation shows promise, but calls for much further research. Subtext suggests that we can make programming radically easier, if we are willing to be radical.",
    "author": [
      {
        "family": "Edwards",
        "given": "Jonathan"
      }
    ],
    "collection-title": "OOPSLA ’05",
    "container-title": "Proceedings of the 20th annual ACM SIGPLAN conference on object-oriented programming, systems, languages, and applications",
    "id": "Edwards2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "505-518",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Subtext: Uncovering the simplicity of programming",
    "title-short": "Subtext",
    "type": "paper-conference"
  },
  {
    "abstract": "Traditional information retrieval systems use query words to identify relevant documents. In difficult retrieval tasks, however, one needs access to a wealth of background knowledge. We present a method that uses Wikipedia-based feature generation to improve retrieval performance. Intuitively, we expect that using extensive world knowledge is likely to improve recall but may adversely affect precision. High quality feature selection is necessary to maintain high precision, but here we do not have the labeled training data for evaluating features, that we have in supervised learning. We present a new feature selection method that is inspired by pseudo-relevance feedback. We use the top-ranked and bottom-ranked documents retrieved by the bag-of-words method as representative sets of relevant and non-relevant documents. The generated features are then evaluated and filtered on the basis of these sets. Experiments on TREC data confirm the superior performance of our method compared to the previous state of the art.",
    "author": [
      {
        "family": "Egozi",
        "given": "Ofer"
      },
      {
        "family": "Gabrilovich",
        "given": "Evgeniy"
      },
      {
        "family": "Markovitch",
        "given": "Shaul"
      }
    ],
    "container-title": "Proceedings of AAAI’08",
    "id": "Egozi2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ir, wikipedia",
    "language": "en-US",
    "page": "1132-1137",
    "title": "Concept-based feature generation and selection for information retrieval",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1961209.1961211",
    "ISSN": "1046-8188",
    "abstract": "Information retrieval systems traditionally rely on textual keywords to index and retrieve documents. Keyword-based retrieval may return inaccurate and incomplete results when different keywords are used to describe the same concept in the documents and in the queries. Furthermore, the relationship between these related keywords may be semantic rather than syntactic, and capturing it thus requires access to comprehensive human world knowledge. Concept-based retrieval methods have attempted to tackle these difficulties by using manually built thesauri, by relying on term cooccurrence data, or by extracting latent word relationships and concepts from a corpus. In this article we introduce a new concept-based retrieval approach based on Explicit Semantic Analysis (ESA), a recently proposed method that augments keyword-based text representation with concept-based features, automatically extracted from massive human knowledge repositories such as Wikipedia. Our approach generates new text features automatically, and we have found that high-quality feature selection becomes crucial in this setting to make the retrieval more focused. However, due to the lack of labeled data, traditional feature selection methods cannot be used, hence we propose new methods that use self-generated labeled training data. The resulting system is evaluated on several TREC datasets, showing superior performance over previous state-of-the-art results.",
    "author": [
      {
        "family": "Egozi",
        "given": "Ofer"
      },
      {
        "family": "Markovitch",
        "given": "Shaul"
      },
      {
        "family": "Gabrilovich",
        "given": "Evgeniy"
      }
    ],
    "container-title": "ACM Transactions on Information Systems",
    "id": "Egozi2011",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2011,
          4
        ]
      ]
    },
    "keyword": "ir",
    "language": "en-US",
    "page": "8:1-8:34",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Concept-based information retrieval using explicit semantic analysis",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "DOI": "10.1016/s0920-5489(02)00006-5",
    "ISSN": "09205489",
    "abstract": "Succession in standardization is often a problem. The advantages of improvements must be weighed against those of compatibility. If compatibility considerations dominate, a grafting process takes place. According to our taxonomy of succession, there are three types of outcomes. A Type I succession, where grafting is successful, entails compatibility between successors, technical paradigm compliance and continuity in the standards trajectory. In this paper, we examine issues of succession and focus on the Extensible Markup Language (XML). It was to be grafted on the Standard Generalized Markup Language (SGML), a stable standard since 1988. However, XML was a profile, a subset and an extension of SGML (1988). Adaptation of SGML was needed (SGML 1999) to forge full (downward) compatibility with XML (1998). We describe the grafting efforts and analyze their outcomes. Our conclusion is that although SGML was a technical exemplar for XML developers, full compatibility was not achieved. The widespread use of HyperText Markup Language (HTML) exemplified the desirability of simplicity in XML standardization. This and HTML’s user market largely explain the discontinuity in SGML–XML succession.",
    "author": [
      {
        "family": "Egyedi",
        "given": "Tineke M."
      },
      {
        "family": "Loeffen",
        "given": "Arjan"
      }
    ],
    "container-title": "Computer Standards & Interfaces",
    "id": "Egyedi1992",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "document_engineering, sgml, standard, xml",
    "language": "en-US",
    "page": "279-290",
    "title": "Succession in standardization: Grafting XML onto SGML",
    "title-short": "Succession in standardization",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "DOI": "10.1057/9781137544582",
    "author": [
      {
        "family": "Eide",
        "given": "Øyvind"
      }
    ],
    "id": "Eide2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Palgrave Macmillan",
    "title": "Media boundaries and conceptual modelling",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Ellström",
        "given": "Per-Erik"
      }
    ],
    "container-title": "Higher Education",
    "id": "Ellstroem1983",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1983,
          4
        ]
      ]
    },
    "page": "231-241",
    "title": "Four faces of educational organizations",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "URL": "http://www.dougengelbart.org/pubs/augment-3906.html",
    "abstract": "This is an initial summary report of a project taking a new and systematic approach to improving the intellectual effectiveness of the individual human being. A detailed conceptual framework explores the nature of the system composed of the individual and the tools, concepts, and methods that match his basic capabilities to his problems. One of the tools that shows the greatest immediate promise is the computer, when it can be harnessed for direct on-line assistance, integrated with new concepts and methods.",
    "author": [
      {
        "family": "Engelbart",
        "given": "Douglas C."
      }
    ],
    "genre": "SRI Summary Report",
    "id": "Engelbart1962",
    "issued": {
      "date-parts": [
        [
          1962,
          10
        ]
      ]
    },
    "keyword": "classic, hci, interactive_editing",
    "language": "en-US",
    "number": "AFOSR-3223",
    "publisher": "Stanford Research Institute",
    "title": "Augmenting human intellect: A conceptual framework",
    "title-short": "Augmenting human intellect",
    "type": "report"
  },
  {
    "DOI": "10.1145/1476589.1476645",
    "abstract": "This paper describes a multisponsor research center at Stanford Research Institute in man-computer interaction.",
    "author": [
      {
        "family": "Engelbart",
        "given": "Douglas C."
      },
      {
        "family": "English",
        "given": "William K."
      }
    ],
    "container-title": "Proceedings of the AFIPS ’68 fall joint computer conference, part i",
    "id": "Engelbart1968",
    "issued": {
      "date-parts": [
        [
          1968
        ]
      ]
    },
    "keyword": "classic, document_engineering, hci, hypertext, interactive_editing",
    "language": "en-US",
    "page": "395-410",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A research center for augmenting human intellect",
    "type": "paper-conference"
  },
  {
    "URL": "http://jasss.soc.surrey.ac.uk/11/4/12.html",
    "abstract": "This address treats some enduring misconceptions about modeling. One of these is that the goal is always prediction. The lecture distinguishes between explanation and prediction as modeling goals, and offers sixteen reasons other than prediction to build a model. It also challenges the common assumption that scientific theories arise from and ’summarize’ data, when often, theories precede and guide data collection; without theory, in other words, it is not clear what data to collect. Among other things, it also argues that the modeling enterprise enforces habits of mind essential to freedom. It is based on the author’s 2008 Bastille Day keynote address to the Second World Congress on Social Simulation, George Mason University, and earlier addresses at the Institute of Medicine, the University of Michigan, and the Santa Fe Institute.",
    "author": [
      {
        "family": "Epstein",
        "given": "Joshua M."
      }
    ],
    "container-title": "Journal of Artificial Societies and Social Simulation",
    "id": "Epstein2008",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "formal_models",
    "language": "en-US",
    "title": "Why model?",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "DOI": "10.16995/lefou.16",
    "ISSN": "2515-2076",
    "abstract": "With the publication of this special issue, Le foucaldien continues its experiment of updating the thought of Michel Foucault. Can historical discourse analyses be carried out with the aid of computers? In order to examine this question, we compare Franco Moretti’s Distant Reading with Foucault’s archaeological method. Despite their common origins in the French Annales School, the two approaches differ fundamentally. While Moretti interprets literary data by means of social history, Foucault seeks the immanent meaning of discourses. Our preliminary conclusion: digital archaeology appears to founder on the operationalization of the complex concept of the statement (énoncé).",
    "author": [
      {
        "family": "Erb",
        "given": "Maurice"
      },
      {
        "family": "Ganahl",
        "given": "Simon"
      },
      {
        "family": "Kilian",
        "given": "Patrick"
      }
    ],
    "container-title": "Le foucaldien",
    "id": "Erb2016",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "8+",
    "title": "Distant reading and discourse analysis",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.1109/ICSC.2011.63",
    "abstract": "We present a comprehensive analysis of link discovery approaches. We classify them with regard to the type of knowledge being used, and identify three commonly used sources of knowledge: The text of a document, the document title, and already existing links. We analyze the influence of the knowledge source as well as of the amount of training data used. Results show that the link-based approach performs best if the amount of training data is huge. In a more realistic setting with fewer training data, the text-based approach yields better results.",
    "author": [
      {
        "family": "Erbs",
        "given": "Nicolai"
      },
      {
        "family": "Zesch",
        "given": "Torsten"
      },
      {
        "family": "Gurevych",
        "given": "Iryna"
      }
    ],
    "container-title": "Proceedings of the 5<sup>th</sup> IEEE international conference on semantic computing (IEEE-ICSC)",
    "id": "Erbs2011",
    "issued": {
      "date-parts": [
        [
          2011,
          7
        ]
      ]
    },
    "language": "en-US",
    "page": "83-86",
    "title": "Link discovery: A comprehensive analysis",
    "title-short": "Link discovery",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1596990.1596995",
    "ISSN": "1551-6857",
    "abstract": "Research on the automatic construction of bilingual dictionaries has achieved impressive results. Bilingual dictionaries are usually constructed from parallel corpora, but since these corpora are available only for selected text domains and language pairs, the potential of other resources is being explored as well. In this article, we want to further pursue the idea of using Wikipedia as a corpus for bilingual terminology extraction. We propose a method that extracts term-translation pairs from different types of Wikipedia link information. After that, an SVM classifier trained on the features of manually labeled training data determines the correctness of unseen term-translation pairs.",
    "author": [
      {
        "family": "Erdmann",
        "given": "Maike"
      },
      {
        "family": "Nakayama",
        "given": "Kotaro"
      },
      {
        "family": "Hara",
        "given": "Takahiro"
      },
      {
        "family": "Nishio",
        "given": "Shojiro"
      }
    ],
    "container-title": "ACM Transactions on Multimedia Computing, Communications, and Applications",
    "id": "Erdmann2009",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "wikipedia",
    "language": "en-US",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Improving the extraction of bilingual terminology from Wikipedia",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.1007/11735106_6",
    "ISBN": "978-3-540-33347-0",
    "abstract": "In this paper, we describe a new approach for retrieval in texts with non-standard spelling, which is important for historic texts in English or German. For this purpose, we present a new algorithm for generating search term variants in ancient orthography. By applying a spell checker on a corpus of historic texts, we generate a list of candidate terms for which the contemporary spellings have to be assigned manually. Then our algorithm produces a set of probabilistic rules. These probabilities can be considered for ranking in the retrieval stage. An experimental comparison shows that our approach outperforms competing methods.",
    "author": [
      {
        "family": "Ernst-Gerlach",
        "given": "Andrea"
      },
      {
        "family": "Fuhr",
        "given": "Norbert"
      }
    ],
    "chapter-number": "6",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Advances in information retrieval. 28th european conference on IR research, ECIR 2006",
    "editor": [
      {
        "family": "Lalmas",
        "given": "Mounia"
      },
      {
        "family": "MacFarlane",
        "given": "Andy"
      },
      {
        "family": "Rüger",
        "given": "Stefan"
      },
      {
        "family": "Tombros",
        "given": "Anastasios"
      },
      {
        "family": "Tsikrika",
        "given": "Theodora"
      },
      {
        "family": "Yavlinsky",
        "given": "Alexei"
      }
    ],
    "id": "Ernst-Gerlach2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage, ir",
    "language": "en-US",
    "page": "49-60",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Generating search term variants for text collections with historic spellings",
    "type": "chapter",
    "volume": "3936"
  },
  {
    "DOI": "10.1145/1255175.1255242",
    "ISBN": "978-1-59593-644-8",
    "abstract": "We present a new approach for the retrieval of texts with non-standard spelling, which is important for historic texts e.g. in English or German. In this paper, we describe the overall architecture of our system, followed by its evaluation. Given a search term as lemma, we use a dictionary of contemporary German for finding all inflected and derived forms of the lemma. Then we apply transformation rules (derived from training data) for generating historic spelling variants. For the evaluation, we regard the resulting retrieval quality. The experimental results show that we can improve the retrieval quality for historic collections substantially.",
    "author": [
      {
        "family": "Gerlach",
        "given": "Andrea E."
      },
      {
        "family": "Fuhr",
        "given": "Norbert"
      }
    ],
    "container-title": "JCDL ’07: Proceedings of the 7th ACM/IEEE-CS joint conference on digital libraries",
    "id": "Ernst-Gerlach2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage, ir",
    "language": "en-US",
    "page": "333-341",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Retrieval in text collections with historic spelling using linguistic and spelling variants",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-17187-1_12",
    "ISBN": "978-3-642-17186-4",
    "abstract": "Retrieval in historic documents with non-standard spelling requires a mapping from search terms onto the historic terms in the document. For describing this mapping, we have developed a rule-based approach. The bottleneck of this method has been the training set construction for the algorithm where an expert has to assign manually current word forms to historic spelling variants. As a better solution, we apply a spell checker on a corpus of historic texts, which gives us a list of candidate terms and associated suggestions. The new method generates possible rules for the suggestions and accepts the most frequent rules. Experimental results with German and English texts from different centuries demonstrate the feasibility of our approach. Thus a training set can be constructed with much less initial effort.",
    "author": [
      {
        "family": "Ernst-Gerlach",
        "given": "Andrea"
      },
      {
        "family": "Fuhr",
        "given": "Norbert"
      }
    ],
    "chapter-number": "12",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Information retrieval technology: 6<sup>th</sup> asia information retrieval societies conference, AIRS 2010 taipei, taiwan, december 1-3, 2010 proceedings",
    "editor": [
      {
        "family": "Cheng",
        "given": "Pu-Jen"
      },
      {
        "family": "Kan",
        "given": "Min-Yen"
      },
      {
        "family": "Lam",
        "given": "Wai"
      },
      {
        "family": "Nakov",
        "given": "Preslav"
      }
    ],
    "id": "Ernst-Gerlach2010a",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, english, german",
    "language": "en-US",
    "page": "131-140",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Advanced training set construction for retrieval in historic documents",
    "type": "paper-conference",
    "volume": "6458"
  },
  {
    "URL": "http://www.kde.cs.uni-kassel.de/conf/lwa10/proceedings/proceedings.pdf",
    "abstract": "Für Retrieval in historischen Texten wird eine Abbildung der Suchbegriffe auf historische Varianten in den Dokumenten benötigt. Dafür wurde ein regel-basierter Ansatz entwickelt. Dessen Engpass ist die manuelle Konstruktion der Trainingsdaten. Zur Verbesserung dieses Verfahrens wird aus jedem Vorschlag der Rechtschreibprüfung und dem zugehörigen unbekannten Wort ein Beleg gebildet. Danach werden Regeln generiert und die häufigsten akzeptiert. Experimentelle Ergebnisse basierend auf unserer Belegkollektion zeigen, dass so ein großer Teil der Regeln generiert werden kann. Dadurch können die Trainingsdaten deutlich schneller und mit geringerem manuellem Aufwand erzeugt werden.",
    "author": [
      {
        "family": "Ernst-Gerlach",
        "given": "Andrea"
      },
      {
        "family": "Fuhr",
        "given": "Norbert"
      }
    ],
    "container-title": "LWA 2010 lernen, wissen & adaptivität workshop proceedings",
    "editor": [
      {
        "family": "Atzmueller",
        "given": "Martin"
      },
      {
        "family": "Benz",
        "given": "Dominik"
      },
      {
        "family": "Hotho",
        "given": "Andreas"
      },
      {
        "family": "Stumme",
        "given": "Gerd"
      }
    ],
    "id": "Ernst-Gerlach2010b",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, german",
    "language": "en-US",
    "page": "193-198",
    "title": "Semiautomatische Konstruktion von Trainingsdaten für historische Dokumente",
    "type": "paper-conference"
  },
  {
    "ISBN": "0-7695-2088-X",
    "URL": "http://portal.acm.org/citation.cfm?id=969369",
    "abstract": "One of the aims of the EU project COLLATE is to design and implement a Web-based collaboratory for archives, scientists and end-users working with digitized cultural material. Since the originals of such a material are often unique and scattered in various archives, severe problems arise for their wide fruition. A solution would be to develop intelligent document processing tools that automatically transform printed documents into a web-accessible form such as XML. Here, we propose the use of a document processing system, WISDOM++, which uses heavily machine learning techniques in order to perform such a task, and report promising results obtained in preliminary experiments.",
    "author": [
      {
        "family": "Esposito",
        "given": "Floriana"
      },
      {
        "family": "Malerba",
        "given": "Donato"
      },
      {
        "family": "Semeraro",
        "given": "Giovanni"
      },
      {
        "family": "Ferilli",
        "given": "Stefano"
      },
      {
        "family": "Altamura",
        "given": "Oronzo"
      },
      {
        "family": "Basile",
        "given": "Teresa M."
      },
      {
        "family": "Berardi",
        "given": "Margherita"
      },
      {
        "family": "Ceci",
        "given": "Michelangelo"
      },
      {
        "family": "Di Mauro",
        "given": "Nicola"
      }
    ],
    "container-title": "Proceedings of the first international workshop on document image analysis for libraries (DIAL’04)",
    "id": "Esposito2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "publisher": "IEEE Computer Society",
    "publisher-place": "Washington, DC, USA",
    "title": "Machine learning methods for automatically processing historical documents: From paper acquisition to XML transformation",
    "title-short": "Machine learning methods for automatically processing historical documents",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1023/B:CHUM.0000009278.73498.f4",
    "ISSN": "0010-4817",
    "abstract": "Current term recognition algorithms have centred mostly on the notion of term based on the assumption that terms are monoreferential and as such independent of context. The characteristics and behaviour of terms in real texts are however far removed from this ideal because factors such as text type or communicative situation greatly influence the linguistic realisation of a concept. Context, therefore, is important for the correct identification of terms (Dubuc and Lauriston, 1997). Based on this assumption, we have shifted our emphasis from terms towards surrounding linguistic context, namely verbs, as verbs are considered the central elements in the sentence. More specifically, we have set out to examine whether verbs and verbal syntax in particular, could help us towards the task of automatic term recognition. Our findings suggest that term occurrence varies significantly in different argument structures and different syntactic positions. Additionally, deviant grammatical structures have proved rich environments for terms. The analysis was carried out in three different specialised subcorpora in order to explore how the effectiveness of verbal syntax as a potential indicator of term occurrence can be constrained by factors such as subject matter and text type.",
    "author": [
      {
        "family": "Eumeridou",
        "given": "Eugenia"
      },
      {
        "family": "Nkwenti-Azeh",
        "given": "Blaise"
      },
      {
        "family": "McNaught",
        "given": "John"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Eumeridou2004",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "corpus_linguistics, terminology",
    "language": "en-US",
    "page": "37-60",
    "publisher": "Springer Netherlands",
    "title": "An analysis of verb subcategorization frames in three special language corpora with a view towards automatic term recognition",
    "type": "article-journal",
    "volume": "38"
  },
  {
    "URL": "http://www.iaas.uni-stuttgart.de/RUS-data/TR-2017-02\\%20MUSE4Music.pdf",
    "abstract": "Als systematische Repräsentation von Wissen werden Ontologien bereits in den verschiedensten Bereichen eingesetzt. Schlagworte wie ‟ Semantic Web ” und ‟ Linked Open Data ” stehen für den Versuch, dieses Wissen zusammenzutragen und zugänglich zu machen. Auch für die Musikforschung könnte eine Definition und Organisation ihrer Begriffe, wie sie eine Ontologie leistet, hilfreich sein. Zumindest zur Musiktheorie jedoch gibt es nach unserem Wissensstand noch keine Ontologie. Hier will die vorliegende Publikation einen Anfang machen und eine erste Annäherung an die Ontologisierung der musiktheoretischen Begriffe für symphonische Musik des 19. Jahrhunderts leisten.",
    "author": [
      {
        "family": "Eusterbrock",
        "given": "Linus"
      },
      {
        "family": "Barzen",
        "given": "Johanna"
      },
      {
        "family": "Hentschel",
        "given": "Frank"
      }
    ],
    "id": "Eusterbrock2017",
    "issued": {
      "date-parts": [
        [
          2017,
          6
        ]
      ]
    },
    "keyword": "music, ontologies",
    "language": "en-US",
    "number": "2017/02",
    "publisher": "Universität Stuttgart, Institut für Architektur von Anwendungssystemen",
    "publisher-place": "Stuttgart",
    "title": "Eine Ontologie symphonischer Musik des 19. Jahrhunderts",
    "type": "report"
  },
  {
    "URL": "http://home.nettskolen.com/~torstein/ICDE2004finalpaper.doc",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Fagerberg",
        "given": "Truls"
      },
      {
        "family": "Rekkedal",
        "given": "Torstein"
      }
    ],
    "container-title": "Distance education and technology: Issues and practice",
    "editor": [
      {
        "family": "Murphy",
        "given": "David"
      },
      {
        "family": "Carr",
        "given": "Ronnie"
      },
      {
        "family": "Taylor",
        "given": "James"
      },
      {
        "family": "Tat-meng",
        "given": "Wong"
      }
    ],
    "id": "Fagerberg2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "335-351",
    "publisher": "Open University of Hong Kong Press",
    "publisher-place": "Hong Kong",
    "title": "Enhancing the flexibility of distance education: Designing and trying out a learning environment for mobile distance learners",
    "title-short": "Enhancing the flexibility of distance education",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/174652.174658",
    "ISSN": "00045411",
    "abstract": "We provide a model for reasoning about knowledge and probability together. We allow explicit mention of probabilities in formulas, so that our language has formulas that essentially say ” according to agent i, formula φ holds with probability at least b.” The language is powerful enough to allow reasoning about higher-order probabilities, as well as allowing explicit comparisons of the probabilities an agent places on distinct events. We present a general framework for interpreting such formulas, and consider various properties that might hold of the interrelationship between agents’ probability assignments at different states. We provide a complete axiomatization for reasoning about knowledge and probability, prove a small model property, and obtain decision procedures. We then consider the effects of adding common knowledge and a probabilistic variant of common knowledge to the language.",
    "author": [
      {
        "family": "Fagin",
        "given": "Ronald"
      },
      {
        "family": "Halpern",
        "given": "Joseph Y."
      }
    ],
    "container-title": "Journal of the ACM",
    "id": "Fagin1994",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "logic, uncertainty",
    "language": "en-US",
    "page": "340-367",
    "title": "Reasoning about knowledge and probability",
    "type": "article-journal",
    "volume": "41"
  },
  {
    "DOI": "10.1145/1961209.1961210",
    "ISSN": "1046-8188",
    "abstract": "Developing effective retrieval models is a long-standing central challenge in information retrieval research. In order to develop more effective models, it is necessary to understand the deficiencies of the current retrieval models and the relative strengths of each of them. In this article, we propose a general methodology to analytically and experimentally diagnose the weaknesses of a retrieval function, which provides guidance on how to further improve its performance. Our methodology is motivated by the empirical observation that good retrieval performance is closely related to the use of various retrieval heuristics. We connect the weaknesses and strengths of a retrieval function with its implementations of these retrieval heuristics, and propose two strategies to check how well a retrieval function implements the desired retrieval heuristics. The first strategy is to formalize heuristics as constraints, and use constraint analysis to analytically check the implementation of retrieval heuristics. The second strategy is to define a set of relevance-preserving perturbations and perform diagnostic tests to empirically evaluate how well a retrieval function implements retrieval heuristics. Experiments show that both strategies are effective to identify the potential problems in implementations of the retrieval heuristics. The performance of retrieval functions can be improved after we fix these problems.",
    "author": [
      {
        "family": "Fang",
        "given": "Hui"
      },
      {
        "family": "Tao",
        "given": "Tao"
      },
      {
        "family": "Zhai",
        "given": "Chengxiang"
      }
    ],
    "container-title": "ACM Transactions on Information Systems",
    "id": "Fang2011",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2011,
          4
        ]
      ]
    },
    "keyword": "evaluation, ir",
    "language": "en-US",
    "page": "7:1-7:42",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Diagnostic evaluation of information retrieval models",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "URL": "http://www.aclweb.org/anthology-new/W/W10/W10-1835.bib",
    "abstract": "E-Dictor is a tool for encoding, applying levels of editions, and assigning part-of-speech tags to ancient texts. In short, it works as a WYSIWYG interface to encode text in XML format. It comes from the experience during the building of the Tycho Brahe Parsed Corpus of Historical Portuguese and from consortium activities with other research groups. Preliminary results show a decrease of at least 50% on the overall time taken on the editing process.",
    "author": [
      {
        "dropping-particle": "de",
        "family": "Faria",
        "given": "Pablo P."
      },
      {
        "family": "Kepler",
        "given": "Fabio N."
      },
      {
        "dropping-particle": "Paixão de",
        "family": "Sousa",
        "given": "Maria C."
      }
    ],
    "container-title": "Proceedings of the fourth linguistic annotation workshop",
    "id": "Faria2010",
    "issued": {
      "date-parts": [
        [
          2010,
          7
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "page": "217-221",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Uppsala, Sweden",
    "title": "An integrated tool for annotating historical corpora",
    "type": "paper-conference"
  },
  {
    "abstract": "This article investigates what German books on typography, book design and similar topics have to say about indexing. Some sample indexes are examined in order to illustrate the way in which indexes are currently presented by German publishers.",
    "author": [
      {
        "family": "Fassbender",
        "given": "Jochen"
      }
    ],
    "container-title": "The Indexer",
    "id": "Fassbender2006",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2006,
          10
        ]
      ]
    },
    "keyword": "cultural_heritage, typesetting, typography",
    "language": "en-US",
    "page": "79-82",
    "title": "German indexing: Some observations on typographical practice",
    "title-short": "German indexing",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "DOI": "10.1162/coli.08-010-r1-07-048",
    "ISSN": "0891-2017",
    "abstract": "Idiomatic expressions are plentiful in everyday language, yet they remain mysterious, as it is not clear exactly how people learn and understand them. They are of special interest to linguists, psycholinguists, and lexicographers, mainly because of their syntactic and semantic idiosyncrasies as well as their unclear lexical status. Despite a great deal of research on the properties of idioms in the linguistics literature, there is not much agreement on which properties are characteristic of these expressions. Because of their peculiarities, idiomatic expressions have mostly been overlooked by researchers in computational linguistics. In this article, we look into the usefulness of some of the identified linguistic properties of idioms for their automatic recognition. Specifically, we develop statistical measures that each model a specific property of idiomatic expressions by looking at their actual usage patterns in text. We use these statistical measures in a type-based classification task where we automatically separate idiomatic expressions (expressions with a possible idiomatic interpretation) from similar-on-the-surface literal phrases (for which no idiomatic interpretation is possible). In addition, we use some of the measures in a token identification task where we distinguish idiomatic and literal usages of potentially idiomatic expressions in context.",
    "author": [
      {
        "family": "Fazly",
        "given": "Afsaneh"
      },
      {
        "family": "Cook",
        "given": "Paul"
      },
      {
        "family": "Stevenson",
        "given": "Suzanne"
      }
    ],
    "container-title": "Computational Linguistics",
    "id": "Fazly2009",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "computational_linguistics, phraseology",
    "language": "en-US",
    "page": "61-103",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Unsupervised type and token identification of idiomatic expressions",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "abstract": "WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets. The purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains.",
    "editor": [
      {
        "family": "Fellbaum",
        "given": "Christiane"
      }
    ],
    "id": "Fellbaum1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "lexicography, wordnet",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "WordNet: An electronic lexical database",
    "title-short": "WordNet",
    "type": "book"
  },
  {
    "URL": "http://cairn.info/revue-francaise-de-linguistique-appliquee-2005-2-page-49.htm",
    "abstract": "We discuss the goals and methods of the lexicographic project ” Collocations and Idioms in the German Language” at the Berlin-Brandenburg Academy of Sciences. A very large corpus is tagged and parsed to enable flexible searches for target structures, German verb phrase idioms. On the basis of relevant tokens, an extensive linguistic-lexicographic analysis is performed and recorded on a set of structured forms, which comprise a kind of digital dictionary entry for the target structure. For transparency and future research, each recorded linguistic-lexicographic phenomenon is linked with appropriate corpus tokens. The resulting resource, which combines an exhaustive description of the idioms’ properties with corpus tokens, allows for multiple search types.",
    "author": [
      {
        "family": "Fellbaum",
        "given": "Christiane"
      },
      {
        "family": "Geyken",
        "given": "Alexander"
      }
    ],
    "container-title": "Revue française de linguistique appliquée",
    "id": "Fellbaum2005",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "german, phraseology",
    "page": "49-62",
    "title": "Transforming a corpus into a lexical resource the Berlin Idiom Project",
    "type": "article-journal",
    "volume": "X"
  },
  {
    "author": [
      {
        "family": "Fenske",
        "given": "Wolfram"
      }
    ],
    "genre": "Diplomarbeit",
    "id": "Fenske2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "publisher": "Otto-von-Guericke-Universität Magdeburg",
    "publisher-place": "Magdeburg",
    "title": "Formen der elektronischen Testaufgabe",
    "type": "thesis"
  },
  {
    "DOI": "10.1111/hith.12047",
    "ISSN": "00182656",
    "abstract": "This essay will argue that the traditional opposition between narrative and theory in historical sciences is dissolved if we conceive of narratives as theoretical devices for understanding events in time through special concepts that abridge typical sequences of events. I shall stress, in the context of the Historical Knowledge Epistemological Square (HKES) that emerged with the scientization of history, that history is always narrative, story has a theoretical ground of itself, and scientific histories address the need for a conceptual progression in ever‐improved narratives. This will lead to identification of three major theoretical levels in historical stories: naming, plotting (or emplotment), and formalizing. We revisit Jörn Rüsen’s theory of history as the best starting point, and explore to what extent it could be developed by (i) taking a deeper look into narratological knowledge, and (ii) reanalyzing logically the conceptual strata in order to bridge the overrated Forschung/Darstellung (research/exposition) divide. The corollary: we should consider (scientific) historical writing as the last step of historical research, not as the next step after research is over. This thesis will drive us to a reconsideration of the German Historik regarding the problem of interpretation and exposition. Far from alienating history from science, narrative links history positively to anthropology and biology. The crossing of our triad name‐plot‐model with Rüsen’s four theoretical levels (categories‐types‐concepts‐names) points to the feasibility of expanding Rüsen’s Historik in logical and semiotic directions. Story makes history, theory makes story, and historical reason may proceed.",
    "author": [
      {
        "family": "Fernandez",
        "given": "Juan L."
      }
    ],
    "container-title": "History and Theory",
    "id": "Fernandez2018",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "keyword": "history, uncertainty",
    "language": "en-US",
    "page": "75-103",
    "title": "Story makes history, theory makes story: Developing Rüsen’s <i>Historik</i> in logical and semiotic directions",
    "title-short": "Story makes history, theory makes story",
    "type": "article-journal",
    "volume": "57"
  },
  {
    "DOI": "10.1007/978-94-007-0120-5_11",
    "ISBN": "978-94-007-0119-9",
    "abstract": "We are in the middle of an historical paradigm shift. It is a change similar in scale to those confronting the Library of Alexandria, twenty-two centuries ago. Metadata, indexes and taxonomies were the paradigm during the age of paper and print, and librarians and publishers leveraged them for searching. Now the amount of documents has grown to levels that make those traditional tools less efficient for users and less affordable for publishers. But, in the last three decades, search technologies have created new solutions such as direct queries, relevance ranking or faceted results, as well as the promises of conceptual search engines and ontologies. However, this integration of legal knowledge has not yet proven scalable in large databases: the improvements in recall have a negative effect on precision and performance. We have focused in one key behavior of legal experts in legal searches: the creation of ” better queries” as a result of knowledge of the domain and search techniques. This is the same that happens on taxonomical classical searches, but in full-text we could try to encode part of that knowledge in a search engine. To achieve this goal, we have developed both the technology to semantically analyze documents and queries, and a methodology to fill a dictionary with 10,000 concepts and 40,000 expressions. This has been put in production with a 3 million legal documents database. In addition to the semantic improvements, these developments have created significant improvements in the relevance algorithm and complementary tools such as dynamic summaries and query reformulation trough local context analysis.",
    "author": [
      {
        "family": "Ferrer",
        "given": "Ángel S."
      },
      {
        "family": "Hernández",
        "given": "Carlos F."
      },
      {
        "family": "Rivero",
        "given": "José M."
      }
    ],
    "chapter-number": "11",
    "collection-title": "Law, governance and technology series",
    "container-title": "Approaches to legal ontologies",
    "editor": [
      {
        "family": "Sartor",
        "given": "Giovanni"
      },
      {
        "family": "Casanovas",
        "given": "Pompeu"
      },
      {
        "family": "Biasiotti",
        "given": "Mariangela A."
      },
      {
        "family": "Fernández-Barrera",
        "given": "Meritxell"
      }
    ],
    "id": "Ferrer2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "ir",
    "language": "en-US",
    "page": "179-200",
    "publisher": "Springer Netherlands",
    "publisher-place": "Dordrecht",
    "title": "From thesaurus towards ontologies in large legal databases approaches to legal ontologies",
    "type": "chapter",
    "volume": "1"
  },
  {
    "author": [
      {
        "family": "Feustel",
        "given": "Thomas"
      }
    ],
    "genre": "Master's Thesis",
    "id": "Feustel2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "publisher": "Fakultät für Informatik, Otto-von-Guericke-Universität",
    "publisher-place": "Magdeburg",
    "title": "Analyse von Texteingaben in einem CAA-Werkzeug zur elektronischen Einreichung und Auswertung von Aufgaben",
    "type": "thesis"
  },
  {
    "author": [
      {
        "family": "Fillmore",
        "given": "Charles J."
      }
    ],
    "container-title": "Proceedings of the 1<sup>st</sup> annual meeting",
    "id": "Fillmore1975",
    "issued": {
      "date-parts": [
        [
          1975
        ]
      ]
    },
    "page": "123-131",
    "publisher": "Berkeley Linguistics Society",
    "title": "An alternative to checklist theories of meaning",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/3078081.3078096",
    "ISBN": "9781450352659",
    "abstract": "In the absence of ground truth it is not possible to automatically determine the exact spectrum and occurrences of OCR errors in an OCR’ed text. Yet, for interactive postcorrection of OCR’ed historical printings it is extremely useful to have a statistical profile available that provides an estimate of error classes with associated frequencies, and that points to conjectured errors and suspicious tokens. The method introduced in [3] computes such a profile, combining lexica, pattern sets and advanced matching techniques in a specialized Expectation Maximization (EM) procedure. Here we improve this method in three respects: First, the method in [3] is not adaptive: user feedback obtained by actual postcorrection steps cannot be used to compute refined profiles. We introduce a variant of the method that is open for adaptivity, taking correction steps of the user into account. This leads to higher precision with respect to recognition of erroneous OCR tokens. Second, during postcorrection often new historical patterns are found. We show that adding new historical patterns to the linguistic background resources leads to a second kind of improvement, enabling even higher precision by telling historical spellings apart from OCR errors. Third, the method in [3] does not make any active use of tokens that cannot be interpreted in the underlying channel model. We show that adding these uninterpretable tokens to the set of conjectured errors leads to a significant improvement of the recall for error detection, at the same time improving precision.",
    "author": [
      {
        "family": "Fink",
        "given": "Florian"
      },
      {
        "family": "Schulz",
        "given": "Klaus U."
      },
      {
        "family": "Springmann",
        "given": "Uwe"
      }
    ],
    "container-title": "Proceedings of the 2<sup>nd</sup> international conference on digital access to textual cultural heritage (DATeCH2017)",
    "id": "Fink2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "ocr",
    "language": "en-US",
    "page": "61-66",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Profiling of OCR’ed historical texts revisited",
    "type": "paper-conference"
  },
  {
    "DOI": "10.2307/2110698",
    "URL": "http://jstor.org/stable/2110698",
    "abstract": "In recent years the journals have published an increasing number of articles which present or utilize formal models of political behavior and political processes. At present, however, this research probably reaches only a small audience. In an effort to broaden that audience this paper attempts (1) to describe and illustrate the use of formal models, (2) to explain why some believe that the construction of models is a useful research method, (3) to identify subfields in which models exist and suggest others where models should exist, (4) to discuss various types of existing models, (5) to offer some basic critical standards according to which research which involves models can be judged.",
    "author": [
      {
        "family": "Fiorina",
        "given": "Morris P."
      }
    ],
    "container-title": "American Journal of Political Science",
    "id": "Fiorina1975",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1975
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "en-US",
    "page": "133-159",
    "title": "Formal models in political science",
    "type": "article-journal",
    "volume": "XIX"
  },
  {
    "DOI": "10.1109/VSMM.2009.26",
    "ISBN": "978-0-7695-3790-0",
    "abstract": "The automatic transcription of historical documents is vital for the creation of digital libraries. In order to make images of valuable old documents amenable to browsing, a transcription of high accuracy is needed. In this paper, two state-of-the art recognizers originally developed for modern scripts are applied to medieval documents. The first is based on Hidden Markov Models and the second uses a Neural Network with a bidirectional Long Short-Term Memory. On a dataset of word images extracted from a medieval manuscript of the 13th century, written in Middle High German by several writers, it is demonstrated that a word accuracy of 93.32% is achievable. This is far above the word accuracy of 77.12% achieved with the same recognizers for unconstrained modern scripts written in English. These results encourage the development of real world systems for automatic transcription of historical documents with a view to image and text browsing in digital libraries.",
    "author": [
      {
        "family": "Fischer",
        "given": "Andreas"
      },
      {
        "family": "Wuthrich",
        "given": "Markus"
      },
      {
        "family": "Liwicki",
        "given": "Marcus"
      },
      {
        "family": "Frinken",
        "given": "Volkmar"
      },
      {
        "family": "Bunke",
        "given": "Horst"
      },
      {
        "family": "Viehhauser",
        "given": "Gabriel"
      },
      {
        "family": "Stolz",
        "given": "Michael"
      }
    ],
    "container-title": "2009 15th international conference on virtual systems and multimedia",
    "id": "Fischer2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, handwriting_recognition, ocr",
    "language": "en-US",
    "page": "137-142",
    "publisher": "IEEE",
    "title": "Automatic transcription of handwritten medieval documents",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/ICFHR.2010.47",
    "ISBN": "978-1-4244-8353-2",
    "abstract": "Automatic transcription of historical documents is vital for the creation of digital libraries. In this paper we propose graph similarity features as a novel descriptor for handwriting recognition in historical documents based on Hidden Markov Models. Using a structural graph-based representation of text images, a sequence of graph similarity features is extracted by means of dissimilarity embedding with respect to a set of character prototypes. On the medieval Parzival data set it is demonstrated that the proposed structural descriptor significantly outperforms two well-known statistical reference descriptors for single word recognition.",
    "author": [
      {
        "family": "Fischer",
        "given": "Andreas"
      },
      {
        "family": "Riesen",
        "given": "Kaspar"
      },
      {
        "family": "Bunke",
        "given": "Horst"
      }
    ],
    "container-title": "2010 12<sup>th</sup> international conference on frontiers in handwriting recognition",
    "id": "Fischer2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, handwriting_recognition, ocr",
    "language": "en-US",
    "page": "253-258",
    "publisher": "IEEE",
    "publisher-place": "Los Alamitos, CA, USA",
    "title": "Graph similarity features for HMM-based handwriting recognition in historical documents",
    "type": "paper-conference"
  },
  {
    "URL": "http://kups.ub.uni-koeln.de/4337/",
    "abstract": "Digital technology changes the way scholars work with manuscripts. This volume deepens the questions raised by the first volume on palaeography and codicology in the digital age, published a year ago, particularly questions on digitisation and cataloguing, on character recognition and the analysis of script.Moreover, the focus has been widened to include the fields of computer-aided manuscript research in musicology and history of art, as well as to methodologies applied in computational and natural sciences. Besides Latin, this volume covers also Greek, Glagolitic, Judeo-Arabic and other scripts. The spatio-temporal frame stretches from ancient Egypt of 1800 BC to Paris of the 20th century. With Contributions by: Pádraig Ó Macháin; Armand Tif; Alison Stones, Ken Sochats; Melissa Terras; Silke Schöttle, Ulrike Mehringer; Marilena Maniaci, Paolo Eleuteri; Ezio Ornato; Toby Burrows; Robert Kummer; Lior Wolf, Nachum Dershowitz, Liza Potikha, Tanya German, Roni Shweka, Yacov Choueka; Daniel Deckers, Leif Glaser; Timothy Stinson; Peter Meinlschmidt, Carmen Kämmerer, Volker Märgner; Peter Stokes—Dominique Stutzmann; Stephen Quirke; Markus Diem, Robert Sablatnig, Melanie Gau, Heinz Miklas; Julia Craig-McFeely; Isabelle Schürch, Martin Rüesch; Carole Dornier, Pierre-Yves Buard; Samantha Saidi, Jean-François Bert, Philippe Artières; Elena Pierazzo, Peter Stokes. Introduction by: Franz Fischer, Patrick Sahle. In Collaboration with: Bernhard Assmann, Malte Rehbein, Patrick Sahle.",
    "collection-title": "Schriften des instituts für dokumentologie und editorik",
    "editor": [
      {
        "family": "Fischer",
        "given": "Franz"
      },
      {
        "family": "Fritze",
        "given": "Christiane"
      },
      {
        "family": "Vogeler",
        "given": "Georg"
      }
    ],
    "id": "Fischer2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_edition",
    "language": "en-US",
    "publisher": "BoD",
    "publisher-place": "Norderstedt, Germany",
    "title": "Kodikologie und paläographie im digitalen zeitalter 2 – codicology and palaeography in the digital age 2",
    "type": "book",
    "volume": "3"
  },
  {
    "ISBN": "0415039347",
    "abstract": "Television Culture provides a comprehensive introduction to television studies. Fiske examines both the economic and cultural aspects of television, and investigates it in terms of both theory and text-based criticism. Fiske introduces the main arguments from current British, American, Australian, and French scholarship in a style accessible to the student, providing an integrated study of approaches to the medium.",
    "author": [
      {
        "family": "Fiske",
        "given": "John"
      }
    ],
    "id": "Fiske1987",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "keyword": "intertextuality",
    "language": "en-US",
    "publisher": "Routledge",
    "publisher-place": "London",
    "title": "Television culture",
    "type": "book"
  },
  {
    "ISBN": "9780415596473",
    "URL": "http://www.worldcat.org/isbn/9780415596473",
    "abstract": "This revised edition of a now classic text includes a new introduction by Henry Jenkins, explaining ’Why Fiske Still Matters’ for today’s students, followed by a discussion between former Fiske students Ron Becker, Aniko Bodroghkozy, Steve Classen, Elana Levine, Jason Mittell, Greg Smith and Pam Wilson on ’John Fiske and Television Culture’. Both underline the continuing relevance of this foundational text in the study of contemporary media and popular culture. Television is unique in its ability to produce so much pleasure and so many meanings for such a wide variety of people. In this book, John Fiske looks at television’s role as an agent of popular culture, and goes on to consider the relationship between this cultural dimension and television’s status as a commodity of the cultural industries that are deeply inscribed with capitalism. He makes use of detailed textual analysis and audience studies to show how television is absorbed into social experience, and thus made into popular culture. Audiences, Fiske argues, are productive, discriminating, and televisually literate. Television Culture provides a comprehensive introduction for students to an integral topic on all communication and media studies courses.",
    "author": [
      {
        "family": "Fiske",
        "given": "John"
      }
    ],
    "edition": "2",
    "id": "Fiske2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "intertextuality",
    "language": "en-US",
    "publisher": "Routledge",
    "publisher-place": "London",
    "title": "Television culture",
    "type": "book"
  },
  {
    "DOI": "10.1515/9783110211429.4.552",
    "author": [
      {
        "family": "Fitschen",
        "given": "Arne"
      },
      {
        "family": "Gupta",
        "given": "Piklu"
      }
    ],
    "chapter-number": "25",
    "collection-title": "Handbooks of linguistics and communication science",
    "container-title": "Corpus linguistics. An international handbook",
    "editor": [
      {
        "family": "Lüdeling",
        "given": "Anke"
      },
      {
        "family": "Kytö",
        "given": "Merja"
      }
    ],
    "id": "Fitschen2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "computational_linguistics",
    "language": "en-US",
    "page": "552-564",
    "publisher": "Mouton de Gruyter",
    "publisher-place": "Berlin/New York",
    "title": "Lemmatising and morphological tagging",
    "type": "chapter",
    "volume": "1"
  },
  {
    "DOI": "10.1002/9781118680605.ch16",
    "author": [
      {
        "family": "Flanders",
        "given": "Julia"
      },
      {
        "family": "Jannidis",
        "given": "Fotis"
      }
    ],
    "chapter-number": "16",
    "container-title": "A new companion to digital humanities",
    "editor": [
      {
        "family": "Schreibman",
        "given": "Susan"
      },
      {
        "family": "Siemens",
        "given": "Ray"
      },
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "id": "Flanders2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "229-237",
    "publisher": "Wiley",
    "publisher-place": "Chichester",
    "title": "Data modeling",
    "type": "chapter"
  },
  {
    "URL": "http://nbn-resolving.org/urn:nbn:de:bvb:20-opus-111270",
    "abstract": "Based on the results of a 3-day workshop at the Brown University (2012) this white paper tries to sum up important topics and problems which came up in the presentations and discussions and to outline some general aspects of data modeling in digital humanities. Starting with an attempt to define data modeling it introduces distinctions like curation-driven vs. research-driven for a more general description of data modeling. The second part discusses specific problems and challenges of data modeling in the Humanities, while the third part outlines practical aspects, like the creation of data models or their evaluation.",
    "author": [
      {
        "family": "Flanders",
        "given": "Julia"
      },
      {
        "family": "Jannidis",
        "given": "Fotis"
      }
    ],
    "id": "FlandersJannidis2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "en-US",
    "title": "Knowledge organization and data modeling in the humanities",
    "type": "report"
  },
  {
    "URL": "http://cairn.info/revue-francaise-de-linguistique-appliquee-2005-2-page-119.htm",
    "abstract": "Spell-checkers and grammar checkers are among the most widely used natural language processing applications. At the heart of these proofing tools, one finds the electronic lexicon, where the various types of lexical information these tools rely on are stored. We describe some of these linguistic properties and show how the border between spell-checker and grammar checker tends to become blurred in the most recent versions of these tools, even if, for the time being at least, the two types of tools keep meeting distinct needs.",
    "author": [
      {
        "family": "Fontenelle",
        "given": "Thierry"
      }
    ],
    "container-title": "Revue française de linguistique appliquée",
    "id": "Fontenelle2005",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "interactive_editing, spelling_correction",
    "language": "en-US",
    "page": "119-128",
    "title": "Dictionnaires et outils de correction linguistique",
    "type": "article-journal",
    "volume": "X"
  },
  {
    "abstract": "This paper addresses several issues of broad concern in the United States: population trends; the quality of urban life; national policy for urban growth; and the unexpected, ineffective, or detrimental results often generated by government programs in these areas. The nation exhibits a growing sense of futility as it repeatedly attacks deficiencies in our social system while the symptoms continue to worsen. Legislation is debated and passed with great promise and hope. But many programs prove to be ineffective. Results often seem unrelated to those expected when the programs were planned. At times programs cause exactly the reverse of desired results. It is now possible to explain how such contrary results can happen. There are fundamental reasons why people misjudge the behavior of social systems. There are orderly processes at work in the creation of human judgment and intuition that frequently lead people to wrong decisions when faced with complex and highly interacting systems. Until we come to a much better understanding of social systems, we should expect that attempts to develop corrective programs will continue to disappoint us. The purpose of this paper is to leave with its readers a sense of caution about continuing to depend on the same past approaches that have led to our present feeling of frustration and to suggest an approach which can eventually lead to a better understanding of our social systems and thereby to more effective policies for guiding the future.",
    "author": [
      {
        "family": "Forrester",
        "given": "Jay W."
      }
    ],
    "container-title": "Technology Review",
    "id": "Forrester1971",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "keyword": "formal_models, mental_models",
    "language": "en-US",
    "page": "52-68",
    "title": "Counterintuitive behavior of social systems",
    "type": "article-journal",
    "volume": "73"
  },
  {
    "DOI": "10.1007/bf00148991",
    "ISSN": "0040-5833",
    "abstract": "This paper addresses several issues of broad concern in the United States: population trends; the quality of urban life; national policy for urban growth; and the unexpected, ineffective, or detrimental results often generated by government programs in these areas. The author does attempt to indicate how multiloop feed-back systems (to which our social systems belong) mislead us because our intuition and judgement have been formed to expect behavior different from that actually possessed by such systems. At times programs cause exactly the reverse of desired results. It is now possible to explain how such contrary results can happen. There are fundamental reasons why people misjudge the behavior of social systems. There are orderly processes at work that frequently lead people to wrong decisions when faced with complex and highly interacting systems. Until we come to a much better understanding of social systems, we should expect that attempts to develop corrective programs will continue to disappoint us.",
    "author": [
      {
        "family": "Forrester",
        "given": "Jay W."
      }
    ],
    "container-title": "Theory and Decision",
    "id": "Forrester1971-TAD",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "keyword": "formal_models, mental_models",
    "language": "en-US",
    "page": "109-140",
    "title": "Counterintuitive behavior of social systems",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "URL": "http://static.clexchange.org/ftp/documents/system-dynamics/SD1993-01CounterintuitiveBe.pdf",
    "abstract": "This paper addresses several social concerns: population trends; quality of urban life; policies for urban growth; and the unexpected, ineffective, or detrimental results often generated by government programs. Society becomes frustrated as repeated attacks on deficiencies in social systems lead only to worse symptoms. Legislation is debated and passed with great hope, but many programs prove to be ineffective. Results are often far short of expectations. Because dynamic behavior of social systems is not understood, government programs often cause exactly the reverse of desired results. The field of system dynamics now can explain how such contrary results happen. Fundamental reasons cause people to misjudge behavior of social systems. Orderly processes in creating human judgment and intuition lead people to wrong decisions when faced with complex and highly interacting systems. Until we reach a much better public understanding of social systems, attempts to develop corrective programs for social troubles will continue to be disappointing. This paper cautions against continuing to depend on the same past approaches that have led to present feelings of frustration. cNew methods developed over the last 30 years will lead to a better understanding of social systems and thereby to more effective policies for guiding the future.",
    "author": [
      {
        "family": "Forrester",
        "given": "Jay W."
      }
    ],
    "id": "Forrester1995",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "keyword": "formal_models",
    "language": "en-US",
    "note": "This is a revised version of the 1971 article published in Technology Review.",
    "title": "Counterintuitive behavior of social systems",
    "type": "report"
  },
  {
    "DOI": "10.1145/1016848.1016879",
    "ISSN": "0362-1340",
    "abstract": "This paper presents a methodology for implementing natural language morphology in the functional language Haskell. The main idea behind is simple: instead of working with untyped regular expressions, which is the state of the art of morphology in computational linguistics, we use finite functions over hereditarily finite algebraic datatypes. The definitions of these datatypes and functions are the language-dependent part of the morphology. The language-independent part consists of an untyped dictionary format which is used for synthesis of word forms, and a decorated trie, which is used for analysis.Functional Morphology builds on ideas introduced by Huet in his computational linguistics toolkit Zen, which he has used to implement the morphology of Sanskrit. The goal has been to make it easy for linguists, who are not trained as functional programmers, to apply the ideas to new languages. As a proof of the productivity of the method, morphologies for Swedish, Italian, Russian, Spanish, and Latin have already been implemented using the library. The Latin morphology is used as a running example in this article.",
    "author": [
      {
        "family": "Forsberg",
        "given": "Markus"
      },
      {
        "family": "Ranta",
        "given": "Aarne"
      }
    ],
    "container-title": "ACM SIGPLAN Notices",
    "id": "Forsberg2004",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "computational_linguistics, morphology, nlp",
    "language": "en-US",
    "page": "213-223",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Functional morphology",
    "type": "article-journal",
    "volume": "39"
  },
  {
    "URL": "http://www.cse.chalmers.se/alumni/markus/phd2007_print_version.pdf",
    "abstract": "Purely functional programming and meta programming based on declarative models are productive approaches to language processing and language resource building. Three tools are presented as evidence of this: BNF Converter, Functional Morphology, and Extract.",
    "author": [
      {
        "family": "Forsberg",
        "given": "Markus"
      }
    ],
    "genre": "PhD thesis",
    "id": "Forsberg2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage, morphology, swedish",
    "language": "en-US",
    "publisher": "Chalmers University of Technology",
    "publisher-place": "Gothenburg, Sweden",
    "title": "Three tools for language processing: BNF Converter, Functional Morphology, and Extract",
    "title-short": "Three tools for language processing",
    "type": "thesis"
  },
  {
    "DOI": "10.1093/llc/fqr029",
    "abstract": "In this study, we use computational methods to evaluate and quantify philological evidence that an eighth century CE Latin poem by Paul the Deacon was influenced by the works of the classical Roman poet Catullus. We employ a hybrid feature set composed of n-gram frequencies for linguistic structures of three different kinds—words, characters, and metrical quantities. This feature set is evaluated using a one-class support vector machine approach. While all three classes of features prove to have something to say about poetic style, the character-based features prove most reliable in validating and quantifying the subjective judgments of the practicing Latin philologist. Word-based features were most useful as a secondary refining tool, while metrical data were not yet able to improve classification. As these features are developed in ongoing work, they are simultaneously being incorporated into an existing online tool for allusion detection in Latin poetry.",
    "author": [
      {
        "family": "Forstall",
        "given": "Christopher W."
      },
      {
        "family": "Jacobson",
        "given": "Sarah L."
      },
      {
        "family": "Scheirer",
        "given": "Walter J."
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Forstall2011",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "intertextuality",
    "language": "en-US",
    "page": "285",
    "title": "Evidence of intertextuality: Investigating Paul the Deacon’s Angustae Vitae",
    "title-short": "Evidence of intertextuality",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "DOI": "10.1093/llc/fqu014",
    "ISSN": "2055-7671",
    "abstract": "The study of intertextuality, or how authors make artistic use of other texts in their works, has a long tradition, and has in recent years benefited from a variety of applications of digital methods. This article describes an approach for detecting the sorts of intertexts that literary scholars have found most meaningful, as embodied in the free Tesserae website http://tesserae.caset.buffalo.edu/. Tests of Tesserae Versions 1 and 2 showed that word-level n-gram matching could recall a majority of parallels identified by scholarly commentators in a benchmark set. But these versions lacked precision, so that the meaningful parallels could be found only among long lists of those that were not meaningful. The Version 3 search described here adds a second stage scoring system that sorts the found parallels by a formula accounting for word frequency and phrase density. Testing against a benchmark set of intertexts in Latin epic poetry shows that the scoring system overall succeeds in ranking parallels of greater significance more highly, allowing site users to find meaningful parallels more quickly. Users can also choose to adjust both recall and precision by focusing only on results above given score levels. As a theoretical matter, these tests establish that lemma identity, word frequency, and phrase density are important constituents of what make a phrase parallel a meaningful intertext.",
    "author": [
      {
        "family": "Forstall",
        "given": "Christopher"
      },
      {
        "family": "Coffee",
        "given": "Neil"
      },
      {
        "family": "Buck",
        "given": "Thomas"
      },
      {
        "family": "Roache",
        "given": "Katherine"
      },
      {
        "family": "Jacobson",
        "given": "Sarah"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Forstall2015",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2015,
          12,
          15
        ]
      ]
    },
    "keyword": "digital_humanities, intertextuality",
    "language": "en-US",
    "page": "503-515",
    "publisher": "Oxford University Press",
    "title": "Modeling the scholars: Detecting intertextuality through enhanced word-level n-gram matching",
    "title-short": "Modeling the scholars",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "author": [
      {
        "family": "Forsythe",
        "given": "George E."
      },
      {
        "family": "Wirth",
        "given": "Niklaus"
      }
    ],
    "container-title": "Commun. ACM",
    "id": "Forsythe1965",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          1965
        ]
      ]
    },
    "language": "en-US",
    "page": "275-278",
    "title": "Automatic grading programs",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "DOI": "10.1023/b:aire.0000036259.68818.1e",
    "ISSN": "0269-2821",
    "abstract": "This paper presents a robust parsing approach which is designed to address the issue of syntactic errors in text. The approach is based on the concept of an error grammar which is a grammar of ungrammatical sentences. An error grammar is derived from a conventional grammar on the basis of an analysis of a corpus of observed ill-formed sentences. A robust parsing algorithm is presented which is applied after a conventional bottom–up parsing algorithm has failed. This algorithm combines a rule from the error grammar with rules from the normal grammar to arrive at a parse for an ungrammatical sentence. This algorithm is applied to 50 test sentences, with encouraging results.",
    "author": [
      {
        "family": "Foster",
        "given": "Jennifer"
      },
      {
        "family": "Vogel",
        "given": "Carl"
      }
    ],
    "container-title": "Artificial Intelligence Review",
    "id": "Foster2004",
    "issue": "3-4",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "robust_parsing",
    "language": "en-US",
    "page": "269-291",
    "title": "Parsing Ill-Formed text using an error grammar",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "DOI": "10.1007/s10032-007-0059-8",
    "ISSN": "1433-2833",
    "abstract": "This article describes how a treebank of ungrammatical sentences can be created from a treebank of well-formed sentences. The treebank creation procedure involves the automatic introduction of frequently occurring grammatical errors into the sentences in an existing treebank, and the minimal transformation of the original analyses in the treebank so that they describe the newly created ill-formed sentences. Such a treebank can be used to test how well a parser is able to ignore grammatical errors in texts (as people do), and can be used to induce a grammar capable of analysing such sentences. This article demonstrates these two applications using the Penn Treebank. In a robustness evaluation experiment, two state-of-the-art statistical parsers are evaluated on an ungrammatical version of Sect. 23 of the Wall Street Journal (WSJ) portion of the Penn treebank. This experiment shows that the performance of both parsers degrades with grammatical noise. A breakdown by error type is provided for both parsers. A second experiment retrains both parsers using an ungrammatical version of WSJ Sections 2–21. This experiment indicates that an ungrammatical treebank is a useful resource in improving parser robustness to grammatical errors, but that the correct combination of grammatical and ungrammatical training data has yet to be determined.",
    "author": [
      {
        "family": "Foster",
        "given": "Jennifer"
      }
    ],
    "container-title": "International Journal on Document Analysis and Recognition",
    "id": "Foster2007",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "noisy_text",
    "language": "en-US",
    "page": "129-145",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Treebanks gone bad",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "URL": "http://aclweb.org/anthology/N10-1060",
    "abstract": "We evaluate the Berkeley parser on text from an online discussion forum. We evaluate the parser output with and without gold tokens and spellings (using Sparseval and Parseval), and we compile a list of problematic phenomena for this domain. The Parseval f-score for a small development set is 77.56. This increases to 80.27 when we apply a set of simple transformations to the input sentences and to the Wall Street Journal (WSJ) training sections.",
    "author": [
      {
        "family": "Foster",
        "given": "Jennifer"
      }
    ],
    "container-title": "Human language technologies: The 2010 annual conference of the north american chapter of the association for computational linguistics",
    "id": "Foster2010",
    "issued": {
      "date-parts": [
        [
          2010,
          6
        ]
      ]
    },
    "keyword": "domain_adaptation, nlp, spelling_correction",
    "language": "en-US",
    "page": "381-384",
    "publisher": "Association for Computational Linguistics",
    "title": "“cba to check the spelling”: Investigating parser performance on discussion forum posts",
    "title-short": "“cba to check the spelling”",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.aaai.org/ocs/index.php/WS/AAAIW11/paper/view/3912",
    "abstract": "We evaluate the statistical dependency parser, Malt, on a new dataset of sentences taken from tweets. We use a version of Malt which is trained on gold standard phrase structure Wall Street Journal (WSJ) trees converted to Stanford labelled dependencies. We observe a drastic drop in performance moving from our in-domain WSJ test set to the new Twitter dataset, much of which has to do with the propagation of part-of-speech tagging errors. Retraining Malt on dependency trees produced by a state-of-the-art phrase structure parser, which has itself been self-trained on Twitter material, results in a signiﬁcant improvement. We analyse this improvement by examining in detail the effect of the retraining on individual dependency types.",
    "author": [
      {
        "family": "Foster",
        "given": "Jennifer"
      },
      {
        "family": "Çetinoğlu",
        "given": "Özlem"
      },
      {
        "family": "Wagner",
        "given": "Joachim"
      },
      {
        "family": "Roux",
        "given": "Joseph L."
      },
      {
        "family": "Hogan",
        "given": "Stephen"
      },
      {
        "family": "Nivre",
        "given": "Joakim"
      },
      {
        "family": "Hogan",
        "given": "Deirdre"
      },
      {
        "dropping-particle": "van",
        "family": "Genabith",
        "given": "Josef"
      }
    ],
    "collection-title": "AAAI workshops",
    "container-title": "Analyzing microtext: Papers from the 2011 AAAI workshop",
    "id": "Foster2011a",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "microtext, nlp, parsing",
    "language": "en-US",
    "page": "20-25",
    "publisher": "AAAI",
    "publisher-place": "Palo Alto, CA, USA",
    "title": "#Hardtoparse: POS tagging and parsing the Twitterverse",
    "title-short": "#Hardtoparse",
    "type": "paper-conference",
    "volume": "WS-11-05"
  },
  {
    "DOI": "10.3115/992730.992798",
    "abstract": "The results of an experiment are presented in which an approach for robust parsing has been applied incrementally. They confirm that due to the robust nature of the underlying technology an arbitrary prefix of a sentence can be analysed into an intermediate structural description which is able to direct the further analysis with a high degree of reliability. Most notably, this result can be achieved without adapting the grammar or the parsing algorithms to the case of incremental processing. The resulting incremental parsing procedure is significantly faster if compared to a non-incremental best-first search. Additionally it turns out that longer sentences benefit most from this acceleration.",
    "author": [
      {
        "family": "Foth",
        "given": "Kilian"
      },
      {
        "family": "Menzel",
        "given": "Wolfgang"
      },
      {
        "family": "Pop",
        "given": "Horia F."
      },
      {
        "family": "Schröder",
        "given": "Ingo"
      }
    ],
    "container-title": "Proceedings of the 18<sup>th</sup> conference on computational linguistics (COLING ’00)",
    "id": "Foth2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "interactive_parsing, robust_parsing",
    "language": "en-US",
    "page": "1026-1030",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "An experiment on incremental analysis using robust parsing techniques",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1017/s1351324903003267",
    "ISSN": "1469-8110",
    "abstract": "Based on constraint optimization techniques, an architecture for robust parsing of natural language utterances has been developed. The resulting system is able to combine possibly contradicting evidence from a variety of information sources, using a plausibility-based arbitration procedure to derive fairly rich structural representations, comprising aspects of syntax, semantics and other description levels of language. The results of a series of experiments are reported which demonstrate the high potential for robust behaviour with respect to ungrammaticality, incomplete utterances, and temporal pressure.",
    "author": [
      {
        "family": "Foth",
        "given": "Kilian"
      },
      {
        "family": "Menzel",
        "given": "Wolfgang"
      },
      {
        "family": "Schröder",
        "given": "Ingo"
      }
    ],
    "container-title": "Natural Language Engineering",
    "id": "Foth2005",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "interactive_parsing, robust_parsing",
    "language": "en-US",
    "page": "1-25",
    "title": "Robust parsing with weighted constraints",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "DOI": "10.1080/01615440.2017.1361879",
    "abstract": "Is a third passage to the past possible, beyond Elton’s and Fogel’s two roads of narrative history and scientific/quantitative history? One that would combine narrative history’s focus on the event, on individuals and their actions, at a particular time and place, to scientific/quantitative history’s emphasis on explicit behavioral models based on social-science theories? That is the question this article addresses. It illustrates a computer-assisted methodology for the study of narrative—quantitative narrative analysis (QNA)—that does just that. Based on the 5 Ws + H of narrative—Who, What, When, Where, Why, and How—QNA quantifies events without losing the event itself, without losing people behind numbers, diachronic time behind synchronic statistical coefficients. When used in conjunction with dynamic and interactive data visualization tools (and new natural language processing tools), QNA may provide a third unforeseen road to the past.",
    "author": [
      {
        "family": "Franzosi",
        "given": "Roberto"
      }
    ],
    "container-title": "Historical Methods: A Journal of Quantitative and Interdisciplinary History",
    "id": "Franzosi2017",
    "issued": {
      "date-parts": [
        [
          2017,
          8,
          31
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "1-18",
    "publisher": "Routledge",
    "title": "A third road to the past? Historical scholarship in the age of big data",
    "type": "article-journal"
  },
  {
    "author": [
      {
        "family": "Friedl",
        "given": "Jeffrey"
      }
    ],
    "edition": "2",
    "id": "Friedl2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "language": "en-US",
    "publisher": "O’Reilly",
    "publisher-place": "Sebastopol, CA, USA",
    "title": "Mastering regular expressions",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Friedl",
        "given": "Jeffrey"
      }
    ],
    "edition": "3",
    "id": "Friedl2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "language": "en-US",
    "publisher": "O’Reilly",
    "publisher-place": "Sebastopol, CA, USA",
    "title": "Mastering regular expressions",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Friedlein",
        "given": "Ashley"
      }
    ],
    "id": "Friedlein2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "publisher": "Morgan Kaufmann",
    "publisher-place": "San Francisco, CA, USA",
    "title": "Web project management: Delivering successful commercial web sites",
    "title-short": "Web project management",
    "type": "book"
  },
  {
    "DOI": "10.1130/0016-7606%281995%29107%3C0960:grgaai%3E2.3.co;2",
    "ISSN": "00167606",
    "abstract": "The standard account of the reasoning process within geology views it as lacking a distinctive methodology of its own. Rather, geology is described as a derivative science, relying on the logical techniques exemplified by physics. I argue that this account is inadequate and skews our understanding of both geology and the scientific process in general. Far from simply taking up and applying the logical techniques of physics, geological reasoning has developed its own distinctive set of logical procedures. I begin with a review of contemporary philosophy of science as it relates to geology. I then discuss the two distinctive features of geological reasoning, which are its nature as (1) an interpretive and (2) a historical science. I conclude that geological reasoning offers us the best model of the type of reasoning necessary for confronting the type of problems we are likely to face in the 21st century.",
    "author": [
      {
        "family": "Frodeman",
        "given": "Robert"
      }
    ],
    "container-title": "Geological Society of America Bulletin",
    "id": "Frodeman1995",
    "issue": "8",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "keyword": "philosophy_of_science, uncertainty",
    "language": "en-US",
    "page": "960-968",
    "title": "Geological reasoning: Geology as an interpretive and historical science",
    "title-short": "Geological reasoning",
    "type": "article-journal",
    "volume": "107"
  },
  {
    "author": [
      {
        "family": "Froger",
        "given": "Jacques"
      }
    ],
    "id": "Froger1968",
    "issued": {
      "date-parts": [
        [
          1968
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_edition",
    "language": "fr-FR",
    "publisher": "Dunod",
    "publisher-place": "Paris",
    "title": "La critique des textes et son automatisation",
    "type": "book"
  },
  {
    "DOI": "10.2307/1583073",
    "ISSN": "00426032",
    "URL": "http://jstor.org/stable/1583073",
    "author": [
      {
        "family": "Froger",
        "given": "Jacques"
      }
    ],
    "container-title": "Vigiliae Christianae",
    "id": "Froger1970",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1970
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_edition",
    "language": "fr-FR",
    "page": "210-217",
    "title": "La critique des textes et l’ordinateur",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "DOI": "10.1145/32206.32212",
    "ISSN": "0001-0782",
    "abstract": "In almost all computer applications, users must enter correct words for the desired objects or actions. For success without extensive training, or in first-tries for new targets, the system must recognize terms that will be chosen spontaneously. We studied spontaneous word choice for objects in five application-related domains, and found the variability to be surprisingly large. In every case two people favored the same term with probability",
    "author": [
      {
        "family": "Furnas",
        "given": "G. W."
      },
      {
        "family": "Landauer",
        "given": "T. K."
      },
      {
        "family": "Gomez",
        "given": "L. M."
      },
      {
        "family": "Dumais",
        "given": "S. T."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Furnas1987",
    "issue": "11",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "keyword": "classic, ir",
    "language": "en-US",
    "page": "964-971",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The vocabulary problem in human-system communication",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "abstract": "In order to improve OCR quality in texts originally typeset in Gothic script, we have built an automated correction system which is highly specialized for the given text. Our approach includes external dictionary resources as well as information derived from the text itself. The focus lies on testing and improving different methods for classifying words as correct or erroneous. Also, different techniques are applied to find and rate correction candidates. In addition, we are working on a web application that enables users to read and edit the digitized text online.",
    "author": [
      {
        "family": "Furrer",
        "given": "Lenz"
      },
      {
        "family": "Volk",
        "given": "Martin"
      }
    ],
    "container-title": "Proceedings of the RANLP 2011 workshop on language technologies for digital humanities and cultural heritage",
    "id": "Furrer2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "language": "en-US",
    "page": "97-103",
    "title": "Reducing OCR errors in Gothic-script documents",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/356887.356891",
    "annote": "Also contained in Nievergelt et al. (eds.): Document Preparation Systems (1982)",
    "author": [
      {
        "family": "Furuta",
        "given": "Richard"
      },
      {
        "family": "Scofield",
        "given": "Jeffrey"
      },
      {
        "family": "Shaw",
        "given": "Alan"
      }
    ],
    "container-title": "ACM Computing Surveys",
    "id": "Furuta1982",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "language": "en-US",
    "page": "417-472",
    "title": "Document formatting systems: Survey, concepts, and issues",
    "title-short": "Document formatting systems",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "author": [
      {
        "family": "Furuta",
        "given": "Richard"
      },
      {
        "family": "Stotts",
        "given": "P. David"
      }
    ],
    "container-title": "Document manipulation and typography. Proceedings of the international conference",
    "editor": [
      {
        "dropping-particle": "van",
        "family": "Vliet",
        "given": "J. C."
      }
    ],
    "id": "Furuta1988",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "page": "109-120",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge",
    "title": "Specifying structured document transformations",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Furuta",
        "given": "Richard"
      }
    ],
    "container-title": "Electronic Publishing",
    "id": "Furuta1992",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "language": "en-US",
    "page": "19-44",
    "title": "Important papers in the history of document preparation systems: Basic sources",
    "title-short": "Important papers in the history of document preparation systems",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "URL": "http://www.gi.de/fileadmin/redaktion/Download/was-ist-informatik-lang.pdf",
    "editor": [
      {
        "family": "GI"
      }
    ],
    "id": "GI2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Positionspapier der Gesellschaft für Informatik",
    "title": "Was ist Informatik?",
    "type": "pamphlet"
  },
  {
    "URL": "http://www.cs.technion.ac.il/~shaulm/papers/pdf/Gabrilovich-Markovitch-ijcai2007.pdf",
    "abstract": "Computing semantic relatedness of natural language texts requires access to vast amounts of common-sense and domain-specific world knowledge. We propose Explicit Semantic Analysis (ESA), a novel method that represents the meaning of texts in a high-dimensional space of concepts derived from Wikipedia. We use machine learning techniques to explicitly represent the meaning of any text as a weighted vector of Wikipedia-based concepts. Assessing the relatedness of texts in this space amounts to comparing the corresponding vectors using conventional metrics (e.g., cosine). Compared with the previous state of the art, using ESA results in substantial improvements in correlation of computed relatedness scores with human judgments: from $r=0.56$ to $0.75$ for individual words and from $r=0.60$ to $0.72$ for texts. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.",
    "author": [
      {
        "family": "Gabrilovich",
        "given": "Evgeniy"
      },
      {
        "family": "Markovitch",
        "given": "Shaul"
      }
    ],
    "container-title": "Proceedings of the 20<sup>th</sup> international joint conference for artificial intelligence",
    "id": "Gabrilovich2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "ir, wikipedia",
    "language": "en-US",
    "page": "1606-1611",
    "publisher-place": "Hyderabad, India",
    "title": "Computing semantic relatedness using Wikipedia-based explicit semantic analysis",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1108/eb046999",
    "ISSN": "0033-0337",
    "abstract": "The article discusses phonetic retrieval of written names or words in computer systems. It deals briefly with the two established coding techniques of Davidson and Soundex, and an improved method called Phonix. Phonetic coding is performed on full words, with the ending sounds of words having special significance during the search process which results in likely, less likely, least likely categories of matches. Phonix has been tested, within the URICA library package, on bibliographic databases where it has been used as a secondary method of retrieval after failure of an initial search.",
    "author": [
      {
        "family": "Gadd",
        "given": "T. N."
      }
    ],
    "container-title": "Program: electronic library and information systems",
    "id": "Gadd1988",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "approximate_matching, ir",
    "language": "en-US",
    "page": "222-237",
    "title": "“Fisching fore werds”: Phonetic retrieval of written text in information systems",
    "title-short": "“Fisching fore werds”",
    "type": "article-journal",
    "volume": "22"
  },
  {
    "DOI": "10.1108/eb047069",
    "ISSN": "0033-0337",
    "abstract": "PHONIX is a phonetic retrieval technique developed for use with the URICA library system. It has been successfully installed at a number of URICA sites in Southern Africa. PHONIX has been found to be particularly useful when applied to personal names, specifically author surnames in the context of a library system. Certain names such as Anton Chekov have been variously transliterated as TSJECHOF, TSJECHOW, TJEKHOW, CHEKHOV, CHEKHOW etc., in the multi-lingual environment of libraries in Southern Africa. PHONIX complements the more conventional information retrieval techniques used by URICA, and the algorithm used is explained as a simple, easy to apply, set of rules. This contribution follows on from a previous paper.",
    "author": [
      {
        "family": "Gadd",
        "given": "T. N."
      }
    ],
    "container-title": "Program: electronic library and information systems",
    "id": "Gadd1990",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "approximate_matching, ir, spelling_correction",
    "language": "en-US",
    "page": "363-366",
    "title": "PHONIX: The algorithm",
    "title-short": "PHONIX",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "DOI": "10.1016/s0261-5177(02)00028-6",
    "ISSN": "02615177",
    "abstract": "Database management systems used in information systems for the tourism industry and based on SQL are flexible insofar as they allow queries regarding the data stored in them. However, the SQL language, in spite of its flexibility, cannot deal with the vagueness of the queries clients present to travel agencies. Questions like ” I want an inexpensive trip to an Italian city not too far from Rome” cannot be directly answered by the system. Rather, the agent has to make different queries about available trips and then has to summarise and adapt them to the needs of the clients. A solution that gives an answer to these requirements is found in fuzzy logic sets applied to databases. Currently, we have implemented a fuzzy SQL (FSQL) server in information systems as a result of our previous work in the field of information systems (IS). This server is available for Oracle databases and allows queries in traditional relational databases, as well as in fuzzy databases, by using the FSQL language. The FSQL language is an extension of the SQL language that allows vague or fuzzy queries to databases by using fuzzy conditions (with fuzzy comparators), fulfilment thresholds, fuzzy constants, etc. In this work we show an application of the FSQL language to the tourism sector, in particular, to the IS database of a travel agency which includes attributes with fuzzy values. That is, the database allows the storing of fuzzy values and queries with conditions involving such values. This allows the management of all the company’s products to be carried out in a more efficient way and offers clients the products that best fit their requirements. Also, the system makes it possible to obtain the degree of match for each product in relation to the set obtained in the fuzzy query.",
    "author": [
      {
        "family": "Galindo",
        "given": "José"
      },
      {
        "family": "Aranda",
        "given": "M. Carmen"
      },
      {
        "family": "Caro",
        "given": "José L."
      },
      {
        "family": "Guevara",
        "given": "Antonio"
      },
      {
        "family": "Aguayo",
        "given": "Andrés"
      }
    ],
    "container-title": "Tourism Management",
    "id": "Galindo2002",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "database, uncertainty",
    "language": "en-US",
    "page": "623-629",
    "title": "Applying fuzzy databases and FSQL to the management of rural accommodation",
    "type": "article-journal",
    "volume": "23"
  },
  {
    "ISBN": "9781591403241",
    "abstract": "This book includes an introduction to fuzzy logic, fuzzy databases and an overview of the state of the art in fuzzy modeling in databases. It proposes also a method to translate FuzzyEER model to a classical DBMS, and defines FSQL (Fuzzy SQL), an extension of the SQL language that allows users to write flexible conditions in queries, using all extensions defined by the FuzzyEER model.",
    "author": [
      {
        "family": "Galindo",
        "given": "Jose"
      },
      {
        "family": "Urrutia",
        "given": "Angelica"
      },
      {
        "family": "Piattini",
        "given": "Mario"
      }
    ],
    "id": "Galindo2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "database, uncertainty",
    "language": "en-US",
    "publisher": "IGI Global",
    "publisher-place": "Hershey, PA, USA",
    "title": "Fuzzy databases: Modeling, design, and implementation",
    "title-short": "Fuzzy databases",
    "type": "book"
  },
  {
    "DOI": "10.1108/00220410510607507",
    "ISSN": "0022-0418",
    "abstract": "Purpose: To propose a categorization of the different conflation procedures at the two basic approaches, non-linguistic and linguistic techniques, and to justify the application of normalization methods within the framework of linguistic techniques. Design/methodology/approach: Presents a range of term conflation methods, that can be used in information retrieval. The uniterm and multiterm variants can be considered equivalent units for the purposes of automatic indexing. Stemming algorithms, segmentation rules, association measures and clustering techniques are well evaluated non-linguistic methods, and experiments with these techniques show a wide variety of results. Alternatively, the lemmatisation and the use of syntactic pattern-matching, through equivalence relations represented in finite-state transducers (FST), are emerging methods for the recognition and standardization of terms. Findings: The survey attempts to point out the positive and negative effects of the linguistic approach and its potential as a term conflation method. Originality/value: Outlines the importance of FSTs for the normalization of term variants.",
    "author": [
      {
        "family": "Galvez",
        "given": "Carmen"
      },
      {
        "family": "Moya-Anegón",
        "given": "Félix"
      },
      {
        "family": "Solana",
        "given": "Víctor H."
      }
    ],
    "container-title": "Journal of Documentation",
    "id": "Galvez2005",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "ir",
    "language": "en-US",
    "page": "520-547",
    "title": "Term conflation methods in information retrieval: Non-linguistic and linguistic approaches",
    "title-short": "Term conflation methods in information retrieval",
    "type": "article-journal",
    "volume": "61"
  },
  {
    "DOI": "10.1093/llc/fqu020",
    "ISSN": "0268-1145",
    "abstract": "For more than 40 years now, modern theories of literature insist on the role of paraphrases, rewritings, citations, reciprocal borrowings, and mutual contributions of many kinds. The notions of ’intertextuality’, ’transtextuality’, and ’hypertextuality/hypotextuality’ were introduced in the seventies and eighties to approach these phenomena. Through the Phœbus project, computer scientists from the computer science laboratory of the University Pierre and Marie Curie collaborate with the literary teams of Paris-Sorbonne University to develop efficient tools for literary studies that take advantage of modern computer science techniques to detect borrowings of huge masses of texts and to help put them in context. In this context, we have developed a piece of software that automatically detects and explores networks of textual reuses in classical literature. This article describes the principles on which our program is based, the significant results that have already been obtained and the prospective for the near future. It is divided into four parts. The first part recalls the distinction between various types of borrowings like plagiarism, pastiches, citations, etc. The second enumerates the criteria that are retained to characterize reuses and citations on which we are focusing here. The third part describes the implementation and shows its efficiency by comparison with manual detection. Finally, we show some of the results that have already been obtained with the Phœbus program.",
    "author": [
      {
        "family": "Ganascia",
        "given": "J. G."
      },
      {
        "family": "Glaudes",
        "given": "P."
      },
      {
        "family": "Del Lungo",
        "given": "A."
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Ganascia2014",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2014,
          6,
          24
        ]
      ]
    },
    "keyword": "digital_humanities, intertextuality",
    "language": "en-US",
    "page": "412-421",
    "publisher": "Oxford University Press",
    "title": "Automatic detection of reuses and citations in literary texts",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "DOI": "10.3233/978-1-60750-535-8-345",
    "ISBN": "978-1-60750-534-1",
    "abstract": "The paper presents preliminary results in the area of ontological engineering for historical research. Historical information systems are still in the initial stage of development. Our experience hitherto shows that the decisive stage in the development of such systems is a conceptual model and ontological engineering seems to be the right tool to build it. Our particular aim is to develop a database system for the history of the administrative structure of the Catholic Church in Central-Eastern Europe in the so-called pre-statical period, i.e. roughly from XII to XIX century. We use DOLCE as a foundational ontology, especially its part concerning social objects. We build an axiomatic system that formally defines the basic notions of those structures and may be interpreted as the conceptual scheme of this database.",
    "author": [
      {
        "family": "Garbacz",
        "given": "Pawel"
      },
      {
        "family": "Trypuz",
        "given": "Robert"
      },
      {
        "family": "Szady",
        "given": "Bogumil"
      },
      {
        "family": "Kulicki",
        "given": "Piotr"
      },
      {
        "family": "Gradzki",
        "given": "Przemyslaw"
      },
      {
        "family": "Lechniak",
        "given": "Marek"
      }
    ],
    "container-title": "Proceedings of the 2010 conference on formal ontology in information systems: Proceedings of the sixth international conference (FOIS 2010)",
    "id": "Garbacz2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "semantic_web",
    "language": "en-US",
    "page": "345-358",
    "publisher": "IOS Press",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "Towards a formal ontology for history of church administration",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.iua.upf.es/~dgriffit/papers/garcia-griffiths_villach.pdf",
    "abstract": "The IMS-QTI, and other related specifications have been developed to support the creation of reusable and pedagogically neutral learning scenarios and content, as stated by the IMS Global Learning Consortium. In this paper we discuss how current specifications both constrain the design of assessment scenarios, and limit content reusability. Key issues regarding reusability such as granularity, localization and self-contained-ness are analyzed from the point of view of current eLearning specifications. We also suggest some solutions to overcome these limitations. The paper is based on our experience developing and testing an IMS QTI Lite compliant assessment authoring tool, QAed. It supports teacher centering, which is quite neglected when designing such tools. In the paper we also discuss how to make compatible standards support and user centering in eLearning applications and provide some recommendations for the design of the user interfaces.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "García-Robles",
        "given": "Rocío"
      },
      {
        "family": "Blat",
        "given": "Josep"
      },
      {
        "family": "Sayago",
        "given": "Sergio"
      },
      {
        "family": "Griffiths",
        "given": "Dai"
      },
      {
        "family": "Casado",
        "given": "Francis"
      },
      {
        "family": "Martinez",
        "given": "Juanjo"
      }
    ],
    "container-title": "Proceedings of the international conference of interactive computer-aided learning, ICL 2004, villach (austria)",
    "editor": [
      {
        "family": "Auer",
        "given": "M."
      },
      {
        "family": "Auer",
        "given": "U."
      }
    ],
    "id": "Garcia-Robles2004",
    "issued": {
      "date-parts": [
        [
          2004,
          9
        ]
      ]
    },
    "publisher": "In cooperation with ACM-Austrian Chapter, and supported by IEEE Education Society",
    "title": "Limitations of some eLearning standards for supporting learning",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/ICALT.2004.1357464",
    "abstract": "The IMS-QTI, and other related specifications have been developed to support the creation of reusable and pedagogically neutral assessment scenarios and content, as stated by the IMS Global Learning Consortium. This paper discusses how current specifications both constrain the design of assessment scenarios, and limit content reusability. Also suggest some solutions to overcome these limitations. The paper is based on our experience developing and testing an IMS QTI Lite compliant assessment authoring tool, QAed. It supports teacher centering, which is quite neglected when designing such tools. In the paper, the author also discuss how to make compatible standards support and user centering in eLearning applications and provide some recommendations for the design of the user interfaces.",
    "author": [
      {
        "family": "García-Robles",
        "given": "Rocío"
      },
      {
        "family": "Blat",
        "given": "Josep"
      },
      {
        "family": "Sayago",
        "given": "Sergio"
      },
      {
        "family": "Griffiths",
        "given": "Dai"
      },
      {
        "family": "Casado",
        "given": "Francis"
      },
      {
        "family": "Martinez",
        "given": "Juanjo"
      }
    ],
    "container-title": "ICALT ’04: Proceedings of the IEEE international conference on advanced learning technologies (ICALT’04)",
    "id": "Garcia-Robles2004a",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "495-499",
    "publisher": "IEEE Computer Society",
    "publisher-place": "Washington, DC, USA",
    "title": "Supporting usability and reusability based on eLearning standards",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1002/ss.245",
    "abstract": "This chapter provides an overview of e-portfolio concepts and designs. It describes a model that outlines an array of dimensions for the categorization of e-portfolio systems, reviews selected systems, and makes observations regarding the importance for student affairs units to understand, collaborate, and include e-portfolio systems within their programs and services.",
    "author": [
      {
        "family": "Garis",
        "given": "Jeff W."
      }
    ],
    "container-title": "New Directions for Student Services",
    "id": "Garis2007",
    "issue": "119",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "e-learning, e-portfolios",
    "language": "en-US",
    "page": "3-16",
    "publisher-place": "Career Center, Florida State University, Tallahassee",
    "title": "e-Portfolios: Concepts, designs, and integration within student affairs",
    "title-short": "e-Portfolios",
    "type": "article-journal",
    "volume": "2007"
  },
  {
    "author": [
      {
        "family": "Garrison",
        "given": "Randy D."
      },
      {
        "family": "Anderson",
        "given": "Terry"
      }
    ],
    "id": "Garrison2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "Routledge Falmer",
    "publisher-place": "London",
    "title": "E-learning in the 21st century: A framework for research and practice",
    "title-short": "E-learning in the 21st century",
    "type": "book"
  },
  {
    "URL": "http://ucrel.lancs.ac.uk/papers/HybridTaggerGS97.pdf",
    "abstract": "In this chapter we discuss in detail how a piece of software can carry out automatically one important task in corpus annotation. The task is part-of-speech (POS) tagging (also called word-class tagging, or grammatical tagging); that is, assigning to each word in a text its correct part of speech in context. The result of this task, as a form of corpus annotation, was discussed in some detail in Chapter 2. It usually forms a basis for more sophisticated annotation, such as full syntactic parsing or semantic annotation, and it carries out the useful supplementary tasks of splitting up the text into individual words and sentences.",
    "author": [
      {
        "family": "Garside",
        "given": "Roger"
      },
      {
        "family": "Smith",
        "given": "Nicholas"
      }
    ],
    "container-title": "Corpus annotation: Linguistic information from computer text corpora",
    "editor": [
      {
        "family": "Garside",
        "given": "Roger"
      },
      {
        "family": "Leech",
        "given": "Geoffrey"
      },
      {
        "family": "McEnery",
        "given": "Tony"
      }
    ],
    "id": "Garside1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "corpus_linguistics, nlp, pos_tagging",
    "language": "en-US",
    "page": "102-121",
    "publisher": "Longman",
    "publisher-place": "London, UK",
    "title": "A hybrid grammatical tagger: CLAWS4",
    "title-short": "A hybrid grammatical tagger",
    "type": "chapter"
  },
  {
    "URL": "http://campussource.de/projekte/docs/rahmenarchitektur.pdf",
    "abstract": "In unserer heutigen Gesellschaft ist Bildung ein zentrales Thema. Schulen, Hochschulen und Unternehmen stehen gleichermaßen vor dem Problem, zu jeder Zeit und an jedem Ort ihre Lehrangebote vermitteln zu müssen. Hierfür sind entsprechende Infrastrukturen zu ergänzen bzw. neu aufzubauen. Webbasierte Lehr-/Lernsysteme stellen eine erste, Erfolg versprechende Antwort auf diese Anforderungen dar. Solche Systeme sind jedoch häufig monolithisch programmiert. Sie sind daher kaum erweiterbar und schwer in eine bestehende IT-Landschaft zu integrieren. Dieses Papier entwickelt eine Rahmenarchitektur für den modularen Aufbau von Lehr-/Lernsystemen, die einfach erweiterbar, anpassbar und integrierbar sind. Ausgehend von der Idee, monolithische Programme in kleine, funktionale Komponenten zu zerlegen, werden zunächst Anforderungen aufgestellt und unter deren Berücksichtigung eine komponentenbasierte Architektur für Lehr-/Lernsysteme erarbeitet. Im Anschluss wird die Komponenten-Idee erweitert und darauf aufbauend eine dienstbasierte Rahmenarchitektur präsentiert. Für deren konkrete Umsetzung wird die Verwendung von Web Services empfohlen. Dabei werden Web Services zunächst vorgestellt und ihre Eignung für die Realisierung heraus gestellt. In einem Vergleich mit anderen Techniken wird die Empfehlung begründet. Abschließend wird der Grundaufbau eines beispielhaften Lehr-/Lernsystems mit minimaler Funktionalität unter Einsatz der Rahmenarchitektur skizziert.",
    "accessed": {
      "date-parts": [
        [
          2007,
          10,
          4
        ]
      ]
    },
    "author": [
      {
        "family": "Gehrke",
        "given": "Matthias"
      },
      {
        "family": "Meyer",
        "given": "Matthias"
      },
      {
        "family": "Schäfer",
        "given": "Wilhelm"
      }
    ],
    "id": "Gehrke2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "publisher": "online",
    "title": "Eine Rahmenarchitektur für verteilte Lehr- und Lernsysteme",
    "type": ""
  },
  {
    "URL": "http://jolt.law.harvard.edu/articles/pdf/v11/11HarvJLTech141.pdf",
    "abstract": "Although CAI demonstrations were given as early as 1965 and 1966 at the American Association of Law Schools (\"AALS\") conventions, 44 actual use of CAI exercises did not begin until the early 1970s at the University o f Illinois College o f Law. .5 Using the PLATO IV com- puter-assisted method o f teaching law, the Illinois system taught future interests and contract law. 46 The system significantly improved upon the workbook exercises by providing a rapid and automatic response to student answers and permitting students to enter full English words, referred to as free language technique, rather than merely yes or no answers. Establishing a free language technique was not easy, however. The programmer was required to anticipate the full range o f responses that students might give to a particular question and to enter such words into the computer’s \"vocabulary. ”47 Although the Illinois system’s creators expressed uncertainty with regard to the exercises’ effective- ness, students using the system were uearly unanimous in their approval.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Geist",
        "given": "Michael A."
      }
    ],
    "container-title": "Harvard Journal of Law & Technology",
    "id": "Geist1997",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "page": "146-183",
    "title": "WHERE CAN YOU GO TODAY?: THE COMPUTERIZATION OF LEGAL EDUCATION FROM WORKBOOKS TO THE Web",
    "title-short": "WHERE CAN YOU GO TODAY?",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "DOI": "10.1145/1458412.1458418",
    "ISBN": "978-1-60558-249-8",
    "abstract": "Simple word matching between the user query and document is common, as are mis-matches of meaning that occur as a consequence, and errors in recall. These defects in the \"bag of words\" model are well known, and raising the semantic level of representation will improve retrieval. This can be done by expanding words and user queries using traditional reference sources such as gazetteers and synonym lists or ontologies.",
    "author": [
      {
        "family": "Gelernter",
        "given": "Judith"
      },
      {
        "family": "Lesk",
        "given": "Michael E."
      }
    ],
    "container-title": "Proceeding of the 2008 ACM workshop on research advances in large digital book repositories (BooksOnline ’08)",
    "id": "Gelernter2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, ir",
    "language": "en-US",
    "page": "17-20",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Traditional resources help interpret texts",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.recherche-qualitative.qc.ca/documents/files/revue/hors_serie/HS-17/rq-hs-17-gendron-richard.pdf",
    "abstract": "Produire du sens en cours d’analyse qualitative invoque la stimulation d’activités cognitives interprétatives et créatrices. Sorte de prétexte à l’élaboration de sens, le recours à diverses formes de schématisation ou de figuration en cours d’analyse qualitative est une pratique avérée qui demeure pourtant peu explicitée. La modélisation systémique est proposée ici comme dispositif d’analyse-conception qualitative pour réfléchir des phénomènes perçus complexes. Des paramètres théoriques et méthodologiques qui appuient une pratique de modélisation systémique en cours d’analyse qualitative sont présentés, suivis d’un exemple afin d’en réfléchir la pertinence, les enjeux et le développement en méthodologie qualitative. Source de pensée innovante, la modélisation systémique en analyse qualitative favorise un espace de dialogue entre modélisateurs-concepteurs et permet de projeter des changements afin d’en raisonner des conséquences possibles. Pour les disciplines préoccupées par la pratique ou l’intervention, cet outil méthodologique favorise l’innovation à travers la mise en réseau d’idées, de savoirs et d’acteurs.",
    "author": [
      {
        "family": "Gendron",
        "given": "Sylvie"
      },
      {
        "family": "Richard",
        "given": "Lauralie"
      }
    ],
    "collection-number": "17",
    "collection-title": "Recherches Qualitatives – Hors-série",
    "container-title": "La recherche qualitative: un vecteur d’innovations. Actes du colloque de l’Association pour la recherche qualitative (ARQ), 24 octobre 2014",
    "editor": [
      {
        "family": "Caouette",
        "given": "Martin"
      },
      {
        "family": "Lussier-Desrochers",
        "given": "Dany"
      },
      {
        "family": "Godin-Tremblay",
        "given": "Valérie"
      }
    ],
    "id": "Gendron2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "formal_models",
    "language": "fr-FR",
    "page": "78-97",
    "publisher": "Association pour la recherche qualitative",
    "title": "La modélisation systémique en analyse qualitative: un potentiel de pensée innovante",
    "title-short": "La modélisation systémique en analyse qualitative",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1600193.1600235",
    "ISBN": "978-1-60558-575-8",
    "abstract": "In this paper, we extend previous work on the automatic structuring of medical documents using content analysis. Our long-term objective is to take advantage of specific rhetoric markers encountered in specialized medical documents (clinical guidelines) to automatically structure free text according to its role in the document. This should enable to generate multiple views of the same document depending on the target audience, generate document summaries, as well as facilitating knowledge extraction from text. We have established in previous work that the structure of clinical guidelines could be refined through the identification of a limited set of deontic operators. We now propose to extend this approach by analyzing the text delimited by these operators using Rhetorical Structure Theory. The emphasis on causality and time in RST proves a powerful complement to the recognition of deontic structures while retaining the same philosophy of high-level recognition of sentence structure, which can be converted into application-specific mark-ups. Throughout the paper, we illustrate our findings through results produced by the automatic processing of English guidelines for the management of hypertension and Alzheimer disease.",
    "author": [
      {
        "family": "Georg",
        "given": "Gersende"
      },
      {
        "family": "Hernault",
        "given": "Hugo"
      },
      {
        "family": "Cavazza",
        "given": "Marc"
      },
      {
        "family": "Prendinger",
        "given": "Helmut"
      },
      {
        "family": "Ishizuka",
        "given": "Mitsuru"
      }
    ],
    "container-title": "DocEng ’09: Proceedings of the 9th ACM symposium on document engineering",
    "id": "Georg2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "document_analysis, document_engineering, nlp, rst",
    "language": "en-US",
    "page": "185-192",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "From rhetorical structures to document structure: Shallow pragmatic analysis for document engineering",
    "title-short": "From rhetorical structures to document structure",
    "type": "paper-conference"
  },
  {
    "ISBN": "978-3-03778-092-3",
    "abstract": "Karl Gerstners Arbeit ist ein Meilenstein in der Geschichte der Gestaltung. Eines seiner wichtigsten Werke ist Programme entwerfen, das hier in einer Neuauflage der Originalpublikation von 1964 erscheint. In vier Essays stellt der Autor die Grundlagen seiner gestalterischen Methode dar. Diese zeigt keine Rezepte, sondern ein Modell für das Entwerfen im beginnenden Computerzeitalter. Das Buch liefert aber auch heute noch anwendbare Denkmodelle: keine Lösungen, kein Richtig oder Falsch, nichts Absolutes, sondern Grundlegendes ist hier auf innovative Art erarbeitet. Gerade vor dem Hintergrund der aktuellen Entwicklungen im Computational Design, das uns die Möglichkeit von programmierter Gestaltung vorhält, gewinnt dieses Buch an Aktualität. Die vielen Beispiele aus Grafik- und Produktdesign, Musik, Architektur und Kunst regen dazu an, das Gezeigte weiterzudenken und in die eigene Arbeit zu integrieren.",
    "author": [
      {
        "family": "Gerstner",
        "given": "Karl"
      }
    ],
    "edition": "Aktualisierter Nachdruck",
    "id": "Gerstner2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "typography",
    "language": "de-DE",
    "publisher": "Lars Müller",
    "publisher-place": "Baden",
    "title": "Programme entwerfen",
    "type": "book"
  },
  {
    "DOI": "10.1145/1463434.1463535",
    "ISBN": "978-1-60558-323-5",
    "abstract": "In digital humanities projects, particularly for historical research and cultural heritage, GIS has played an increasingly important role. However, most implementations have concentrated on displays which ignore the temporal dimension or express it as multiple snapshots for fixed or periodic points in time. Our project concentrates on historical biography and expresses a biography as a sequence of life events with in time and space. We utilize named entity recognition and extraction to automatically mark up biographies so that they can be displayed as dynamic maps. In so doing, contextual features and related happenings and people can be overlaid to facilitate serendipitous discovery of unanticipated and seemingly unrelated connections.",
    "author": [
      {
        "family": "Gey",
        "given": "Fredric"
      },
      {
        "family": "Shaw",
        "given": "Ryan"
      },
      {
        "family": "Larson",
        "given": "Ray"
      },
      {
        "family": "Pateman",
        "given": "Barry"
      }
    ],
    "container-title": "GIS ’08: Proceedings of the 16th ACM SIGSPATIAL international conference on advances in geographic information systems",
    "id": "Gey2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, geo_ir",
    "language": "en-US",
    "page": "1-2",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Biography as events in time and space",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1038/nature14541",
    "ISSN": "0028-0836",
    "abstract": "How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.",
    "author": [
      {
        "family": "Ghahramani",
        "given": "Zoubin"
      }
    ],
    "container-title": "Nature",
    "id": "Ghahramani2015",
    "issue": "7553",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "machine_learning, uncertainty",
    "language": "en-US",
    "page": "452-459",
    "title": "Probabilistic machine learning and artificial intelligence",
    "type": "article-journal",
    "volume": "521"
  },
  {
    "DOI": "10.1109/icsmc.2009.5346027",
    "abstract": "The prediction task in national language processing means to guess the missing letter, word, phrase, or sentence that likely follow in a given segment of a text. Since 1980s many systems with different methods were developed for different languages. In this paper an overview of the existing prediction methods that have been used for more than two decades are described and a general classification of the approaches is presented. The three main categories of the classification are statistical modeling, knowledge-based modeling, and heuristic modeling (adaptive).",
    "author": [
      {
        "family": "Ghayoomi",
        "given": "Masood"
      },
      {
        "family": "Momtazi",
        "given": "Saeedeh"
      }
    ],
    "id": "Ghayoomi2009",
    "issued": {
      "date-parts": [
        [
          2009,
          10
        ]
      ]
    },
    "keyword": "authoring, interactive_editing",
    "language": "en-US",
    "page": "5083-5087",
    "title": "An overview on the existing language models for prediction systems as writing assistant tools",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Ghosh",
        "given": "Moumita"
      },
      {
        "family": "Verma",
        "given": "Brijesh Kumar"
      },
      {
        "family": "Nguyen",
        "given": "Anne T."
      }
    ],
    "container-title": "International conference on information technology and applications, IT in engineering: AI, signal/image processing, ICITA02",
    "editor": [
      {
        "family": "Tien",
        "given": "David"
      }
    ],
    "id": "Ghosh2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "page": "274-279",
    "title": "An automatic assessment marking and plagiarism detection",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqv036",
    "author": [
      {
        "family": "Giacometti",
        "given": "Alejandro"
      },
      {
        "family": "Campagnolo",
        "given": "Alberto"
      },
      {
        "family": "MacDonald",
        "given": "Lindsay"
      },
      {
        "family": "Mahony",
        "given": "Simon"
      },
      {
        "family": "Robson",
        "given": "Stuart"
      },
      {
        "family": "Weyrich",
        "given": "Tim"
      },
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Gibson",
        "given": "Adam"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Giacometti2017",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "page": "101-122",
    "title": "The value of critical destruction: Evaluating multispectral image processing methods for the analysis of primary historical texts",
    "title-short": "The value of critical destruction",
    "type": "article-journal",
    "volume": "32"
  },
  {
    "DOI": "10.1145/1600193.1600247",
    "abstract": "The W3C (World Wide Web Consortium) is in the process of developing the second major version of XSL-FO (eXtensible Stylesheet Language ­ Formatting Objects) [1], the formatting specification component of XSL. XSL-FO is widely deployed in industry and academia where multiple output forms (typically print and online) are needed from single source XML. It is used in many diverse applications and countries on a large number of implementations to create technical documentation, reports and contracts, terms and conditions, invoices and other forms processing, such as driver’s licenses, postal forms, etc. XSL-FO is also widely used for heavy multilingual work because of the internationalization aspects provided in 1.0 to accommodate multiple and mixed writing modes (writing directions such as left-to-right, top-to-bottom, right-to-left, etc.) of the world’s languages. The primary goals of the W3C XSL Working Group in developing XSL 2.0 are to provide more sophisticated formatting and layout, enhanced internationalization to provide special formatting objects for Japanese and other Asian and non-Western languages and scripts and to improve integration with other technologies such as SVG (Scalable Vector Graphics) [2] and MathML (Mathematical Markup Language) [3]. A number of XSL 1.0 implementations already support dynamic inclusion of vector graphics using W3C SVG. The XSL and SVG WGs want to define a tighter interface between XSL-FO and SVG to provide enhanced functionality. Experiments [4] with the use of SVG paths to create non-rectangular text regions, or \"run-arounds\", have helped to motivate further work on deeper integration of SVG graphics inside XSL-FO documents, and to work with the SVG WG on specifying the meaning of XSL-FO markup inside SVG graphics. A similar level of integration with MathML is contemplated.",
    "author": [
      {
        "family": "Giannetti",
        "given": "Fabio"
      }
    ],
    "collection-title": "DocEng ’09",
    "container-title": "Proceedings of the 9th ACM symposium on document engineering",
    "id": "Giannetti2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "page": "245-246",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "XSL-FO 2.0: Automated publishing for graphic documents",
    "title-short": "XSL-FO 2.0",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1515/9783110198539.3.429",
    "ISBN": "9783110198539",
    "author": [
      {
        "family": "Gibbons",
        "given": "John"
      }
    ],
    "chapter-number": "14",
    "container-title": "Handbook of language and communication: Diversity and change",
    "editor": [
      {
        "family": "Hellinger",
        "given": "Marlis"
      },
      {
        "family": "Pauwels",
        "given": "Anne"
      }
    ],
    "id": "Gibbons2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "linguistics",
    "language": "en-US",
    "page": "429-458",
    "publisher": "Mouton de Gruyter",
    "publisher-place": "Berlin, New York",
    "title": "Forensic linguistics",
    "type": "chapter"
  },
  {
    "URL": "http://www.glos.ac.uk/shareddata/dms/2B70988BBCD42A03949CB4F3CB78A516.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Gibbs",
        "given": "Graham"
      },
      {
        "family": "Simpson",
        "given": "Claire"
      }
    ],
    "container-title": "Learning and Teaching in Higher Education",
    "id": "Gibbs2004",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "3-31",
    "title": "Conditions under which assessment supports students’ learning",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "author": [
      {
        "family": "Gibbs",
        "given": "Fred"
      }
    ],
    "chapter-number": "21",
    "container-title": "Defining digital humanities",
    "editor": [
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Vanhoutte",
        "given": "Edward"
      }
    ],
    "id": "Gibbs2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "289-297",
    "publisher": "Ashgate",
    "publisher-place": "Farnham",
    "title": "Digital humanities definitions by type",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/1568296.1568303",
    "ISBN": "978-1-60558-496-6",
    "abstract": "Scanned and OCRed data leads to large file sizes if facsimile images are included. This makes storage of, and providing online access to large data sets costly. Manually analyzing such data is cumbersome because of long download and processing times. It may thus be advantageous to reconstruct the scanned documents as documents without scanned images which nevertheless closely resemble the original. We have done this reconstruction for a data set of Dutch parliamentary proceedings with positive results. 1.5% of the original storage space was needed, while the documents resembled the originals to a high degree. We describe the reconstruction process and evaluate the costs, the benefits and the quality.",
    "author": [
      {
        "family": "Gielissen",
        "given": "Tim"
      },
      {
        "family": "Marx",
        "given": "Maarten"
      }
    ],
    "container-title": "AND ’09: Proceedings of the third workshop on analytics for noisy unstructured text data",
    "id": "Gielissen2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_library, ocr, typesetting",
    "language": "en-US",
    "page": "25-31",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Digital weight watching: Reconstruction of scanned documents",
    "title-short": "Digital weight watching",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/62437.62461",
    "ISBN": "2-7061-0309-4",
    "abstract": "This paper describes some aspects of a project with the aim of developing a user-friendly interface to a classical Information Retrieval (IR) System in order to improve the effectiveness of retrieval. The character by character approach to IR has been abandoned in favor of an approach based on the meaning of both the queries and the texts containing the information to be sought. The concept space, locally derived from a thesaurus, is used to represent a query as well as documents retrieved in atomic concept units. Dependencies between the search terms are taken into account. The meanings of the query and the retrieved documents (results of Elementary Logical Conjuncts (ELCs)) are compared. The ranking method on the semantical level is used in connection with existing data of a classical IR system. The user enters queries without using complex Boolean expressions.",
    "author": [
      {
        "family": "Giger",
        "given": "H. P."
      }
    ],
    "container-title": "Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval (SIGIR ’88)",
    "id": "Giger1988",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "classic, ir",
    "language": "en-US",
    "page": "275-289",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Concept based retrieval in classical IR systems",
    "type": "paper-conference"
  },
  {
    "ISBN": "1-55860-615-7",
    "URL": "http://portal.acm.org/citation.cfm?id=671516",
    "abstract": "The nearest- or near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately, all known techniques for solving this problem fall prey to the \"curse of dimensionality.\" That is, the data structures scale poorly with data dimensionality; in fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should suffice for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our method gives significant improvement in running time over other methods for searching in high-dimentisonal spaces based on hierarchical tree decomposition. Experimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).",
    "author": [
      {
        "family": "Gionis",
        "given": "Aristides"
      },
      {
        "family": "Indyk",
        "given": "Piotr"
      },
      {
        "family": "Motwani",
        "given": "Rajeev"
      }
    ],
    "container-title": "Proceedings of the 25<sup>th</sup> international conference on very large data bases (VLDB ’99)",
    "id": "Gionis1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "approximate_matching, clustering",
    "language": "en-US",
    "page": "518-529",
    "publisher": "Morgan Kaufmann",
    "publisher-place": "San Francisco, CA, USA",
    "title": "Similarity search in high dimensions via hashing",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1998076.1998124",
    "ISBN": "978-1-4503-0744-4",
    "abstract": "Various approaches for plagiarism detection exist. All are based on more or less sophisticated text analysis methods such as string matching, fingerprinting or style comparison. In this paper a new approach called Citation-based Plagiarism Detection is evaluated using a doctoral thesis, in which a volunteer crowd-sourcing project called GuttenPlag identified substantial amounts of plagiarism through careful manual inspection. This new approach is able to identify similar and plagiarized documents based on the citations used in the text. It is shown that citation-based plagiarism detection performs significantly better than text-based procedures in identifying strong paraphrasing, translation and some idea plagiarism. Detection rates can be improved by combining citation-based with text-based plagiarism detection.",
    "author": [
      {
        "family": "Gipp",
        "given": "Bela"
      },
      {
        "family": "Meuschke",
        "given": "Norman"
      },
      {
        "family": "Beel",
        "given": "Joeran"
      }
    ],
    "container-title": "Proceedings of the 11<sup>th</sup> annual international ACM/IEEE joint conference on digital libraries (JCDL ’11)",
    "id": "Gipp2011a",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "plagiarism",
    "language": "en-US",
    "page": "255-258",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Comparative evaluation of text- and citation-based plagiarism detection approaches using guttenplag",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/2034691.2034741",
    "ISBN": "978-1-4503-0863-2",
    "abstract": "Plagiarism Detection Systems have been developed to locate instances of plagiarism e.g. within scientific papers. Studies have shown that the existing approaches deliver reasonable results in identifying copy&paste plagiarism, but fail to detect more sophisticated forms such as paraphrased plagiarism, translation plagiarism or idea plagiarism. The authors of this paper demonstrated in recent studies that the detection rate can be significantly improved by not only relying on text analysis, but by additionally analyzing the citations of a document. Citations are valuable language independent markers that are similar to a fingerprint. In fact, our examinations of real world cases have shown that the order of citations in a document often remains similar even if the text has been strongly paraphrased or translated in order to disguise plagiarism. This paper introduces three algorithms and discusses their suitability for the purpose of citation-based plagiarism detection. Due to the numerous ways in which plagiarism can occur, these algorithms need to be versatile. They must be capable of detecting transpositions, scaling and combinations in a local and global form. The algorithms are coined Greedy Citation Tiling, Citation Chunking and Longest Common Citation Sequence. The evaluation showed that if these algorithms are combined, common forms of plagiarism can be detected reliably.",
    "author": [
      {
        "family": "Gipp",
        "given": "Bela"
      },
      {
        "family": "Meuschke",
        "given": "Norman"
      }
    ],
    "container-title": "DocEng 2011: Proceedings of the 11<sup>th</sup> ACM symposium on document engineering",
    "id": "Gipp2011b",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "document_engineering, e-learning, intertextuality, plagiarism",
    "language": "en-US",
    "page": "249-258",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Citation pattern matching algorithms for citation-based plagiarism detection: Greedy citation tiling, citation chunking and longest common citation sequence",
    "title-short": "Citation pattern matching algorithms for citation-based plagiarism detection",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s13740-012-0005-x",
    "abstract": "We concentrate on geospatial ontologies. Our main contribution in this paper is a methodology and a minimal set of guiding principles, inspired by the faceted approach, as originally developed in library science, and a large-scale ontology for Space that we have constructed following the methodology proposed. The approach we propose, centered on the fundamental notions of domain and facet, guarantees the creation of high-quality ontologies in terms of robustness, extensibility, reusability, compactness and flexibility. Taking into account the different aspects of Space, the ontology we have developed, and that we have obtained from the refinement and extension of some existing resources including GeoNames, WordNet and the Italian part of MultiWordNet, provides knowledge about places of the world, their classes, their attributes and the spatial relations between them. The construction procedure was manual for the identification and categorization into facets of the terms denoting classes, relations and attribute names, while it was automatic for the population of the ontology with entities and corresponding attribute values. This has allowed us to obtain a very satisfactory quantitative and qualitative result.",
    "author": [
      {
        "family": "Giunchiglia",
        "given": "Fausto"
      },
      {
        "family": "Dutta",
        "given": "Biswanath"
      },
      {
        "family": "Maltese",
        "given": "Vincenzo"
      },
      {
        "family": "Farazi",
        "given": "Feroz"
      }
    ],
    "container-title": "Journal on Data Semantics",
    "id": "Giunchiglia2012",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "geo_ir, geocoding",
    "language": "en-US",
    "page": "57-73",
    "title": "A Facet-Based methodology for the construction of a Large-Scale geospatial ontology",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "URL": "http://ucrel.lancs.ac.uk/publications/CL2007/paper/238_Paper.pdf",
    "abstract": "The Historical Dictionary of Brazilian Portuguese (HDBP), the first of its kind, is based on a corpus of Brazilian Portuguese (BP) texts from the sixteenth through the eighteenth centuries (and some texts from the beginning of the nineteenth century), being developed under the sponsorship of the Brazilian funding agency CNPq (Conselho Nacional de Desenvolvimento Científico e Tecnológico). It is a three-year project that started in 2006 to fill a gap in Brazilian culture with a dictionary describing the vocabulary of Brazilian Portuguese from the beginning of the country’s history. The corpus totals more than 3,000 texts with approximately 7.5 million words. Our working corpus, i.e. the corpus already processed by the corpus processing system UNITEX (http://www-igm.univ-mlv.fr/ unitex/), is coded in Unicode (UTF-16) and totals 1,733 texts, 57.1 MB, and 4.9 million words. A difficulty in dealing with historical corpora to carry out lexicographic tasks is the identification of all spelling variants of a specific word, since spelling variation distorts frequency counts, a usual criterion to select dictionary entries. In our project, another challenge is to select all variants of a dictionary entry that are in the corpus to illustrate the absence of an orthographical system in the aforementioned centuries and to provide example sentences for them. This paper introduces both an approach based on transformation rules to cluster distinct spelling variations around a common form, which is not always the orthographic (or modern) form, and the choices made to build a dictionary of spelling variants of BP based on these clusters. Currently, we have forty-three rules manually developed, which generated 12,189 clusters of spelling variants, totalling 27,199 variants from our working corpus. After a careful analysis of these clusters, we adopted the DELA format to build our dictionary. The BP dictionary of spelling variants enables sophisticated searches in the historical corpus using UNITEX, giving support to build the main dictionary of the HDBP project. Moreover, the variants of a given word can be searched using an application named Dicionário we have developed to display dictionaries in DELA format. As we also use Philologic (http://philologic.uchicago.edu/index.php) to support the building of the HDPB, we carried out a comparative evaluation between our approach to cluster distinct spelling variants and AGREP (http://www.tgries.de/agrep/), which is used in Philologic to check for similar or alternative spellings.",
    "author": [
      {
        "family": "Giusti",
        "given": "Rafael"
      },
      {
        "family": "Candido Jr.",
        "given": "Arnaldo"
      },
      {
        "family": "Muniz",
        "given": "Marcelo"
      },
      {
        "family": "Cucatto",
        "given": "Lívia"
      },
      {
        "family": "Aluísio",
        "given": "Sandra"
      }
    ],
    "container-title": "Proceedings of the corpus linguistics conference CL2007",
    "editor": [
      {
        "family": "Davies",
        "given": "Matthew"
      },
      {
        "family": "Rayson",
        "given": "Paul"
      },
      {
        "family": "Hunston",
        "given": "Susan"
      },
      {
        "family": "Danielsson",
        "given": "Pernilla"
      }
    ],
    "id": "Giusti2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, portuguese",
    "language": "en-US",
    "publisher": "University of Birmingham",
    "title": "Automatic detection of spelling variation in historical corpus: An application to build a Brazilian Portuguese spelling variants dictionary",
    "title-short": "Automatic detection of spelling variation in historical corpus",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Гладкий",
        "given": "Алексей Всеволодович"
      },
      {
        "family": "Мельчук",
        "given": "Игорь Александрович"
      }
    ],
    "id": "Gladkij1969",
    "issued": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "language": "ru-RU",
    "publisher": "Наука",
    "publisher-place": "Москва",
    "title": "Элементы математической лингвистики",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Gladkij",
        "given": "Aleksej Vsevolodovič"
      },
      {
        "family": "Melʹčuk",
        "given": "Igor Aleksandrovič"
      }
    ],
    "id": "Gladkij1969-Lat",
    "issued": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "language": "ru-RU",
    "publisher": "Nauka",
    "publisher-place": "Moskva",
    "title": "Elementy matematičeskoj lingvistiki",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Gladkij",
        "given": "Aleksej Vsevolodovič"
      },
      {
        "family": "Melʹčuk",
        "given": "Igor Aleksandrovič"
      }
    ],
    "id": "Gladkij1972",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Dunod",
    "publisher-place": "Paris",
    "title": "Éléments de linguistique mathématique",
    "translator": [
      {
        "family": "Cohen",
        "given": "J."
      },
      {
        "family": "Hérault",
        "given": "D."
      }
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Gladkij",
        "given": "Aleksej Vsevolodovič"
      },
      {
        "family": "Mel’čuk",
        "given": "Igor Aleksandrovič"
      }
    ],
    "id": "Gladkij1973",
    "issued": {
      "date-parts": [
        [
          1973
        ]
      ]
    },
    "language": "de-DE",
    "original-date": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "publisher": "Wilhelm Fink",
    "publisher-place": "München/Salzburg",
    "title": "Elemente der mathematischen Linguistik",
    "translator": [
      {
        "family": "Balzer",
        "given": "A."
      },
      {
        "family": "Böhme",
        "given": "U."
      },
      {
        "family": "Eckert",
        "given": "A."
      },
      {
        "family": "Haltof",
        "given": "B."
      },
      {
        "family": "Koch",
        "given": "G."
      },
      {
        "family": "Krause",
        "given": "H."
      },
      {
        "family": "Schrader",
        "given": "G."
      },
      {
        "family": "Strauch",
        "given": "M."
      }
    ],
    "type": "book"
  },
  {
    "ISBN": "978-1-4244-5732-8",
    "ISSN": "1330-1012",
    "URL": "http://ieeexplore.ieee.org/xpls/abs\\_all.jsp?arnumber=5546369",
    "abstract": "The WissKI system provides a framework for ontology-based science communication and cultural heritage documentation. In many cases, the documentation consists of semi-structured data records with free text fields. Most references in the texts comprise of person and place names, as well as time specifications. We present the WissKI tools for semantic annotation using controlled vocabularies and formal ontologies derived from CIDOC Conceptual Reference Model (CRM). Current research deals with the annotations as building blocks for event recognition. Finally, we outline how the CRM helps to build bridges between documentation in different scientific disciplines.",
    "author": [
      {
        "family": "Görz",
        "given": "Günther"
      },
      {
        "family": "Scholz",
        "given": "Martin"
      }
    ],
    "container-title": "32<sup>nd</sup> international conference on information technology interfaces (ITI 2010)",
    "id": "Goerz2010a",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, nlp, semantic_web",
    "page": "79-84",
    "publisher": "IEEE",
    "publisher-place": "New York, NY, USA",
    "title": "Adaptation of NLP techniques to cultural heritage research and documentation",
    "type": "paper-conference"
  },
  {
    "DOI": "10.2498/cit.1001918",
    "URL": "http://hrcak.srce.hr/file/95778",
    "abstract": "The WissKI system provides a framework for ontology based science communication and cultural heritage documentation. In many cases, the documentation consists of semi-structured data records with free text fields. Most references in the texts comprise of person and place names, as well as time specifications. We present the WissKI tools for semantic annotation using controlled vocabularies and formal ontologies derived from CIDOC Conceptual Reference Model (CRM). Current research deals with the annotations as building blocks for event recognition. Finally, we outline how the CRM helps to build bridges between documentation in different scientific disciplines.",
    "author": [
      {
        "family": "Görz",
        "given": "Günther"
      },
      {
        "family": "Scholz",
        "given": "Martin"
      }
    ],
    "container-title": "Journal of Computing and Information Technology",
    "id": "Goerz2010b",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, semantic_web",
    "page": "317-324",
    "title": "Adaptation of NLP techniques to cultural heritage research and documentation",
    "type": "article-journal",
    "volume": "18"
  },
  {
    "URL": "http://www.kunstgeschichte-ejournal.net/167/",
    "abstract": "Mit dem WissKI-System wird die Konzeption für eine virtuelle Forschungsumgebung vorgestellt, die aus Anforderungen an die kooperative Forschung im Bereich des Kulturerbes und seiner Dokumentation im digitalen Medium entstand und die wir im Rahmen des DFG-geförderten Projekts WissKI (›Wissenschaftliche Kommunikations-Infrastruktur‹, http://www.wiss-ki.eu/) umsetzen. Es geht dabei nicht nur um die einfache Bereitstellung und offene Verfügbarkeit von Quellmaterialien – strukturierte Texte, Grafiken, Bilder, Video, Audio – und Metadaten in digitaler Form, sondern um eine Infrastruktur für interaktives und vernetztes Arbeiten auf der Basis semantischer Tiefenerschließung. Eine Schlüsselrolle kommt hierbei dem ›Conceptual Reference Model‹ von ICOM-CIDOC als formaler Referenzontologie zu, die um geeignete Anwendungsontologien erweitert werden kann. Ihre Implementation in der ›Web Ontology Language‹ bildet in Verbindung mit verschiedenen Werkzeugen des ›Semantic Web‹ den Kern des WissKI-Systems.",
    "author": [
      {
        "family": "Görz",
        "given": "Günther"
      }
    ],
    "container-title": "Kunstgeschichte. Open Peer Reviewed Journal",
    "id": "Goerz2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, semantic_web",
    "language": "en-US",
    "title": "WissKI: Semantische Annotation, Wissensverarbeitung und Wissenschaftskommunikation in einer virtuellen Forschungsumgebung",
    "type": "article-journal"
  },
  {
    "URL": "http://fgg-erlangen.de/fgg/ojs/index.php/mfgg/article/view/339",
    "abstract": "Behaims “Erdapfel” von 1492 ist der älteste erhaltene Erdglobus. Er ist ein frühes Meisterwerk, das von einer Reihe früher wissenschaftlicher und technischer Innovationen zeugt. Heute gehört er zu den prominentesten Exponaten des Germanischen National-Museums in Nürnberg. Sein Kartenbild ist primär ptolemäisch, es enthält aber auch Elemente der mittelalterlichen Universalkartographie und von Portulanen. Seine umfangreiche und kunstvolle Ausstattung umfaßt über 100 Miniaturen und sechzig Fahnen und Wappen, mehr als 2000 Ortsnamen und über 50 lange Inschriften. Der Behaim-Globus ist unter den erhaltenen kartographischen Werken nahezu einzigartig dadurch, dass in ihm verschiedene Traditionen der spätmittelalterlichen Kartographie vereinigt sind. Wir berichten über ein laufendes Forschungsprojekt, das eine digitale und eine gedruckte Edition des Globus zum Ziel hat. Im Jahr 2011 wurden hochaufgelöste digitale Fotografien aufgenommen und ein neues 3D-Modell erstellt. Darüber hinaus gibt es eine Datenbank mit digitalisierten analogen Fotos der Globusoberfläche von 1990 und Schwarzweiß-Fotos von 1940. Auf der Basis dieser Bilddaten wird ein umfassender Katalog aller visuell relevanten Stellen einschließlich der zahlreichen Textfelder mithilfe einer in Beschreibungslogik (OWL-DL) formulierten Domänenontologie für die mittelalterliche Kartographie aufgebaut. Bisher wurden fast 3000 Datensätze (Katalogeinträge und Kommentare) erzeugt, die jedoch noch nicht den ganzen Globus abdecken. Diese Datensätze stellen den Kern der geplanten Edition dar. Was die digitale Seite betrifft, ist es heute weitgehend üblich, generische Begriffe und Eigenschaften für Objekte, Zeit und Raum, Ereignisse, Handlungsträger, Prozesse, etc. in einer sog. Referenzontologie zusammenzufassen, so dass die domänenspezifischen Begriffe aus den allgemeinen abgeleitet werden können. Das Conceptual Reference Model (CRM) von CIDOC ist eine solche Referenzontologie; unsere Implementation Erlangen CRM stellt eine semantische Basis für die kartographische Domänenontologie bereit. Der Behaim-Globus ist ein herauragendes Beispiel für eine übergreifende georeferentielle Organisation des zeotgenössischen Wissens, welche eine neue Dimension für die semantische Erschließung eröffnet. Historische Karten sind an erster Stelle kognitive Karten, wofür eine formale qualitative Darstellung von (abstrakten) Regionen und ihren relativen Positionen zueinander, aber auch von Richtung, Orientierung und Entfernung erforderlich ist. Diese können effizient durch spezielle Datentypen für sog. Constraint-Löser repräsentiert werden; ihre Integration in einen allgemeinen logischen Darstellungsrahmen ergibt ein System für hybrides Schließen zur Bearbeitung komplexer räumlicher Anfragen über dem Stellenkatalog des Globus.",
    "author": [
      {
        "family": "Görz",
        "given": "Günther"
      },
      {
        "family": "Scholz",
        "given": "Martin"
      }
    ],
    "container-title": "Zeitschrift der Fränkischen Geographischen Gesellschaft",
    "id": "Goerz2014a",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "de-DE",
    "page": "1-15",
    "title": "Semantische Erschließung mittelalterlicher Kartographie: Das Beispiel des Behaim-Globus von 1492",
    "title-short": "Semantische Erschließung mittelalterlicher Kartographie",
    "type": "article-journal",
    "volume": "60"
  },
  {
    "DOI": "10.5749/minnesota/9780816677948.001.0001",
    "ISBN": "9780816677948",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      }
    ],
    "id": "Gold2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "publisher": "University of Minnesota Press",
    "title": "Debates in the digital humanities",
    "type": "book"
  },
  {
    "DOI": "10.5749/minnesota/9780816677948.003.0002",
    "ISBN": "9780816677948",
    "author": [
      {
        "family": "Fitzpatrick",
        "given": "Kathleen"
      }
    ],
    "chapter-number": "2",
    "container-title": "Debates in the digital humanities",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      }
    ],
    "id": "Fitzpatrick2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "page": "12-15",
    "publisher": "University of Minnesota Press",
    "title": "The humanities, done digitally",
    "type": "chapter"
  },
  {
    "ISBN": "9780816699544",
    "URL": "http://dhdebates.gc.cuny.edu/book/2",
    "abstract": "Pairing full-length scholarly essays with shorter pieces drawn from scholarly blogs and conference presentations, as well as commissioned interviews and position statements, Debates in the Digital Humanities 2016 reveals a dynamic view of a field in negotiation with its identity, methods, and reach. Pieces in the book explore how DH can and must change in response to social justice movements and events like #Ferguson; how DH alters and is altered by community college classrooms; and how scholars applying DH approaches to feminist studies, queer studies, and black studies might reframe the commitments of DH analysts. Numerous contributors examine the movement of interdisciplinary DH work into areas such as history, art history, and archaeology, and a special forum on large-scale text mining brings together position statements on a fast-growing area of DH research. In the multivalent aspects of its arguments, progressing across a range of platforms and environments, Debates in the Digital Humanities 2016 offers a vision of DH as an expanded field—new possibilities, differently structured. Published simultaneously in print, e-book, and interactive webtext formats, each DH annual will be a book-length publication highlighting the particular debates that have shaped the discipline in a given year. By identifying key issues as they unfold, and by providing a hybrid model of open-access publication, these volumes and the Debates in the Digital Humanities series will articulate the present contours of the field and help forge its future.",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      },
      {
        "family": "Klein",
        "given": "Lauren F."
      }
    ],
    "id": "Gold2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "publisher": "University of Minnesota Press",
    "publisher-place": "Minneapolis, MN, USA",
    "title": "Debates in the digital humanities 2016",
    "type": "book"
  },
  {
    "DOI": "10.1145/2740908.2742021",
    "abstract": "The PeriodO period gazetteer collects definitions of time periods made by archaeologists and other historical scholars. In constructing the gazetteer, we sought to make period definitions parsable and comparable by computers while also retaining the broader scholarly context in which they were conceived. Our approach resulted in a dataset of period definitions and their provenances that resemble what data scientists working in the e-science domain have dubbed “nanopublications.” In this paper we describe the origin and goals of nanopublications, provide an overview of the design and implementation of a database of period definitions, and highlight the similarities and differences between the two.",
    "author": [
      {
        "family": "Golden",
        "given": "Patrick"
      },
      {
        "family": "Shaw",
        "given": "Ryan"
      }
    ],
    "container-title": "SAVE-SD 2015",
    "id": "Golden2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities, nanopublications, rdf, semantic_web",
    "language": "en-US",
    "page": "1013-1018",
    "title": "Period assertion as nanopublication: The PeriodO period gazetteer",
    "title-short": "Period assertion as nanopublication",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/800209.806456",
    "author": [
      {
        "family": "Goldfarb",
        "given": "Charles F."
      }
    ],
    "container-title": "Proceedings of the ACM SIGPLAN SIGOA symposium on text manipulation",
    "event-place": "Portland, Oregon, United States",
    "id": "Goldfarb1981",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "language": "en-US",
    "page": "68-73",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A generalized approach to document markup",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Goldfarb",
        "given": "Charles F."
      }
    ],
    "id": "Goldfarb1990",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "markup, xml",
    "language": "en-US",
    "publisher": "Oxford University Press",
    "publisher-place": "Oxford, UK",
    "title": "The SGML handbook",
    "type": "book"
  },
  {
    "DOI": "10.1002/(sici)1097-4571(199707)48:7%3C656::aid-asi13%3E3.0.co;2-t",
    "ISSN": "0002-8231",
    "abstract": "This article is a commentary—over a quarter-century after the fact—on the first published paper to suggest the need for (and hint at the existence of) what is now the Standard Generalized Markup Language. It was presented at the 33rd annual meeting of the American Society for Information Science in Philadelphia, October 15, 1970, and published in Volume 7 of the ASIS Proceedings. The editors of this Special Issue of JASIS felt that that meeting was worth remembering here because of its hitherto unpublicized connection with the origin of SGML. In addition, it is also worth remembering because of its closing banquet, which featured an erudite and witty speech by a professor with two doctorates, a 70-piece balalaika orchestra, the entire Philadelphia Mummers band (replete with banjos, saxophones, and feathered headdresses ) , and a middle-eastern belly dancer who worked on the table tops! I’ve spoken at some hundred conferences since then and none of them has even come close.",
    "author": [
      {
        "family": "Goldfarb",
        "given": "Charles F."
      }
    ],
    "container-title": "Journal of the American Society for Information Science",
    "id": "Goldfarb1997",
    "issue": "7",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "sgml",
    "language": "en-US",
    "page": "656-661",
    "title": "SGML: The reason why and the first published hint",
    "title-short": "SGML",
    "type": "article-journal",
    "volume": "48"
  },
  {
    "author": [
      {
        "family": "Gooch",
        "given": "Sherwin J."
      }
    ],
    "id": "Gooch1980",
    "issued": {
      "date-parts": [
        [
          1980,
          6
        ]
      ]
    },
    "note": "Filing Date: February 28, 1977",
    "publisher": "United States Patent 4 206 675",
    "title": "Cybernetic music system",
    "type": ""
  },
  {
    "DOI": "10.1093/llc/fqs054",
    "ISSN": "1477-4615",
    "abstract": "There has been widespread excitement in recent years about the emergence of large-scale digital initiatives (LSDIs) such as Google Book Search. Although many have become excited at the prospect of a digital recreation of the Library of Alexandria, there has also been great controversy surrounding these projects. This article looks at one of these controversies: the suggestion that mass digitization is creating a virtual rubbish dump of our cultural heritage. It discusses some of the quantitative methods being used to analyse the big data that have been created, and two major concerns that have arisen as a result. First, there is the concern that quantitative analysis has inadvertently fed a culture that favours information ahead of traditional research methods. Second, little information exists about how LSDIs are used for any research other than quantitative methods. These problems have helped to fuel the idea that digitization is destroying the print medium, when in many respects it still closely remediates the bibliographic codes of the Gutenberg era. The article concludes that more work must be done to understand what impact mass digitization has had on all researchers in the humanities, rather than just the early adopters, and briefly mentions the work that the author is undertaking in this area.",
    "author": [
      {
        "family": "Gooding",
        "given": "Paul"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Gooding2013a",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "425-431",
    "title": "Mass digitization and the garbage dump: The conflicting needs of quantitative and qualitative methods",
    "title-short": "Mass digitization and the garbage dump",
    "type": "article-journal",
    "volume": "28"
  },
  {
    "DOI": "10.1093/llc/fqt051",
    "ISSN": "1477-4615",
    "abstract": "This article presents the theoretical background to a wider project that is attempting to increase our understanding of the impact and uses of large-scale digitization, being undertaken by the first author at University College London with the working title ’What is the impact of large-scale digitization upon researchers and the information sector?’ It discusses the controversy surrounding the emergence of mass digitization: the creation and collection of huge resources containing millions of pages of textual cultural content. It demonstrates that the polarized nature of the literature about this technological development is far from unprecedented, and in fact can be traced through the theory of a number of varied fields: the debate surrounding mechanization and digital technologies, our understanding of the role of the sublime in modern representations of technology, the similarities between the sociology of city life and digital information overload, and the way in which innovations are diffused throughout society. It proposes that these theories explain why debates around technological innovation often become so hyperbolic, creating an almost mythological view of technological determination. It concludes that, as a result of the processes outlined in this theory, mass digitization has become stuck between two conflicting rhetorical movements, and that it is therefore necessary to begin working to increase our understanding of this technology and to move the debate onwards using evidence from the real world.",
    "author": [
      {
        "family": "Gooding",
        "given": "Paul"
      },
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Warwick",
        "given": "Claire"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Gooding2013b",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "629-639",
    "title": "The myth of the new: Mass digitization, distant reading, and the future of the book",
    "title-short": "The myth of the new",
    "type": "article-journal",
    "volume": "28"
  },
  {
    "ISBN": "9781107151000",
    "URL": "http://www.worldcat.org/isbn/9781107151000",
    "abstract": "Commonsense psychology refers to the implicit theories that we all use to make sense of people’s behavior in terms of their beliefs, goals, plans, and emotions. These are also the theories we employ when we anthropomorphize complex machines and computers as if they had humanlike mental lives. In order to successfully cooperate and communicate with people, these theories will need to be represented explicitly in future artificial intelligence systems. This book provides a large-scale logical formalization of commonsense psychology in support of humanlike artificial intelligence. It uses formal logic to encode the deep lexical semantics of the full breadth of psychological words and phrases, providing fourteen hundred axioms of first-order logic organized into twenty-nine commonsense psychology theories and sixteen background theories. This in-depth exploration of human commonsense reasoning for artificial intelligence researchers, linguists, and cognitive and social psychologists will serve as a foundation for the development of humanlike artificial intelligence.",
    "author": [
      {
        "family": "Gordon",
        "given": "Andrew S."
      },
      {
        "family": "Hobbs",
        "given": "Jerry R."
      }
    ],
    "id": "Gordon2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "logic, psychology, uncertainty",
    "language": "en-US",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge",
    "title": "A formal theory of commonsense psychology: How people think people think",
    "title-short": "A formal theory of commonsense psychology",
    "type": "book"
  },
  {
    "URL": "http://www.gorissen.info/Pierre/qti/Quickscan_QTI_UK.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Gorissen",
        "given": "Pierre"
      }
    ],
    "id": "Gorissen2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "publisher": "De Digitale Universiteit",
    "publisher-place": "Utrecht",
    "title": "Quickscan QTI: Usability study of QTI for De Digitale Universiteit",
    "title-short": "Quickscan QTI",
    "type": ""
  },
  {
    "URL": "http://www.gorissen.info/Pierre/qti/Quickscan_QTI_2006.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Gorissen",
        "given": "Pierre"
      }
    ],
    "id": "Gorissen2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "publisher": "De Digitale Universiteit",
    "publisher-place": "Utrecht",
    "title": "Quickscan QTI – 2006: Usability study of QTI for De Digitale Universiteit",
    "title-short": "Quickscan QTI – 2006",
    "type": ""
  },
  {
    "DOI": "10.1007/978-1-4612-3606-1",
    "ISBN": "978-1-4612-8175-7",
    "abstract": "This book is an introduction to NeWS: the Networked, Extensible, Window System from Sun Microsystems. It is oriented towards people who have a basic knowledge of programming and window systems who would like to understand more about window systems in general and NeWS in particular. A significant portion of the book is devoted to an overview and history of window systems. While there is enough detail here to allow readers to write simple NeWS applications, the NeWS Reference Manual should be consulted for a more complete treatment. This book was written to refer to the NeWS 1.1 product, available from Sun and also available from several non-Sun suppliers. Shortly after this book is published, Sun will be releasing the next version of NeWS—the X11/NeWS merged window system. Chapter 10 is dedicated to an overview of that product, but X11/NeWS deserves a book of its own. All the code examples in this book have been tested on both NeWS and the X11/NeWS merge. Should there be another edition of this book, we will discuss some of the new development being done in the user interface tool­ kit area on NeWS. Significantly, the NeWS Development Environment (NDE) is now being developed at Sun; NDE promises to eclipse existing user interface toolkit designs and window programming environments.",
    "author": [
      {
        "family": "Gosling",
        "given": "James"
      },
      {
        "family": "Rosenthal",
        "given": "David S. H."
      },
      {
        "family": "Arden",
        "given": "Michelle J."
      }
    ],
    "id": "Gosling1989",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "hci, postscript",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "New York, NY, USA",
    "title": "The NeWS book",
    "type": "book"
  },
  {
    "DOI": "10.1145/1600193.1600236",
    "abstract": "Many European libraries are currently engaged in mass digitization projects that aim to make historical documents and corpora online available in the Internet. In this context, appropriate lexical resources play a double role. They are needed to improve OCR recognition of historical documents, which currently does not lead to satisfactory results. Second, even assuming a perfect OCR recognition, since historical language differs considerably from modern language, the matching process between queries submitted to search engines and variants of the search terms found in historical documents needs special support. While the usefulness of special dictionaries for both problems seems undisputed, concrete knowledge and experience are still missing. There are no hints about what optimal lexical resources for historical documents should look like. The real benefit reached by optimized lexical resources is unclear. Both questions are rather complex since answers depend on the point in history when documents were born. We present a series of experiments which illuminate these points. For our evaluations we collected a large corpus covering German historical documents from before 1500 to 1950 and constructed various types of dictionaries. We present the coverage reached with each dictionary for ten subperiods of time. Additional experiments illuminate the improvements for OCR accuracy and Information Retrieval that can be reached, again looking at distinct dictionaries and periods of time. For both OCR and IR, our lexical resources lead to substantial improvements.",
    "author": [
      {
        "family": "Gotscharek",
        "given": "Annette"
      },
      {
        "family": "Reffle",
        "given": "Ulrich"
      },
      {
        "family": "Ringlstetter",
        "given": "Christoph"
      },
      {
        "family": "Schulz",
        "given": "Klaus U."
      }
    ],
    "container-title": "DocEng ’09: Proceedings of the 9<sup>th</sup> ACM symposium on document engineering",
    "id": "Gotscharek2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "page": "193-200",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "On lexical resources for digitization of historical documents",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1568296.1568309",
    "abstract": "Due to the large number of spelling variants found in historical texts, standard methods of Information Retrieval (IR) fail to produce satisfactory results on historical document collections. In order to improve recall for search engines, modern words used in queries have to be associated with corresponding historical variants found in the documents. In the literature, the use of (1) special matching procedures and (2) lexica for historical language have been suggested as two ways to solve this problem. In the first part of the paper we show how the construction of matching procedures and lexica may benefit from each other, leading the way to a combination of both approaches. A tool is presented where matching rules and a historical lexicon are built in an interleaved way based on corpus analysis. A crucial question considered in the second part of the paper is if matching procedures alone suffice to lift IR on historical texts to a satisfactory level. Since historical language changes over centuries it is not simple to obtain an answer. We present experiments where the performance of matching procedures in text collections from four centuries is studied. After classifying missed vocabulary, we measure precision and recall of the matching procedure for each period. Our results indicate that for earlier periods historical lexica represent an important corrective to matching procedures in IR applications.",
    "author": [
      {
        "family": "Gotscharek",
        "given": "Annette"
      },
      {
        "family": "Neumann",
        "given": "Andreas"
      },
      {
        "family": "Reffle",
        "given": "Ulrich"
      },
      {
        "family": "Ringlstetter",
        "given": "Christoph"
      },
      {
        "family": "Schulz",
        "given": "Klaus U."
      }
    ],
    "container-title": "Proceedings of the third workshop on analytics for noisy unstructured text data (AND 2009)",
    "id": "Gotscharek2009a",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "page": "69-76",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Enabling information retrieval on historical document collections: The role of matching procedures and special lexica",
    "title-short": "Enabling information retrieval on historical document collections",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s10032-010-0132-6",
    "ISSN": "1433-2833",
    "abstract": "Due to the large number of spelling variants found in historical texts, standard methods of Information Retrieval (IR) fail to produce satisfactory results on historical document collections. In order to improve recall for search engines, modern words used in queries have to be associated with corresponding historical variants found in the documents. In the literature, the use of (1) special matching procedures and (2) lexica for historical language have been suggested as two alternative ways to solve this problem. In the first part of the paper, we show how the construction of matching procedures and lexica may benefit from each other, leading the way to a combination of both approaches. A tool is presented where matching rules and a historical lexicon are built in an interleaved way based on corpus analysis. In the second part of the paper, we ask if matching procedures alone suffice to lift IR on historical texts to a satisfactory level. Since historical language changes over centuries, it is not simple to obtain an answer. We present experiments where the performance of matching procedures in text collections from four centuries is studied. After classifying missed vocabulary, we measure precision and recall of the matching procedure for each period. Results indicate that for earlier periods, matching procedures alone do not lead to satisfactory results. We then describe experiments where the gain for recall obtained from historical lexica of distinct sizes is estimated.",
    "author": [
      {
        "family": "Gotscharek",
        "given": "Annette"
      },
      {
        "family": "Reffle",
        "given": "Ulrich"
      },
      {
        "family": "Ringlstetter",
        "given": "Christoph"
      },
      {
        "family": "Schulz",
        "given": "Klaus U."
      },
      {
        "family": "Neumann",
        "given": "Andreas"
      }
    ],
    "container-title": "International Journal on Document Analysis and Recognition",
    "id": "Gotscharek2011",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr, spelling_correction",
    "language": "en-US",
    "page": "159-171",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Towards information retrieval on historical document collections: The role of matching procedures and special lexica",
    "title-short": "Towards information retrieval on historical document collections",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "URL": "http://aclweb.org/anthology/W11-2210.pdf",
    "abstract": "Microtexts, like SMS messages, Twitter posts, and Facebook status updates, are a popular medium for real-time communication. In this paper, we investigate the writing conventions that different groups of users use to express themselves in microtexts. Our empirical study investigates properties of lexical transformations as observed within Twitter microtexts. The study reveals that different populations of users exhibit different amounts of shortened English terms and different shortening styles. The results reveal valuable insights into how human language technologies can be effectively applied to microtexts.",
    "author": [
      {
        "family": "Gouws",
        "given": "Stephan"
      },
      {
        "family": "Metzler",
        "given": "Donald"
      },
      {
        "family": "Cai",
        "given": "Congxing"
      },
      {
        "family": "Hovy",
        "given": "Eduard"
      }
    ],
    "container-title": "Proceedings of the workshop on language in social media (LSM 2011)",
    "id": "Gouws2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "microtext, spelling_normalization",
    "language": "en-US",
    "page": "20-29",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Contextual bearing on linguistic variation in social media",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1016/j.websem.2011.09.001",
    "ISSN": "1570-8268",
    "URL": "http://www.sciencedirect.com/science/article/pii/S1570826811000783",
    "author": [
      {
        "family": "Gracia",
        "given": "Jorge"
      },
      {
        "family": "Montiel-Ponsoda",
        "given": "Elena"
      },
      {
        "family": "Cimiano",
        "given": "Philipp"
      },
      {
        "family": "Gómez-Pérez",
        "given": "Asunción"
      },
      {
        "family": "Buitelaar",
        "given": "Paul"
      },
      {
        "family": "McCrae",
        "given": "John"
      }
    ],
    "container-title": "Web Semantics: Science, Services and Agents on the World Wide Web",
    "id": "Gracia2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "linked_data",
    "page": "63-71",
    "title": "Challenges for the multilingual web of data",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "abstract": "The preface usually contains one of four pleasures, says anthologist Alasdair Gray. There is the biographical snippet, full of gossipy details that \"make us feel at home in earlier times.\" There is the author’s attempt to forestall criticism (in first editions) or to answer it (in later ones). There is the report on the state of civilization, both favorable (see Walt Whitman) and unfavorable (see Karl Marx). And there is the attack on other writers or translators, sometimes bridging centuries and containing spears thrown at the long dead. All four pleasures are well represented in this 640-page treasury of English and American intros, which runs from an A.D. 675 translation of Genesis to the 1920 poems of Wilfred Owen. Why stop there? \"The flow is stopped at 1920,\" admits Gray in his own disarmingly self-effacing preface, \"by costs of using work still in copyright.\" This is anything but anthology-on-the-cheap, however. Gray (Lanark and A History Maker) poured 16 years of research into The Book of Prefaces, and adds considerable value with his own running commentary, which straggles down the margins in brash red ink. Gray on the God of Genesis: \"This God, with revenge in mind, first makes earth ugly as hell.\" Among God’s anthologized fellows are Mark Twain, who defends his use of Southern dialect in The Adventures of Huckleberry Finn; Lewis Carroll, who anticipates his critics’ charges of writing nonsense in The Hunting of the Snark and proceeds to prove their case; and Charles Darwin, who recalls how the seeds of The Origin of Species were sown aboard the HMS Beagle. Gray mixes scholarly research with playful eccentricities: When was the last time you saw a book’s typesetter, typist, and publisher memorialized in pen-and-ink drawings? And \"with this in their lavatory,\" writes the cheeky author, \"everyone else can read nothing but newspaper supplements and still seem educated.\" He may be right. –Claire Dederer",
    "author": [
      {
        "family": "Gray",
        "given": "Alasdair"
      }
    ],
    "id": "Gray2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "english",
    "language": "en-US",
    "publisher": "Bloomsbury",
    "publisher-place": "New York/London",
    "title": "The book of prefaces",
    "type": "book"
  },
  {
    "DOI": "10.1145/1506250.1506252",
    "ISBN": "978-1-60558-430-0",
    "abstract": "Within most domains of discourse, there exists different terminology used by distinct sub-groups. Often the terms used can be, or have already been, organised into controlled vocabularies which can be encoded into SKOS, a W3C standard for representing vocabularies. This terminology can then be used to help users to search for and discover resources. This requires a search mechanism to go from a user-supplied string to a vocabulary concept. In this paper, we discuss the issues encountered in developing a web service for searching and exploring the concepts in SKOS encoded astronomical vocabularies. Our prototype service takes in a query and responds with the concepts which are the \"best match\". It then supports the user in exploring the concepts’ formal definition and alternative forms, as well as their relationship to other concepts. When we add mappings between the concepts in different vocabularies (where available), these further enrich the explorations of vocabulary concepts.",
    "author": [
      {
        "family": "Gray",
        "given": "Alasdair J. G."
      },
      {
        "family": "Gray",
        "given": "Norman"
      },
      {
        "family": "Ounis",
        "given": "Iadh"
      }
    ],
    "container-title": "Proceedings of the WSDM ’09 workshop on exploiting semantic annotations in information retrieval (ESAIR ’09)",
    "id": "Gray2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "indices",
    "page": "1-5",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Searching and exploring controlled vocabularies",
    "type": "paper-conference"
  },
  {
    "URL": "http://http://www.xrce.xerox.com/Research-Development/Publications/1995-012",
    "author": [
      {
        "family": "Grefenstette",
        "given": "Gregory"
      }
    ],
    "container-title": "Proceedings of the 3international conference on statistical analysis of textual data (JADT 95)",
    "id": "Grefenstette1995",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "language": "en-US",
    "page": "263-268",
    "title": "Comparing two language identification schemes",
    "type": "paper-conference"
  },
  {
    "URL": "http://jstor.org/stable/40381874",
    "author": [
      {
        "family": "Greg",
        "given": "Walter W."
      }
    ],
    "container-title": "Studies in Bibliography",
    "id": "Greg1950",
    "issued": {
      "date-parts": [
        [
          1950
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_edition, digital_humanities",
    "language": "en-US",
    "page": "19-36",
    "publisher": "Bibliographical Society of the University of Virginia",
    "title": "The rationale of Copy-Text",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.1007/s10579-009-9094-z",
    "ISSN": "1574-020X",
    "abstract": "This article describes the design and implementation of a Dutch Electronic Lexicon of Multiword Expressions (DuELME). DuELME describes the core properties of over 5,000 Dutch multiword expressions. This article gives an overview of the decisions made in order to come to a standard lexical representation and discusses the description fields this representation comprises. We discuss the approach taken, which is innovative since it is based on the Equivalence Class Method (ECM). It is shown that introducing parameters to the ECM optimizes the method. The selection of the lexical entries and their properties is corpus-based. We describe the extraction of candidate expressions from corpora and discuss the selection criteria of the lexical entries. Moreover, we present the results of an evaluation of the standard representation in Alpino, a Dutch dependency parser.",
    "author": [
      {
        "family": "Grégoire",
        "given": "Nicole"
      }
    ],
    "container-title": "Language Resources and Evaluation",
    "id": "Gregoire2010",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "computational_linguistics, dutch, lexicography, phraseology",
    "language": "en-US",
    "page": "23-39",
    "title": "DuELME: A Dutch electronic lexicon of multiword expressions",
    "title-short": "DuELME",
    "type": "article-journal",
    "volume": "44"
  },
  {
    "DOI": "10.1093/llc/fqr022",
    "abstract": "Corpus linguistics and Geographical Information Systems (GIS) are approaches exploiting computer-based methodologies in the study of, respectively, language and language usage, and spatial patterns in geographical databases. We present an approach that uses corpus methods to bridge the gap between the textual content of a corpus (and, thus, the typically textual concerns of many branches of the humanities) and the geo-referenced database at the heart of a GIS. Using part-of-speech tagging to extract instances of proper nouns from a corpus, and a gazetteer to limit these instances to those representing place–names, a database of the places mentioned in a corpus can be created, visualized, and analysed using GIS technology. It is also possible to visualize the meanings associated with particular place–names, by building GIS databases on the collocation of place–names with particular semantic categories in their immediate context. In this way, we can create maps that visualize the geographical distribution of mentions of concepts such as war, government, or money in a particular data set. The approach cannot be entirely automated and some manual intervention is required. Nevertheless, the method is clearly valuable for the interpretation of spatial phenomena in text corpora.",
    "author": [
      {
        "family": "Gregory",
        "given": "Ian N."
      },
      {
        "family": "Hardie",
        "given": "Andrew"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Gregory2011",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, geo_ir, gis",
    "language": "en-US",
    "page": "297-314",
    "publisher": "Oxford University Press",
    "title": "Visual GISting: Bringing together corpus linguistics and geographical information systems",
    "title-short": "Visual GISting",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "URL": "http://jstor.org/stable/j.ctt16gz7s5.4",
    "abstract": "When geographical information systems (GIS) first began to be used by academic geographers in the late 1980s, their use was nothing if not controversial. Proponents of the new field argued that it had the potential to reinvigorate geography as a discipline under a more computational paradigm. Opponents argued that it marked a lurch toward an unacceptable form of positivism with no epistemology or treatment of ethical or political issues. One thing on which they both agreed – or perhaps took for granted – was that GIS was a quantitative technology that was to be used in a social scientific manner.",
    "author": [
      {
        "family": "Gregory",
        "given": "Ian N."
      },
      {
        "family": "Geddes",
        "given": "Alistair"
      }
    ],
    "container-title": "Toward spatial humanities: Historical GIS and spatial history",
    "id": "Gregory2013",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities, gis",
    "language": "en-US",
    "page": "ix-xxii",
    "publisher": "Indiana University Press",
    "title": "From historical GIS to spatial humanities: Deepening scholarship and broadening technology",
    "title-short": "From historical GIS to spatial humanities",
    "type": "chapter"
  },
  {
    "DOI": "10.3366/ijhac.2015.0135",
    "ISSN": "1753-8548",
    "author": [
      {
        "family": "Gregory",
        "given": "Ian"
      },
      {
        "family": "Donaldson",
        "given": "Christopher"
      },
      {
        "family": "Murrieta-Flores",
        "given": "Patricia"
      },
      {
        "family": "Rayson",
        "given": "Paul"
      }
    ],
    "container-title": "International Journal of Humanities and Arts Computing",
    "id": "Gregory2015",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities, gis",
    "language": "en-US",
    "page": "1-14",
    "title": "Geoparsing, GIS, and textual analysis: Current developments in spatial humanities research",
    "title-short": "Geoparsing, GIS, and textual analysis",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "abstract": "Cognitive Linguistics, the branch of linguistics that tries to \"make one’s account of human language accord with what is generally known about the mind and the brain,\" has become one of the most flourishing fields of contemporary linguistics. The chapters address many classic topics of Cognitive Linguistics. These topics include studies on the semantics of specific words (including polysemy and synonymy) as well as semantic characteristics of particular syntactic patterns / constructions (including constructional synonymy and the schematicity of constructions), the analysis of causatives, transitivity, and image-schematic aspects of posture verbs. The key characteristic of this volume is that all papers adopt the methodological perspective of Corpus Linguistics, the rapidly evolving branch of linguistics based on the computerized analysis of language used in authentic settings. Thus, the contributions do not only all provide various new insights in their respective fields, they also introduce new data as well as new corpus-based and quantitative methods of analysis. On the basis of their findings, the authors discuss both theoretical implications going well beyond the singular topics of the studies and show how the discipline of Cognitive Linguistics can benefit from the rigorous analysis of naturally-occurring language. The languages which are investigated are English, German, Dutch, and Russian, and the data come from a variety of different corpora. As such, the present volume will be of interest to a wide range of scholars with many different foci and interests and should pave the way for further integration of usage-based techniques of analysis within this exciting paradigm.",
    "editor": [
      {
        "family": "Gries",
        "given": "Stefan T."
      },
      {
        "family": "Stefanowitsch",
        "given": "Anatol"
      }
    ],
    "id": "Gries2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "linguistics",
    "language": "en-US",
    "publisher": "De Gruyter Mouton",
    "publisher-place": "Berlin, New York",
    "title": "Corpora in cognitive linguistics: Corpus-Based approaches to syntax and lexis",
    "title-short": "Corpora in cognitive linguistics",
    "type": "book"
  },
  {
    "DOI": "10.1111/j.1749-818x.2009.00149.x",
    "ISSN": "1749818X",
    "abstract": "Corpus linguistics is one of the fastest-growing methodologies in contemporary linguistics. In a conversational format, this article answers a few questions that corpus linguists regularly face from linguists who have not used corpus-based methods so far. It discusses some of the central assump- tions (’formal distributional differences reflect functional differences’), notions (corpora, representa- tivity and balancedness, markup and annotation), and methods of corpus linguistics (frequency lists, concordances, collocations), and discusses a few ways in which the discipline still needs to mature.",
    "author": [
      {
        "family": "Gries",
        "given": "Stefan T."
      }
    ],
    "container-title": "Language and Linguistics Compass",
    "id": "Gries2009",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "corpus_linguistics",
    "language": "en-US",
    "page": "1225-1241",
    "title": "What is corpus linguistics?",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "URL": "http://cit.zesoi.fer.hr/downloadPaper.php?paper=555",
    "abstract": "Every modern institution involved in higher education needs a Learning Management System (LMS) to handle learning and teaching processes. It is necessary to offer e.g. electronic lecture materials to the students for download via the internet. In some educational contexts, it is also necessary to offer internet tutorials to be able to give the students more personal support and accompany them through the whole lecture period. Many organisations have introduced commercial LMS and gained the experience that monolithic solutions do not fulfil the dynamic requirements of complex educational institutions and are very cost-intensive. Therefore, many universities face the decision to stick to their commercial LMS or to switch to a potentially more cost-effective and flexible solution, for instance by adopting available Open Source LMS. Since we have made profound experience in developing and operating an Open Source LMS, this contribution enlightens the main characteristics of this alternative. This paper describes a use case dealing with a full product lifecycle (development, deployment, use and evaluation) of an Open Source LMS at the University of Muenster (Germany). It identifies relevant instruments and aspects of system design which software architects in practical application domains should pay attention to.",
    "author": [
      {
        "family": "Grob",
        "given": "Heinz L."
      },
      {
        "family": "Bensberg",
        "given": "Frank"
      },
      {
        "family": "Dewanto",
        "given": "Blasius L."
      }
    ],
    "container-title": "Journal of Computing and Information Technology",
    "id": "Grob2004",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2004,
          6
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "127-134",
    "title": "Developing, deploying, using and evaluating an open source learning management system",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "URL": "http://jds.fass.org/cgi/content/abstract/61/9/1308",
    "abstract": "Computer-based education can assist in the interactive teaching of animal breeding. Based on the proposition that learning is facilitated by immediate feedback, computer-based education combines the computational capabilities of a computer with individual instruction. Three laboratory lessons were written on the PLATO IV system which was developed at the University of Illinois. These lessons, written for undergraduates studying the mathematical principles of population and quantitative genetics as applied to animal breeding, are coordinated with lectures and demonstrate through computer simulation the relative importance of factors determining genetic improvement in livestock. The PLATO system and the three lessons are described.",
    "author": [
      {
        "family": "Grossman",
        "given": "M."
      },
      {
        "family": "Walter",
        "given": "D."
      }
    ],
    "container-title": "J. Dairy Sci.",
    "id": "Grossman1978",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          1978,
          9,
          1
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "language": "en-US",
    "page": "1308-1311",
    "title": "Teaching with interactive computer capabilities (PLATO: Computer-Based education for animal breeding)",
    "title-short": "Teaching with interactive computer capabilities (PLATO",
    "type": "article-journal",
    "volume": "61"
  },
  {
    "author": [
      {
        "family": "Grotefend",
        "given": "Hermann"
      }
    ],
    "edition": "14",
    "id": "Grotefend2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "publisher": "Hahnsche Buchhandlung",
    "publisher-place": "Hannover",
    "title": "Taschenbuch der Zeitrechnung des deutschen Mittelalters und der frühen Neuzeit",
    "type": "book"
  },
  {
    "DOI": "10.3233/ISU-2010-0613",
    "ISSN": "0167-5265",
    "abstract": "As the amount of scholarly communication increases, it is increasingly difficult for specific core scientific statements to be found, connected and curated. Additionally, the redundancy of these statements in multiple fora makes it difficult to determine attribution, quality and provenance. To tackle these challenges, the Concept Web Alliance has promoted the notion of nanopublications (core scientific statements with associated context). In this document, we present a model of nanopublications along with a Named Graph/RDF serialization of the model. Importantly, the serialization is defined completely using already existing community-developed technologies. Finally, we discuss the importance of aggregating nanopublications and the role that the Concept Wiki plays in facilitating it.",
    "author": [
      {
        "family": "Groth",
        "given": "Paul"
      },
      {
        "family": "Gibson",
        "given": "Andrew"
      },
      {
        "family": "Velterop",
        "given": "Jan"
      }
    ],
    "container-title": "Information Services and Use",
    "id": "Groth2010",
    "issue": "1–2",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "nanopublications, rdf, semantic_web",
    "language": "en-US",
    "page": "51-56",
    "publisher": "IOS Press",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "The anatomy of a nanopublication",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/pdf/249_paper.pdf",
    "abstract": "In this paper we present two experiments conducted for comparison of different language identification algorithms. Short words-, frequent words- and n-gram-based approaches are considered and combined with the Ad-Hoc Ranking classification method. The language identification process can be subdivided into two main steps: First a document model is generated for the document and a language model for the language; second the language of the document is determined on the basis of the language model and is added to the document as additional information. In this work we present our evaluation results and discuss the importance of a dynamic value for the out-of-place measure.",
    "author": [
      {
        "family": "Grothe",
        "given": "Lena"
      },
      {
        "family": "De Luca",
        "given": "Ernesto W."
      },
      {
        "family": "Nürnberger",
        "given": "Andreas"
      }
    ],
    "container-title": "Proceedings of the sixth international language resources and evaluation (LREC’08)",
    "id": "Grothe2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "language": "en-US",
    "page": "980-985",
    "title": "A comparative study on language identification methods",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.ltg.ed.ac.uk/np/publications/ltg/papers/bopcris-lrec.pdf",
    "abstract": "We describe and evaluate a prototype system for recognising person and place names in digitised records of British parliamentary proceedings from the late 17th and early 19th centuries. The output of an OCR engine is the input for our system and we describe certain issues and errors in this data and discuss the methods we have used to overcome the problems. We describe our rule-based named entity recognition system for person and place names which is implemented using the LT-XML2 and LT-TTT2 text processing tools. We discuss the annotation of a development and testing corpus and provide results of an evaluation of our system on the test corpus.",
    "author": [
      {
        "family": "Grover",
        "given": "Claire"
      },
      {
        "family": "Givon",
        "given": "Sharon"
      },
      {
        "family": "Tobin",
        "given": "Richard"
      },
      {
        "family": "Ball",
        "given": "Julian"
      }
    ],
    "container-title": "Proceedings of the 6<sup>th</sup> international conference on language resources and evaluation (LREC 2008)",
    "id": "Grover2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, proper_names",
    "language": "en-US",
    "page": "1343-1346",
    "publisher": "European Language Resources Association (ELRA)",
    "title": "Named entity recognition for digitised historical texts",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1098/rsta.2010.0149",
    "PMID": "20643682",
    "abstract": "We report on two JISC-funded projects that aimed to enrich the metadata of digitized historical collections with georeferences and other information automatically computed using geoparsing and related information extraction technologies. Understanding location is a critical part of any historical research, and the nature of the collections makes them an interesting case study for testing automated methodologies for extracting content. The two projects (GeoDigRef and Embedding GeoCrossWalk) have looked at how automatic georeferencing of resources might be useful in developing improved geographical search capacities across collections. In this paper, we describe the work that was undertaken to configure the geoparser for the collections as well as the evaluations that were performed.",
    "author": [
      {
        "family": "Grover",
        "given": "Claire"
      },
      {
        "family": "Tobin",
        "given": "Richard"
      },
      {
        "family": "Byrne",
        "given": "Kate"
      },
      {
        "family": "Woollard",
        "given": "Matthew"
      },
      {
        "family": "Reid",
        "given": "James"
      },
      {
        "family": "Dunn",
        "given": "Stuart"
      },
      {
        "family": "Ball",
        "given": "Julian"
      }
    ],
    "container-title": "Philosophical Transactions of the Royal Society A",
    "id": "Grover2010",
    "issue": "1925",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, geo_ir, geocoding",
    "language": "en-US",
    "page": "3875-3889",
    "title": "Use of the Edinburgh geoparser for georeferencing digitized historical collections",
    "type": "article-journal",
    "volume": "368"
  },
  {
    "DOI": "10.1145/62266.62273",
    "abstract": "Many systems, applications, and features that support cooperative work share two characteristics: A significant investment has been made in their development, and their successes have consistently fallen far short of expectations. Examination of several application areas reveals a common dynamic: 1) A factor contributing to the application’s failure is the disparity between those who will benefit from an application and those who must do additional work to support it. 2) A factor contributing to the decision-making failure that leads to ill-fated development efforts is the unique lack of management intuition for CSCW applications. 3) A factor contributing to the failure to learn from experience is the extreme difficulty of evaluating these applications. These three problem areas escape adequate notice due to two natural but ultimately misleading analogies: the analogy between multi-user application programs and multi-user computer systems, and the analogy between multi-user applications and single-user applications. These analogies influence the way we think about cooperative work applications and designers and decision-makers fail to recognize their limits. Several CSCW application areas are examined in some detail.",
    "author": [
      {
        "family": "Grudin",
        "given": "Jonathan"
      }
    ],
    "container-title": "CSCW ’88: Proceedings of the 1988 ACM conference on computer-supported cooperative work",
    "id": "Grudin1988",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "cscw",
    "language": "en-US",
    "page": "85-93",
    "publisher": "ACM",
    "title": "Why CSCW applications fail: Problems in the design and evaluation of organizational interfaces",
    "title-short": "Why CSCW applications fail",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-03555-5_10",
    "ISBN": "978-3-642-03554-8",
    "abstract": "BaseX is an early adopter of the upcoming XQuery Full Text Recommendation. This paper presents some of the enhancements made to the XML database to fully support the language extensions. The system’s data and index structures are described, and implementation details are given on the XQuery compiler, which supports sequential scanning, index-based, and hybrid processing of full-text queries. Experimental analysis and an insight into visual result presentation of query results conclude the presentation.",
    "author": [
      {
        "family": "Grün",
        "given": "Christian"
      },
      {
        "family": "Gath",
        "given": "Sebastian"
      },
      {
        "family": "Holupirek",
        "given": "Alexander"
      },
      {
        "family": "Scholl",
        "given": "Marc H."
      }
    ],
    "container-title": "Proceedings of the 6th international XML database symposium on database and XML technologies",
    "id": "Gruen2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "database, xml, xquery",
    "language": "en-US",
    "page": "114-128",
    "publisher": "Springer",
    "publisher-place": "Berlin, Heidelberg",
    "title": "XQuery full text implementation in BaseX",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-04930-9_17",
    "abstract": "This paper explores the application of restricted relationship graphs (RDF) and statistical NLP techniques to improve named entity annotation in challenging Informal English domains. We validate our approach using on-line forums discussing popular music. Named entity annotation is particularly difficult in this domain because it is characterized by a large number of ambiguous entities, such as the Madonna album ” Music” or Lilly Allen’s pop hit ” Smile”. We evaluate improvements in annotation accuracy that can be obtained by restricting the set of possible entities using real-world constraints. We find that constrained domain entity extraction raises the annotation accuracy significantly, making an infeasible task practical. We then show that we can further improve annotation accuracy by over 50% by applying SVM based NLP systems trained on word-usages in this domain.",
    "author": [
      {
        "family": "Gruhl",
        "given": "Daniel"
      },
      {
        "family": "Nagarajan",
        "given": "Meena"
      },
      {
        "family": "Pieper",
        "given": "Jan"
      },
      {
        "family": "Robson",
        "given": "Christine"
      },
      {
        "family": "Sheth",
        "given": "Amit"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "The semantic web – ISWC 2009",
    "editor": [
      {
        "family": "Bernstein",
        "given": "Abraham"
      },
      {
        "family": "Karger",
        "given": "David R."
      },
      {
        "family": "Heath",
        "given": "Tom"
      },
      {
        "family": "Feigenbaum",
        "given": "Lee"
      },
      {
        "family": "Maynard",
        "given": "Diana"
      },
      {
        "family": "Motta",
        "given": "Enrico"
      },
      {
        "family": "Thirunarayan",
        "given": "Krishnaprasad"
      }
    ],
    "id": "Gruhl2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "nlp, proper_names, rdf, semantic_web",
    "language": "en-US",
    "page": "260-276",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Context and domain knowledge enhanced entity spotting in informal text",
    "type": "chapter",
    "volume": "5823"
  },
  {
    "DOI": "10.2307/3529080",
    "author": [
      {
        "family": "Grundlehner",
        "given": "Philip E."
      }
    ],
    "container-title": "Die Unterrichtspraxis / Teaching German",
    "id": "Grundlehner1974",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1974
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "language": "en-US",
    "page": "96-105",
    "publisher": "Blackwell Publishing on behalf of the American Association of Teachers of German",
    "title": "Computer-Based education: PLATO in german",
    "title-short": "Computer-Based education",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "URL": "http://www.ssrq-sds-fds.ch/fileadmin/user_upload/Literatur/ZSR-I_2007-05_03_Gschwend1.pdf",
    "abstract": "1894 traf der Schweizerische Juristenverein den Entscheid, eine Sammlung der bis 1798 im Gebiet der Schweiz entstandenen Rechtsquellen herauszugeben. Im Geiste der historischen Rechtsschule und gepaart mit nationalem Selbstbewusstsein wurde die Arbeit an diesem monumentalen Projekt von «vaterländischer» Bedeutung aufgenommen. Im Vordergrund stand die systematische und authentische Quellenwiedergabe, um die Rechtsquellen im Sinne eines Grundlagenwerkes von unbeschränkter Verwendungsdauer v.a. der rechts-, geschichts- und sprachwissenschaftlichen Forschung zugänglich zu machen. Im Verlauf des 20. Jahrhunderts entwickelte sich die Sammlung Schweizerischer Rechtsquellen zu einem modernen, interdisziplinären Editionsprojekt, welches über mehrere Jahrhunderte hinweg die Kulturgeschichte institutioneller Konflikterledigung widerspiegelt und zunehmend über die Grenzen der Schweiz hinaus bei Forscherinnen und Forschern verschiedenster Fachrichtungen auf grosses Interesse stösst.",
    "author": [
      {
        "family": "Gschwend",
        "given": "Lukas"
      }
    ],
    "container-title": "Zeitschrift für Schweizerisches Recht",
    "id": "Gschwend2007",
    "issue": "I",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "ssrq",
    "language": "de-DE",
    "page": "435-457",
    "title": "Die Sammlung Schweizerischer Rechtsquellen, herausgegeben von der Rechtsquellenstiftung des Schweizerischen Juristenvereins: Ein Monumentalwerk rechtshistorischer Grundlagenforschung",
    "type": "article-journal",
    "volume": "126"
  },
  {
    "URL": "http://www.sgg-ssh.ch/de/szg/detail.php?id=1317",
    "abstract": "In 1894 the Swiss Association of Jurists (Schweizerischer Juristenverein) decided to edit a compilation of local and regional law sources, generated on Swiss territory until 1798. The project was equally motivated by the tradition of Historism and the desire for national reinforcement as by the pending unification of civil and criminal law. Thereby, a systematic and genuine rendition was the main objective to customize those law sources – in the sense of a scientific basic work with unrestricted shelf life – particularly to jurisprudential, historic and linguistic research. In the course of the 20th century the Compilation of Swiss Law Sources (Sammlung Schweizerischer Rechtsquellen) emerged as a modern and interdisciplinary edition project. It reflects the cultural history of institutional conflict settlement and increasingly meets with interest of researchers from different fields even beyond Swiss national boundaries.",
    "author": [
      {
        "family": "Gschwend",
        "given": "Lukas"
      }
    ],
    "container-title": "Schweizerische Zeitschrift für Geschichte",
    "id": "Gschwend2008",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ssrq",
    "language": "de-DE",
    "page": "4-19",
    "title": "Rechtshistorische Grundlagenforschung: Die Sammlung Schweizerischer Rechtsquellen",
    "type": "article-journal",
    "volume": "58"
  },
  {
    "DOI": "10.1075/ijcl.1.2.07gue",
    "author": [
      {
        "family": "Guenthner",
        "given": "Franz"
      }
    ],
    "container-title": "International Journal of Corpus Linguistics",
    "id": "Guenthner1996",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "computational_linguistics, language_resources",
    "page": "287-301",
    "title": "Electronic lexica and corpora research at CIS",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.1016/j.ins.2016.10.032",
    "ISSN": "00200255",
    "abstract": "Since the beginning of the twenty-first century, the cultural industry has been through a massive and historical mutation induced by the rise of digital technologies. The comic books industry keeps looking for the right solution and has not yet produced anything as convincing as the music or movie have. A lot of energy has been spent to transfer printed material to digital supports so far. The specificities of those supports are not always exploited at the best of their capabilities, while they could potentially be used to create new reading conventions. In spite of the needs induced by the large amount of data created since the beginning of the comics history, content indexing has been left behind. It is indeed quite a challenge to index such a composition of textual and visual information. While a growing number of researchers are working on comic books’ image analysis from a low-level point of view, only a few are tackling the issue of representing the content at a high semantic level. We propose in this article a framework to handle the content of a comic book, to support the automatic extraction of its visual components and to formalize the semantic of the domain’s codes. We tested our framework over two applications: 1) the unsupervised content discovery of comic books’ images, 2) its capabilities to handle complex layouts and to produce a respectful browsing experience to the digital comics reader.",
    "author": [
      {
        "family": "Guérin",
        "given": "Clément"
      },
      {
        "family": "Rigaud",
        "given": "Christophe"
      },
      {
        "family": "Bertet",
        "given": "Karell"
      },
      {
        "family": "Revel",
        "given": "Arnaud"
      }
    ],
    "container-title": "Information Sciences",
    "id": "Guerin2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "annotation, ontologies",
    "language": "en-US",
    "page": "109-130",
    "title": "An ontology-based framework for the automated analysis and interpretation of comic books’ images",
    "type": "article-journal",
    "volume": "378"
  },
  {
    "ISSN": "2180-1266",
    "URL": "http://oa.upm.es/6816/",
    "abstract": "This study approaches a methodology for the integration of temporal information belonging to a historical corpus in a Geographic Information System (GIS), with the purpose of analyzing and visualizing the textual information. The selected corpus is composed of business letters of the Castilian merchant Simón Ruiz (1553-1597), in the context of the DynCoopNet Project (Dynamic Complexity of Cooperation-Based Self-Organizing Commercial Networks in the First Global Age), that aims to analyze the dynamic cooperation procedures of social networks. The integration of historical corpus into a GIS has involved the following phases: (1) recognition and normalization of temporal expressions and events in 16th century Castilian following the TimeML annotation guidelines and (2) storage of tagged expressions into a Geodatabase. The implementation of this process in a GIS would allow to later carrying out temporal queries, dynamic visualization of historical events and thus, it addresses the recognition of human activity patterns and behaviours over time.",
    "author": [
      {
        "family": "Guerrero Nieto",
        "given": "Marta"
      },
      {
        "family": "García Rodríguez",
        "given": "María J."
      },
      {
        "family": "Urrutia Zambrana",
        "given": "Adolfo"
      },
      {
        "family": "Siabato Vaca",
        "given": "Willington L."
      },
      {
        "family": "Bernabe Poveda",
        "given": "Miguel A."
      }
    ],
    "container-title": "International Journal of Computational Linguistics and Applications",
    "id": "Guerrero2010",
    "issue": "1–2",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, geo_ir",
    "language": "en-US",
    "page": "269-283",
    "title": "Incorporating TimeML into a GIS",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.5406/jsporthistory.44.2.0146",
    "ISSN": "00941700",
    "abstract": "Exploring praxis as a key construct, this article disputes the understandings of digital history as a relatively recent phenomena by providing an alternative narrative of digital history’s development. Understanding that ” digital” history is a constellation of practices drawn from humanities and computing disciplines, this paper argues that digital sport history must demonstrate critical, intentional engagement with interdisciplinary research to achieve its fullest future. Using Michael Oriard’s Reading Football: How the Popular Press Created an American Spectacle as a basis for a speculative design exercise, the paper suggests three alternative research methods that scholars could now use to explore Oriard’s sources. The exercise illustrates how digital sport historians must recognize the digital and its multiplicity of forms as historical objects that are produced, interpreted, and contested. As important, the article closes by presenting core values for our consideration as we move toward recognized methodologies for digital sport history.",
    "author": [
      {
        "family": "Guiliano",
        "given": "Jennifer"
      }
    ],
    "container-title": "Journal of Sport History",
    "id": "Guiliano2017",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities, history",
    "language": "en-US",
    "page": "146+",
    "title": "Toward a praxis of critical digital sport history",
    "type": "article-journal",
    "volume": "44"
  },
  {
    "author": [
      {
        "family": "Guillot",
        "given": "Céline"
      },
      {
        "family": "Lavrentiev",
        "given": "Alexei"
      },
      {
        "family": "Marchello-Nizia",
        "given": "Christiane"
      }
    ],
    "collection-title": "Zeitschrift für französische sprache und literatur – beihefte. Neue folge (ZFSL-b)",
    "container-title": "Le nouveau corpus d’amsterdam. Actes de l’atelier de lauterbad, 23–26 février 2006",
    "editor": [
      {
        "family": "Kunstmann",
        "given": "Pierre"
      },
      {
        "family": "Stein",
        "given": "Achim"
      }
    ],
    "id": "Guillot2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, french",
    "language": "en-US",
    "page": "143-152",
    "publisher": "Steiner",
    "publisher-place": "Stuttgart, Germany",
    "title": "La Base de Français Médiéval (BFM): états et perspectives",
    "type": "chapter",
    "volume": "34"
  },
  {
    "URL": "http://corpus.revues.org/index1495.html",
    "abstract": "Ce numéro spécial dédié aux corpus d’ancien et moyen français, à leur constitution et à leur exploitation, vient après bien d’autres numéros de revues présentantl’usage des corpus en linguistique depuis une dizaine d’années. Mais celui-ci est spécifiquement consacré aux corpus concernant les périodes les plus anciennes de la langue, c’est-à-dire aux formes et pratiques linguistiques qui sont les plus différentes de ce que connaît et pratique le locuteur moderne, et pour lesquelles la base textuelle de référence FRANTEXT ne fournit rien puisque les textes qu’elle comporte ne remontent pas au-delà de 1520.",
    "accessed": {
      "date-parts": [
        [
          2011,
          11,
          8
        ]
      ]
    },
    "author": [
      {
        "family": "Guillot",
        "given": "Céline"
      },
      {
        "family": "Heiden",
        "given": "Serge"
      },
      {
        "family": "Lavrentiev",
        "given": "Alexei"
      },
      {
        "family": "Marchello-Nizia",
        "given": "Christiane"
      }
    ],
    "container-title": "Corpus",
    "id": "Guillot2008",
    "issued": {
      "date-parts": [
        [
          2008,
          11
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, french",
    "language": "en-US",
    "title": "Constitution et exploitation des corpus d’ancien et de moyen français",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "DOI": "10.7202/1005464ar",
    "ISSN": "1705-8546",
    "abstract": "As much as the concept of intertextuality has known a great success within social sciences because it represented the closure of a conception of the world understood as readable, in the same way, intermediality will have a true reach only if it permits us to liberate ourselves from this model and open a new array of problematics. Through this rapid reconstruction of conceptual paradigms, what is at stake is the possibility of rethinking medias outside of the procedures of hermeneutics and the sole universe of significance.",
    "author": [
      {
        "family": "Gumbrecht",
        "given": "Hans U."
      }
    ],
    "container-title": "Intermédialités: Histoire et théorie des arts, des lettres et des techniques",
    "id": "Gumbrecht2003",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "intertextuality",
    "language": "en-US",
    "page": "173-178",
    "title": "Why intermediality — if at all?",
    "type": "article-journal"
  },
  {
    "URL": "http://www.faz.net/aktuell/feuilleton/geisteswissenschaften/neue-serie-das-digitale-denken-das-denken-muss-nun-auch-den-daten-folgen-12840532.html",
    "abstract": "Die Geisteswissenschaften reagieren ratlos auf die digitale Revolution. Doch die verändert die Welt und die Art, wie wir uns selbst erleben, dramatisch. Es ist überlebenswichtig, diesen Wandel mit neuen Begriffen fassen und beeinflussen zu können.",
    "author": [
      {
        "family": "Gumbrecht",
        "given": "Hans U."
      }
    ],
    "container-title": "Frankfurter Allgemeine Zeitung",
    "id": "Gumbrecht2014",
    "issued": {
      "date-parts": [
        [
          2014,
          3
        ]
      ]
    },
    "keyword": "digital_humanities, in_the_media",
    "language": "de-DE",
    "title": "Das Denken muss nun auch den Daten folgen",
    "type": "article-journal"
  },
  {
    "abstract": "Multi-category classification of short dialogues is a common task performed by humans. When assigning a question to an expert, a customer service operator tries to classify the customer query into one of N different classes for which experts are available. Similarly, questions on the web (for example questions at Yahoo Answers) can be automatically forwarded to a restricted group of people with a specific expertise. Typical questions are short and assume background world knowledge for correct classification. With exponentially increasing amount of knowledge available, with distinct properties (labeled vs unlabeled, structured vs unstructured), no single knowledge-transfer algorithm such as transfer learning, multi-task learning or self-taught learning can be applied universally. In this work we show that bag-of-words classifiers performs poorly on noisy short conversational text snippets. We present an algorithm for leveraging heterogeneous data sources and algorithms with significant improvements over any single algorithm, rivaling human performance. Using different algorithms for each knowledge source we use mutual information to aggressively prune features. With heterogeneous data sources including Wikipedia, Open Directory Project (ODP), and Yahoo Answers, we show 89.4% and 96.8% correct classification on Google Answers corpus and Switchboard corpus using only 200 features/class. This reflects a huge improvement over bag of words approaches and 48-65% error reduction over previously published state of art (Gabrilovich et. al. 2006).",
    "author": [
      {
        "family": "Gupta",
        "given": "Rakesh"
      },
      {
        "family": "Ratinov",
        "given": "Lev-Arie"
      }
    ],
    "container-title": "Proceedings of AAAI’08",
    "id": "Gupta2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ir, wikipedia",
    "language": "en-US",
    "page": "842-847",
    "title": "Text categorization with knowledge transfer from heterogeneous data sources",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/P07-1130",
    "abstract": "We present a study aimed at investigating the use of semantic information in a novel NLP application, Electronic Career Guidance (ECG), in German. ECG is formulated as an information retrieval (IR) task, whereby textual descriptions of professions (documents) are ranked for their relevanceto natural language descriptions of a person’s professional interests (the topic). We compare the performance of two semantic IR models: (IR-1) utilizing semantic relatedness (SR) measures based on either wordnet or Wikipedia and a set of heuristics, and (IR-2) measuring the similarity between the topic and documents based on Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). We evaluate the performance of SR measures intrinsically on the tasks of (T-1) computing SR, and (T-2) solving Reader’s Digest Word Power (RDWP) questions.",
    "author": [
      {
        "family": "Gurevych",
        "given": "Iryna"
      },
      {
        "family": "Müller",
        "given": "Christof"
      },
      {
        "family": "Zesch",
        "given": "Torsten"
      }
    ],
    "container-title": "Proceedings of the 45th annual meeting of the association of computational linguistics (ACL’07)",
    "id": "Gurevych2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "ir, wikipedia",
    "page": "1032-1039",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "What to be? – electronic career guidance based on semantic relatedness",
    "type": "paper-conference"
  },
  {
    "URL": "https://aclanthology.org/E12-1059",
    "abstract": "We present UBY, a large-scale lexical-semantic resource combining a wide range of information from expert-constructed and collaboratively constructed resources for English and German. It currently contains nine resources in two languages: English WordNet, Wiktionary, Wikipedia, FrameNet and VerbNet, German Wikipedia, Wiktionary and GermaNet, and multilingual OmegaWiki modeled according to the LMF standard. For FrameNet, VerbNet and all collaboratively constructed resources, this is done for the first time. Our LMF model captures lexical information at a fine-grained level by employing a large number of Data Categories from ISOCat and is designed to be directly extensible by new languages and resources. All resources in UBY can be accessed with an easy to use publicly available API.",
    "author": [
      {
        "family": "Gurevych",
        "given": "Iryna"
      },
      {
        "family": "Eckle-Kohler",
        "given": "Judith"
      },
      {
        "family": "Hartmann",
        "given": "Silvana"
      },
      {
        "family": "Matuschek",
        "given": "Michael"
      },
      {
        "family": "Meyer",
        "given": "Christian M."
      },
      {
        "family": "Wirth",
        "given": "Christian"
      }
    ],
    "container-title": "Proceedings of the 13<sup>th</sup> conference of the european chapter of the association for computational linguistics (EACL 2012)",
    "id": "Gurevych2012",
    "issued": {
      "date-parts": [
        [
          2012,
          4
        ]
      ]
    },
    "language": "en-US",
    "page": "580-590",
    "title": "UBY – a large-scale unified lexical-semantic resource based on LMF",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2006/pdf/357_pdf.pdf",
    "abstract": "Data sparsity is a large problem in natural language processing that refers to the fact that language is a system of rare events, so varied and complex, that even using an extremely large corpus, we can never accurately model all possible strings of words. This paper examines the use of skip-grams (a technique where by n-grams are still stored to model language, but they allow for tokens to be skipped) to overcome the data sparsity problem. We analyze this by computing all possible skip-grams in a training corpus and measure how many adjacent (standard) n-grams these cover in test documents. We examine skip-gram modelling using one to four skips with various amount of training data and test against similar documents as well as documents generated from a machine translation system. In this paper we also determine the amount of extra training data required to achieve skip-gram coverage using standard adjacent tri-grams.",
    "author": [
      {
        "family": "Guthrie",
        "given": "David"
      },
      {
        "family": "Allison",
        "given": "Ben"
      },
      {
        "family": "Liu",
        "given": "Wei"
      },
      {
        "family": "Guthrie",
        "given": "Louise"
      },
      {
        "family": "Wilks",
        "given": "Yorick"
      }
    ],
    "container-title": "Proceedings of the 5<sup>th</sup> international conference on language resources and evaluation (LREC 2006)",
    "id": "Guthrie2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "approximate_matching, nlp",
    "language": "en-US",
    "page": "1222-1225",
    "publisher": "European Language Resources Association (ELRA)",
    "publisher-place": "Paris",
    "title": "A closer look at skip-gram modelling",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/tkde.2007.34",
    "ISSN": "1041-4347",
    "abstract": "The resource description framework (RDF) is a metadata model and language recommended by the W3C. This paper presents a framework to incorporate temporal reasoning into RDF, yielding temporal RDF graphs. We present a semantics for these kinds of graphs which includes the notion of temporal entailment and a syntax to incorporate this framework into standard RDF graphs, using the RDF vocabulary plus temporal labels. We give a characterization of temporal entailment in terms of RDF entailment and show that the former does not yield extra asymptotic complexity with respect to nontemporal RDF graphs. We also discuss temporal RDF graphs with anonymous timestamps, providing a theoretical framework for the study of temporal anonymity. Finally, we sketch a temporal query language for RDF, along with complexity results for query evaluation that show that the time dimension preserves the tractability of answers",
    "author": [
      {
        "family": "Gutierrez",
        "given": "Claudio"
      },
      {
        "family": "Hurtado",
        "given": "Carlos A."
      },
      {
        "family": "Vaisman",
        "given": "Alejandro"
      }
    ],
    "container-title": "IEEE Transactions on Knowledge and Data Engineering",
    "id": "Gutierrez2007",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "rdf, spatio-temporal_annotation, temporal_data",
    "language": "en-US",
    "page": "207-218",
    "publisher": "IEEE",
    "publisher-place": "New York, NY, USA",
    "title": "Introducing time into RDF",
    "type": "article-journal",
    "volume": "19"
  },
  {
    "DOI": "10.1145/3363181",
    "author": [
      {
        "family": "Guzdial",
        "given": "Mark"
      },
      {
        "family": "Kay",
        "given": "Alan"
      },
      {
        "family": "Norris",
        "given": "Cathie"
      },
      {
        "family": "Soloway",
        "given": "Elliot"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Guzdial2019",
    "issue": "11",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "keyword": "computational_thinking",
    "language": "en-US",
    "page": "28-30",
    "title": "Computational thinking should just be good thinking",
    "type": "article-journal",
    "volume": "62"
  },
  {
    "URL": "http://www.hrgdigital.de/",
    "abstract": "Das Handwörterbuch zur deutschen Rechtsgeschichte (HRG) zählt zu den Standardwerken fast aller historisch arbeitenden Institute, Bibliotheken und Archive. Das Werk präsentiert weitgehend die Summe des Wissens über die Geschichte des Rechts.",
    "edition": "2<sup>nd</sup>",
    "editor": [
      {
        "family": "Cordes",
        "given": "Albrecht"
      },
      {
        "family": "Lück",
        "given": "Heiner"
      },
      {
        "family": "Werkmüller",
        "given": "Dieter"
      },
      {
        "family": "Schmidt-Wiegand",
        "given": "Ruth"
      }
    ],
    "id": "HRG",
    "issued": {
      "date-parts": []
    },
    "keyword": "legal, ssrq",
    "language": "en-US",
    "publisher": "Erich Schmidt",
    "publisher-place": "Berlin, Germany",
    "title": "Handwörterbuch zur deutschen rechtsgeschichte",
    "type": "book"
  },
  {
    "ISBN": "9780631196792",
    "abstract": "In this important new work, Haack develops an original theory of empirical evidence or justification, and argues its appropriateness to the goals of inquiry. In so doing, Haack provides detailed critical case studies of Lewis’s foundationalism; Davidson’s and Bonjour’s coherentism; Popper’s ’epistemology without a knowing subject’; Quine’s naturalism; Goldman’s reliabilism; and Rorty’s, Stich’s, and the Churchlands’ recent obituaries of epistemology.",
    "author": [
      {
        "family": "Haack",
        "given": "Susan"
      }
    ],
    "id": "Haack2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "argumentation, uncertainty",
    "language": "en-US",
    "publisher": "Blackwell",
    "publisher-place": "London",
    "title": "Evidence and inquiry towards reconstruction in epistemology",
    "type": "book"
  },
  {
    "DOI": "10.1087/20120404",
    "ISSN": "0953-1513",
    "abstract": "The Open Researcher & Contributor ID (ORCID) registry presents a unique opportunity to solve the problem of author name ambiguity. At its core the value of the ORCID registry is that it crosses disciplines, organizations, and countries, linking ORCID with both existing identifier schemes as well as publications and other research activities. By supporting linkages across multiple datasets – clinical trials, publications, patents, datasets – such a registry becomes a switchboard for researchers and publishers alike in managing the dissemination of research findings. We describe use cases for embedding ORCID identifiers in manuscript submission workflows, prior work searches, manuscript citations, and repository deposition. We make recommendations for storing and displaying ORCID identifiers in publication metadata to include ORCID identifiers, with CrossRef integration as a specific example. Finally, we provide an overview of ORCID membership and integration tools and resources.",
    "author": [
      {
        "family": "Haak",
        "given": "Laurel L."
      },
      {
        "family": "Fenner",
        "given": "Martin"
      },
      {
        "family": "Paglione",
        "given": "Laura"
      },
      {
        "family": "Pentz",
        "given": "Ed"
      },
      {
        "family": "Ratner",
        "given": "Howard"
      }
    ],
    "container-title": "Learned Publishing",
    "id": "Haak2012",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "bibliography",
    "language": "en-US",
    "page": "259-264",
    "publisher": "Association of Learned and Professional Society Publishers",
    "title": "ORCID: A system to uniquely identify researchers",
    "title-short": "ORCID",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "collection-title": "LNI",
    "editor": [
      {
        "family": "Haake",
        "given": "Jörg M."
      },
      {
        "family": "Lucke",
        "given": "Ulrike"
      },
      {
        "family": "Tavangarian",
        "given": "Djamshid"
      }
    ],
    "id": "Haake2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "publisher": "GI",
    "title": "DeLFI 2005: 3. Deutsche e-learning fachtagung informatik, der gesellschaft für informatik e.v. (GI) 13.-16. September 2005 in rostock",
    "title-short": "DeLFI 2005",
    "type": "book",
    "volume": "66"
  },
  {
    "URL": "https://files.ifi.uzh.ch/cl/volk/LexMorphVorl/Lexikon04.Gertwol.html",
    "accessed": {
      "date-parts": [
        [
          2010,
          10,
          7
        ]
      ]
    },
    "author": [
      {
        "family": "Haapalainen",
        "given": "Mariikka"
      },
      {
        "family": "Majorin",
        "given": "Ari"
      }
    ],
    "id": "Haapalainen1994",
    "issued": {
      "date-parts": [
        [
          1994,
          9
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Lingsoft Oy",
    "publisher-place": "Helsinki",
    "title": "GERTWOL: Ein System zur automatischen Wortformerkennung deutscher Wörter",
    "type": "report"
  },
  {
    "DOI": "10.16995/lefou.22",
    "ISSN": "2515-2076",
    "abstract": "In this interview with Ruben Hackler and Guido Kirsten, Franco Moretti elaborates on his biography and intellectual development, on ’distant reading’ and his work at the Stanford Literary Lab, on the complicated relationships between literature, market, and ideology, and above all on the question what literary history might contribute to a social critique of the present. With regard to his experiences in the growing field of digital humanities, Moretti makes an argument for ’computational criticism’ as a combination of data-driven research methods and in-depth analyses of literary texts.",
    "author": [
      {
        "family": "Hackler",
        "given": "Ruben"
      },
      {
        "family": "Kirsten",
        "given": "Guido"
      }
    ],
    "container-title": "Le foucaldien",
    "id": "Hackler2016",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "7+",
    "title": "Distant reading, computational criticism, and social critique: An interview with Franco Moretti",
    "title-short": "Distant reading, computational criticism, and social critique",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.1145/1536274.1536289",
    "ISBN": "978-1-60558-415-7",
    "abstract": "The objective of the current study is to research, design, implement and test a tool that provides a repository of learning objects and records a profile of the users, in order to deliver a personalized and customized version of the learning object to the user taking into account their optimal learning profile and inclusive design in mind. The intention is to package and deliver content taking into account the diversity in learning subjects and materials encountered by teachers and learners in a classroom setting. To this end, intensional logic and programming is introduced to accommodate user diversity in a web-based teaching and learning environment. The Markup Macro Processor Language (MMP) [1] is used as an interpreter language that makes adaptations on the server as it transforms web pages \"on the fly\" without requiring web content to be rewritten for each specific user.",
    "author": [
      {
        "family": "Hadian",
        "given": "Shohreh"
      },
      {
        "family": "Wadge",
        "given": "Bill"
      }
    ],
    "container-title": "Proceedings of the 14th western canadian conference on computing education (WCCCE ’09)",
    "id": "Hadian2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "e-learning, troff",
    "language": "en-US",
    "page": "45-52",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Diversity in an environment for accessible learning",
    "type": "paper-conference"
  },
  {
    "DOI": "10.4242/balisagevol19.dekker01",
    "author": [
      {
        "family": "Haentjens Dekker",
        "given": "Ronald"
      },
      {
        "family": "Birnbaum",
        "given": "David J."
      }
    ],
    "collection-title": "Balisage series on markup technologies",
    "container-title": "Proceedings of balisage: The markup conference 2017",
    "id": "Haentjens_Dekker2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "title": "It’s more than just overlap: Text as graph",
    "title-short": "It’s more than just overlap",
    "type": "paper-conference",
    "volume": "17"
  },
  {
    "URL": "http://www.nzz.ch/feuilleton/1.18355018",
    "abstract": "Auch für die Sozial- und Geisteswissenschaften, so glauben manche, liege die Zukunft in der Auswertung grosser Datenmengen. Ein Beispiel aus der Kulturgeschichte ist in «Science» publiziert worden.",
    "author": [
      {
        "family": "Hafner",
        "given": "Urs"
      }
    ],
    "container-title": "Neue Zürcher Zeitung",
    "id": "Hafner2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities, in_the_media",
    "language": "de-DE",
    "title": " und die Kulturgeschichte: Zahlenspiele für Fortgeschrittene",
    "title-short": " und die Kulturgeschichte",
    "type": "article-journal"
  },
  {
    "URL": "http://www.nzz.ch/feuilleton/1.18582482",
    "abstract": "Wenn die Geisteswissenschaften nicht wie die Naturwissenschaften konsequent auf digitale Daten setzten, hätten sie keine Zukunft, findet die modisch gewordene Bewegung der «Digital Humanities».",
    "author": [
      {
        "family": "Hafner",
        "given": "Urs"
      }
    ],
    "container-title": "Neue Zürcher Zeitung",
    "id": "Hafner2015a",
    "issue": "165",
    "issued": {
      "date-parts": [
        [
          2015,
          7,
          20
        ]
      ]
    },
    "keyword": "digital_humanities, in_the_media",
    "language": "de-DE",
    "title": "Geist unter Strom",
    "type": "article-journal",
    "volume": "236"
  },
  {
    "URL": "http://www.nzz.ch/feuilleton/1.18632316",
    "abstract": "«Big Data» ist das Zauber- und Reizwort der historischen Stunde. Eine Tagung von Infoclio.ch, der digitalen Plattform der Geschichtswissenschaften in der Schweiz, setzte kritische Akzente.",
    "author": [
      {
        "family": "Hafner",
        "given": "Urs"
      }
    ],
    "container-title": "Neue Zürcher Zeitung",
    "id": "Hafner2015b",
    "issued": {
      "date-parts": [
        [
          2015,
          10,
          20
        ]
      ]
    },
    "keyword": "digital_humanities, in_the_media",
    "language": "de-DE",
    "title": " und die Geschichtswissenschaft: Gemachte Tatsachen",
    "title-short": " und die Geschichtswissenschaft",
    "type": "article-journal"
  },
  {
    "DOI": "10.1007/978-3-540-78135-6_20",
    "ISBN": "978-3-540-78134-9",
    "abstract": "We present in this paper the work that has been developed at Xerox Research Centre Europe to build a robust temporal text processor. The aim of this processor is to extract events described in texts and to link them, when possible, to a temporal anchor. Another goal is to be able to establish temporal ordering between the events expressed in texts. One of the originalities of this work is that the temporal processor is coupled with a syntactic-semantic analyzer. The temporal module takes then advantage of syntactic and semantic information extracted from text and at the same time, syntactic and semantic processing benefits from the temporal processing performed. As a result, analysis and management of temporal information is combined with other kinds of syntactic and semantic information, making possible a more refined text understanding processor that takes into account the temporal dimension.",
    "author": [
      {
        "family": "Hagège",
        "given": "Caroline"
      },
      {
        "family": "Tannier",
        "given": "Xavier"
      }
    ],
    "chapter-number": "20",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Computational linguistics and intelligent text processing. 9<sup>th</sup> international conference, CICLing 2008",
    "editor": [
      {
        "family": "Gelbukh",
        "given": "Alexander"
      }
    ],
    "id": "Hagege2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "spatio-temporal_annotation",
    "language": "en-US",
    "page": "231-240",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "XTM: A robust temporal text processor computational linguistics and intelligent text processing",
    "title-short": "XTM",
    "type": "chapter",
    "volume": "4919"
  },
  {
    "DOI": "10.1002/1099-0542(2000)8:2<127::AID-CAE7>3.0.CO;2-H",
    "abstract": "By the mid-1970s, the U.S. National Science Foundation was funding two large projects, PLATO and TICCIT, designed to demonstrate the efficacy of teaching with the help of computers. Although neither project proved successful, they nevertheless propelled teaching with computers to a level that laid the foundation for most later efforts. © 2000 John Wiley & Sons, Inc. Comput Appl Eng Educ 8: 127-131, 2000",
    "author": [
      {
        "family": "Hagler",
        "given": "Marion O."
      },
      {
        "family": "Marcy",
        "given": "William M."
      }
    ],
    "container-title": "Computer Applications in Engineering Education",
    "id": "Hagler2000",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "e-learning, plato_system, ticcit",
    "language": "en-US",
    "page": "127-131",
    "publisher-place": "College of Engineering, Texas Tech University, Lubbock, Texas 79409",
    "title": "The legacy of PLATO and TICCIT for learning with computers",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "DOI": "10.1109/MAHC.2006.70",
    "abstract": "Historians have not yet explored word processing’s development, and so to provide a rounded treatment, we examine the story from multiple perspectives. We review the conceptual development of word processing and office automation; the development of word processing’s constituent hardware and software technologies; the relationship of word processing to changes in the organization of office work; and the business history of the word processing industry. Word processing entered the American office in 1970 as an idea about reorganizing typists, but its meaning soon shifted to describe computerized text editing. The designers of word processing systems combined existing technologies to exploit the falling costs of interactive computing, creating a new business quite separate from the emerging world of the personal computer",
    "author": [
      {
        "family": "Haigh",
        "given": "Thomas"
      }
    ],
    "container-title": "IEEE Annals of the History of Computing",
    "id": "Haigh2006",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "6-31",
    "title": "Remembering the office of the future: The origins of word processing and office automation",
    "title-short": "Remembering the office of the future",
    "type": "article-journal",
    "volume": "28"
  },
  {
    "URL": "http://ufal.mff.cuni.cz/pdt2.0/publications/HajicHladkaPajas2001.pdf",
    "abstract": "The contents of the Prague Dependency Treebank (recently released by the Linguistic Data Consortium in its version 1.0) is described, from morphology to surface syntax to the deep (underlying) syntax layers of annotation. For each layer, the basic assumptions are given, followed by a more detailed description of the annotation scheme. Annotation software currently in use is characterized and its distinguishing features are emphasized. Finally, the checking schema and procedures adopted for the release of the Prague Dependency Treebank version 1.0 are discussed.",
    "author": [
      {
        "family": "Hajič",
        "given": "Jan"
      },
      {
        "family": "Vidová-Hladká",
        "given": "Barbora"
      },
      {
        "family": "Pajas",
        "given": "Petr"
      }
    ],
    "container-title": "Proceedings of the IRCS workshop on linguistic databases",
    "id": "HajicHladkaPajas2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "corpus_linguistics, czech",
    "language": "en-US",
    "page": "105-114",
    "publisher": "University of Pennsylvania",
    "publisher-place": "Philadelphia, PA, USA",
    "title": "The Prague Dependency Treebank: Annotation structure and support",
    "title-short": "The Prague Dependency Treebank",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1835449.1835520",
    "ISBN": "978-1-4503-0153-4",
    "abstract": "In this paper, we present a novel near-duplicate document detection method that can easily be tuned for a particular domain. Our method represents each document as a real-valued sparse k-gram vector, where the weights are learned to optimize for a specified similarity function, such as the cosine similarity or the Jaccard coefficient. Near-duplicate documents can be reliably detected through this improved similarity measure. In addition, these vectors can be mapped to a small number of hash-values as document signatures through the locality sensitive hashing scheme for efficient similarity computation. We demonstrate our approach in two target domains: Web news articles and email messages. Our method is not only more accurate than the commonly used methods such as Shingles and I-Match, but also shows consistent improvement across the domains, which is a desired property lacked by existing methods.",
    "author": [
      {
        "family": "Hajishirzi",
        "given": "Hannaneh"
      },
      {
        "family": "Yih",
        "given": "Wen T."
      },
      {
        "family": "Kołcz",
        "given": "Aleksander"
      }
    ],
    "container-title": "Proceedings of the 33rd international ACM SIGIR conference on research and development in information retrieval (SIGIR ’10)",
    "id": "Hajishirzi2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "approximate_matching",
    "language": "en-US",
    "page": "419-426",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Adaptive near-duplicate detection via similarity learning",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/P07-2053.pdf",
    "abstract": "In the world of non-proprietary NLP software the standard, and perhaps the best, HMM-based POS tagger is TnT (Brants, 2000). We argue here that some of the criticism aimed at HMM performance on languages with rich morphology should more properly be directed at TnT’s peculiar license, free but not open source, since it is those details of the implementation which are hidden from the user that hold the key for improved POS tagging across a wider variety of languages. We present HunPos, a free and open source (LGPL-licensed) alternative, which can be tuned by the user to fully utilize the potential of HMM architectures, offering performance comparable to more complex models, but preserving the ease and speed of the training and tagging process.",
    "author": [
      {
        "family": "Halácsy",
        "given": "Péter"
      },
      {
        "family": "Kornai",
        "given": "András"
      },
      {
        "family": "Oravecz",
        "given": "Csaba"
      }
    ],
    "container-title": "Proceedings of the ACL 2007 demo and poster sessions",
    "id": "Halacsy2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "pos_tagging",
    "language": "en-US",
    "page": "209-212",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "HunPos: An open source trigram tagger",
    "title-short": "HunPos",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1023/a:1012801612483",
    "ISSN": "09259902",
    "abstract": "Cluster analysis aims at identifying groups of similar objects and, therefore helps to discover distribution of patterns and interesting correlations in large data sets. It has been subject of wide research since it arises in many application domains in engineering, business and social sciences. Especially, in the last years the availability of huge transactional and experimental data sets and the arising requirements for data mining created needs for clustering algorithms that scale and can be applied in diverse domains.",
    "author": [
      {
        "family": "Halkidi",
        "given": "Maria"
      },
      {
        "family": "Batistakis",
        "given": "Yannis"
      },
      {
        "family": "Vazirgiannis",
        "given": "Michalis"
      }
    ],
    "container-title": "Journal of Intelligent Information Systems",
    "id": "Halkidi2001",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "clustering",
    "language": "en-US",
    "page": "107-145",
    "publisher": "Springer Netherlands",
    "title": "On clustering validation techniques",
    "type": "article-journal",
    "volume": "17"
  },
  {
    "ISBN": "9780262533805",
    "abstract": "Formal ways of representing uncertainty and various logics for reasoning about it; updated with new material on weighted probability measures, complexity-theoretic considerations, and other topics. In order to deal with uncertainty intelligently, we need to be able to represent it and reason about it. In this book, Joseph Halpern examines formal ways of representing uncertainty and considers various logics for reasoning about it. While the ideas presented are formalized in terms of definitions and theorems, the emphasis is on the philosophy of representing and reasoning about uncertainty. Halpern surveys possible formal systems for representing uncertainty, including probability measures, possibility measures, and plausibility measures; considers the updating of beliefs based on changing information and the relation to Bayes’ theorem; and discusses qualitative, quantitative, and plausibilistic Bayesian networks. This second edition has been updated to reflect Halpern’s recent research. New material includes a consideration of weighted probability measures and how they can be used in decision making; analyses of the Doomsday argument and the Sleeping Beauty problem; modeling games with imperfect recall using the runs-and-systems approach; a discussion of complexity-theoretic considerations; the application of first-order conditional logic to security. Reasoning about Uncertainty is accessible and relevant to researchers and students in many fields, including computer science, artificial intelligence, economics (particularly game theory), mathematics, philosophy, and statistics.",
    "author": [
      {
        "family": "Halpern",
        "given": "Joseph Y."
      }
    ],
    "edition": "2",
    "id": "Halpern2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Reasoning about uncertainty",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Hamlet",
        "given": "Richard"
      }
    ],
    "container-title": "Text processing and document manipulation: Proceedings of the international conference",
    "editor": [
      {
        "dropping-particle": "van",
        "family": "Vliet",
        "given": "J. C."
      }
    ],
    "id": "Hamlet1986",
    "issued": {
      "date-parts": [
        [
          1986,
          4
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "78-89",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge, UK",
    "title": "A disciplined text environment",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/P11-1038",
    "abstract": "Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.",
    "author": [
      {
        "family": "Han",
        "given": "Bo"
      },
      {
        "family": "Baldwin",
        "given": "Timothy"
      }
    ],
    "container-title": "Proceedings of the 49<sup>th</sup> annual meeting of the association for computational linguistics: Human language technologies",
    "id": "Han2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "microtext, spelling_normalization",
    "language": "en-US",
    "page": "368-378",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Lexical normalisation of short text messages: Makn Sens a #twitter",
    "title-short": "Lexical normalisation of short text messages",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/W11-1502",
    "abstract": "The paper describes a tagger for Old Czech (1200-1500 AD), a fusional language with rich morphology. The practical restrictions (no native speakers, limited corpora and lexicons, limited funding) make Old Czech an ideal candidate for a resource-light cross-lingual method that we have been developing (e.g. Hana et al., 2004; Feldman and Hana, 2010).We use a traditional supervised tagger. However, instead of spending years of effort to create a large annotated corpus of Old Czech, we approximate it by a corpus of Modern Czech. We perform a series of simple transformations to make a modern text look more like a text in Old Czech and vice versa. We also use a resource-light morphological analyzer to provide candidate tags. The results are worse than the results of traditional taggers, but the amount of language-specific work needed is minimal.",
    "author": [
      {
        "family": "Hana",
        "given": "Jirka"
      },
      {
        "family": "Feldman",
        "given": "Anna"
      },
      {
        "family": "Aharodnik",
        "given": "Katsiaryna"
      }
    ],
    "container-title": "Proceedings of the 5th ACL-HLT workshop on language technology for cultural heritage, social sciences, and humanities",
    "id": "Hana2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, czech, pos_tagging",
    "language": "en-US",
    "page": "10-18",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Portland, OR, USA",
    "title": "A low-budget tagger for Old Czech",
    "type": "paper-conference"
  },
  {
    "URL": "http://library.oclc.org/cdm/ref/collection/p267701coll27/id/379",
    "abstract": "The accuracy of optical character recognition (OCR) can be improved by processing a document page with three OCR systems and merging the results. The algorithm for merging three text pages is based on a dynamic programming tree alignment algorithm. To make the algorithm computationally feasible for large strings such as pages with 3000 characters, the alignment algorithm is executed in two stages. First, the alignment algorithm is run with each page considered as strings of lines. When the algorithm choose among three non-null lines, the alignment algorithm is invoked on the lines.",
    "author": [
      {
        "family": "Handley",
        "given": "John C."
      },
      {
        "family": "Hickey",
        "given": "Thomas B."
      }
    ],
    "container-title": "Proceedings of RIAO’91",
    "id": "Handley1991",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "keyword": "ocr",
    "language": "en-US",
    "page": "160-175",
    "title": "Merging optical character recognition outputs for improved accuracy",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/ICSMC.1998.727527",
    "ISBN": "0-7803-4778-1",
    "abstract": "Optical character recognition is perhaps the most studied application of pattern recognition. Recent work has increased accuracy in two ways. Combination of individual classifier outputs overcomes deficiencies of features and trainability of single classifiers. OCR systems take page images as input and output strings of recognized characters. Due to character segmentation errors, characters can be split or merged preventing output combination character-by-character. Merging of output strings is done using string alignment algorithms.",
    "author": [
      {
        "family": "Handley",
        "given": "John C."
      }
    ],
    "container-title": "SMC’98 conference proceedings. 1998 IEEE international conference on systems, man, and cybernetics",
    "id": "Handley1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "ocr",
    "language": "en-US",
    "page": "4330-4333",
    "publisher": "IEEE",
    "publisher-place": "New York, NY, USA",
    "title": "Improving OCR accuracy through combination: A survey",
    "title-short": "Improving OCR accuracy through combination",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Hanrieder",
        "given": "Gerhard"
      }
    ],
    "container-title": "Linguistische verifikation. Dokumentation zur ersten morpholympics 1994",
    "editor": [
      {
        "family": "Hausser",
        "given": "Roland"
      }
    ],
    "id": "Hanrieder1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "morphology",
    "language": "en-US",
    "page": "53-66",
    "publisher": "Niemeyer",
    "publisher-place": "Tübingen",
    "title": "MORPH – Ein modulares und robustes Morphologieprogramm für das Deutsche in Common Lisp",
    "type": "chapter"
  },
  {
    "URL": "http://latte.brandeis.edu/project/LTS-statement-on-CMS-at-Brandeis.pdf",
    "accessed": {
      "date-parts": [
        [
          2007,
          4,
          19
        ]
      ]
    },
    "author": [
      {
        "family": "Hanson",
        "given": "Perry"
      },
      {
        "family": "Tomecka",
        "given": "Anna"
      },
      {
        "family": "Wawrzaszek",
        "given": "Susan"
      }
    ],
    "id": "Hanson2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "publisher": "Brandeis University, Library & Technology Services",
    "publisher-place": "Waltham, MA, USA",
    "title": "LTS statement on course management systems at Brandeis University",
    "type": "report"
  },
  {
    "ISBN": "0596102429",
    "abstract": "This reference is a fascinating and complete guide to using fonts and typography on the Web and across a variety of operating systems and application software. Fonts & Encodings shows you how to take full advantage of the incredible number of typographic options available, with advanced material that covers everything from designing glyphs to developing software that creates and processes fonts. The era of ASCII characters on green screens is long gone, and industry leaders such as Apple, HP, IBM, Microsoft, and Oracle have adopted the Unicode Worldwide Character Standard. Yet, many software applications and web sites still use a host of standards, including PostScript, TrueType, TeX/Omega, SVG, Fontlab, FontForge, Metafont, Panose, and OpenType. This book explores each option in depth, and provides background behind the processes that comprise today’s \"digital space for writing\": * Part I introduces Unicode, with a brief history of codes and encodings including ASCII. Learn about the morass of the data that accompanies each Unicode character, and how Unicode deals with normalization, the bidirectional algorithm, and the handling of East Asian characters. * Part II discusses font management, including installation, tools for activation/deactivation, and font choices for three different systems: Windows, the Mac OS, and the X Window System (Unix). * Part III deals with the technical use of fonts in two specific cases: the TeX typesetting system (and its successor, W, which the author co-developed) and web pages. * Part IV describes methods for classifying fonts: Vox, Alessandrini, and Panose-1, which is used by Windows and the CSS standard. Learn about existing tools for creating (or modifying) fonts, including FontLab and FontForge, and become familiar with OpenType properties and AAT fonts. Nowhere else will you find the valuable technical information on fonts and typography that software developers, web developers, and graphic artists need to know to get typography and fonts to work properly.",
    "author": [
      {
        "family": "Haralambous",
        "given": "Yannis"
      }
    ],
    "id": "Haralambous2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "typography, unicode",
    "language": "en-US",
    "publisher": "O’Reilly",
    "publisher-place": "Sebastopol, CA, USA",
    "title": "Fonts & encodings",
    "type": "book"
  },
  {
    "DOI": "10.1145/585058.585077",
    "ISBN": "1-58113-594-7",
    "URL": "http://portal.acm.org/citation.cfm?id=585058.585077\\&coll=ACM\\&dl=ACM\\&CFID=22733996\\&CFTOKEN=39955741",
    "abstract": "Documents are often marked up in XML-based tagsets to delineate major structural components such as headings, paragraphs, figure captions and so on, without much regard to their eventual displayed appearance. And yet these same abstract documents, after many transformations and ’typesetting’ processes, often emerge in the popular format of Adobe PDF, either for dissemination or archiving.Until recently PDF has been a totally display-based document representation, relying on the underlying PostScript semantics of PDF. Early versions of PDF had no mechanism for retaining any form of abstract document structure but recent releases have now introduced an internal structure tree to create the so called ’Tagged PDF’.This paper describes the development of a plugin for Adobe Acrobat which creates a two-window display. In one window is shown an XML document original and in the other its Tagged PDF counterpart is seen, with an internal structure tree that, in some sense, matches the one seen in XML. If a component is highlighted in either window then the corresponding structured item, with any attendant text, is also highlighted in the other window.Important applications of correctly Tagged PDF include making PDF documents reflow intelligently on small screen devices and enabling them to be read out in correct reading order, via speech synthesiser software, for the visually impaired. By tracing structure transformation from source document to destination one can implement the repair of damaged PDF structure or the adaptation of an existing structure tree to an incrementally updated document.",
    "author": [
      {
        "family": "Hardy",
        "given": "Matthew R. B."
      },
      {
        "family": "Brailsford",
        "given": "David F."
      }
    ],
    "container-title": "Proceedings of the 2002 ACM symposium on document engineering",
    "id": "Hardy2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "document_engineering, pdf, xml",
    "page": "95-102",
    "publisher": "ACM",
    "publisher-place": "McLean, Virginia, USA",
    "title": "Mapping and displaying structural transformations between XML and PDF",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-33876-2_1",
    "abstract": "Technology advances in the last decade have led to a “digital revolution” in biomedical research. Much greater volumes of data can be generated in much less time, transforming the way researchers work [1]. Yet, for those seeking to develop new drugs to treat human disease, the task of assembling a coherent picture of existing knowledge from molecular biology to clinical investigation, can be daunting and frustrating. Individual electronic resources remain mostly disconnected, making it difficult to follow information between them. Those that contain similar types of data can describe them very differently, compounding the confusion. It can also be difficult to understand exactly where specific facts or data points originated or how to judge their quality or reliability. Finally, scientists routinely wish to ask questions that the system does not allow, or ask questions that span multiple different resources. Often the result of this is to simply abandon the enquiry, significantly diminishing the value to be gained from existing knowledge. Within pharmaceutical companies, such concerns have led to majorprogrammes in data integration; downloading, parsing, mapping, transforming and presenting public, commercial and private data. Much of this work is redundant between companies and significant resources could be saved by collaboration [2]. In an industry facing major economic pressures [3], the idea of combining forces to “get more for less” is very attractive and is arguably the only feasible route to dealing with the exponentially growing information landscape.",
    "author": [
      {
        "family": "Harland",
        "given": "Lee"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Knowledge engineering and knowledge management. 18th international conference, EKAW 2012, galway city, ireland, october 8-12, 2012. proceedings",
    "editor": [
      {
        "dropping-particle": "ten",
        "family": "Teije",
        "given": "Annette"
      },
      {
        "family": "Völker",
        "given": "Johanna"
      },
      {
        "family": "Handschuh",
        "given": "Siegfried"
      },
      {
        "family": "Stuckenschmidt",
        "given": "Heiner"
      },
      {
        "family": "Acquin",
        "given": "Mathieu",
        "non-dropping-particle": "d’"
      },
      {
        "family": "Nikolov",
        "given": "Andriy"
      },
      {
        "family": "Aussenac-Gilles",
        "given": "Nathalie"
      },
      {
        "family": "Hernandez",
        "given": "Nathalie"
      }
    ],
    "id": "Harland2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "nanopublications, rdf, semantic_web",
    "language": "en-US",
    "page": "1-7",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Open PHACTS: A semantic knowledge infrastructure for public and commercial drug discovery research",
    "title-short": "Open PHACTS",
    "type": "paper-conference",
    "volume": "7603"
  },
  {
    "abstract": "If you’re a developer working with XML, you know there’s a lot to know about XML, and the XML space is evolving almost moment by moment. But you don’t need to commit every XML syntax, API, or XSLT transformation to memory; you only need to know where to find it. And if it’s a detail that has to do with XML or its companion standards, you’ll find it–clear, concise, useful, and well-organized–in the updated third edition of <i>XML in a Nutshell</i>. With <i>XML in a Nutshell</i> beside your keyboard, you’ll be able to: <ul><li>Quick-reference syntax rules and usage examples for the core XML technologies, including XML, DTDs, Xpath, XSLT, SAX, and DOM </li><li>Develop an understanding of well-formed XML, DTDs, namespaces, Unicode, and W3C XML Schema </li><li>Gain a working knowledge of key technologies used for narrative XML documents such as web pages, books, and articles technologies like XSLT, Xpath, Xlink, Xpointer, CSS, and XSL-FO </li><li>Build data-intensive XML applications </li><li>Understand the tools and APIs necessary to build data-intensive XML applications and process XML documents, including the event-based Simple API for XML (SAX2) and the tree-oriented Document Object Model (DOM)</li></ul> This powerful new edition is the comprehensive XML reference. Serious users of XML will find coverage on just about everything they need, from fundamental syntax rules, to details of DTD and XML Schema creation, to XSLT transformations, to APIs used for processing XML documents. <i>XML in a Nutshell</i> also covers XML 1.1, as well as updates to SAX2 and DOM Level 3 coverage. If you need explanation of how a technology works, or just need to quickly find the precise syntax for a particular piece, <i>XML in a Nutshell</i> puts the information at your fingertips. Simply put, <i>XML in a Nutshell</i> is the critical, must-have reference for any XML developer.",
    "author": [
      {
        "family": "Harold",
        "given": "Elliotte R."
      },
      {
        "family": "Means",
        "given": "W. Scott"
      }
    ],
    "edition": "3rd",
    "id": "Harold2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "xml",
    "language": "en-US",
    "publisher": "O’Reilly",
    "publisher-place": "Sebastopol, CA",
    "title": "XML in a Nutshell",
    "type": "book"
  },
  {
    "DOI": "10.1300/j104v43n03_03",
    "abstract": "This article discusses how various controlled vocabularies, classification schemes, and thesauri can serve as some of the building blocks of the Semantic Web. These vocabularies have been developed over the course of decades, and can be put to great use in the development of robust Web services and Semantic Web technologies. The article covers how initial collaboration between the Semantic Web, Library and Metadata communities are creating partnerships to complete work in this area. It then discusses some core principles of authority control before talking more specifically about subject and genre vocabularies and name authority. It is hoped that future systems for internationally shared authority data will link the world’s authority data from trusted sources to benefit users worldwide. Finally, the article looks at how encoding and markup of vocabularies can help ensure compatibility with the current and future state of Semantic Web development and provides examples of how this work can help improve the findability and navigation of information on the World Wide Web.",
    "author": [
      {
        "family": "Harper",
        "given": "Corey A."
      },
      {
        "family": "Tillett",
        "given": "Barbara B."
      }
    ],
    "container-title": "Cataloging & Classification Quarterly",
    "id": "Harper2009",
    "issue": "3-4",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "semantic_web, vocabularies",
    "language": "en-US",
    "page": "47-68",
    "publisher": "Routledge",
    "title": "Library of Congress controlled vocabularies and their application to the Semantic Web",
    "type": "article-journal",
    "volume": "43"
  },
  {
    "collection-title": "SUNY series in computer-mediated communication",
    "editor": [
      {
        "family": "Harrison",
        "given": "Teresa M."
      },
      {
        "family": "Stephen",
        "given": "Timothy D."
      }
    ],
    "id": "Harrison1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "publisher": "SUNY Press",
    "publisher-place": "Albany, NY",
    "title": "Computer networking and scholarly communication in twenty-first-century university",
    "type": "book"
  },
  {
    "URL": "https://eric.ed.gov/?id=ED218931",
    "abstract": "Technical aspects of the PLATO system, the language work done on PLATO thus far (specifically in foreign languages), and areas for further research in computer-based language instruction are reviewed. PLATO-IV, designed and implemented by the Computer-Based Educational Research Laboratory of the University of Illinois at Urbana-Champaign, is unusual in being a large interactive system designed solely for instructional use. On-line management and administration of instruction with PLATO-IV has been accomplished by group files and router files, which respond to an increasing need for centralizing instructional sequencing logic. Additional management developments include an instructional management software product called \"PLATO Learning Management,\" developed by Control Data Corporation; various PLATO on-line communications facilities that aid in course administration; and site-management software available with PLATO IV. The only programming language available for interactive use on PLATO IV is TUTOR. Another valuable contribution to language instruction is the TUTOR \"answer judging\" algorithm. TUTOR has evolved a set of file and database manipulation capabilities and also offers a built-in data collection system. PLATO-compatible micro-processors exist in the form of several intelligent terminals. Most PLATO language materials have been developed by practicing language teachers. Examples of PLATO-based curricula developed for a number of languages are identified. After considering techniques for presenting a single topic through PLATO-based language instruction and evaluation needs for computer materials, attention is directed to future directions, including intelligent processing of grammar and meaning and assessment of grammar competence.",
    "author": [
      {
        "family": "Hart",
        "given": "Robert S."
      }
    ],
    "container-title": "Studies in Language Learning",
    "id": "Hart1981",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "keyword": "call, e-learning, plato_system",
    "language": "en-US",
    "page": "1-24",
    "title": "Language study and the PLATO system.",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.1558/cj.v12i4.15-37",
    "URL": "https://journals.equinoxpub.com/CALICO/article/view/23408",
    "author": [
      {
        "family": "Hart",
        "given": "Robert S."
      }
    ],
    "container-title": "CALICO Journal",
    "id": "Hart1995",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "language": "en-US",
    "page": "15-37",
    "title": "The illinois PLATO foreign languages project",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "DOI": "10.1007/978-3-642-02121-3_5",
    "abstract": "Today a large amount of RDF data is published on the Web. However, the openness of the Web and the ease to combine RDF data from different sources creates new challenges. The Web of data is missing a uniform way to assess and to query the trustworthiness of information. In this paper we present tSPARQL, a trust-aware extension to SPARQL. Two additional keywords enable users to describe trust requirements and to query the trustworthiness of RDF data. Hence, tSPARQL allows adding trust to RDF-based applications in an easy manner. As the foundation we propose a trust model that associates RDF statements with trust values and we extend the SPARQL semantics to access these trust values in tSPARQL. Furthermore, we discuss opportunities to optimize the execution of tSPARQL queries.",
    "author": [
      {
        "family": "Hartig",
        "given": "Olaf"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "The Semantic Web: Research and applications. Proceedings of the 6<sup>th</sup> European Semantic Web Conference (ESWC 2009)",
    "editor": [
      {
        "family": "Aroyo",
        "given": "Lora"
      },
      {
        "family": "Traverso",
        "given": "Paolo"
      },
      {
        "family": "Ciravegna",
        "given": "Fabio"
      },
      {
        "family": "Cimiano",
        "given": "Philipp"
      },
      {
        "family": "Heath",
        "given": "Tom"
      },
      {
        "family": "Hyvönen",
        "given": "Eero"
      },
      {
        "family": "Mizoguchi",
        "given": "Riichiro"
      },
      {
        "family": "Oren",
        "given": "Eyal"
      },
      {
        "family": "Sabou",
        "given": "Marta"
      },
      {
        "family": "Simperl",
        "given": "Elena"
      }
    ],
    "id": "Hartig2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "semantic_web, uncertainty",
    "language": "en-US",
    "page": "5-20",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Querying trust in RDF data with tSPARQL",
    "type": "paper-conference",
    "volume": "5554"
  },
  {
    "URL": "http://ceur-ws.org/Vol-1912/paper12.pdf",
    "abstract": "The standard approach to annotate statements in RDF with metadata has a number of shortcomings including data size blow-up and unnecessarily complicated queries. We propose an alternative approach that is based on nesting of RDF triples and of query patterns. The approach allows for a more compact representation of data and queries, and it is backwards compatible with the standard. In this paper we present the formal foundations of our proposal and of different approaches to implement it. More specifically, we formally capture the necessary extensions of the RDF data model and its query language SPARQL, and we define mappings based on which our extended notions can be converted back to ordinary RDF and SPARQL. Additionally, for such type of mappings we define two desirable properties, information preservation and query result equivalence, and we show that the introduced mappings possess these properties.",
    "author": [
      {
        "family": "Hartig",
        "given": "Olaf"
      }
    ],
    "container-title": "Proceedings of the 11<sup>th</sup> Alberto Mendelzon International Workshop on Foundations of Data Management and the Web (AMW 2017)",
    "editor": [
      {
        "family": "Reutter",
        "given": "Juan"
      },
      {
        "family": "Srivastava",
        "given": "Divesh"
      }
    ],
    "id": "Hartig2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "rdf, semantic_web",
    "language": "en-US",
    "title": "Foundations of RDF* and SPARQL*: An alternative approach to statement-level metadata in RDF",
    "title-short": "Foundations of RDF* and SPARQL*",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.irrodl.org/index.php/irrodl/article/view/227/310",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Harvey",
        "given": "Brian"
      }
    ],
    "container-title": "International Review of Research in Open and Distance Learning",
    "id": "Harvey2005",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "title": "Learning objects and instructional design",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "DOI": "10.1145/1386352.1386408",
    "ISBN": "978-1-60558-070-8",
    "abstract": "Semantic similarity between words or phrases is frequently used to find matching correlations between search queries and documents when straightforward matching of terms fails. This is particularly important for searching in visual databases, where pictures or video clips have been automatically tagged with a small set of semantic concepts based on analysis and classification of the visual content. Here, the textual description of documents is very limited, and semantic similarity based on WordNet’s cognitive synonym structure, along with information content derived from term frequencies, can help to bridge the gap between an arbitrary textual query and a limited vocabulary of visual concepts. This approach, termed concept-based retrieval, has received significant attention over the last few years, and its success is highly dependent on the quality of the similarity measure used to map textual query terms to visual concepts. In this paper, we consider some issues of semantic similarity measures based on Information Content (IC), and propose a way to improve them. In particular, we note that most IC-based similarity measures are derived from a small and relatively outdated corpus (the Brown corpus), which does not adequately capture the usage pattern of many contemporary terms: for example, out of more than 150,000 WordNet terms, only about 36,000 are represented. This shortcoming reflects very negatively on the coverage of typical search query terms. We therefore suggest using alternative IC corpora that are larger and better aligned with the usage of modern vocabulary. We experimentally derive two such corpora using the WWW Google search engine, and show that they provide better coverage of vocabulary, while showing comparable frequencies for Brown corpus terms. Finally, we evaluate the two proposed IC corpora in the context of a concept-based video retrieval application using the TRECVID 2005, 2006, and 2007 datasets, and we show that they increase average precision results by up to 200%.",
    "author": [
      {
        "family": "Haubold",
        "given": "Alexander"
      },
      {
        "family": "Natsev",
        "given": "Apostol"
      }
    ],
    "container-title": "Proceedings of the 2008 international conference on content-based image and video retrieval (CIVR ’08)",
    "id": "Haubold2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ir",
    "language": "en-US",
    "page": "437-446",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Web-based information content and its application to concept-based video retrieval",
    "type": "paper-conference"
  },
  {
    "URL": "http://atala.org/IMG/pdf/TAL-2009-50-2-01-Haug.pdf",
    "abstract": "This paper reports on the development of the PROIEL parallel corpus of New Testament texts, which contains the Greek original of the New Testament and its earliest Indo-European translations, into Latin, Gothic, Old Church Slavic and Classical Armenian. A web application has been constructed specifically for the purpose of annotating the texts at multiple levels: morphology, syntax, alignment at sentence, dependency graph and token level, information structure and semantics. We describe this web application and our annotation schemes. Although designed for investigating pragmatic resources, the corpus with its rich annotation is an important resource in contrastive and historical Indo-European syntax and pragmatics, easily expandable to include other old Indo-European languages.",
    "author": [
      {
        "family": "Haug",
        "given": "Dag T. T."
      },
      {
        "family": "Jøhndal",
        "given": "Marius L."
      },
      {
        "family": "Eckhoff",
        "given": "Hanne M."
      },
      {
        "family": "Hertzenberg",
        "given": "Mari J. B."
      },
      {
        "family": "Müth",
        "given": "Angelika"
      }
    ],
    "container-title": "Traitement Automatique des Langues",
    "id": "Haug2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "armenian, corpus_linguistics, cultural_heritage, gothic, greek, latin, old-church-slavic",
    "language": "en-US",
    "page": "17-45",
    "title": "Computational and linguistic issues in designing a syntactically annotated parallel corpus of Indo-European languages",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "URL": "http://www.menota.org/HB2_index.xml",
    "abstract": "The purpose of these guidelines is to define a framework for machine-readable editions of medieval Nordic texts. These guidelines are recommended for any scholar who wishes to produce detailed, machine-readable editions of primary works, that is, medieval Nordic manuscripts.",
    "editor": [
      {
        "family": "Haugen",
        "given": "Odd E."
      }
    ],
    "id": "Haugen2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, norse",
    "language": "en-US",
    "publisher": "Medieval Nordic Text Archive",
    "publisher-place": "Bergen, Norway",
    "title": "The Menota handbook 2.0: Guidelines for the electronic encoding of medieval Nordic primary sources",
    "title-short": "The Menota handbook 2.0",
    "type": "book"
  },
  {
    "URL": "http://research.ihost.com/and2007/cd/Proceedings_files/p147.pdf",
    "abstract": "With the new interest in historical documents insight grew that electronic access to these texts causes many specific problems. In the first part of the paper we survey the present role of digital historical documents. After collecting central facts and observations on historical language change we comment on the difficulties that result for retrieval and data mining on historical texts. In the second part of the paper we report on our own work in the area with a focus on special matching strategies that help to relate modern language keywords with old variants. The basis of our studies is a collection of documents from the Early New High German period. These texts come with a very rich spectrum on word variants and spelling variations.",
    "author": [
      {
        "family": "Hauser",
        "given": "Andreas"
      },
      {
        "family": "Heller",
        "given": "Markus"
      },
      {
        "family": "Leiss",
        "given": "Elisabeth"
      },
      {
        "family": "Schulz",
        "given": "Klaus U."
      },
      {
        "family": "Wanzeck",
        "given": "Christiane"
      }
    ],
    "container-title": "Proceedings of IJCAI-2007 workshop on analytics for noisy unstructured text data (AND 2007)",
    "editor": [
      {
        "family": "Knoblock",
        "given": "Craig"
      },
      {
        "family": "Lopresti",
        "given": "Daniel"
      },
      {
        "family": "Roy",
        "given": "Shourya"
      },
      {
        "family": "Subramaniam",
        "given": "L. Venkata"
      }
    ],
    "id": "Hauser2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage, ir, spelling_normalization",
    "language": "en-US",
    "page": "147-154",
    "title": "Information access to historical documents from the Early New High German period",
    "type": "paper-conference"
  },
  {
    "URL": "http://drops.dagstuhl.de/opus/volltexte/2007/1057",
    "abstract": "With the new interest in historical documents insight grew that electronic access to these texts causes many specific problems. In the first part of the paper we survey the present role of digital historical documents. After collecting central facts and observations on historical language change we comment on the difficulties that result for retrieval and data mining on historical texts. In the second part of the paper we report on our own work in the area with a focus on special matching strategies that help to relate modern language keywords with old variants. The basis of our studies is a collection of documents from the Early New High German period. These texts come with a very rich spectrum on word variants and spelling variations.",
    "author": [
      {
        "family": "Hauser",
        "given": "Andreas"
      },
      {
        "family": "Heller",
        "given": "Markus"
      },
      {
        "family": "Leiss",
        "given": "Elisabeth"
      },
      {
        "family": "Schulz",
        "given": "Klaus U."
      },
      {
        "family": "Wanzeck",
        "given": "Christiane"
      }
    ],
    "collection-number": "06491",
    "collection-title": "Dagstuhl seminar proceedings",
    "container-title": "Digital historical corpora—architecture, annotation, and retrieval",
    "editor": [
      {
        "family": "Burnard",
        "given": "Lou"
      },
      {
        "family": "Dobreva",
        "given": "Milena"
      },
      {
        "family": "Fuhr",
        "given": "Norbert"
      },
      {
        "family": "Lüdeling",
        "given": "Anke"
      }
    ],
    "id": "Hauser2007a",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "german, ir, spelling_normalization",
    "language": "en-US",
    "publisher": "Internationales Begegnungs- und Forschungszentrum für Informatik (IBFI), Schloss Dagstuhl",
    "publisher-place": "Dagstuhl, Germany",
    "title": "Information access to historical documents from the Early New High German period",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.cip.ifi.lmu.de/\\~{}hauser/papers/histOCRNachkorrektur.pdf",
    "abstract": "More and more OCR software is available for recognizing historical texts, which are often written in Blackletter fonts. Recognition rates are still far lower, then on current fonts. Postcorrection of the OCR result is therefore an essential part of the process of getting better results. This work concentrates on the postcorrection of historical German texts, printed since the 14th century. This covers the Early New High German and New High German period of the German language.",
    "author": [
      {
        "family": "Hauser",
        "given": "Andreas"
      }
    ],
    "genre": "Master’s thesis",
    "id": "Hauser2007b",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "german, ocr, spelling_correction, spelling_normalization",
    "language": "en-US",
    "publisher": "Ludwig-Maximilians-Universität München",
    "publisher-place": "Munich, Germany",
    "title": "OCR postcorrection of historical texts",
    "type": "thesis"
  },
  {
    "URL": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.7720",
    "abstract": "For the computer, word forms in an online text are simply letter sequences between blanks. A rule-based automatic language analyses presupposes, however, that the computer can recognize the individual word forms. This includes assigning the base form (lemmatization) and determining the morphosyntactic properties (categorization). It is shown that there are three principled methods of automatic word form recognition, based on word forms, morphemes, and allomorphs, respectively. After describing these different methods using the notions of traditional morphology, they are compared with regards to their handling of neologisms, time efficiency, and space requirements.",
    "author": [
      {
        "family": "Hausser",
        "given": "Roland"
      }
    ],
    "container-title": "VEXTAL: Proceedings of the conference, 22–24 november 1999, università ca’ foscari, venice, italy",
    "id": "Hausser1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "morphology, nlp",
    "page": "91-100",
    "title": "Three principled methods of automatic word form recognition.",
    "type": "paper-conference"
  },
  {
    "abstract": "The central task of a future-oriented computational linguistics is the development of cognitive machines which humans can freely talk with in their respective natural language. In the long run, this task will ensure the development of a functional theory of language, an objective method of verification,and a wide range of practical applications. Natural communication requires not only verbal processing, but also non-verbal perception and action. Therefore the content of this textbook is organized as a theory of language for the construction of talking robots. The main topic is the mechanism of natural language communication in both the speaker and the hearer. The book contains more than 700 exercises for reviewing key ideas and important problems. In the 2nd edition, Chapters 22-24 have been completely rewritten. They present a declarative outline for programming the semantic and pragmatic interpretation of natural language communication.",
    "author": [
      {
        "family": "Hausser",
        "given": "Roland"
      }
    ],
    "edition": "2",
    "id": "Hausser2001a",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "lag",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Foundations of computational linguistics: Human-computer communication in natural language",
    "title-short": "Foundations of computational linguistics",
    "type": "book"
  },
  {
    "ISBN": "978-1-58603-163-3",
    "URL": "http://www.linguistik.uni-erlangen.de/clue/fileadmin/docs/rrh/papers/2001finn.pdf",
    "abstract": "This paper compares the semantic interpretation of logical, programming, and natural languages. It shows that they are based on different ontologies, and investigates the relation between the ontology assumed and the analysis of empirical phenomena such as truth, the Epimenides paradox, propositional attitudes, and vagueness. Furthermore, it shows that there is a basic difference between a metalanguage-based and a procedural semantics, and that the choice between them depends on the ontology presumed.",
    "author": [
      {
        "family": "Hausser",
        "given": "Roland"
      }
    ],
    "collection-number": "67",
    "collection-title": "Frontiers in artificial intelligence and applications",
    "container-title": "Information modelling and knowledge bases XII",
    "editor": [
      {
        "family": "Jaakkola",
        "given": "Hannu"
      },
      {
        "family": "Kangassalo",
        "given": "Hannu"
      },
      {
        "family": "Kawaguchi",
        "given": "Eiji"
      }
    ],
    "id": "Hausser2001b",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "semantics",
    "language": "en-US",
    "page": "21-40",
    "publisher": "IOS Press",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "The four basic ontologies of semantic interpretation",
    "type": "chapter"
  },
  {
    "DOI": "10.1016/8755-4615(88)80002-1",
    "author": [
      {
        "family": "Hawisher",
        "given": "Gail E."
      }
    ],
    "container-title": "Computers and Composition",
    "id": "Hawisher1988",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "language": "en-US",
    "page": "7-27",
    "title": "Research update: Writing and word processing",
    "title-short": "Research update",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "URL": "https://eric.ed.gov/?id=EJ360278",
    "abstract": "There has been a shift of emphasis in research on the teaching of writing. The focus changed from the products of writing to writing processes. This has strong implications for instruction. Writing curriculum should be based on the organization of the cognitive processes involved in writing skills.",
    "author": [
      {
        "family": "Hayes",
        "given": "John R."
      },
      {
        "family": "Flower",
        "given": "Linda S."
      }
    ],
    "container-title": "American Psychologist",
    "id": "Hayes1986",
    "issue": "10",
    "issued": {
      "date-parts": [
        [
          1986
        ]
      ]
    },
    "keyword": "writing_research",
    "language": "en-US",
    "page": "1106-1113",
    "title": "Writing research and the writer",
    "type": "article-journal",
    "volume": "41"
  },
  {
    "DOI": "10.1109/ipcc.1994.347495",
    "ISBN": "0-7803-1936-2",
    "abstract": "Considers the advantages and disadvantages of the Standard Generalized Markup Language (SGML) which has been adopted by the International Standards Organization, the American National Standards Institute, and the US Department of Defense as a standard for describing the components of documents.",
    "author": [
      {
        "family": "Hayhoe",
        "given": "George F."
      }
    ],
    "container-title": "IPCC 94 proceedings. Scaling new heights in technical communication",
    "id": "Hayhoe1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "sgml",
    "language": "en-US",
    "page": "378-379",
    "publisher": "IEEE",
    "publisher-place": "New York, NY, USA",
    "title": "Strategy or SNAFU? The virtues and vulnerabilities of SGML",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Hayles",
        "given": "N. Katherine"
      }
    ],
    "id": "Hayles2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "publisher": "University of Chicago Press",
    "publisher-place": "Chicago, MI, USA",
    "title": "How we think: Digital media and the contemporary technogenesis",
    "title-short": "How we think",
    "type": "book"
  },
  {
    "DOI": "10.2200/s00334ed1v01y201102wbe001",
    "abstract": "The World Wide Web has enabled the creation of a global information space comprising linked documents. As the Web becomes ever more enmeshed with our daily lives, there is a growing desire for direct access to raw data not currently available on the Web or bound up in hypertext documents. Linked Data provides a publishing paradigm in which not only documents, but also data, can be a first class citizen of the Web, thereby enabling the extension of the Web with a global data space based on open standards - the Web of Data. In this Synthesis lecture we provide readers with a detailed technical introduction to Linked Data. We begin by outlining the basic principles of Linked Data, including coverage of relevant aspects of Web architecture. The remainder of the text is based around two main themes - the publication and consumption of Linked Data. Drawing on a practical Linked Data scenario, we provide guidance and best practices on: architectural approaches to publishing Linked Data; choosing URIs and vocabularies to identify and describe resources; deciding what data to return in a description of a resource on the Web; methods and frameworks for automated linking of data sets; and testing and debugging approaches for Linked Data deployments. We give an overview of existing Linked Data applications and then examine the architectures that are used to consume Linked Data from the Web, alongside existing tools and frameworks that enable these. Readers can expect to gain a rich technical understanding of Linked Data fundamentals, as the basis for application development, research or further study. Table of Contents: List of Figures / Introduction / Principles of Linked Data / The Web of Data / Linked Data Design Considerations / Recipes for Publishing Linked Data / Consuming Linked Data / Summary and Outlook",
    "author": [
      {
        "family": "Heath",
        "given": "Tom"
      },
      {
        "family": "Bizer",
        "given": "Christian"
      }
    ],
    "collection-number": "1",
    "collection-title": "Synthesis lectures on the semantic web: Theory and technology",
    "id": "Heath2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "rdf, semantic_web",
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "publisher-place": "San Rafael, CA, USA",
    "title": "Linked data: Evolving the web into a global data space",
    "title-short": "Linked data",
    "type": "book"
  },
  {
    "DOI": "10.1109/47.650005",
    "abstract": "As multimedia communication continues to grow, online technologies have dramatically changed the ways we use and present information-so much so, that we need new theories and models for understanding how technology and content are related in the new communication environment. The paper presents a theory of digital architecture, explains how SGML, HTML, and information architectures are related in the creation of a new online literacy and rhetoric, and discusses concepts, skills, and resources needed for educating tomorrow’s technical communicators",
    "author": [
      {
        "family": "Heba",
        "given": "Gary"
      }
    ],
    "container-title": "Professional Communication, IEEE Transactions on",
    "id": "Heba1997",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "document_research, sgml",
    "language": "en-US",
    "page": "275-283",
    "title": "Digital architectures: A rhetoric of electronic document structures",
    "title-short": "Digital architectures",
    "type": "article-journal",
    "volume": "40"
  },
  {
    "ISBN": "1-932432-83-3",
    "URL": "http://aclweb.org/anthology/W06-1100",
    "abstract": "We examine various string distance measures for suitability in modeling dialect distance, especially its perception. We find measures superior which do not normalize for word length, but which are are sensitive to order. We likewise find evidence for the superiority of measures which incorporate a sensitivity to phonological context, realized in the form of n -grams–although we cannot identify which form of context (bigram, trigram, etc.) is best. However, we find no clear benefit in using gradual as opposed to binary segmental difference when calculating sequence distances.",
    "author": [
      {
        "family": "Heeringa",
        "given": "Wilbert"
      },
      {
        "family": "Kleiweg",
        "given": "Peter"
      },
      {
        "family": "Gooskens",
        "given": "Charlotte"
      },
      {
        "family": "Nerbonne",
        "given": "John"
      }
    ],
    "container-title": "LD ’06: Proceedings of the workshop on linguistic distances",
    "id": "Heeringa2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage, language_identification, nlp",
    "language": "en-US",
    "page": "51-62",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Morristown, NJ, USA",
    "title": "Evaluation of string distance algorithms for dialectology",
    "type": "paper-conference"
  },
  {
    "abstract": "We report on a set of procedures to extract juridical terminology and in particular collocations and chains of collocations from German texts from the field of the protection of intellectual property. The extraction work is based on standard corpus technology, and it produces different types of output which can be made available to lexicographers: lists of term candidates, of collocation candidates, as well as example sentences which show the use of these items in context. German juridical language is a particularly interesting domain to carry out analyses of multiword expressions, as it is very rich in collocations and in chains of collocations. Many of these multiword expressions correspond to juridical concepts and thus merit being extracted and described in an electronic dictionary. We describe our sources, our tools, and examples of the output of the extraction procedures; and we discuss options for the presentation of these results towards lexicographers.",
    "author": [
      {
        "family": "Heid",
        "given": "Ulrich"
      },
      {
        "family": "Fritzinger",
        "given": "Fabienne"
      },
      {
        "family": "Hauptmann",
        "given": "Susanne"
      },
      {
        "family": "Weidenkaff",
        "given": "Julia"
      },
      {
        "family": "Weller",
        "given": "Marion"
      }
    ],
    "collection-title": "Text, translation, computational processing",
    "container-title": "Text resources and lexical knowledge",
    "editor": [
      {
        "family": "Storrer",
        "given": "Angelika"
      },
      {
        "family": "Geyken",
        "given": "Alexander"
      },
      {
        "family": "Siebert",
        "given": "Alexander"
      },
      {
        "family": "Würzner",
        "given": "Kay-Michael"
      }
    ],
    "id": "Heid2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "computational_linguistics, cultural_heritage",
    "language": "en-US",
    "page": "131-144",
    "publisher": "Mouton de Gruyter",
    "publisher-place": "Berlin/New York",
    "title": "Providing corpus data for a dictionary for German juridical phraseology",
    "type": "chapter",
    "volume": "8"
  },
  {
    "URL": "http://cairn.info/revue-francaise-de-linguistique-appliquee-2004-1-page-99.htm",
    "abstract": "Two approaches to the development of medieval text corpora can be distinguished among the projects carried out since a few decades. The first one consists of digitizing modern critical editions, and the second one is concerned with the production of precise diplomatic transcriptions of manuscripts, often directly linked to the photographs of the originals. These approaches are in fact complementary rather than contradictory, as they make it possible for scholars to choose between the quantity (representativeness) and the quality (accuracy and richness) of the data depending on the goals of their research. For both types of corpora, the challenges of their XML-TEI encoding related to the tools of their processing and analyzing are considered. Many methodological problems which arise from creating and processing medieval text corpora also concern other types of linguistic corpora.",
    "author": [
      {
        "family": "Heiden",
        "given": "Serge"
      },
      {
        "family": "Lavrentiev",
        "given": "Alexei"
      }
    ],
    "container-title": "Revue française de linguistique appliquée",
    "id": "Heiden2004",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, tei",
    "language": "en-US",
    "page": "99-118",
    "title": "Ressources électroniques pour l’étude des textes médiévaux : Approches et outils",
    "title-short": "Ressources électroniques pour l’étude des textes médiévaux ",
    "type": "article-journal",
    "volume": "IX"
  },
  {
    "URL": "http://bfm.ens-lyon.fr/IMG/pdf/Manuel_Encodage_TEI.pdf",
    "abstract": "Ce document de travail est élaboré dans le cadre des opérations de relecture et de balisage en XML-TEI des textes de la BFM. Il sert de référence à l’équipe des relecteurs/encodeurs de la BFM et pour des échanges de textes avec nos partenaires. Ce document est publié librement sur le web à destination de la communauté scientifique dans le cadre de la licence Creative Commons « Paternité-Pas d’Utilisation Commerciale-Partage des Conditions Initiales à l’Identique 2.0 France ». En accord avec cette licence, si vous utilisez ce document dans vos travaux, vous êtes prié de mentionner sa référence (projet BFM, titre, auteurs).",
    "author": [
      {
        "family": "Heiden",
        "given": "Serge"
      },
      {
        "family": "Guillot",
        "given": "Céline"
      },
      {
        "family": "Lavrentiev",
        "given": "Alexei"
      },
      {
        "family": "Bertrand",
        "given": "Lauranne"
      }
    ],
    "edition": "4",
    "id": "Heiden2010",
    "issued": {
      "date-parts": [
        [
          2010,
          8
        ]
      ]
    },
    "keyword": "cultural_heritage, french, markup, tei",
    "language": "en-US",
    "publisher": "UMR ICAR/ENS-LSH",
    "publisher-place": "Lyon, France",
    "title": "Manuel d’encodage XML-TEI des textes de la Base de Français Médiéval",
    "type": "book"
  },
  {
    "abstract": "Cette étude présente une méthode d’analyse comparative destinée à explorer le processus de « différenciation » considéré ici comme un principe fondamental de l’interaction des cultures et de la création littéraire. Celle-ci tire sa capacité de créer des effets de sens toujours nouvellement pertinents de sa façon de se différencier des voix et des façons de dire déjà existantes. L’étude présente les présupposés, fondements théoriques et objectifs d’une telle « comparaison différentielle » qui accorde une importance constitutive aux langues et aux contextes énonciatifs dont émanent les oeuvres littéraires. Le souci d’explorer ce qui est « différentiel » ne mène pas au constat d’irréductibles différences, mais à la découverte du dialogisme intertextuel et interculturel qui sous-tend toute création littéraire. Les études littéraires en reçoivent un nouvel attrait, car elles nous apprennent quelque chose d’essentiel qu’Eduard Glissant résume dans cette formule : « C’est par la différence que fonctionne ce que j’appelle la Relation avec un grand R ».",
    "author": [
      {
        "family": "Heidmann",
        "given": "Ute"
      }
    ],
    "container-title": "Nouveaux regards sur le texte littéraire",
    "editor": [
      {
        "family": "Jouve",
        "given": "Vincent"
      }
    ],
    "id": "Heidmann2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities, literature",
    "language": "fr-FR",
    "page": "203-222",
    "publisher": "Éditions et Presses universitaires de Reims",
    "publisher-place": "Reims",
    "title": "La comparaison différentielle comme approche littéraire",
    "type": "chapter"
  },
  {
    "abstract": "This chapter discusses intelligent writing assistance, in particular that which is known as text critiquing.",
    "author": [
      {
        "family": "Heidorn",
        "given": "George E."
      }
    ],
    "container-title": "Handbook of natural language processing",
    "editor": [
      {
        "family": "Dale",
        "given": "Robert"
      },
      {
        "family": "Moisl",
        "given": "Hermann"
      },
      {
        "family": "Somers",
        "given": "Harold"
      }
    ],
    "id": "Heidorn2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "interactive_editing, nlp, post-writing_tools",
    "language": "en-US",
    "page": "181-207",
    "publisher": "Dekker",
    "publisher-place": "New York, NY, USA",
    "title": "Intelligent writing assistance",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Henning",
        "given": "John"
      }
    ],
    "id": "Henning2008",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "publisher": "Routledge",
    "publisher-place": "New York, NY, USA and London, UK",
    "title": "The art of discussion-based teaching: Opening up conversation in the classroom",
    "title-short": "The art of discussion-based teaching",
    "type": "book"
  },
  {
    "URL": "https://calico.org/a-527-Answer\\%20Markup\\%20Algorithms\\%20for\\%20Southeast\\%20Asian\\%20Languagespresented\\%20at\\%20the\\%208th\\%20Annual\\%20International\\%20CALICO\\%20Symposium\\%20April\\%206\\%201991.html",
    "abstract": "Hart, Nesbit and Nakayama, and others have described answer markup methods for providing feedback to short answers entered by foreign language learners. These methods are not directly applicable to the languages of Southeast Asia, which are not written in a strictly linear fashion. Instead, these languages contain written tones and vowel fragments which appear above and/ or below the main line. Thus a given column may have several written letters making unmodified columnwise edit markup misleading. This paper describes the modification of Hart’s edit markup software and a second variation based on a simple edit distance algorithm adapted to a general Southeast Asian font system devised by the author.",
    "author": [
      {
        "family": "Henry",
        "given": "George M."
      }
    ],
    "container-title": "CALICO Journal",
    "id": "Henry1991",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "keyword": "call, e-learning, plato_system",
    "language": "en-US",
    "page": "27-38",
    "title": "Answer markup algorithms for southeast asian languages",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "URL": "http://edoc.mpg.de/315521",
    "abstract": "This paper describes a type of on-line test, the Sentence Completion Test (SCT), that tries to fill the gap between rigid MC tests and unreliable automatic essay grading approaches. We give a short overview of the main concepts, the implementation and show examplary use and applications. SCTs are used as one component in a fully operational virtual laboratory of Computational Linguistics in use at the University of Zurich.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Hess",
        "given": "Michael"
      },
      {
        "family": "Mahlow",
        "given": "Cerstin"
      }
    ],
    "container-title": "German e-science conference 2007, baden-baden",
    "id": "Hess2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "e-learning",
    "publisher": "Max Planck Society - eDocument Server [http://edoc.mpg.de/ac_oai.pl] (Germany)",
    "title": "Sentence completion tests in a virtual laboratory",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1524/9783486755732.149",
    "ISBN": "9783486755732",
    "author": [
      {
        "family": "Heßbrüggen-Walter",
        "given": "Stefan"
      }
    ],
    "container-title": "Historyblogosphere",
    "editor": [
      {
        "family": "Haber",
        "given": "Peter"
      },
      {
        "family": "Pfanzelter",
        "given": "Eva"
      }
    ],
    "id": "Hessbrueggen2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities, nanopublications, rdf",
    "language": "de-DE",
    "publisher": "Oldenbourg",
    "publisher-place": "München",
    "title": "Tatsachen im semantischen Web: Nanopublikationen in den digitalen Geisteswissenschaften?",
    "type": "chapter"
  },
  {
    "DOI": "10.17175/sb001_001",
    "abstract": "Traditionally, ontology engineering is based on the presumption that the meaning of a proposition results from the combination of the meaning of its elements (concepts) and its syntactical structure. The reach of this ›principle of compositionality‹ is, however, a contested topic in semantics. Its opponents defend the primacy of propositional meaning and derive the meaning of concepts from their contribution to propositional meaning. In this situation, this paper argues for an approach to ontology design that does not presuppose a stance in this debate. The proposed “minimal doxographical ontology” is intended as a heuristic tool charting unknown or complex domains. It regards propositional meaning as atomic and relates it to a bearer of propositional content (persons or texts). The strengths of such an approach are first discussed in a simplified example, the analysis of legal stipulations on alcoholic beverages. A more complex use case concerns the doxographical analysis of debates in the history of early modern philosophy. In closing, the paper sketches briefly how this approach may be extended using ontologies as hermeneutic tools in the interpretation of sources from the history of philosophy.",
    "author": [
      {
        "family": "Heßbrüggen-Walter",
        "given": "Stefan"
      }
    ],
    "container-title": "Zeitschrift für Digital Humanities",
    "id": "Hessbrueggen2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities, nanopublications, rdf",
    "language": "en-US",
    "title": "What people said: The theoretical foundations of a minimal doxographical ontology and its use in the history of philosophy",
    "title-short": "What people said",
    "type": "article-journal",
    "volume": "Sonderband 1"
  },
  {
    "DOI": "10.1057/9781137372406",
    "author": [
      {
        "family": "Hewitson",
        "given": "Mark"
      }
    ],
    "id": "Hewitson2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Palgrave Macmillan",
    "title": "History and causality",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Hext",
        "given": "J. B."
      },
      {
        "family": "Winings",
        "given": "J. W."
      }
    ],
    "container-title": "Commun. ACM",
    "id": "Hext1969",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "language": "en-US",
    "page": "272-275",
    "title": "An automatic grading scheme for simple programming exercises",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "DOI": "10.1007/978-3-642-73533-2_15",
    "abstract": "Die zentrale Aufgabe der maschinellen Repräsentation von Wissen wird heute allgemein darin gesehen, Beschreibungen einer Anwendungswelt in einer solchen Weise anzugeben, daß ein Computer durch Manipulation dieser Beschreibungen und unter Verwendung geeigneter Inferenztechniken möglichst effizient zu neuen und angemessenen Folgerungen über seine Anwendungswelt kommen kann. Vom Standpunkt der KI bezeichnet der Terminus «Wissensrepräsentation» damit sowohl eine Repräsentationssprache als auch die entsprechenden Inferenzmechanismen.",
    "author": [
      {
        "family": "Heyer",
        "given": "Gerhard"
      }
    ],
    "collection-title": "Informatik-Fachberichte",
    "container-title": "Wissensarten und ihre Darstellung",
    "editor": [
      {
        "family": "Heyer",
        "given": "Gerhard"
      },
      {
        "family": "Krems",
        "given": "Josef"
      },
      {
        "family": "Görz",
        "given": "Günther"
      }
    ],
    "id": "Heyer1988",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "ai, knowledge_representation",
    "language": "de-DE",
    "page": "216-218",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Wissensrepräsentation und Wissensakquisition: Einführung",
    "title-short": "Wissensrepräsentation und Wissensakquisition",
    "type": "chapter",
    "volume": "169"
  },
  {
    "DOI": "10.1145/544414.544431",
    "author": [
      {
        "family": "Higgins",
        "given": "Colin"
      },
      {
        "family": "Symeonidis",
        "given": "Pavlos"
      },
      {
        "family": "Tsintsifas",
        "given": "Athanasios"
      }
    ],
    "container-title": "ITiCSE ’02: Proceedings of the 7<sup>th</sup> annual conference on innovation and technology in computer science education",
    "id": "Higgins2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "page": "46-50",
    "publisher": "ACM",
    "title": "The marking system for CourseMaster",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.springerlink.com/content/h1n7r24940306324",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Higgins",
        "given": "Colin"
      },
      {
        "family": "Hegazy",
        "given": "Tarek"
      },
      {
        "family": "Symeonidis",
        "given": "Pavlos"
      },
      {
        "family": "Tsintsifas",
        "given": "Athanasios"
      }
    ],
    "container-title": "Education and Information Technologies",
    "id": "Higgins2003",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2003,
          9
        ]
      ]
    },
    "page": "287-304",
    "title": "The CourseMarker CBA system: Improvements over Ceilidh",
    "title-short": "The CourseMarker CBA system",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "DOI": "10.1145/1163405.1163410",
    "author": [
      {
        "family": "Higgins",
        "given": "Colin A."
      },
      {
        "family": "Gray",
        "given": "Geoffrey"
      },
      {
        "family": "Symeonidis",
        "given": "Pavlos"
      },
      {
        "family": "Tsintsifas",
        "given": "Athanasios"
      }
    ],
    "container-title": "Journal on Educational Resources in Computing",
    "id": "Higgins2005",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "page": "5",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Automated assessment and experiences of teaching programming",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "URL": "http://journals.tdl.org/jodi/article/view/76/75",
    "abstract": "This essay questions the XML doctrine of \"one input, many outputs\". In the area of publishing the doctrine says that from one book one can produce many formats and end-products. Supported by insights of linguistics and experiences of writers and editors, I shall claim this assertion to be basically wrong. By examining the main properties of XML I will further, in contrast to the doctrine, argue that XML and related technologies add to the complexity of publishing. New media, new formats and new genres will, powered by XML, lead publishers into a new and challenging state of \"many outputs, many inputs\".",
    "accessed": {
      "date-parts": [
        [
          2011,
          4,
          15
        ]
      ]
    },
    "author": [
      {
        "family": "Hillesund",
        "given": "Terje"
      }
    ],
    "container-title": "Journal of Digital Information",
    "id": "Hillesund2002",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "document_research, xml",
    "language": "en-US",
    "title": "Many outputs – many inputs: XML for publishers and e-book designers",
    "title-short": "Many outputs – many inputs",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "ISBN": "9780419134701",
    "URL": "http://eprints.ucl.ac.uk/15007/",
    "abstract": "Scientific approaches to architecture usually avoid the issue of building form, preferring to focus on function. But how can there be a theory of function without a systematic analysis of the key architectural variable of form? A theory of description is required. In this paper it is argued that such a theory can be built through the analysis of spatial form in buildings. Then once spatial form is describable in terms of a descriptive theory, a more powerfully scientific - and architectural - understanding of function is possible. The argument draws on several pieces of research carried out by the authors and their students, but focusses eventually on various types of medical building in order to illustrate certain general principles.",
    "author": [
      {
        "family": "Hillier",
        "given": "Bill"
      },
      {
        "family": "Hanson",
        "given": "Julienne"
      },
      {
        "family": "Peponis",
        "given": "John"
      }
    ],
    "container-title": "Designing for building utilisation",
    "editor": [
      {
        "family": "Powell",
        "given": "James A."
      },
      {
        "family": "Cooper",
        "given": "Ian"
      },
      {
        "family": "Lera",
        "given": "Sebastian"
      }
    ],
    "id": "Hillier1984",
    "issued": {
      "date-parts": [
        [
          1984
        ]
      ]
    },
    "keyword": "space_syntax",
    "language": "en-US",
    "page": "61-72",
    "publisher": "Spon",
    "publisher-place": "London, UK",
    "title": "What do we mean by building function?",
    "type": "paper-conference"
  },
  {
    "URL": "https://eric.ed.gov/?id=ED152238",
    "abstract": "The goal of this investigation was to find a method of reducing revision time and improving instruction in some 370 extension courses taken by airmen while on the job to upgrade their training. The goal of the pilot project was to determine which of two systems—TICCIT or PLATO—was sufficiently more attractive than textbooks to justify exploring the purchase of an economical computer based education system for the delivery of the extension courses wherever appropriate. The report describes the history, design, staffing, training, and courseware development processes of both systems as used in the project.",
    "author": [
      {
        "family": "Himwich",
        "given": "H. Alec"
      }
    ],
    "id": "Himwich1977",
    "issued": {
      "date-parts": [
        [
          1977
        ]
      ]
    },
    "keyword": "e-learning, plato_system, ticcit",
    "language": "en-US",
    "publisher": "University of Illinois",
    "publisher-place": "Urbana, IL",
    "title": "A comparison of the TICCIT and PLATO IV systems in a military setting",
    "type": "report"
  },
  {
    "URL": "http://csli-publications.stanford.edu/koskenniemi-festschrift/",
    "author": [
      {
        "family": "Hinrichs",
        "given": "Erhard W."
      }
    ],
    "collection-title": "CSLI studies in computational linguistics ONLINE",
    "container-title": "Inquiries into words, constraints, and contexts. Festschrift for kimmo koskenniemi on his 60th birthday",
    "editor": [
      {
        "family": "Arppe",
        "given": "Antti"
      },
      {
        "family": "Carlson",
        "given": "Lauri"
      },
      {
        "family": "Lindén",
        "given": "Krister"
      },
      {
        "family": "Piitulainen",
        "given": "Jussi"
      },
      {
        "family": "Suominen",
        "given": "Mickael"
      },
      {
        "family": "Vainio",
        "given": "Martti"
      },
      {
        "family": "Westerlund",
        "given": "Hanna"
      },
      {
        "family": "Yli-Jyrä",
        "given": "Anssi"
      }
    ],
    "id": "Hinrichs2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "chunking, nlp",
    "language": "en-US",
    "page": "35-44",
    "publisher": "CSLI Publications",
    "publisher-place": "Stanford, CA, USA",
    "title": "Finite-State parsing of german",
    "type": "chapter"
  },
  {
    "DOI": "10.1515/9783110211818.2.119",
    "author": [
      {
        "family": "Hinrichs",
        "given": "Erhard"
      },
      {
        "family": "Zastrow",
        "given": "Thomas"
      }
    ],
    "collection-title": "Text, translation, computational processing",
    "container-title": "Text resources and lexical knowledge",
    "editor": [
      {
        "family": "Storrer",
        "given": "Angelika"
      },
      {
        "family": "Geyken",
        "given": "Alexander"
      },
      {
        "family": "Siebert",
        "given": "Alexander"
      },
      {
        "family": "Würzner",
        "given": "Kay-Michael"
      }
    ],
    "id": "Hinrichs2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "computational_linguistics, cultural_heritage",
    "language": "en-US",
    "page": "119-129",
    "publisher": "Mouton de Gruyter",
    "publisher-place": "Berlin/New York",
    "title": "Visualization of dialect data",
    "type": "chapter",
    "volume": "8"
  },
  {
    "DOI": "10.1093/llc/fqv046",
    "ISSN": "2055-768X",
    "abstract": "Large-scale digitization efforts and the availability of computational methods, including text mining and information visualization, have enabled new approaches to historical research. However, we lack case studies of how these methods can be applied in practice and what their potential impact may be. Trading Consequences is an interdisciplinary research project between environmental historians, computational linguists, and visualization specialists. It combines text mining and information visualization alongside traditional research methods in environmental history to explore commodity trade in the 19th century from a global perspective. Along with a unique data corpus, this project developed three visual interfaces to enable the exploration and analysis of four historical document collections, consisting of approximately 200,000 documents and 11 million pages related to commodity trading. In this article, we discuss the potential and limitations of our approach based on feedback from historians we elicited over the course of this project. Informing the design of such tools in the larger context of digital humanities projects, our findings show that visualization-based interfaces are a valuable starting point to large-scale explorations in historical research. Besides providing multiple visual perspectives on the document collection to highlight general patterns, it is important to provide a context in which these patterns occur and offer analytical tools for more in-depth investigations.",
    "author": [
      {
        "family": "Hinrichs",
        "given": "Uta"
      },
      {
        "family": "Alex",
        "given": "Beatrice"
      },
      {
        "family": "Clifford",
        "given": "Jim"
      },
      {
        "family": "Watson",
        "given": "Andrew"
      },
      {
        "family": "Quigley",
        "given": "Aaron"
      },
      {
        "family": "Klein",
        "given": "Ewan"
      },
      {
        "family": "Coates",
        "given": "Colin M."
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Hinrichs2015",
    "issue": "suppl 1",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities, geocoding",
    "language": "en-US",
    "page": "i50-i75",
    "title": "Trading consequences: A case study of combining text mining and visualization to facilitate document exploration",
    "title-short": "Trading consequences",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "DOI": "10.1017/S1351324904003560",
    "abstract": "Spelling errors that happen to result in a real word in the lexicon cannot be detected by a conventional spelling checker. We present a method for detecting and correcting many such errors by identifying tokens that are semantically unrelated to their context and are spelling variations of words that would be related to the context. Relatedness to context is determined by a measure of semantic distance initially proposed by Jiang and Conrath (1997). We tested the method on an artificial corpus of errors; it achieved recall of 23â50% and precision of 18â25%.",
    "author": [
      {
        "family": "Hirst",
        "given": "Graeme"
      },
      {
        "family": "Budanitsky",
        "given": "Alexander"
      }
    ],
    "container-title": "Natural Language Engineering",
    "id": "Hirst2005",
    "issue": "01",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "spelling_correction",
    "language": "en-US",
    "page": "87-111",
    "title": "Correcting real-word spelling errors by restoring lexical cohesion",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "URL": "http://ftp.cs.toronto.edu/pub/gh/Hirst-2008-Word.pdf",
    "abstract": "Microsoft Office Word 2007 includes a “contextual spelling checker” that is intended to find misspellings that nonetheless form correctly spelled words. In an evaluation on 1400 examples, it is found to have high precision but low recall – that is, it fails to find most errors, but when it does flag a possible error, it is almost always correct. However, its performance in terms of F is inferior to that of the trigrams-based method of Mays, Damerau, and Mercer (1991).",
    "author": [
      {
        "family": "Hirst",
        "given": "Graeme"
      }
    ],
    "id": "Hirst2008",
    "issued": {
      "date-parts": [
        [
          2008,
          1
        ]
      ]
    },
    "keyword": "interactive_editing, spelling_correction",
    "language": "en-US",
    "title": "An evaluation of the contextual spelling checker of Microsoft Office Word 2007",
    "type": ""
  },
  {
    "author": [
      {
        "family": "Dahl",
        "given": "Ole-Johan"
      },
      {
        "family": "Dijkstra",
        "given": "Edsger W."
      },
      {
        "family": "Hoare",
        "given": "C. A. R."
      }
    ],
    "collection-number": "8",
    "collection-title": "A.p.i.c. Studies in data processing",
    "id": "Dahl1972",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Academic Press",
    "title": "Structured programming",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Dijkstra",
        "given": "Edsger W."
      }
    ],
    "chapter-number": "I",
    "collection-number": "8",
    "collection-title": "A.p.i.c. Studies in data processing",
    "container-author": [
      {
        "family": "Dahl",
        "given": "Ole-Johan"
      },
      {
        "family": "Dijkstra",
        "given": "Edsger W."
      },
      {
        "family": "Hoare",
        "given": "C. A. R."
      }
    ],
    "container-title": "Structured programming",
    "id": "Dijkstra1972",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "language": "en-US",
    "page": "1-82",
    "publisher": "Academic Press",
    "title": "Notes on structured programming",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Hoare",
        "given": "C. A. R."
      }
    ],
    "chapter-number": "II",
    "collection-number": "8",
    "collection-title": "A.p.i.c. Studies in data processing",
    "container-author": [
      {
        "family": "Dahl",
        "given": "Ole-Johan"
      },
      {
        "family": "Dijkstra",
        "given": "Edsger W."
      },
      {
        "family": "Hoare",
        "given": "C. A. R."
      }
    ],
    "container-title": "Structured programming",
    "id": "Hoare1972",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "language": "en-US",
    "page": "83-174",
    "publisher": "Academic Press",
    "title": "Notes on data structuring",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Dahl",
        "given": "Ole-Johan"
      },
      {
        "family": "Hoare",
        "given": "C. A. R."
      }
    ],
    "chapter-number": "III",
    "collection-number": "8",
    "collection-title": "A.p.i.c. Studies in data processing",
    "container-author": [
      {
        "family": "Dahl",
        "given": "Ole-Johan"
      },
      {
        "family": "Dijkstra",
        "given": "Edsger W."
      },
      {
        "family": "Hoare",
        "given": "C. A. R."
      }
    ],
    "container-title": "Structured programming",
    "id": "Dahl1972a",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "language": "en-US",
    "page": "175-220",
    "publisher": "Academic Press",
    "title": "Hierarchical program structures",
    "type": "chapter"
  },
  {
    "ISBN": "978-3-486-58286-4",
    "author": [
      {
        "family": "Hockerts",
        "given": "Hans G."
      }
    ],
    "container-title": "“… für die deutsche Geschichts- und Quellenforschung”: 150 Jahre Historische Kommission bei der Bayerischen Akademie der Wissenschaften",
    "editor": [
      {
        "family": "Gall",
        "given": "Lothar"
      }
    ],
    "id": "Hockerts2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "history",
    "language": "de-DE",
    "page": "229-269",
    "publisher": "Oldenburg",
    "publisher-place": "München",
    "title": "Vom nationalen Denkmal zum biographischen Portal: Die Geschichte von ADB und NDB 1885–2008",
    "title-short": "Vom nationalen Denkmal zum biographischen Portal",
    "type": "chapter"
  },
  {
    "URL": "http://www.vcolr.org/jiol/issues/PDF/2.3.1.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Hodges",
        "given": "Charles B."
      }
    ],
    "container-title": "The Journal of Interactive Online Learning",
    "id": "Hodges2004",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "1-7",
    "title": "Designing to motivate: Motivational techniques to incorporate in e-learning experiences",
    "title-short": "Designing to motivate",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.1007/978-3-642-25093-4_9",
    "ISBN": "978-3-642-25092-7",
    "abstract": "This paper introduces the MetaLex Document Server (MDS), an ongoing project to improve access to legal sources (regulations, court rulings) by means of a generic legal XML syntax (CEN MetaLex) and Linked Data. The MDS defines a generic conversion mechanism from legacy legal XML syntaxes to CEN MetaLex, RDF and Pajek network files, and discloses content by means of HTTP-based content negotiation, a SPARQL endpoint and a basic search interface. MDS combines a transparent (versioned) and opaque (content-based) naming scheme for URIs of parts of legal texts, allowing for tracking of version information at the URI-level, as well as reverse engineering of versioned metadata from sources that provide only partial information, such as many web-based legal content services. The MDS hosts all 28k national regulations of the Netherlands available since May 2011, comprising some 100M triples.",
    "author": [
      {
        "family": "Hoekstra",
        "given": "Rinke"
      }
    ],
    "chapter-number": "9",
    "collection-title": "Lecture notes in computer science",
    "container-title": "The semantic web –- ISWC 2011",
    "editor": [
      {
        "family": "Aroyo",
        "given": "Lora"
      },
      {
        "family": "Welty",
        "given": "Chris"
      },
      {
        "family": "Alani",
        "given": "Harith"
      },
      {
        "family": "Taylor",
        "given": "Jamie"
      },
      {
        "family": "Bernstein",
        "given": "Abraham"
      },
      {
        "family": "Kagal",
        "given": "Lalana"
      },
      {
        "family": "Noy",
        "given": "Natasha"
      },
      {
        "family": "Blomqvist",
        "given": "Eva"
      }
    ],
    "id": "Hoekstra2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "document_engineering, document_management, legal, rdf, temporal_data",
    "language": "en-US",
    "page": "128-143",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "The MetaLex document server",
    "type": "chapter",
    "volume": "7032"
  },
  {
    "abstract": "We present our approaches for link discovery in document collections with or without existing links. In collections containing links, we discover links using measures of link anchor ranking based on existing links. In collections without links, we gather noun phrases as anchor candidates. To discover targets, we use a measure of semantic relatedness between texts. We find that semantic relatedness is useful to identify targets for ambiguous link anchors. In collections that contain no existing links, using only document titles as anchor candidates can be enhanced by using arbitrary noun phrases extracted from documents.",
    "author": [
      {
        "family": "Hoffart",
        "given": "Johannes"
      },
      {
        "family": "Bär",
        "given": "Daniel"
      },
      {
        "family": "Zesch",
        "given": "Torsten"
      },
      {
        "family": "Gurevych",
        "given": "Iryna"
      }
    ],
    "container-title": "INEX 2009 workshop preproceedings",
    "editor": [
      {
        "family": "Geva",
        "given": "Shlomo"
      },
      {
        "family": "Kamps",
        "given": "Jaap"
      },
      {
        "family": "Trotman",
        "given": "Andrew"
      }
    ],
    "id": "Hoffart2009",
    "issued": {
      "date-parts": [
        [
          2009,
          12
        ]
      ]
    },
    "language": "en-US",
    "page": "314-325",
    "title": "Discovering links using semantic relatedness",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqq003",
    "abstract": "Categorization and taxonomy are topical issues in intertextuality studies. Instead of increasing the number of overlapping or contradictory definitions (often established with reference to limited databases) which exist even for key concepts such as “allusion ” or “quotation”, we propose an electronically implemented data-driven approach based on the isolation, analysis and description of a number of relevant parameters such as general text relation, marking for quotation, modification etc. If a systematic parameter analysis precedes discussions of possible correlations and the naming of features bundles as composite categories, a dynamic approach to categorization emerges which does justice to the varied and complex phenomena in this field. The database is the HyperHamlet corpus, a chronologically and generically wide-ranging collection of Hamlet references that confront linguistic and literary researchers with a comprehensive range of formal and stylistic issues. Its multi-dimensional encodings and search facilities provide the indispensable ’freedom from the analytic limits of hardcopy’, as Jerome McGann put it. The methodological and heuristic gains include a more complete description of possible parameter settings, a clearer recognition of multiple parameter settings (as implicit in existing genre definitions), a better understanding of how parameters interact, descriptions of disregarded literary phenomena that feature unusual parameter combinations and, finally, descriptive labels for the most polysemous areas that may clarify matters without increasing taxonomical excess.",
    "author": [
      {
        "family": "Hohl Trillini",
        "given": "Regula"
      },
      {
        "family": "Quassdorf",
        "given": "Sixta"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "HohlTrillini2010",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "intertextuality",
    "language": "en-US",
    "page": "269",
    "title": "A “key to all quotations”? A corpus-based parameter model of intertextuality",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "DOI": "10.1045/march2009-holley",
    "abstract": "This article details the work undertaken by the National Library of Australia Newspaper Digitisation Program on identifying and testing solutions to improve OCR accuracy in large scale newspaper digitisation programs. In 2007 and 2008 several different solutions were identified, applied and tested on digitised material now available in the Australian Newspapers Digitisation Program beta service <http://ndpbeta.nla.gov.au/ndp/del/home>. This article gives a state of the art overview of how OCR software works on newspapers, factors that effect OCR accuracy, methods of measuring accuracy, methods of improving accuracy, and testing methods and results for specific solutions that were considered viable for large scale text digitisation projects.",
    "author": [
      {
        "family": "Holley",
        "given": "Rose"
      }
    ],
    "container-title": "D-Lib Magazine",
    "id": "Holley2009",
    "issue": "3/4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "language": "en-US",
    "title": "How good can it get? Analysing and improving OCR accuracy in large scale historic newspaper digitisation programs",
    "type": "article-journal",
    "volume": "15"
  },
  {
    "URL": "http://www.nla.gov.au/ndp/project_details/documents/ANDP_ManyHands.pdf",
    "author": [
      {
        "family": "Holley",
        "given": "Rose"
      }
    ],
    "id": "Holley2009b",
    "issued": {
      "date-parts": [
        [
          2009,
          3
        ]
      ]
    },
    "keyword": "crowdsourcing, cultural_heritage, english, ocr, spelling_correction",
    "language": "en-US",
    "publisher": "National Library of Australia",
    "title": "Many hands make light work: Public collaborative OCR text correction in Australian Historic Newspapers",
    "title-short": "Many hands make light work",
    "type": "report"
  },
  {
    "DOI": "10.1045/march2010-holley",
    "ISSN": "1082-9873",
    "abstract": "The definition and purpose of crowdsourcing and its relevance to libraries is discussed with particular reference to the Australian Newspapers service, FamilySearch, Wikipedia, Distributed Proofreaders, Galaxy Zoo and The Guardian MP’s Expenses Scandal. These services have harnessed thousands of digital volunteers who transcribe, create, enhance and correct text, images and archives. Known facts about crowdsourcing are presented and helpful tips and strategies for libraries beginning to crowdsource are given.",
    "author": [
      {
        "family": "Holley",
        "given": "Rose"
      }
    ],
    "container-title": "D-Lib Magazine",
    "id": "Holley2010",
    "issue": "3/4",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "crowdsourcing, cultural_heritage, ocr, spelling_correction",
    "language": "en-US",
    "title": "Crowdsourcing: How and why should libraries do it?",
    "title-short": "Crowdsourcing",
    "type": "article-journal",
    "volume": "16"
  },
  {
    "URL": "http://hdl.handle.net/10760/15510",
    "abstract": "Trove has achieved recognition as an exemplary search service for finding and getting Australian information in a very short time. It is now embedded in the Australian information landscape. Australians can easily explore the richness and diversity of Australia’s cultural heritage online, for free, in a simple search interface. Over 118 million resources have been aggregated from over 1,000 Australian organisations. This figure continues to grow which is a testament to the strong spirit of collaboration and information sharing in Australia. Helping Australians, especially those living in remote areas, to access information about their country and their heritage is vitally important. Providing tools for Australians to enrich the data and add their own context is equally as important. Trove is now a trusted search and collaboration tool that has the potential to grow, develop and evolve even further to meet its users’ needs.",
    "author": [
      {
        "family": "Holley",
        "given": "Rose"
      }
    ],
    "id": "Holley2011",
    "issued": {
      "date-parts": [
        [
          2011,
          2,
          25
        ]
      ]
    },
    "keyword": "crowdsourcing, cultural_heritage, ocr",
    "language": "en-US",
    "publisher": "National Library of Australia",
    "publisher-place": "Canberra, Australia",
    "title": "Trove: The first year. January 2010 – January 2011",
    "title-short": "Trove",
    "type": "report"
  },
  {
    "DOI": "10.1109/2.947101",
    "ISSN": "0018-9162",
    "abstract": "Its lack of a versatile and visible markup language can make using Microsoft Word a nightmare-and reflects poorly on our profession.",
    "author": [
      {
        "family": "Holmes",
        "given": "Neville"
      }
    ],
    "container-title": "Computer",
    "id": "Holmes2001",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "interactive_editing, markup",
    "language": "en-US",
    "page": "128, 126-127",
    "publisher": "IEEE",
    "publisher-place": "Los Alamitos, CA, USA",
    "title": "Crouching error, hidden markup [microsoft word]",
    "type": "article-journal",
    "volume": "34"
  },
  {
    "DOI": "10.1006/jtbi.1994.1211",
    "ISSN": "00225193",
    "abstract": "The biological world is a physical system whose properties and behaviors seem entirely foreign to physics. The origins of this discrepancy lie in the very high information content in biological systems (the large amount of dynamically broken symmetry) and the evolutionary value placed on predicting the future (computation) in an environment which is inhomogeneous in time and in space. Within this context, \"free will\" can be described as a useful predictive myth.",
    "author": [
      {
        "family": "Hopfield",
        "given": "J. J."
      }
    ],
    "container-title": "Journal of Theoretical Biology",
    "id": "Hopfield1994",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "en-US",
    "page": "53-60",
    "title": "Physics, computation, and why biology looks so different",
    "type": "article-journal",
    "volume": "171"
  },
  {
    "DOI": "10.1007/s10707-009-0088-1",
    "ISSN": "1384-6175",
    "abstract": "This paper describes an approach for automatically combining geospatial and temporal ontologies such that a geospatial domain can be analyzed over multiple temporal granularities. Terms from a geospatial ontology are combined with terms from a temporal ontology to form cross products that provide an integrated spatiotemporal framework. This framework is multi-granular, highlighting elements from the geospatial ontology at different domain times. We show how pairs of ontologies represented in Protégé can be used as the input for deriving cross products and how the results of this technique can be used as a basis for querying and retrieving new perspectives on geospatial domains. Visualizations of cross product spaces highlight the geospatial–temporal combinations of terms as well as the different relations that link these terms and improve the understanding of the structure of the spatiotemporal framework. Methods for filtering terms from the cross products are also investigated in order to prune the resulting frameworks and remove irrelevant or unnecessary terms.",
    "author": [
      {
        "family": "Hornsby",
        "given": "Kathleen S."
      },
      {
        "family": "Joshi",
        "given": "Kripa"
      }
    ],
    "container-title": "GeoInformatica",
    "id": "Hornsby2010",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "rdf, spatio-temporal_annotation, temporal_data",
    "language": "en-US",
    "page": "481-505",
    "publisher": "Springer Netherlands",
    "title": "Combining ontologies to automatically generate temporal perspectives of geospatial domains",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "DOI": "10.1108/eb050493",
    "ISSN": "0001-253X",
    "abstract": "This approach to co-operation between libraries and information centres considers the use of switching languages. These can be employed in situations where several centres with a common interest wish to co-operate in the collection and indexing of information bearing material, yet prefer to retain the systems often developed and tailored to their individual requirements. To facilitate the exchange of subject information between the centres, a series of concordances are constructed between each of the indexing languages and the switching language. Thus the translation of subject indexing expressed in the indexing languages of the other participating centres can then proceed automatically. Among the possible benefits are the economic saving due to the sharing by several centres in the collection and indexing of material, and the increase in coverage enjoyed by all the centres. To determine the feasibility of such devices, a switching language, the Intermediate Lexicon for Information Science, has been developed and is under test at the Polytechnic of North London School of Librarianship. The work is supported by a grant from British Library Research and Development Department.",
    "author": [
      {
        "family": "Horsnell",
        "given": "Verina"
      }
    ],
    "container-title": "Aslib Proceedings",
    "id": "Horsnell1975",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1975
        ]
      ]
    },
    "keyword": "terminology, vocabularies",
    "language": "en-US",
    "page": "57-66",
    "title": "The intermediate lexicon: An aid to international co-operation",
    "title-short": "The intermediate lexicon",
    "type": "article-journal",
    "volume": "27"
  },
  {
    "DOI": "10.1007/978-3-642-23032-5_4",
    "ISBN": "978-3-642-23031-8",
    "abstract": "As more and more data is provided in RDF format, storing huge amounts of RDF data and efficiently processing queries on such data is becoming increasingly important. The first part of the lecture will introduce state-of-the-art techniques for scalably storing and querying RDF with relational systems, including alternatives for storing RDF, efficient index structures, and query optimization techniques. As centralized RDF repositories have limitations in scalability and failure tolerance, decentralized architectures have been proposed. The second part of the lecture will highlight system architectures and strategies for distributed RDF processing. We cover search engines as well as federated query processing, highlight differences to classic federated database systems, and discuss efficient techniques for distributed query processing in general and for RDF data in particular. Moreover, for the last part of this chapter, we argue that extracting knowledge from the Web is an excellent showcase – and potentially one of the biggest challenges – for the scalable management of uncertain data we have seen so far. The third part of the lecture is thus intended to provide a close-up on current approaches and platforms to make reasoning (e.g., in the form of probabilistic inference) with uncertain RDF data scalable to billions of triples.",
    "author": [
      {
        "family": "Hose",
        "given": "Katja"
      },
      {
        "family": "Schenkel",
        "given": "Ralf"
      },
      {
        "family": "Theobald",
        "given": "Martin"
      },
      {
        "family": "Weikum",
        "given": "Gerhard"
      }
    ],
    "chapter-number": "4",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Reasoning web. Semantic technologies for the web of data",
    "editor": [
      {
        "family": "Polleres",
        "given": "Axel"
      },
      {
        "family": "Amato",
        "given": "Claudia",
        "non-dropping-particle": "d’"
      },
      {
        "family": "Arenas",
        "given": "Marcelo"
      },
      {
        "family": "Handschuh",
        "given": "Siegfried"
      },
      {
        "family": "Kroner",
        "given": "Paula"
      },
      {
        "family": "Ossowski",
        "given": "Sascha"
      },
      {
        "family": "Patel-Schneider",
        "given": "Peter F."
      }
    ],
    "id": "Hose2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "database, rdf, temporal_data",
    "language": "en-US",
    "page": "202-249",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Database foundations for scalable RDF processing",
    "type": "chapter",
    "volume": "6848"
  },
  {
    "URL": "http://www.aset.org.au/confs/edtech88/hosie.html",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Hosie",
        "given": "Peter"
      },
      {
        "family": "Jamieson",
        "given": "Duncan"
      }
    ],
    "container-title": "Proceedings of EdTech’88",
    "editor": [
      {
        "family": "Steele",
        "given": "J."
      },
      {
        "family": "Hedberg",
        "given": "J. G."
      }
    ],
    "id": "Hosie1988",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "page": "208-223",
    "publisher": "AJET Publications",
    "publisher-place": "Canberra",
    "title": "Plenty of chaff – some wheat: Evaluating CBT authoring packages for industry and education",
    "title-short": "Plenty of chaff – some wheat",
    "type": "paper-conference"
  },
  {
    "URL": "http://chronicle.com/article/Google-Begins-to-Scale-Back/131109/",
    "accessed": {
      "date-parts": [
        [
          2012,
          6,
          4
        ]
      ]
    },
    "author": [
      {
        "family": "Howard",
        "given": "Jennifer"
      }
    ],
    "container-title": "The Chronicle of Higher Education",
    "id": "Howard2012",
    "issued": {
      "date-parts": [
        [
          2012,
          3,
          9
        ]
      ]
    },
    "title": "Google begins to scale back its scanning of books from university libraries",
    "type": "article-journal"
  },
  {
    "URL": "http://www.hrionline.ac.uk/scc/db/scc/manual.html",
    "accessed": {
      "date-parts": [
        [
          2011,
          10,
          24
        ]
      ]
    },
    "author": [
      {
        "family": "Hu",
        "given": "Xiaoling"
      },
      {
        "family": "McLaughlin",
        "given": "Jamie"
      }
    ],
    "edition": "First",
    "id": "Hu2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "publisher": "University of Sheffield",
    "publisher-place": "Sheffield, UK",
    "title": "The sheffield corpus of chinese (SCC)",
    "type": "book"
  },
  {
    "URL": "http://ismir2009.ismir.net/proceedings/PS3-4.pdf",
    "abstract": "This research examines the role lyric text can play in improving audio music mood classification. A new method is proposed to build a large ground truth set of 5,585 songs and 18 mood categories based on social tags so as to reflect a realistic, user-centered perspective. A relatively complete set of lyric features and representation models were investigated. The best performing lyric feature set was also compared to a leading audio-based system. In combining lyric and audio sources, hybrid feature sets built with three different feature selection methods were also examined. The results show patterns at odds with findings in previous studies: audio features do not always outperform lyrics features, and combining lyrics and audio features can improve performance in many mood categories, but not all of them.",
    "author": [
      {
        "family": "Hu",
        "given": "Xiao"
      },
      {
        "family": "Downie",
        "given": "J. Stephen"
      },
      {
        "family": "Ehmann",
        "given": "Andreas F."
      }
    ],
    "container-title": "10th international society for music information retrieval conference (ISMIR 2009)",
    "id": "Hu2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "language_identification",
    "language": "en-US",
    "page": "411-416",
    "title": "Lyric text mining in music mood classification",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/3-540-46154-X_15",
    "ISBN": "978-3-540-44129-8",
    "abstract": "Classical Chinese is essentially different from Modern Chinese, in both syntax and morphology. While there has recently been a number of works on part-of-speech (PoS) tagging for Modern Chinese, the PoS tagging for Classical Chinese is largely neglected. To the best of our knowledge, this is the first work in the area. Fortunately however, in terms of tagging, Classical Chinese is easier than Modern Chinese in that most Classical Chinese words are single-character-formed, thus no segmentation is needed. So in this paper, we will propose and analyze a simple statistical approach for PoS tagging of Classical Chinese. We first designed a tagset for Classical Chinese that is later shown to be accurate and efficient. Then we apply the hidden Markov model (HMM) Viterbi algorithm and made several improvements, such as sparse data problem handling and unknown word guessing, both designed particularly for Classical Chinese. As the training set grows larger, the accuracies for bigram and trigram increase to 94.9% and 97.6%, respectively. The contribution of our work also lies in proposing and solving some previously unseen problems in processing Classical Chinese.",
    "author": [
      {
        "family": "Huang",
        "given": "Liang"
      },
      {
        "family": "Peng",
        "given": "Yinan"
      },
      {
        "family": "Wang",
        "given": "Huan"
      },
      {
        "family": "Wu",
        "given": "Zhenyu"
      }
    ],
    "chapter-number": "15",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Text, speech and dialogue. 5<sup>th</sup> international conference (TSD 2002)",
    "editor": [
      {
        "family": "Sojka",
        "given": "Petr"
      },
      {
        "family": "Kopecek",
        "given": "Ivan"
      },
      {
        "family": "Pala",
        "given": "Karel"
      }
    ],
    "id": "Huang2002a",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "chinese, cultural_heritage",
    "language": "en-US",
    "page": "296-311",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Statistical part-of-speech tagging for Classical Chinese",
    "type": "paper-conference",
    "volume": "2448"
  },
  {
    "DOI": "10.3115/1118824.1118830",
    "abstract": "The Probabilistic Context-Free Grammar (PCFG) model is widely used for parsing natural languages, including Modern Chinese. But for Classical Chinese, the computer processing is just commencing. Our previous study on the part-of-speech (POS) tagging of Classical Chinese is a pioneering work in this area. Now in this paper, we move on to the PCFG parsing of Classical Chinese texts. We continue to use the same tagset and corpus as our previous study, and apply the bigram-based forward-backward algorithm to obtain the context-dependent probabilities. Then for the PCFG model, we restrict the rewriting rules to be binary/unary rules, which will simplify our programming. A small-sized rule-set was developed that could account for the grammatical phenomena occurred in the corpus. The restriction of texts lies in the limitation on the amount of proper nouns and difficult characters. In our preliminary experiments, the parser gives a promising accuracy of 82.3%.",
    "author": [
      {
        "family": "Huang",
        "given": "Liang"
      },
      {
        "family": "Peng",
        "given": "Yinan"
      },
      {
        "family": "Wang",
        "given": "Huan"
      },
      {
        "family": "Wu",
        "given": "Zhenyu"
      }
    ],
    "container-title": "Proceedings of the first SIGHAN workshop on chinese language processing (SIGHAN ’02)",
    "id": "Huang2002b",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "chinese, cultural_heritage, parsing",
    "language": "en-US",
    "page": "1-6",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "PCFG parsing for restricted Classical Chinese texts",
    "type": "paper-conference"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=1858791",
    "abstract": "Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging \"equivalent\" stacks based on feature values. Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy. Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.",
    "author": [
      {
        "family": "Huang",
        "given": "Liang"
      },
      {
        "family": "Sagae",
        "given": "Kenji"
      }
    ],
    "container-title": "Proceedings of the 48th annual meeting of the association for computational linguistics (ACL 2010)",
    "id": "Huang2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "interactive_parsing",
    "language": "en-US",
    "page": "1077-1086",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Dynamic programming for linear-time incremental parsing",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.helsinki.fi/varieng/journal/volumes/01/huber/",
    "abstract": "In their search for records of authentic spoken language predating the invention of audio recording technology in the second half of the 19th century, historical linguists have turned to written genres that are believed to be closer to speech than the average written document. Such genres include drama, dialogue in prose fiction, sermons, and trial proceedings. The proceedings of the Old Bailey, London’s central criminal court, were published from 1674 to 1834 and constitute a large body of texts from the beginning of Late Modern English. The Proceedings contain over 100,000 trials, totalling ca. 52 million words and its verbatim passages are arguably as near as we can get to the spoken word of the period. The material thus offers the rare opportunity of analyzing everyday language in a period that has been neglected both with regard to the compilation of primary linguistic data and the description of the structure, variability, and change of English.",
    "author": [
      {
        "family": "Huber",
        "given": "Magnus"
      }
    ],
    "container-title": "Annotating variation and change",
    "editor": [
      {
        "family": "Meurman-Solin",
        "given": "Anneli"
      },
      {
        "family": "Nurmi",
        "given": "Arja"
      }
    ],
    "id": "Huber2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage, english",
    "language": "en-US",
    "publisher": "Research Unit for Variation, Contacts and Change in English (VARIENG), University of Helsinki",
    "publisher-place": "Helsinki, Finland",
    "title": "The Old Bailey Proceedings, 1674–1834: Evaluating and annotating a corpus of 18th- and 19th-century spoken english",
    "title-short": "The Old Bailey Proceedings, 1674–1834",
    "type": "chapter",
    "volume": "1"
  },
  {
    "abstract": "Researches concerning Temporal Databases are being developed for more than 20 years. However, very few implemented systems are available. Several temporal data models were proposed, extending traditional data models in a way to capture also the temporal features. A feasible way of implementing a Temporal Database is using a conventional commercial database mapping the temporal data model to it. This mapping shall provide the explicit representation of the temporal information related to the intended temporal data model. A Temporal Management System is presented in this paper. Data and rules related to a temporal data model are managed by this system, implemented on a conventional DBMS.",
    "author": [
      {
        "family": "Hübler",
        "given": "Patrícia N."
      },
      {
        "family": "Edelweiss",
        "given": "Nina"
      }
    ],
    "container-title": "XV simpósio brasileiro de banco de dados (SBBD 2000)",
    "id": "Huebler2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "temporal_data",
    "language": "en-US",
    "page": "259-272",
    "title": "Implementing a temporal database on top of a conventional database: Mapping of the data model and data definition management",
    "title-short": "Implementing a temporal database on top of a conventional database",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.ifi.unizh.ch/ifiadmin/staff/rofrei/DA/DA_Arbeiten_2004/Hugentobler_Urs.pdf",
    "abstract": "E-Learning is experiencing considerable growth in the educational system. Often it is however difficult for the school organization to estimate the respective costs. Many institutions have experienced considerable losses and are therefore very hesitant with respect to e-learning. It is the aim of this report to develop a cost model for an IT-supported education. The Cost model for defined teaching scenarios (CDTS Model) should enable schooling organizations to budget the implementation of IT supported learning tools. It focuses on collaborative teaching scenarios in order to achieve dialogue and high interaction among students. The CDTS model is not only suitable for schools but can be implemented as well in corporate teaching organisations.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Hugentobler",
        "given": "Urs"
      }
    ],
    "genre": "Diplomarbeit",
    "id": "Hugentobler2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "publisher": "Universität Zürich",
    "publisher-place": "Zürich",
    "title": "Interaktion und Dialog im computerunterstützten Lernen – Entwicklung eines Kostenmodells",
    "type": "thesis"
  },
  {
    "URL": "http://eprints.infodiv.unimelb.edu.au/archive/00001744/01/459_pdf.pdf",
    "abstract": "The task of identifying the language in which a given document (ranging from a sentence to thousands of pages) is written has been relatively well studied over several decades. Automated approaches to written language identification are used widely throughout research and industrial contexts, over both oral and written source materials. Despite this widespread acceptance, a review of previous research in written language identification reveals a number of questions which remain open and ripe for further investigation.",
    "author": [
      {
        "family": "Hughes",
        "given": "Baden"
      },
      {
        "family": "Baldwin",
        "given": "Timothy"
      },
      {
        "family": "Bird",
        "given": "Steven"
      },
      {
        "family": "Nicholson",
        "given": "Jeremy"
      },
      {
        "family": "Mackinlay",
        "given": "Andrew"
      }
    ],
    "container-title": "Proceedings of the 5<sup>th</sup> international conference on language resources and evaluation (LREC 2006)",
    "id": "Hughes2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "language": "en-US",
    "page": "485-488",
    "title": "Reconsidering language identification for written language resources",
    "type": "paper-conference"
  },
  {
    "DOI": "10.2307/2504863",
    "URL": "http://jstor.org/stable/2504863",
    "abstract": "A central subject is the main strand around which the fabric of an historical narrative is woven. Such a subject must possess both spatial and temporal continuity. It is integrated into an historical entity through the relationship between those properties which make it an individual, and their interaction with the historical event. Scientific theory is useful in the reconstruction of past events and the definition of the central subject. Ideas used as central subjects present the problem of finding internal principles of integration which will make the idea continuous over time. The purpose of narratives is to explain an event by integrating it into an organized whole.",
    "author": [
      {
        "family": "Hull",
        "given": "David L."
      }
    ],
    "container-title": "History and Theory",
    "id": "Hull1975",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1975
        ]
      ]
    },
    "keyword": "history, philosophy_of_science, uncertainty",
    "language": "en-US",
    "page": "253-274",
    "title": "Central subjects and historical narratives",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "URL": "http://computersandcomposition.osu.edu/archives/v5/5_2_html/5_2_2_Hult.html",
    "abstract": "In the absence of appropriate instruction, word processing programs in general and stylistic analysis programs in particular can reinforce the unproductive revision strategies of inexperienced student writers. For example, the predilection of inexperienced writers to see text as parts (words) rather than as whole (communication) can be reinforced by writing with computers, since only a small amount of text fits on the screen and the entire text is relatively inaccessible until a printout is made. Of even greater concern is the tendency of inexperienced writers to substitute and delete rather than add and rearrange words. The problem is heightened when students use text analysis programs, which concentrate on words rather than on the whole text. The inexperienced writer’s concern about rule violations is also reinforced by spelling checkers since they identify such a limited set of errors. Students need instruction in word processing functions that allow them to add and rearrange as well as to substitute and delete; class time should be spent on rearranging text using block movements. Analysis programs should not be run until concerns about the whole essay have been addressed through conferences and repeated revision sessions. In order to use word processing effectively, students must understand the principles of effective composition and apply those principles to writing with a word processor. (HOD)",
    "author": [
      {
        "family": "Hult",
        "given": "Christine A."
      }
    ],
    "container-title": "Computers and Composition",
    "id": "Hult1988",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1988,
          4
        ]
      ]
    },
    "keyword": "interactive_editing, writing_research",
    "language": "en-US",
    "page": "29-38",
    "title": "The computer and the inexperienced writer",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "author": [
      {
        "family": "Hunt",
        "given": "J. W."
      },
      {
        "family": "McIlroy",
        "given": "M. D."
      }
    ],
    "genre": "CSTR",
    "id": "Hunt1976",
    "issued": {
      "date-parts": [
        [
          1977,
          7
        ]
      ]
    },
    "number": "41",
    "publisher": "Bell Laboratories",
    "publisher-place": "Murray Hill, NJ",
    "title": "An algorithm for differential file comparison",
    "type": "report"
  },
  {
    "DOI": "10.1007/3-540-49426-x",
    "ISBN": "978-3-540-65312-7",
    "abstract": "An introductory review of uncertainty formalisms by the volume editors begins the volume. The first main part of the book introduces some of the general problems dealt with in research. The second part is devoted to case studies; each presentation in this category has a well-delineated application problem and an analyzed solution based on an uncertainty formalism. The final part reports on developments of uncertainty formalisms and supporting technology, such as automated reasoning systems, that are vital to making these formalisms applicable. The book ends with a useful subject index. There is considerable synergy between the papers presented. The representative collection of case studies and associated techniques make the volume a particularly coherent and valuable resource. It will be indispensable reading for researchers and professionals interested in the application of uncertainty formalisms as well as for newcomers to the topic.",
    "editor": [
      {
        "family": "Hunter",
        "given": "Anthony"
      },
      {
        "family": "Parsons",
        "given": "Simon"
      }
    ],
    "id": "Hunter1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Applications of uncertainty formalisms",
    "type": "book",
    "volume": "1455"
  },
  {
    "DOI": "10.1007/11853107_12",
    "ISBN": "978-3-540-39586-7",
    "abstract": "Time management is a key feature needed in any query language for web and semistructured data. However, only recently this has been addressed by the Semantic Web community, through the study of temporal extensions to RDF ( Resource Description Framework ). In this paper we show that the ability of the RDF data model of handling unknown resources by means of blank nodes, naturally yields a rich framework for temporal reasoning in RDF. That is, even without knowing the interval of validity of some statements we can still entail useful knowledge from temporal RDF databases. To take advantage of this, we incorporate a class of temporal constraints over anonymous timestamps based on Allen’s interval algebra. We show that testing entailment in temporal graphs with the constraints proposed reduces to closure computation and mapping discovery, that is, an extended form of the standard approach for testing entailment in non-temporal RDF graphs.",
    "author": [
      {
        "family": "Hurtado",
        "given": "Carlos"
      },
      {
        "family": "Vaisman",
        "given": "Alejandro"
      }
    ],
    "chapter-number": "12",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Principles and Practice of Semantic Web Reasoning",
    "editor": [
      {
        "family": "Alferes",
        "given": "Jóse J."
      },
      {
        "family": "Bailey",
        "given": "James"
      },
      {
        "family": "May",
        "given": "Wolfgang"
      },
      {
        "family": "Schwertel",
        "given": "Uta"
      }
    ],
    "id": "Hurtado2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "rdf, temporal_data",
    "language": "en-US",
    "page": "164-178",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Reasoning with temporal constraints in RDF",
    "type": "chapter",
    "volume": "4187"
  },
  {
    "DOI": "10.2200/s00452ed1v01y201210wbe003",
    "abstract": "Abstract Cultural Heritage (CH) data is syntactically and semantically heterogeneous, multilingual, semantically rich, and highly interlinked. It is produced in a distributed, open fashion by museums, libraries, archives, and media organizations, as well as individual persons. Managing publication of such richness and variety of content on the Web, and at the same time supporting distributed, interoperable content creation processes, poses challenges where traditional publication approaches need to be re-thought. Application of the principles and technologies of Linked Data and the Semantic Web is a new, promising approach to address these problems. This development is leading to the creation of large national and international CH portals, such as Europeana, to large open data repositories, such as the Linked Open Data Cloud, and massive publications of linked library data in the U.S., Europe, and Asia. Cultural Heritage has become one of the most successful application domains of Linked Data and Semantic Web technologies. This book gives an overview on why, when, and how Linked (Open) Data and Semantic Web technologies can be employed in practice in publishing CH collections and other content on the Web. The text first motivates and presents a general semantic portal model and publishing framework as a solution approach to distributed semantic content creation, based on an ontology infrastructure. On the Semantic Web, such an infrastructure includes shared metadata models, ontologies, and logical reasoning, and is supported by shared ontology and other Web services alleviating the use of the new technology and linked data in legacy cataloging systems. The goal of all this is to provide layman users and researchers with new, more intelligent and usable Web applications that can be utilized by other Web applications, too, via well-defined Application Programming Interfaces (API). At the same time, it is possible to provide publishing organizations with more cost-efficient solutions for content creation and publication. This book is targeted to computer scientists, museum curators, librarians, archivists, and other CH professionals interested in Linked Data and CH applications on the Semantic Web. The text is focused on practice and applications, making it suitable to students, researchers, and practitioners developing Web services and applications of CH, as well as to CH managers willing to understand the technical issues and challenges involved in linked data publication. Table of Contents: Cultural Heritage on the Semantic Web / Portal Model for Collaborative CH Publishing / Requirements for Publishing Linked Data / Metadata Schemas / Domain Vocabularies and Ontologies / Logic Rules for Cultural Heritage / Cultural Content Creation / Semantic Services for Human and Machine Users / Conclusions",
    "author": [
      {
        "family": "Hyvönen",
        "given": "Eero"
      }
    ],
    "collection-number": "1",
    "collection-title": "Synthesis lectures on the semantic web: Theory and technology",
    "id": "Hyvonen2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "cultural_heritage, semantic_web",
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "publisher-place": "San Rafael, CA, USA",
    "title": "Publishing and using cultural heritage linked data on the semantic web",
    "type": "book",
    "volume": "3"
  },
  {
    "URL": "http://ieeexplore.ieee.org/servlet/opac?punumber=8897",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          23
        ]
      ]
    },
    "author": [
      {
        "literal": "IEEE"
      }
    ],
    "id": "IEEE-LTSA",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "language": "en-US",
    "title": "1484.1-2003 IEEE Standard for Learning Technology—Learning Technology Systems Architecture (LTSA)",
    "type": ""
  },
  {
    "URL": "http://www.ietf.org/rfc/rfc4122.txt",
    "author": [
      {
        "family": "Leach",
        "given": "Paul J."
      },
      {
        "family": "Mealling",
        "given": "Michael"
      },
      {
        "family": "Salz",
        "given": "Rich"
      }
    ],
    "genre": "RFC",
    "id": "IETFUUID",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "number": "4122",
    "publisher": "Internet Engineering Task Force",
    "title": "A Universally Unique IDentifier (UUID) URN Namespace",
    "type": "report"
  },
  {
    "URL": "http://imsglobal.org/simplesequencing/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "id": "IMSSS1.0",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "note": "Version 1.0",
    "publisher": "IMS Global Learning Consortium",
    "title": "IMS Simple Sequencing Specification",
    "type": "book"
  },
  {
    "id": "ISO10179",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "publisher": "ISO",
    "publisher-place": "Geneva",
    "title": "ISO/IEC 10179:1996. Information technology — Processing languages — Document Style Semantics and Specification Language (DSSSL)",
    "type": ""
  },
  {
    "author": [
      {
        "literal": "International Organization for Standardization"
      }
    ],
    "id": "ISO10179-alt",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "publisher-place": "Geneva",
    "title": "ISO/IEC 10179:1996. Information technology — Processing languages — Document Style Semantics and Specification Language (DSSSL)",
    "type": "legislation"
  },
  {
    "author": [
      {
        "literal": "ISO (International Organization for Standardization)"
      }
    ],
    "id": "ISO14492",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "title": "ISO/IEC 14492:2001. Information technology — Lossy/lossless coding of bi-level images",
    "type": ""
  },
  {
    "author": [
      {
        "literal": "ISO (International Organization for Standardization)"
      }
    ],
    "id": "ISO14977",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "title": "ISO/IEC 14977:1996. Information technology – Syntactic metalanguage – Extended BNF",
    "type": ""
  },
  {
    "author": [
      {
        "literal": "ISO (International Organization for Standardization)"
      }
    ],
    "id": "ISO15836",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "title": "ISO 15836:2009. Information and documentation — The Dublin Core metadata element set",
    "type": ""
  },
  {
    "author": [
      {
        "literal": "ISO (International Organization for Standardization)"
      }
    ],
    "id": "ISO19757",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "title": "ISO/IEC 19757-2:2003. Information technology — Document Schema Definition Language (DSDL) — Part 2: Regular-grammar-based validation — RELAX NG",
    "type": ""
  },
  {
    "author": [
      {
        "literal": "ISO (International Organization for Standardization)"
      }
    ],
    "id": "ISO26300",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "title": "ISO/IEC 26300:2006. Information technology — Open Document Format for Office Applications (OpenDocument) v1.0",
    "type": ""
  },
  {
    "id": "ISO8879",
    "issued": {
      "date-parts": [
        [
          1986
        ]
      ]
    },
    "publisher": "ISO",
    "publisher-place": "Geneva",
    "title": "ISO 8879:1986. Information processing — Text and office systems — Standard Generalized Markup Language (SGML)",
    "type": ""
  },
  {
    "author": [
      {
        "literal": "International Organization for Standardization"
      }
    ],
    "id": "ISO8879-alt",
    "issued": {
      "date-parts": [
        [
          1986
        ]
      ]
    },
    "publisher-place": "Geneva",
    "title": "ISO 8879:1986. Information processing — Text and office systems — Standard Generalized Markup Language (SGML)",
    "type": "legislation"
  },
  {
    "URL": "http://www.isu.edu/itrc/resources/LMS_FINAL_REPORT_MOODLE.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          12,
          11
        ]
      ]
    },
    "author": [
      {
        "literal": "Instructional Technology Resource Center"
      }
    ],
    "id": "ISU2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "publisher": "Idaho State University",
    "publisher-place": "Pocatello, ID, USA",
    "title": "Final evaluation and recommendation report",
    "type": "report"
  },
  {
    "URL": "http://www.itu.int/rec/T-REC-T.411/en",
    "author": [
      {
        "literal": "International Telecommunication Union"
      }
    ],
    "id": "ITU-T411",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "document_engineering, markup",
    "language": "en-US",
    "title": "ITU-T Recommendation T.411: Information technology – Open Document Architecture (ODA) and Interchange Format: Introduction and General Principles",
    "type": ""
  },
  {
    "DOI": "10.1145/1066157.1066280",
    "ISBN": "1-59593-060-4",
    "abstract": "Management of multihierarchical XML encodings has attracted attention of a number of researchers both in databases [8] and in humanities [10]. Encoding documents using multiple hierarchies can yield overlapping markup. Previously proposed solutions to management of document-centric XML with overlapping markup rely on the XML expertise of humans and their ability to maintain correct schemas for complex markup languages.We demonstrate a unified solution for management of complex, multihierarchical document-centric XML. Our framework includes software for storing, parsing, in-memory access, editing and querying, multihierarchical XML documents with conflicting structures.",
    "author": [
      {
        "family": "Iacob",
        "given": "Ionut E."
      },
      {
        "family": "Dekhtyar",
        "given": "Alex"
      }
    ],
    "collection-title": "SIGMOD ’05",
    "container-title": "Proceedings of the 2005 ACM SIGMOD international conference on management of data",
    "id": "Iacob2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "document_engineering, markup_overlap, xml",
    "language": "en-US",
    "page": "897-899",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A framework for processing complex document-centric XML with overlapping structures",
    "type": "paper-conference"
  },
  {
    "id": "Idiotikon",
    "issued": {
      "date-parts": []
    },
    "publisher": "Huber, Frauenfeld",
    "title": "Wörterbuch der schweizerdeutschen Sprache",
    "type": ""
  },
  {
    "DOI": "10.1007/978-3-319-46604-0_61",
    "abstract": "We look at Aby Warburg’s concept of Pathosformel, the repeatable formula for the expression of emotion, through the depiction of human pose in art. Using crowdsourcing, we annotate 2D human pose in one-third of the panels of Warburg’s atlas of art, and perform some exploratory data analysis. Concentrating only on the relative angles of limbs, we find meaningful clusters of related poses, explore the structure using a hierarchical model, and describe a novel method for visualising salient characteristics of the cluster. We find characteristic pose-clusters which correspond to Pathosformeln, and investigate their historical distribution; at the same time, we find morphologically similar poses can represent wildly different emotions. We hypothesise that this ambiguity comes from the static nature of our encoding, and conclude with some remarks about static and dynamic representations of human pose in art.",
    "author": [
      {
        "family": "Impett",
        "given": "Leonardo"
      },
      {
        "family": "Süsstrunk",
        "given": "Sabine"
      }
    ],
    "collection-number": "9913",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Computer vision – ECCV 2016 workshops",
    "editor": [
      {
        "family": "Hua",
        "given": "Gang"
      },
      {
        "family": "Jégou",
        "given": "Hervé"
      }
    ],
    "id": "Impett2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "888-902",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Pose and Pathosformel in Aby Warburg’s Bilderatlas",
    "type": "chapter",
    "volume": "1"
  },
  {
    "URL": "http://intrallect.com/index.php/intrallect/content/download/497/1974/file/intrallectnewslettera3.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "Intrallect Limited"
      }
    ],
    "id": "Intrallect2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "publisher-place": "Linlithgow, U.K.",
    "title": "Intrallect newsletter",
    "type": "pamphlet"
  },
  {
    "URL": "http://jorum.ac.uk/docs/pdf/JORUM_osswatch_final.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "JORUM Team"
      }
    ],
    "id": "JORUM2005",
    "issued": {
      "date-parts": [
        [
          2005,
          11
        ]
      ]
    },
    "publisher": "JORUM Consortium",
    "title": "Report on open source learning object repository systems",
    "type": "report"
  },
  {
    "abstract": "New concepts and technologies are being introduced continuously for application development in the World-Wide Web. Selecting the right implementation strategies and tools when building a Web application has become a tedious task, requiring in-depth knowledge and significant experience from both software developers and software managers. The mission of this book is to guide the reader through the opaque jungle of Web technologies. Based on their long industrial and academic experience, Stefan Jablonski and his coauthors provide a framework architecture for Web applications which helps choose the best strategy for a given project. The authors classify common technologies and standards like .NET, CORBA, J2EE, DCOM, WSDL and many more with respect to platform, architectural layer, and application package, and guide the reader through a three-phase development process consisting of preparation, design, and technology selection steps. The whole approach is exemplified using a real-world case: the architectural design of an order-entry management system.",
    "author": [
      {
        "family": "Jablonski",
        "given": "Stefan"
      },
      {
        "family": "Petrov",
        "given": "Ilia"
      },
      {
        "family": "Meiler",
        "given": "Christian"
      },
      {
        "family": "Mayer",
        "given": "Udo"
      }
    ],
    "id": "Jablonski2004",
    "issued": {
      "date-parts": [
        [
          2004,
          11
        ]
      ]
    },
    "keyword": "document_management",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Guide to web application and platform architectures",
    "type": "book"
  },
  {
    "abstract": "IF Theory Reader by Kevin Jackson-Mead and J. Robinson Wheeler was published in late February 2011. It is intended to be a book about both the craft and theory of interactive fiction. It reviews past IF achievements, summarizes current IF discussions, and suggests possible ideas to pursue in future works of IF. This work was called the \"IF Theory Book\" during its long development. Eventually after a long hiatus and a change of editors, the book was finally published.",
    "editor": [
      {
        "family": "Jackson-Mead",
        "given": "Kevin"
      },
      {
        "family": "Wheeler",
        "given": "J. Robinson"
      }
    ],
    "id": "Jackson-Mead2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "interactive_fiction",
    "language": "en-US",
    "publisher": "Transcript On Press",
    "publisher-place": "Boston, MA, USA",
    "title": "IF theory reader",
    "type": "book"
  },
  {
    "DOI": "10.1093/llc/fqv070",
    "author": [
      {
        "family": "Jackson",
        "given": "Cornell"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Jackson2017",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "page": "336-343",
    "title": "Using social network analysis to reveal unseen relationships in medieval Scotland",
    "type": "article-journal",
    "volume": "32"
  },
  {
    "DOI": "10.2312/eurovisstar.20151113",
    "abstract": "We present an overview of the last ten years of research on visualizations that support close and distant reading of textual data in the digital humanities. We look at various works published within both the visualization and digital humanities communities. We provide a taxonomy of applied methods for close and distant reading, and illustrate approaches that combine both reading techniques to provide a multifaceted view of the data. Furthermore, we list toolkits and potentially beneficial visualization approaches for research in the digital humanities. Finally, we summarize collaboration experiences when developing visualizations for close and distant reading, and give an outlook on future challenges in that research area.",
    "author": [
      {
        "family": "Jänicke",
        "given": "Stefan"
      },
      {
        "family": "Franzini",
        "given": "Greta"
      },
      {
        "family": "Cheema",
        "given": "Muhammad F."
      },
      {
        "family": "Scheuermann",
        "given": "Gerik"
      }
    ],
    "container-title": "Eurographics Conference on Visualization (EuroVis) – STARs",
    "editor": [
      {
        "family": "Borgo",
        "given": "R."
      },
      {
        "family": "Ganovelli",
        "given": "F."
      },
      {
        "family": "Viola",
        "given": "I."
      }
    ],
    "id": "Jaenicke2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities, uncertainty",
    "language": "en-US",
    "publisher": "The Eurographics Association",
    "title": "On close and distant reading in digital humanities: A survey and future challenges",
    "title-short": "On close and distant reading in digital humanities",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1016/j.ipm.2006.09.016",
    "ISSN": "03064573",
    "abstract": "n-grams have been used widely and successfully for approximate string matching in many areas. s-grams have been introduced recently as an n-gram based matching technique, where di-grams are formed of both adjacent and non-adjacent characters. s-grams have proved successful in approximate string matching across language boundaries in Information Retrieval (IR). s-grams however lack precise definitions. Also their similarity comparison lacks precise definition. In this paper, we give precise definitions for both. Our definitions are developed in a bottom-up manner, only assuming character strings and elementary mathematical concepts. Extending established practices, we provide novel definitions of s-gram profiles and the L1 distance metric for them. This is a stronger string proximity measure than the popular Jaccard similarity measure because Jaccard is insensitive to the counts of each n-gram in the strings to be compared. However, due to the popularity of Jaccard in IR experiments, we define the reduction of s-gram profiles to binary profiles in order to precisely define the (extended) Jaccard similarity function for s-grams. We also show that n-gram similarity/distance computations are special cases of our generalized definitions.",
    "author": [
      {
        "family": "Järvelin",
        "given": "Anni"
      },
      {
        "family": "Järvelin",
        "given": "Antti"
      },
      {
        "family": "Järvelin",
        "given": "Kalervo"
      }
    ],
    "container-title": "Information Processing & Management",
    "id": "Jaervelin2007",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "approximate_matching, ir",
    "language": "en-US",
    "page": "1005-1019",
    "publisher": "Pergamon Press, Inc.",
    "publisher-place": "Tarrytown, NY, USA",
    "title": "S-grams: Defining generalized n-grams for information retrieval",
    "title-short": "S-grams",
    "type": "article-journal",
    "volume": "43"
  },
  {
    "DOI": "10.1007/978-3-319-39931-7_30",
    "abstract": "Formal models of international relations have a long history of exploiting representations and algorithms from artificial intelligence. As more news sources move online, there is an increasing wealth of data that can inform the creation of such models. The Global Database of Events, Language, and Tone (GDELT) extracts events from news articles from around the world, where the events represent actions taken by geopolitical actors, reflecting the actors’ relationships. We can apply existing machine-learning algorithms to automatically construct a Bayesian network that represents the distribution over the actions between actors. Such a network model allows us to analyze the interdependencies among events and generate the relative likelihoods of different events. By examining the accuracy of the learned network over different years and different actor pairs, we are able to identify aspects of international relations from a data-driven approach. We are also able to identify weaknesses in the model that suggest needs for additional domain knowledge.",
    "author": [
      {
        "family": "Jalal-Kamali",
        "given": "Ali"
      },
      {
        "family": "Pynadath",
        "given": "David V."
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Social, cultural, and behavioral modeling. Proceedings of SBP-BRiMS 2016",
    "editor": [
      {
        "family": "Xu",
        "given": "Kevin S."
      },
      {
        "family": "Reitter",
        "given": "David"
      },
      {
        "family": "Lee",
        "given": "Dongwon"
      },
      {
        "family": "Osgood",
        "given": "Nathaniel"
      }
    ],
    "id": "Jalal-Kamali2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "formal_models",
    "language": "en-US",
    "page": "311-322",
    "publisher": "Springer",
    "publisher-place": "Cham",
    "title": "Toward a bayesian network model of events in international relations",
    "type": "paper-conference",
    "volume": "9708"
  },
  {
    "DOI": "10.1080/03080188.2016.1223651",
    "ISSN": "0308-0188",
    "abstract": "The notion of the existence of two opposed cultures, one literary and one scientific, has a long pedigree going back to nineteenth century. However, it was C.P. Snow’s formulation of the idea in 1959 and F.R. Leavis’s 1962 critique, which brought it to the fore in cultural discourse, where it has more or less remained ever since. The papers in this special double issue of Interdisciplinary Science Reviews examine the debate and its legacies from a variety of perspectives, while this introduction seeks to contextualise the issues raised and draw some contemporary lessons.",
    "author": [
      {
        "family": "James",
        "given": "Frank A. J. L."
      }
    ],
    "container-title": "Interdisciplinary Science Reviews",
    "id": "James2016",
    "issue": "2-3",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "philosophy_of_science",
    "language": "en-US",
    "page": "107-117",
    "title": "Introduction: Some significances of the two cultures debate",
    "title-short": "Introduction",
    "type": "article-journal",
    "volume": "41"
  },
  {
    "ISBN": "9783476026224",
    "abstract": "Computerbasierte Verfahren greifen in viele Bereiche der Geistes- und Kulturwissenschaften ein und spielen eine zunehmende Rolle in der universitären Bildung. Dieser Band bietet eine fundierte Einführung in die grundlegenden Konzepte, Methoden und Werkzeuge der Digital Humanities. Sie präsentiert Grundlagen wie Digitalisierung, Aufbau von Datensammlungen, Datenmodellierung und XML. Darüber hinaus behandelt sie Anwendungsgebiete wie Digitale Edition, Information Retrieval, Netzwerkanalyse, Geographische Informationssysteme, Simulation ebenso weiterführende Aspekte wie die Rolle der Bibliotheken, Archive und Museen sowie rechtliche und ethische Fragen.",
    "editor": [
      {
        "family": "Jannidis",
        "given": "Fotis"
      },
      {
        "family": "Kohle",
        "given": "Hubertus"
      },
      {
        "family": "Rehbein",
        "given": "Malte"
      }
    ],
    "id": "Jannidis2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "de-DE",
    "publisher": "J. B. Metzler",
    "publisher-place": "Stuttgart",
    "title": "Digital Humanities: Eine Einführung",
    "title-short": "Digital Humanities",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Jannidis",
        "given": "Fotis"
      }
    ],
    "chapter-number": "7",
    "container-title": "Digital Humanities: Eine Einführung",
    "editor": [
      {
        "family": "Jannidis",
        "given": "Fotis"
      },
      {
        "family": "Kohle",
        "given": "Hubertus"
      },
      {
        "family": "Rehbein",
        "given": "Malte"
      }
    ],
    "id": "Jannidis2017chap7",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "de-DE",
    "page": "99-108",
    "publisher": "J. B. Metzler",
    "publisher-place": "Stuttgart",
    "title": "Grundlagen der Datenmodellierung",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/11863939_14",
    "ISBN": "978-3-540-44526-5",
    "abstract": "Acquisition and semantic annotation of data are fundamental tasks within the domain of cultural heritage. With the increasing amount of available data and ad hoc cross linking between their providers and users (e.g. through web services), data integration and knowledge refinement becomes even more important. To integrate information from several sources it has to be guaranteed that objects of discourse (which may be artifacts, events, persons, places or periods) refer to the same real world phenomena within all involved data sources. Local (database) identifiers however only disambiguate internal data, but fail in establishing connections to/between external data, while global identifiers can only partially solve this problem. Software assistants should support users in establishing such connections by delivering identity assumptions, i.e. by estimating whether examined data actually concerns the same real word phenomenon. This paper points out how similarity measures can act as groundwork for such assistants by introducing a similarity-based identity assumption assistant for historical places to support scholars in establishing links between distributed historical knowledge.",
    "author": [
      {
        "family": "Janowicz",
        "given": "Krzysztof"
      }
    ],
    "chapter-number": "14",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Geographic information science",
    "editor": [
      {
        "family": "Raubal",
        "given": "Martin"
      },
      {
        "family": "Miller",
        "given": "Harvey J."
      },
      {
        "family": "Frank",
        "given": "Andrew U."
      },
      {
        "family": "Goodchild",
        "given": "Michael F."
      }
    ],
    "id": "Janowicz2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage, rdf, spatio-temporal_annotation, temporal_data",
    "language": "en-US",
    "page": "199-216",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Towards a Similarity-Based identity assumption service for historical places",
    "type": "chapter",
    "volume": "4197"
  },
  {
    "URL": "http://www.xml.com/pub/a/2004/03/03/sgmlwiki.html",
    "abstract": "Wikis are nice for typing. XML is nice for processing. SGML is a standard compiler compiler language for specifying conversions from one to the other.",
    "author": [
      {
        "family": "Jelliffe",
        "given": "Rick"
      }
    ],
    "container-title": "xml.com",
    "id": "Jelliffe2004",
    "issued": {
      "date-parts": [
        [
          2004,
          3,
          3
        ]
      ]
    },
    "keyword": "sgml, wiki, xml",
    "language": "en-US",
    "title": "From wiki to XML, through SGML",
    "type": "article-journal"
  },
  {
    "DOI": "10.1145/377435.377472",
    "ISBN": "1-58113-330-8",
    "abstract": "Students approach the study of computing in Higher Education in increasing numbers from an increasingly wide variety of backgrounds. In most degree level courses one of the first modules students will encounter is intended to teach them to program.As the students become more diverse, so do their motivations for taking their degree. Anecdotal evidence from many institutions is that students are becoming more tactical, and will engage only in those activities that they see as contributing to an eventual highly paid job.This paper describes an investigation into the motivations of students for taking a degree in computing, and for studying programming in particular. The results raise a number of issues for the teaching of programming.",
    "author": [
      {
        "family": "Jenkins",
        "given": "Tony"
      }
    ],
    "container-title": "ITiCSE ’01: Proceedings of the 6th annual conference on innovation and technology in computer science education",
    "id": "Jenkins2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "53-56",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The motivation of students of programming",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.digitalhumanities.org/dhq/vol/001/2/000009/000009.html",
    "abstract": "Because so little primary historical work has been done on the classic text computer game \"Colossal Cave Adventure\", academic and popular references to it frequently perpetuate inaccuracies. \"Adventure\" was the first in a series of text-based games (\"interactive fiction\") that emphasize exploring, puzzles, and story, typically in a fantasy setting; these games had a significant cultural impact in the late 1970s and a significant commercial presence in the early 1980s. Will Crowther based his program on a real cave in Kentucky; Don Woods expanded this version significantly. The expanded work has been examined as an occasion for narrative encounters [Buckles 1985] and as an aesthetic masterpiece of logic and utility [Knuth 1998]; however, previous attempts to assess the significance of \"Adventure\" remain incomplete without access to Crowther’s original source code and Crowther’s original source cave. Accordingly, this paper analyzes previously unpublished files recovered from a backup of Woods’s student account at Stanford, and documents an excursion to the real Colossal Cave in Kentucky in 2005. In addition, new interviews with Crowther, Woods, and their associates (particularly members of Crowther’s family) provide new insights on the precise nature of Woods’s significant contributions. Real locations in the cave and several artifacts (such as an iron rod and an axe head) correspond to their representation in Crowther’s version; however, by May of 1977, Woods had expanded the game to include numerous locations that he invented, along with significant technical innovations (such as scorekeeping and a player inventory). Sources that incorrectly date Crowther’s original to 1972 or 1974, or that identify it as a cartographic data file with no game or fantasy elements, are sourced thinly if at all. The new evidence establishes that Crowther wrote the game during the 1975-76 academic year and probably abandoned it in early 1976. The original game employed magic, humor, simple combat, and basic puzzles, all of which Woods greatly expanded. While Crowther remained largely faithful to the geography of the real cave, his original did introduce subtle changes to the environment in order to improve the gameplay.",
    "author": [
      {
        "family": "Jerz",
        "given": "Dennis G."
      }
    ],
    "container-title": "Digital Humanities Quarterly",
    "id": "Jerz2007",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "interactive_fiction",
    "language": "en-US",
    "title": "Somewhere nearby is Colossal Cave: Examining Will Crowther’s original  in code and in Kentucky",
    "title-short": "Somewhere nearby is Colossal Cave",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.1007/978-3-658-00897-0",
    "editor": [
      {
        "family": "Jeschke",
        "given": "Sabina"
      },
      {
        "family": "Jakobs",
        "given": "Eva-Maria"
      },
      {
        "family": "Dröge",
        "given": "Alicia"
      }
    ],
    "id": "Jeschke2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Springer Gabler",
    "title": "Exploring Uncertainty: Ungewissheit und Unsicherheit im interdisziplinären Diskurs",
    "title-short": "Exploring Uncertainty",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-658-11755-9",
    "editor": [
      {
        "family": "Jeschke",
        "given": "Sabina"
      },
      {
        "family": "Schmitt",
        "given": "Robert"
      },
      {
        "family": "Dröge",
        "given": "Alicia"
      }
    ],
    "id": "Jeschke2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Springer Gabler",
    "title": "Exploring Cybernetics: Kybernetik im interdisziplinären Diskurs",
    "title-short": "Exploring Cybernetics",
    "type": "book"
  },
  {
    "DOI": "10.1093/llc/fqm041",
    "abstract": "Information about place and location is an essential part of research in the humanities. There are many ways that methods and tools for structuring, visualizing and analysing space, spatial behaviour, and spatial relationships can benefit humanities research but the use of spatial information in digital scholarship by humanists remains very limited. The developing role of the study of place and location through geographical information systems (GIS) and other digital tools is discussed briefly before examining the factors that are inhibiting the use of spatial data in our research. The influences of current research practice and the attitudes of scholarly institutions in the humanities are examined. This article will explore some of the potential research applications but, possibly more importantly; it will also examine why that potential is being developed so slowly and discuss a possible way forward for the community.",
    "author": [
      {
        "family": "Jessop",
        "given": "Martyn"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Jessop2008a",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "digital_humanities, geo_ir",
    "language": "en-US",
    "page": "39-50",
    "title": "The inhibition of geographical information in digital humanities scholarship",
    "type": "article-journal",
    "volume": "23"
  },
  {
    "DOI": "10.1093/llc/fqn016",
    "abstract": "Thought processes are enhanced when ways are found to link external perception with internal mental processes by the use of graphic aids. Such aids range from scribbled diagrams to sophisticated linkages between thought, images, and text such as those employed by Leonardo da Vinci. These tools allow visual perception to be harnessed in the dynamic processes associated with the creation or discovery of new knowledge. Digital humanists are applying digital versions of these age-old tools in many areas of research, from the graphs generated by text analysis applications to virtual reality models of ancient buildings, methods known collectively as ’digital visualization’. This article begins with a brief review of the current application of visualization in the digital humanities before moving on to establish a context for digital visualization within ’traditional’ humanities scholarship. This provides a context for an examination of what is required in order to ensure that digital visualization work is performed with identifiable intellectual rigour. The London Charter is used as a case study for a possible framework for the development of appropriate methods and standards. Digital visualization as a scholarly methodology is discussed and demonstrated as being part of a continuum of established academic practice rather than something that is in some way new, ’revolutionary’, or lacking in rigorous scholarly value.",
    "author": [
      {
        "family": "Jessop",
        "given": "Martyn"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Jessop2008b",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "digital_humanities, geo_ir",
    "language": "en-US",
    "page": "281-293",
    "title": "Digital visualization as a scholarly activity",
    "type": "article-journal",
    "volume": "23"
  },
  {
    "DOI": "10.1145/1268784.1268807",
    "author": [
      {
        "family": "Ji",
        "given": "Jeong-Hoon"
      },
      {
        "family": "Woo",
        "given": "Gyun"
      },
      {
        "family": "Cho",
        "given": "Hwan-Gue"
      }
    ],
    "container-title": "ITiCSE ’07: Proceedings of the 12th annual SIGCSE conference on innovation and technology in computer science education",
    "id": "Ji2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "page": "73-77",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A source code linearization technique for detecting plagiarized programs",
    "type": "paper-conference"
  },
  {
    "URL": "http://cognet.mit.edu/sites/default/files/books/9780262281805/pdfs/9780262281805_chap12.pdf",
    "author": [
      {
        "family": "Johnson-Laird",
        "given": "Philip N."
      }
    ],
    "chapter-number": "12",
    "container-title": "The foundations of cognitive science",
    "editor": [
      {
        "family": "Posner",
        "given": "Michael I."
      }
    ],
    "id": "Johnson-Laird1989",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "cogsci, formal_models",
    "language": "en-US",
    "page": "439-499",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Mental models",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Johnson",
        "given": "Stephen C."
      }
    ],
    "genre": "CSTR",
    "id": "Johnson1977",
    "issued": {
      "date-parts": [
        [
          1977,
          12
        ]
      ]
    },
    "number": "65",
    "publisher": "Bell Laboratories",
    "publisher-place": "Murray Hill, NJ",
    "title": "Lint, a C program checker",
    "type": "report"
  },
  {
    "DOI": "10.1109/WMTE.2005.26",
    "abstract": "This paper introduces a novel open source platform independent quiz engine developer (QED). This is a standards compliant (IMS QTI) system which permits the user to develop questions at the lowest level of granularity (termed quiz objects). Quizzes are a natural symbiotic partner to learning and therefore are equally as important to complete the learning experience. These quiz objects are structured similarly to learning objects (IEEE LOM) and similarly this object-orientated approach supports interoperability and reusability by the development of quiz banks that are searchable and shareable between independent developers. The quiz engine that has been designed is Web-based, cross platform (mobile & desktop) and feature rich and allows several modes of interaction with the user (e.g. template driven, power user). An important aspect of the system is its ability to search individual quiz objects, which allows greater reuse of questions.",
    "author": [
      {
        "family": "Johnson",
        "given": "K."
      },
      {
        "family": "Hall",
        "given": "T."
      },
      {
        "family": "O’Keeffe",
        "given": "D."
      }
    ],
    "container-title": "WMTE 2005. IEEE international workshop on wireless and mobile technologies in education, 2005",
    "id": "Johnson2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "120-122",
    "title": "Generation of quiz objects (QO) with a quiz engine developer (QED)",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.physsci.heacademy.ac.uk/publications/practiceguide/effectivepracticeinobjectiveassessment.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Johnstone",
        "given": "Alex"
      }
    ],
    "id": "Johnstone2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "publisher": "LTSN Physical Sciences Centre",
    "title": "Effective practice in objective assessment",
    "type": "book"
  },
  {
    "abstract": "This paper starts by tracing the architecture of document preparation systems. Two basic types of document representations appear: at the page level or at logical level. The paper then focuses on logical level representations and tries to sorvey three existing formalisms: SGML, Interscript and ODA.",
    "author": [
      {
        "family": "Joloboff",
        "given": "Vania"
      }
    ],
    "container-title": "Text Processing and Document Manipulation. Proceedings of the International Conference",
    "editor": [
      {
        "dropping-particle": "van",
        "family": "Vliet",
        "given": "J. C."
      }
    ],
    "id": "Joloboff1986",
    "issued": {
      "date-parts": [
        [
          1986
        ]
      ]
    },
    "keyword": "markup, sgml",
    "language": "en-US",
    "page": "107-124",
    "publisher": "British Computer Society; Cambridge University Press",
    "publisher-place": "Cambridge",
    "title": "Trends and standards in document representation",
    "type": "paper-conference"
  },
  {
    "URL": "https://www.calico.org/a-587-TICCIT\\%20and\\%20CLIPS\\%20The\\%20Early\\%20Years.html",
    "author": [
      {
        "family": "Jones",
        "given": "Randall L."
      }
    ],
    "container-title": "CALICO Journal",
    "id": "Jones1995",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "keyword": "e-learning, plato_system, ticcit",
    "language": "en-US",
    "page": "84-96",
    "title": "TICCIT and CLIPS: The early years",
    "title-short": "TICCIT and CLIPS",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "URL": "http://sleid.cqu.edu.au/viewarticle.php?id=27",
    "abstract": "The implementation of e-learning within an institution of higher learning can be seen as a design problem. A first step in a design problem is the generation of an appropriate representation of the initial state. The nature of this representation has significant influence over the characteristics and suitability of any solution derived from it. This paper uses the work systems framework to present a shortened conceptualisation of this initial state. This conceptualisation reveals lessons and implications to improve future institutional approaches to e-learning.",
    "author": [
      {
        "family": "Jones",
        "given": "David"
      }
    ],
    "container-title": "Studies in Learning, Evaluation, Innovation and Development",
    "id": "Jones2004",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2004,
          10
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "47-55",
    "title": "The conceptualisation of e-learning: Lessons and implications",
    "title-short": "The conceptualisation of e-learning",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "author": [
      {
        "family": "Jones",
        "given": "Steven E."
      }
    ],
    "id": "Jones2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Routledge",
    "publisher-place": "Abingdon",
    "title": "Roberto Busa, S. J., And the emergence of humanities computing: The priest and the punched cards",
    "title-short": "Roberto Busa, S. J., And the emergence of humanities computing",
    "type": "book"
  },
  {
    "DOI": "10.1145/2254129.2254184",
    "ISBN": "978-1-4503-0915-8",
    "abstract": "Recent work in digital humanities has seen researchers increasingly producing online editions of texts and manuscripts, particularly in adoption of the TEI XML format for online publishing. The benefits of semantic web techniques are underexplored in such research, however, with a lack of sharing and communication of research information. The Sharing Ancient Wisdoms (SAWS) project applies linked data practices to enhance and expand on what is possible with these digital text editions. Focussing on Greek and Arabic collections of ancient wise sayings, which are often related to each other, we use RDF to annotate and extract semantic information from the TEI documents as RDF triples. This allows researchers to explore the conceptual networks that arise from these interconnected sayings. The SAWS project advocates a semantic-web-based methodology, enhancing rather than replacing current workflow processes, for digital humanities researchers to share their findings and collectively benefit from each other’s work.",
    "author": [
      {
        "family": "Jordanous",
        "given": "Anna"
      },
      {
        "family": "Lawrence",
        "given": "K. Faith"
      },
      {
        "family": "Hedges",
        "given": "Mark"
      },
      {
        "family": "Tupman",
        "given": "Charlotte"
      }
    ],
    "collection-title": "WIMS ’12",
    "container-title": "Proceedings of the 2<sup>nd</sup> international conference on web intelligence, mining and semantics (WIMS ’12)",
    "id": "Jordanous2012a",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "cultural_heritage, rdf, semantic_web, tei",
    "language": "en-US",
    "page": "A44+",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Exploring manuscripts: Sharing ancient wisdoms across the semantic web",
    "title-short": "Exploring manuscripts",
    "type": "paper-conference"
  },
  {
    "DOI": "10.4242/balisagevol8.jordanous01",
    "abstract": "In this paper we primarily consider what we can gain from enhancing TEI-encoded texts with RDF, though there are other choices of re-representation which could also be profitable in the future. We consider the use of OAC annotations as part of our work for the future. To illustrate our approach, we take as a case study the Sharing Ancient Wisdoms (SAWS) project, which explores and analyses the tradition of wisdom literatures in ancient Greek, Arabic and other languages. Our methods for representing semantic links within and between specific sections of these texts, and describing the relationships that exist between them in a systematic way, are documented and explained. We consider that this approach has the potential to be used widely to link and describe related sections of a variety of different types of texts. Given the common practice of publishing TEI documents as part of Digital Humanities research output, our central contribution is to demonstrate how the usefulness of these TEI documents can be developed further in diverse directions, beyond their current application for digital edition publication.",
    "author": [
      {
        "family": "Jordanous",
        "given": "Anna"
      },
      {
        "family": "Stanley",
        "given": "Alan"
      },
      {
        "family": "Tupman",
        "given": "Charlotte"
      }
    ],
    "collection-title": "Balisage series on markup technologies",
    "container-title": "Proceedings of balisage: The markup conference 2012",
    "id": "Jordanous2012b",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "nanopublications, rdf, tei",
    "language": "en-US",
    "title": "Contemporary transformation of ancient documents for recording and retrieving maximum information: When one form of markup is not enough",
    "title-short": "Contemporary transformation of ancient documents for recording and retrieving maximum information",
    "type": "paper-conference",
    "volume": "8"
  },
  {
    "URL": "http://www.translationautomation.com/perspectives/minority-report-helping-less-resourced-languages-to-share-data.html",
    "accessed": {
      "date-parts": [
        [
          2011,
          11,
          8
        ]
      ]
    },
    "author": [
      {
        "family": "Joscelyne",
        "given": "Andrew"
      }
    ],
    "id": "Joscelyne2010",
    "issued": {
      "date-parts": [
        [
          2010,
          3
        ]
      ]
    },
    "keyword": "nlp",
    "title": "Minority report: Helping less-resourced languages to share data",
    "title-short": "Minority report",
    "type": ""
  },
  {
    "DOI": "10.1145/1163405.1163407",
    "author": [
      {
        "family": "Joy",
        "given": "Mike"
      },
      {
        "family": "Griffiths",
        "given": "Nathan"
      },
      {
        "family": "Boyatt",
        "given": "Russell"
      }
    ],
    "container-title": "Journal on Educational Resources in Computing",
    "id": "Joy2005",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "page": "2",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The BOSS online submission and assessment system",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "abstract": "An explosion of Web-based language techniques, merging of distinct fields,availability of phone-based dialogue systems, and much more make this anexciting time in speech and language processing. The first of its kind tothoroughly cover language technology – at all levels and with all moderntechnologies – this book takes an empirical approach to the subject, based onapplying statistical and other machine-learning algorithms to largecorporations. Builds each chapter around one or more worked examplesdemonstrating the main idea of the chapter, usingthe examples to illustratethe relative strengths and weaknesses of various approaches. Adds coverage ofstatistical sequence labeling, information extraction, question answering andsummarization, advanced topics in speech recognition, speech synthesis. Revises coverage of language modeling, formal grammars, statistical parsing,machine translation, and dialog processing. A useful reference forprofessionals in any of the areas of speech and language processing.",
    "author": [
      {
        "family": "Jurafsky",
        "given": "Daniel"
      },
      {
        "family": "Martin",
        "given": "James H."
      }
    ],
    "edition": "2",
    "id": "Juarafsky2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "nlp",
    "language": "en-US",
    "publisher": "Prentice Hall",
    "publisher-place": "Upper Saddle River, NJ, USA",
    "title": "Speech and language processing",
    "type": "book"
  },
  {
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Judson",
        "given": "Eugene"
      },
      {
        "family": "Sawada",
        "given": "Daiyo"
      }
    ],
    "container-title": "Journal of Computers in Mathematics and Science Teaching",
    "id": "Judson2002",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "page": "167-181",
    "title": "Learning from past and present: Electronic response systems in college lecture halls",
    "title-short": "Learning from past and present",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "DOI": "10.1145/1324960.1324964",
    "author": [
      {
        "family": "Junghans",
        "given": "Martin"
      },
      {
        "family": "Riehle",
        "given": "Dirk"
      },
      {
        "family": "Gurram",
        "given": "Rama"
      },
      {
        "family": "Kaiser",
        "given": "Matthias"
      },
      {
        "family": "Lopes",
        "given": "Mario"
      },
      {
        "family": "Yalcinalp",
        "given": "Umit"
      }
    ],
    "container-title": "SIGWEB Newsletter",
    "id": "Junghans2007a",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "page": "4",
    "title": "An EBNF grammar for Wiki Creole 1.0",
    "type": "article-journal",
    "volume": "2007"
  },
  {
    "DOI": "10.1145/1324960.1324965",
    "author": [
      {
        "family": "Junghans",
        "given": "Martin"
      },
      {
        "family": "Riehle",
        "given": "Dirk"
      },
      {
        "family": "Yalcinalp",
        "given": "Umit"
      }
    ],
    "container-title": "SIGWEB Newsletter",
    "id": "Junghans2007b",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "page": "5",
    "title": "An XML interchange format for Wiki Creole 1.0",
    "type": "article-journal",
    "volume": "2007"
  },
  {
    "DOI": "10.1145/1822258.1822287",
    "abstract": "Today’s wiki engines are not interoperable. The rendering engine is tied to the processing tools which are tied to the wiki editors. This is an unfortunate consequence of the lack of rigorously specified standards. This paper discusses an EBNF-based grammar for Wiki Creole 1.0, a community standard for wiki markup, and demonstrates its benefits. Wiki Creole is being specified using prose, so our grammar revealed several categories of ambiguities, showing the value of a more formal approach to wiki markup specification. The formalization of Wiki Creole using a grammar shows performance problems that today’s regular-expression-based wiki parsers might face when scaling up. We present an implementation of a wiki markup parser and demonstrate our test cases for validating Wiki Creole parsers. We view the work presented in this paper as an important step towards decoupling wiki rendering engines from processing tools and from editing tools by means of a precise and complete wiki markup specification. This decoupling layer will then allow innovation on these different parts to proceed independently and as is expected at a faster pace than before.",
    "author": [
      {
        "family": "Junghans",
        "given": "Martin"
      },
      {
        "family": "Riehle",
        "given": "Dirk"
      },
      {
        "family": "Gurram",
        "given": "Rama"
      },
      {
        "family": "Kaiser",
        "given": "Matthias"
      },
      {
        "family": "Lopes",
        "given": "Mário"
      },
      {
        "family": "Yalcinalp",
        "given": "Umit"
      }
    ],
    "container-title": "WikiSym ’08: Proceedings of the 4<sup>th</sup> international symposium on wikis",
    "id": "Junghans2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "markup",
    "language": "en-US",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A grammar for standardized wiki markup",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1561/1500000005",
    "ISSN": "1554-0669",
    "abstract": "Authorship attribution, the science of inferring characteristics of the author from the characteristics of documents written by that author, is a problem with a long history and a wide range of application. Recent work in \"non-traditional\" authorship attribution demonstrates the practicality of automatically analyzing documents based on authorial style, but the state of the art is confusing. Analyses are difficult to apply, little is known about type or rate of errors, and few \"best practices\" are available. In part because of this confusion, the field has perhaps had less uptake and general acceptance than is its due.",
    "author": [
      {
        "family": "Juola",
        "given": "Patrick"
      }
    ],
    "container-title": "Foundations and Trends® in Information Retrieval",
    "id": "Juola2007",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "authorship_attribution",
    "language": "en-US",
    "page": "233-334",
    "publisher": "Now Publishers Inc.",
    "publisher-place": "Hanover, MA, USA",
    "title": "Authorship attribution",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.1515/9783110211818.1.27",
    "abstract": "Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any technique or system requiring reference to a fixed lexicon accessed by orthographic form. This paper presents two methods for mapping unknown historical text types to one or more synchronically active canonical types: conflation by phonetic form, and conflation by lemma instantiation heuristics. Implementation details and evaluation of both methods are provided for a corpus of historical German verse quotation evidence from the digital edition of the Deutsches Wörterbuch.",
    "author": [
      {
        "family": "Jurish",
        "given": "Bryan"
      }
    ],
    "collection-title": "Text, translation, computational processing",
    "container-title": "Text resources and lexical knowledge",
    "editor": [
      {
        "family": "Storrer",
        "given": "Angelika"
      },
      {
        "family": "Geyken",
        "given": "Alexander"
      },
      {
        "family": "Siebert",
        "given": "Alexander"
      },
      {
        "family": "Würzner",
        "given": "Kay-Michael"
      }
    ],
    "id": "Jurish2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "computational_linguistics, cultural_heritage, german",
    "language": "en-US",
    "page": "27-37",
    "publisher": "Mouton de Gruyter",
    "publisher-place": "Berlin/New York",
    "title": "Finding canonical forms for historical German text",
    "type": "chapter",
    "volume": "8"
  },
  {
    "abstract": "Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a fixed lexicon accessed by orthographic form, such as information retrieval systems, part-of-speech taggers, simple word stemmers, or more sophisticated morphological analyzers.",
    "author": [
      {
        "family": "Jurish",
        "given": "Bryan"
      }
    ],
    "container-title": "Journal for Language Technology and Computational Linguistics",
    "id": "Jurish2010",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, german, spelling_correction",
    "language": "en-US",
    "page": "23-39",
    "title": "More than words: Using token context to improve canonicalization of historical German",
    "title-short": "More than words",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "URL": "http://aclweb.org/anthology/W10-2209",
    "abstract": "Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a static lexicon accessed by orthographic form. In this paper, we present three methods for associating unknown historical word forms with synchronically active canonical cognates and evaluate their performance on an information retrieval task over a manually annotated corpus of historical German verse.",
    "author": [
      {
        "family": "Jurish",
        "given": "Bryan"
      }
    ],
    "container-title": "Proceedings of the 11th meeting of the ACL special interest group on computational morphology and phonology",
    "editor": [
      {
        "family": "Heinz",
        "given": "Jeffrey"
      },
      {
        "family": "Cahill",
        "given": "Lynne"
      },
      {
        "family": "Wicentowski",
        "given": "Richard"
      }
    ],
    "id": "Jurish2010a",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, german",
    "language": "en-US",
    "page": "72-77",
    "publisher": "Association for Computational Linguistics",
    "title": "Comparing canonicalizations of historical German text",
    "type": "paper-conference"
  },
  {
    "URL": "http://opus.kobv.de/ubp/volltexte/2012/5578/",
    "abstract": "This work addresses issues in the automatic preprocessing of historical German input text for use by conventional natural language processing techniques. Conventional techniques cannot adequately account for historical input text due to conventional tools’ reliance on a fixed application-specific lexicon keyed by contemporary orthographic surface form on the one hand, and the lack of consistent orthographic conventions in historical input text on the other. Historical spelling variation is treated here as an error-correction problem or \"canonicalization\" task: an attempt to automatically assign each (historical) input word a unique extant canonical cognate, thus allowing direct application-specific processing (tagging, parsing, etc.) of the returned canonical forms without need for any additional application-specific modifications. In the course of the work, various methods for automatic canonicalization are investigated and empirically evaluated, including conflation by phonetic identity, conflation by lemma instantiation heuristics, canonicalization by weighted finite-state rewrite cascade, and token-wise disambiguation by a dynamic Hidden Markov Model.",
    "author": [
      {
        "family": "Jurish",
        "given": "Bryan"
      }
    ],
    "genre": "PhD thesis",
    "id": "Jurish2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "cultural_heritage, german, spelling_correction, spelling_normalization",
    "language": "en-US",
    "publisher": "Universität Potsdam",
    "publisher-place": "Potsdam, Germany",
    "title": "Finite-state canonicalization techniques for historical German",
    "type": "thesis"
  },
  {
    "DOI": "10.1016/j.fss.2015.06.011",
    "ISSN": "01650114",
    "abstract": "This comprehensive, bird’s view research note combines the state of the art, a brief presentation of the history and some original solutions, and position like views of some prospective future developments of one of the most relevant and interesting areas related to the use of fuzzy logic in database management systems, notably in its querying component, and – to some extent – in a broader issue of data and information management. We briefly summarize the roots of those new applications of fuzzy logic, more relevant proposals and development in the context of fuzzification of the basic relational database model, and then some of its further generalizations. We particularly focus on fuzzy querying as a human consistent and friendly way of retrieving information due to real human intentions and preferences expressed in natural language represented via fuzzy logic and possibility theory. We mention some extensions, notably fuzzy queries with linguistic quantifiers, and point their close relation to linguistic summaries. As for newer, prospective developments, we mainly focus on bipolar queries that can accomodate the users’ intentions and preferences involving some sort of a required and desired, mandatory and optional, etc. conditions. We show various ways of handling such queries. We conclude with some brief position statements of our view on relevant and promising directions, and challenges.",
    "author": [
      {
        "family": "Kacprzyk",
        "given": "Janusz"
      },
      {
        "family": "Zadrożny",
        "given": "Sławomir"
      },
      {
        "family": "De Tré",
        "given": "Guy"
      }
    ],
    "container-title": "Fuzzy Sets and Systems",
    "id": "Kacprzyk2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "database, uncertainty",
    "language": "en-US",
    "page": "300-307",
    "title": "Fuzziness in database management systems: Half a century of developments and future prospects",
    "title-short": "Fuzziness in database management systems",
    "type": "article-journal",
    "volume": "281"
  },
  {
    "abstract": "Several improvements are suggested to the syntax of SGML, the recent international standard for the description of electronic document types. These improvements ease processing by existing tools, remove ambiguity cleanly, and increase human usability. They also indicate some guidelines that should be followed in the design and specification of computer-software standards. By following accepted computer-science conventions for the description of languages the design of a standard may be improved, and the subsequent implementation task simplified.",
    "author": [
      {
        "family": "Kaelbling",
        "given": "Michael"
      }
    ],
    "container-title": "Electronic Publishing",
    "id": "Kaelbling1990",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "document_engineering, sgml",
    "language": "en-US",
    "page": "93-98",
    "title": "On improving SGML",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "URL": "http://www.wired.com/gadgets/mac/commentary/cultofmac/2002/08/54370",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Kahney",
        "given": "Leander"
      }
    ],
    "id": "Kahney2002",
    "issued": {
      "date-parts": [
        [
          2002,
          8
        ]
      ]
    },
    "title": "HyperCard: What could have been",
    "title-short": "HyperCard",
    "type": ""
  },
  {
    "DOI": "10.1145/280765.280899",
    "ISBN": "0-89791-983-1",
    "abstract": "Recently, encouraging progress has been made in integrating independent components in complete agents for real-world environments. While such systems demonstrate component integration, they often do not explicitly utilize synergistic interactions, which allow each component to function beyond its original capabilities because of the presence of other components. This abstract presents an implemented illustration of such explicit component synergy and its usefulness in dynamic multi-agent environments.",
    "author": [
      {
        "family": "Kaminka",
        "given": "Gal A."
      },
      {
        "family": "Tambe",
        "given": "Milind"
      }
    ],
    "container-title": "Autonomous agents ’98: Proceedings of the second international conference on autonomous agents",
    "id": "Kaminka1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "software_components",
    "language": "en-US",
    "page": "459-460",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A synergy of agent components: Social comparison for failure detection",
    "title-short": "A synergy of agent components",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqr009",
    "ISSN": "0268-1145",
    "abstract": "The Electronic Manipulus florum Project (<http://manipulusflorum.com/>) is digitizing a large collection of Latin quotations. We examine the design of the Janus search engine, which finds overlaps between keywords, supplied text, and Manipulus florum quotations in the presence of typographic, orthographic, and complex variants. Janus can be used to determine whether, and to what extent, the Manipulus florum was used as a resource by later writers. It can also be used to determine which previous works Thomas of Ireland used to compile the Manipulus florum. In addition, the Janus engine could be used to find overlaps between two arbitrary texts.",
    "author": [
      {
        "family": "Kane",
        "given": "A."
      },
      {
        "family": "Tompa",
        "given": "Frank"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Kane2011",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2011,
          5,
          4
        ]
      ]
    },
    "keyword": "digital_humanities, intertextuality",
    "language": "en-US",
    "page": "407-415",
    "publisher": "Oxford University Press",
    "title": "Janus: The intertextuality search engine for the electronic manipulus florum project",
    "title-short": "Janus",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "URL": "http://www.zeit.de/2014/35/big-data-lingustik-geisteswissenschaften/komplettansicht",
    "abstract": "Mit Hilfe des Computers lesen Geisteswissenschaftler riesige Textmengen. So kommen sie dem Zeitgeist auf die Spur und entdecken manch überraschenden Sinneswandel – auch in 335.878 Artikeln der ZEIT.",
    "author": [
      {
        "family": "Kara",
        "given": "Stefanie"
      }
    ],
    "container-title": "Die Zeit",
    "id": "Kara2014",
    "issued": {
      "date-parts": [
        [
          2014,
          8
        ]
      ]
    },
    "keyword": "digital_humanities, in_the_media",
    "language": "de-DE",
    "title": "Big Data: Zwischen den Zeilen",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "DOI": "10.1145/1922649.1922654",
    "ISSN": "0360-0300",
    "abstract": "Machine transliteration is the process of automatically transforming the script of a word from a source language to a target language, while preserving pronunciation. The development of algorithms specifically for machine transliteration began over a decade ago based on the phonetics of source and target languages, followed by approaches using statistical and language-specific methods. In this survey, we review the key methodologies introduced in the transliteration literature. The approaches are categorized based on the resources and algorithms used, and the effectiveness is compared.",
    "author": [
      {
        "family": "Karimi",
        "given": "Sarvnaz"
      },
      {
        "family": "Scholer",
        "given": "Falk"
      },
      {
        "family": "Turpin",
        "given": "Andrew"
      }
    ],
    "container-title": "ACM Computing Surveys",
    "id": "Karimi2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "transliteration",
    "language": "en-US",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Machine transliteration survey",
    "type": "article-journal",
    "volume": "43"
  },
  {
    "author": [
      {
        "family": "Karlsson",
        "given": "Fred"
      }
    ],
    "container-title": "Proceedings of the 13<sup>th</sup> international conference of computational linguistics",
    "editor": [
      {
        "family": "Karlgren",
        "given": "Hans"
      }
    ],
    "id": "Karlsson1990",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "page": "168-173",
    "publisher-place": "Helsinki",
    "title": "Constraint grammar as a framework for parsing unrestricted text",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1162/coli.2007.33.4.443",
    "ISSN": "0891-2017",
    "abstract": "This article is a perspective on some important developments in semantics and in computational linguistics over the past forty years. It reviews two lines of research that lie at opposite ends of the field: semantics and morphology. The semantic part deals with issues from the 1970s such as discourse referents, implicative verbs, presuppositions , and questions . The second part presents a brief history of the application of finite-state transducers to linguistic analysis starting with the advent of two-level morphology in the early 1980s and culminating in successful commercial applications in the 1990s. It offers some commentary on the relationship, or the lack thereof, between computational and paper-and-pencil linguistics. The final section returns to the semantic issues and their application to currently popular tasks such as textual inference and question answering.",
    "author": [
      {
        "family": "Karttunen",
        "given": "Lauri"
      }
    ],
    "container-title": "Computational Linguistics",
    "id": "Karttunen2007",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "computational_linguistics, semantics",
    "language": "en-US",
    "page": "443-467",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Word play",
    "type": "article-journal",
    "volume": "33"
  },
  {
    "URL": "http://ltrc.iiit.ac.in/icon_archives/ICON2010/10Dec2010/Paper4-File33-Paper189.pdf",
    "abstract": "The use of computer mediated communication such as emailing, microblogs, Short Messaging System (SMS), and chat rooms has created corpora which contain incredibly noisy text. Tweets, messages sent by users on Twitter.com, are an especially noisy form of communication. Twitter.com contains billions of these tweets, but in their current state they contain so much noise that it is difficult to extract useful information. Tweets often contain highly irregular syntax and nonstandard use of English. This paper describes a novel system which normalizes these Twitter posts, converting them into a more standard form of English, so that standard machine translation (MT) and natural language processing (NLP) techniques can be more easily applied to them. In order to normalize Twitter tweets, we take a two step approach. We first preprocess tweets to remove as much noise as possible and then feed them into a machine translation model to convert them into standard English. Together, these two steps allow us to achieve improvement in BLEU scores comporable to the improvements achieved by SMS normalization.",
    "author": [
      {
        "family": "Kaufmann",
        "given": "Max"
      },
      {
        "family": "Kalita",
        "given": "Jugal"
      }
    ],
    "container-title": "Proceedings of the 8<sup>th</sup> international conference on natural language processing (ICON 2010)",
    "id": "Kaufmann2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "microtext, parsing, spelling_normalization",
    "language": "en-US",
    "publisher": "Macmillan India",
    "publisher-place": "Chennai, India",
    "title": "Syntactic normalization of Twitter messages",
    "type": "paper-conference"
  },
  {
    "ISSN": "0891-2017",
    "abstract": "We present an algorithm for aligning texts with their translations that is based only on internal evidence. The relaxation process rests on a notion of which word in one text corresponds to which word in the other text that is essentially based on the similarity of their distributions. It exploits a partial alignment of the word level to induce a maximum likelihood alignment of the sentence level, which is in turn used, in the next iteration, to refine the word level estimate. The algorithm appears to converge to the correct sentence alignment in only a few iterations.",
    "author": [
      {
        "family": "Kay",
        "given": "Martin"
      },
      {
        "family": "Röscheisen",
        "given": "Martin"
      }
    ],
    "container-title": "Computational Linguistics",
    "id": "Kay1993",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "machine_translation",
    "language": "en-US",
    "page": "121-142",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Text-translation alignment",
    "type": "article-journal",
    "volume": "19"
  },
  {
    "DOI": "10.1145/958220.958221",
    "abstract": "This is an extended abstract of the talk given by Michael Kay in the keynote address of the DocEng2003 symposium.",
    "author": [
      {
        "family": "Kay",
        "given": "Michael"
      }
    ],
    "container-title": "Proceedings of the 2003 ACM symposium on document engineering",
    "id": "Kay2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "xml,xquery,xslt",
    "page": "29-31",
    "publisher": "ACM",
    "publisher-place": "Grenoble, France",
    "title": "XML five years on: A review of the achievements so far and the challenges ahead",
    "title-short": "XML five years on",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/358557.358569",
    "ISSN": "0001-0782",
    "author": [
      {
        "family": "Kearsley",
        "given": "Greg"
      }
    ],
    "container-title": "Commun. ACM",
    "id": "Kearsley1982",
    "issue": "7",
    "issued": {
      "date-parts": [
        [
          1982,
          7
        ]
      ]
    },
    "keyword": "e-learning, plato_system, ticcit",
    "language": "en-US",
    "page": "429-437",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Authoring systems in computer based education",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "URL": "http://www.contempaesthetics.org/newvolume/pages/article.php?articleID=754",
    "abstract": "A summary of the possible persistence of so-called useless humanistic research against the diktat of the Edufactory, the essay “No-media – Against the Coming Singularity” problematizes the complex field of forces and factors currently leading the life of universities toward the servicing of reduced aspirations for scholarship in an ultra-monetized society – plus neo-liberal academia’s penchant for the manufacturing of events and reputations at the expense of impersonal (confraternal) intellectual inquiry proper. An oblique critique of “vertical integration” strategies derived from corporate business models, foremost in media empires, and as applied to the production and management of knowledge, the essay prefigures a return to forms of scholarly and artistic production in alliance with universal moral and ethical precepts as preserved in droit moral – the Enlightenment-era concept of the Moral Rights of Authors.",
    "author": [
      {
        "family": "Keeney",
        "given": "Gavin"
      }
    ],
    "container-title": "Contemporary Aesthetics",
    "id": "Keeney2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "title": "No-media: Against the coming singularity",
    "title-short": "No-media",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "DOI": "10.1207/s15516709cog1102_5",
    "abstract": "This paper presents a theory of the syntactic aspects of human sentence production. An important characteristic of unprepared speech is that overt pronunciation of a sentence can be initiated before the speaker has completely worked out the meaning content he or she is going to express in that sentence. Apparently, the speaker is able to build up a syntactically coherent utterance out of a series of syntactic fragments each rendering a new part of the meaning content. This incremental, left-to-right mode of sentence production is the central capability of the proposed Incremental Procedural Grammar (IPG). Certain other properties of spontaneous speech, as derivable from speech errors, hesitations, self-repairs, and language pathology, are accounted for as well. The psychological plausibility thus gained by the grammar appears compatible with a satisfactory level of linguistic plausibility in that sentences receive structural descriptions which are in line with current theories of grammar. More importantly, an explanation for the existence of configurational conditions on transformations and other linguistics rules is proposed. The basic design feature of IPG which gives rise to these psychologically and linguistically desirable properties, is the ” Procedures + Stack” concept. Sentences are built not by a central constructing agency which overlooks the whole process but by a team of syntactic procedures (modules) which work—in parallel—on small parts of the sentence, have only a limited overview, and whose sole communication channel is a stack. IPG covers object complement constructions, interrogatives, and word order in main and subordinate clauses. It handles unbounded dependencies, crossserial dependencies and coordination phenomena such as gapping and conjunction reduction. It is also capable of generating self-repairs and elliptical answers to questions. IPG has been implemented as an incremental Dutch sentence generator written in LISP.",
    "author": [
      {
        "family": "Kempen",
        "given": "Gerard"
      },
      {
        "family": "Hoenkamp",
        "given": "Edward"
      }
    ],
    "container-title": "Cognitive Science",
    "id": "Kempen1987",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1987,
          4,
          1
        ]
      ]
    },
    "keyword": "interactive_parsing",
    "language": "en-US",
    "page": "201-258",
    "publisher": "Lawrence Erlbaum Associates, Inc.",
    "title": "An incremental procedural grammar for sentence formulation",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "DOI": "10.1007/978-0-387-34747-9_31",
    "ISBN": "978-0-387-34654-0",
    "abstract": "This paper describes the comparison of selected distance measures in their applicability for supporting retrieval of historical spelling variants (hsv). The interdisciplinary project Rule-based search in text databases with nonstandard orthography develops a fuzzy full-text search engine for historical text documents. This engine should provide easier text access for experts as well as interested amateurs. The FlexMetric framework enhances the distance measure algorithm found to be most efficient according to the results of the evaluation. This measure can be used for multiple applications, including searching, post-ranking, transformation and even reflection about oneâs own language.",
    "author": [
      {
        "family": "Kempken",
        "given": "Sebastian"
      },
      {
        "family": "Luther",
        "given": "Wolfram"
      },
      {
        "family": "Pilz",
        "given": "Thomas"
      }
    ],
    "chapter-number": "31",
    "collection-title": "IFIP international federation for information processing",
    "container-title": "Artificial intelligence in theory and practice",
    "editor": [
      {
        "family": "Bramer",
        "given": "Max"
      }
    ],
    "id": "Kempken2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage, ir",
    "language": "en-US",
    "page": "295-304",
    "publisher": "Springer",
    "publisher-place": "Boston",
    "title": "Comparison of distance measures for historical spelling variants",
    "type": "paper-conference",
    "volume": "217"
  },
  {
    "author": [
      {
        "family": "Kenner",
        "given": "Hugh"
      }
    ],
    "container-title": "BYTE",
    "id": "Kenner1989",
    "issued": {
      "date-parts": [
        [
          1989,
          10
        ]
      ]
    },
    "page": "360-361",
    "title": "Curtains for Lady Buxley",
    "type": "article-journal"
  },
  {
    "DOI": "10.1109/OCEANS.1995.526779",
    "abstract": "The global economy is in a constant state of change. Companies maintain a competitive edge by continuously revising their products as new technologies merge. Manufacturers expect their employees to perform new jobs with new roles, new equipment and new responsibilities. Consumers demand high-quality products at low cost and better customer service. Success in business increasingly demands new ways to improve product design, manufacturing, distribution, and marketing. Those who adapt well to the unsettling impact of this new world order will discover their unrealized potential by using change as a strength. These will be the survivors. However, to not only survive, but to thrive a company must gain a competitive advantage by adapting rapidly to this changing environment. This paper outlines the requirement for education and training in the USA for the manning of industry in the future",
    "author": [
      {
        "family": "Keramas",
        "given": "James G."
      }
    ],
    "container-title": "OCEANS ’95. MTS/IEEE. Challenges of our changing global environment. Conference proceedings",
    "id": "Keramas1995",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "242-246",
    "title": "The impact of new technologies on our changing global environment",
    "type": "paper-conference",
    "volume": "1"
  },
  {
    "URL": "http://cm.bell-labs.com/cm/cs/cstr/97.ps.gz",
    "abstract": "Although TROFF has been the mainstay of document preparation at Bell Labs for several years, it has heretofore been very dependent on one particular typesetter, the Graphic Systems CAT. This paper describes conversion of TROFF to deal with a wide class of typesetters. Some of these typesetters provide many more facilities than the CAT does. Typical extra features include more sizes and fonts, larger alphabets, and the ability to create new characters and to draw graphical objects. The paper describes the enhancements that permit TROFF to take advantage of some of these capabilities as well.",
    "author": [
      {
        "family": "Kernighan",
        "given": "Brian W."
      }
    ],
    "genre": "CSTR",
    "id": "Kernighan1982",
    "issued": {
      "date-parts": [
        [
          1982,
          3
        ]
      ]
    },
    "language": "en-US",
    "number": "97",
    "publisher": "Bell Laboratories",
    "publisher-place": "Murray Hill, NJ",
    "title": "A typesetter-independent TROFF",
    "type": "report"
  },
  {
    "DOI": "10.1007/978-3-540-39984-1_19",
    "ISBN": "978-3-540-20177-9",
    "abstract": "Untranslatable query keys pose a problem in dictionary-based cross-language information retrieval (CLIR). One solution consists of using approximate string matching methods for finding the spelling variants of the source key among the target database index. In such a setting, it is important to select a matching method suited especially for CLIR. This paper focuses on comparing the effectiveness of several matching methods in a cross-lingual setting. Search words from five domains were expressed in six languages (French, Spanish, Italian, German, Swedish, and Finnish). The target data consisted of the index of an English full-text database. In this setting, we first established the best method among six baseline matching methods for each language pair. Secondly, we tested novel matching methods based on binary digrams formed of both adjacent and non-adjacent characters of words. The latter methods consistently outperformed all baseline methods.",
    "author": [
      {
        "family": "Keskustalo",
        "given": "Heikki"
      },
      {
        "family": "Pirkola",
        "given": "Ari"
      },
      {
        "family": "Visala",
        "given": "Kari"
      },
      {
        "family": "Leppänen",
        "given": "Erkka"
      },
      {
        "family": "Järvelin",
        "given": "Kalervo"
      }
    ],
    "chapter-number": "19",
    "collection-title": "Lecture notes in computer science",
    "container-title": "String processing and information retrieval",
    "editor": [
      {
        "family": "Nascimento",
        "given": "Mario A."
      },
      {
        "family": "Moura",
        "given": "Edleno S."
      },
      {
        "family": "Oliveira",
        "given": "Arlindo L."
      }
    ],
    "id": "Keskustalo2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "approximate_matching, ir",
    "language": "en-US",
    "page": "252-265",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Non-adjacent digrams improve matching of Cross-Lingual spelling variants",
    "type": "chapter",
    "volume": "2857"
  },
  {
    "DOI": "10.1093/llc/fqq011",
    "abstract": "This article deals with the lemmatization of Middle Dutch literature. This text collection—like any other medieval corpus—is characterized by an enormous spelling variation, which makes it difficult to perform a computational analysis of this kind of data. Lemmatization is therefore an essential preprocessing step in many applications, since it allows the abstraction from superficial textual variation, for instance in spelling. The data we will work with is the Corpus-Gysseling, containing all surviving Middle Dutch literary manuscripts dated before 1300 AD. In this article we shall present a language-independent system that can “learn” intra-lemma spelling variation. We describe a series of experiments with this system, using Memory-Based Machine Learning and propose two solutions for the lemmatization of our data: the first procedure attempts to generate new spelling variants, the second one seeks to implement a novel string distance metric to better detect spelling variants. The latter system attempts to rerank candidates suggested by a classic Levenshtein distance, leading to a substantial gain in lemmatization accuracy. This research result is encouraging and means a substantial step forward in the computational study of Middle Dutch literature. Our techniques might be of interest to other research domains as well because of their language-independent nature.",
    "author": [
      {
        "family": "Kestemont",
        "given": "Mike"
      },
      {
        "family": "Daelemans",
        "given": "Walter"
      },
      {
        "family": "De Pauw",
        "given": "Guy"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Kestemont2010",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2010,
          9
        ]
      ]
    },
    "keyword": "cultural_heritage, dutch, morphology",
    "language": "en-US",
    "page": "287-301",
    "title": "Weigh your words—memory-based lemmatization for Middle Dutch",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "URL": "http://www.mike-kestemont.org/pdf/benelearn2010.pdf",
    "abstract": "In this paper we highlight an aspect of previous research into lemmatization for Middle Dutch, a medieval language characterized by a lot of spelling variation. We briefly present a novel, memory-based learning method that assigns a similarity score to pairs of tokens. This method is based on assessing the “soundness” of a given class label, an untypical question in a kNN setting.",
    "author": [
      {
        "family": "Kestemont",
        "given": "Mike"
      },
      {
        "family": "Daelemans",
        "given": "Walter"
      },
      {
        "family": "De Pauw",
        "given": "Guy"
      }
    ],
    "container-title": "Proceedings of the 19<sup>th</sup> annual belgian-dutch conference on machine learning (benelearn 2010)",
    "id": "Kestemont2010b",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, dutch, spelling_normalization",
    "language": "en-US",
    "publisher-place": "Louvain (Belgium)",
    "title": "Space traveling: Assessing the “soundness” of class labels in memory-based learning and the case of Middle Dutch spelling variation",
    "title-short": "Space traveling",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/BF00054027",
    "abstract": "This article focuses on typographical spellchecking. Typographical spellchecking verifies the use of characters such as ? ! - ; :  $ @ and other special purpose characters in respect to spaces or null elements. The author claims that this kind of spellchecking has not been developed to any substantial degree, although it could be of considerable practical use as writer’s aid. The article discusses the basic challenges of typographical spellchecking and shows that some of the difficulties are greater than might be expected at first sight. Rules for describing the behavior of typographical characters are proposed.",
    "author": [
      {
        "family": "Kettunen",
        "given": "Kimmo"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Kettunen1996",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "interactive_editing, spelling_correction, typography",
    "language": "en-US",
    "publisher": "Springer",
    "title": "Low-level typographical spellchecking: A proposal",
    "title-short": "Low-level typographical spellchecking",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "DOI": "10.2200/s00862ed1v01y201807dtm048",
    "ISSN": "2153-5418",
    "abstract": "Large-scale, highly interconnected networks, which are often modeled as graphs, pervade both our society and the natural world around us. Uncertainty, on the other hand, is inherent in the underlying data due to a variety of reasons, such as noisy measurements, lack of precise information needs, inference and prediction models, or explicit manipulation, e.g., for privacy purposes. Therefore, uncertain, or probabilistic, graphs are increasingly used to represent noisy linked data in many emerging application scenarios, and they have recently become a hot topic in the database and data mining communities. Many classical algorithms such as reachability and shortest path queries become #P-complete and, thus, more expensive over uncertain graphs. Moreover, various complex queries and analytics are also emerging over uncertain networks, such as pattern matching, information diffusion, and influence maximization queries. In this book, we discuss the sources of uncertain graphs and their applications, uncertainty modeling, as well as the complexities and algorithmic advances on uncertain graphs processing in the context of both classical and emerging graph queries and analytics. We emphasize the current challenges and highlight some future research directions.",
    "author": [
      {
        "family": "Khan",
        "given": "Arijit"
      },
      {
        "family": "Ye",
        "given": "Yuan"
      },
      {
        "family": "Chen",
        "given": "Lei"
      }
    ],
    "collection-title": "Synthesis lectures on data management",
    "id": "Khan2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "keyword": "database, graphs, uncertainty",
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "publisher-place": "San Rafael, CA, USA",
    "title": "On uncertain graphs",
    "type": "book",
    "volume": "10"
  },
  {
    "DOI": "10.1145/162754.162882",
    "ISBN": "0-89791-567-4",
    "abstract": "Syntax-directed editing environments are designed for the development and maintenance of formal documents such as specifications, computer programs, and structured software documentation. The purpose of such environments is to provide a variety of facilities for the construction of the final documents in an integrated and easy to use manner. Based on this premise, the paper identifies the usability and design of syntax-directed editing environments as critical issues and investigates the factors contributing to these issues. A number of features are formulated for the design and/or the use of such environments. Some of the existing editing environments are analyzed using the formulated features and the result of the analysis is presented in a tabular fashion.",
    "author": [
      {
        "family": "Khwaja",
        "given": "Amir A."
      },
      {
        "family": "Urban",
        "given": "Joseph E."
      }
    ],
    "container-title": "SAC ’93: Proceedings of the 1993 ACM/SIGAPP symposium on applied computing",
    "id": "Khwaja1993",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "230-237",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Syntax-directed editing environments: issues and features",
    "title-short": "Syntax-directed editing environments",
    "type": "paper-conference"
  },
  {
    "URL": "http://hdl.handle.net/2060/19930013023",
    "abstract": "An application of fuzzy sets and Dempster Shafter Theory (DST) in modeling the interpretational process of organic geochemistry data for predicting the level of maturities of oil and source rock samples is presented. This was accomplished by (1) representing linguistic imprecision and imprecision associated with experience by a fuzzy set theory, (2) capturing the probabilistic nature of imperfect evidences by a DST, and (3) combining multiple evidences by utilizing John Yen’s generalized Dempster-Shafter Theory (GDST), which allows DST to deal with fuzzy information. The current prototype provides collective beliefs on the predicted levels of maturity by combining multiple evidences through GDST’s rule of combination.",
    "author": [
      {
        "family": "Kim",
        "given": "Charlie S."
      },
      {
        "family": "Isaksen",
        "given": "Gary H."
      }
    ],
    "container-title": "Proceedings of the third international workshop on neural networks and fuzzy logic",
    "id": "Kim1993",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "273-281",
    "publisher": "NASA",
    "title": "Application of fuzzy set and Dempster–Shafer theory to organic geochemistry interpretation",
    "type": "paper-conference",
    "volume": "2"
  },
  {
    "DOI": "10.1007/978-1-4757-1388-6_9",
    "ISBN": "978-1-4757-1390-9",
    "abstract": "A current research problem of great intellectual and commerical interest is the construction of interactive document processing systems that can easily handle the large variety of objects that typically appear in electronic and paper documents. These objects can be divided roughly into four classes: textual, tabular, mathematical, and pictorial. It has proven difficult to gracefully integrate the treatment of these different classes so that dissimilar objects may be used together, for example, mathematics within pictures, pictures within text, or text within mathematics inside a table. Ideally, there should be a common underlying structural model, similar editing and specification languages, and a uniform way to store, traverse, and display all objects.",
    "author": [
      {
        "family": "Kimura",
        "given": "Gary D."
      },
      {
        "family": "Shaw",
        "given": "Alan C."
      }
    ],
    "chapter-number": "9",
    "container-title": "Languages for automation",
    "editor": [
      {
        "family": "Chang",
        "given": "Shi-Kuo"
      }
    ],
    "id": "Kimura1985",
    "issued": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "keyword": "classic, document_engineering, document_research",
    "language": "en-US",
    "page": "179-196",
    "publisher": "Plenum",
    "publisher-place": "New York, NY, USA",
    "title": "The structure of abstract document objects",
    "type": "chapter"
  },
  {
    "ISBN": "0-387-50011-1",
    "URL": "http://portal.acm.org/citation.cfm?id=52850.52851",
    "author": [
      {
        "family": "King",
        "given": "Margaret"
      }
    ],
    "container-title": "Scientific symposium on syntax and semantics on natural language at the computer",
    "id": "King1988",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "computational_linguistics",
    "language": "en-US",
    "page": "9-30",
    "publisher": "Springer-Verlag New York, Inc.",
    "publisher-place": "New York, NY, USA",
    "title": "Computational linguistics: Issues and solutions",
    "title-short": "Computational linguistics",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Kingston",
        "given": "Jeffrey H."
      }
    ],
    "container-title": "Software: Practice and Experience",
    "id": "Kingston1993",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "page": "1001-­1041",
    "title": "The design and implementation of the Lout document formatting language",
    "type": "article-journal",
    "volume": "23"
  },
  {
    "URL": "http://dlist.sir.arizona.edu/1873/01/OpenCourseWare.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Kirkpatrick",
        "given": "Karie L."
      }
    ],
    "container-title": "Searcher",
    "id": "Kirkpatrick2006",
    "issue": "10",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "page": "53-58",
    "title": "OpenCourseWare: An “MIT thing”?",
    "title-short": "OpenCourseWare",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "URL": "https://newrepublic.com/article/117428/",
    "abstract": "The humanities are in crisis again, or still. But there is one big exception: digital humanities, which is a growth industry. In 2009, the nascent field was the talk of the Modern Language Association (MLA) convention: ” among all the contending subfields,” a reporter wrote about that year’s gathering, ” the digital humanities seem like the first ’next big thing’ in a long time.” Even earlier, the National Endowment for the Humanities created its Office of Digital Humanities to help fund projects. And digital humanities continues to go from strength to strength, thanks in part to the Mellon Foundation, which has seeded programs at a number of universities with large grants—most recently, $1 million to the University of Rochester to create a graduate fellowship.",
    "accessed": {
      "date-parts": [
        [
          2020,
          4,
          15
        ]
      ]
    },
    "author": [
      {
        "family": "Kirsch",
        "given": "Adam"
      }
    ],
    "container-title": "The New Republic",
    "id": "Kirsch2014",
    "issued": {
      "date-parts": [
        [
          2014,
          5,
          2
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "title": "Technology is taking over English departments",
    "type": "webpage"
  },
  {
    "DOI": "doi:10.1632/ade.150.55",
    "abstract": "What is (or are) the “digital humanities,” aka “humanities computing”? It’s tempting to say that whoever asks the question has not gone looking very hard for an answer. “What is digital humanities?” essays like this one are already genre pieces. Willard McCarty has been contributing papers on the subject for years (a monograph too). Under the earlier appellation, John Unsworth has advised us “what is humanities computing and what is not.” Most recently Patrik Svensson has been publishing a series of well-documented articles on multiple aspects of the topic, including the lexical shift from humanities computing to digital humanities. Moreover, as Cynthia Selfe in an ADE Bulletin from 1988 reminds us, computers have been part of our disciplinary lives for well over two decades now. During this time digital humanities has accumulated a robust professional apparatus that is probably more rooted in English than any other departmental home.",
    "author": [
      {
        "family": "Kirschenbaum",
        "given": "Matthew G."
      }
    ],
    "container-title": "ADE Bulletin",
    "id": "Kirschenbaum2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "55-61",
    "title": "What is digital humanities and what’s it doing in English departments?",
    "type": "article-journal",
    "volume": "150"
  },
  {
    "URL": "http://dhdebates.gc.cuny.edu/debates/text/38",
    "abstract": "What is (or are) the “digital humanities,” aka “humanities computing”? It’s tempting to say that whoever asks the question has not gone looking very hard for an answer. “What is digital humanities?” essays like this one are already genre pieces. Willard McCarty has been contributing papers on the subject for years (a monograph too). Under the earlier appellation, John Unsworth has advised us “what is humanities computing and what is not.” Most recently Patrik Svensson has been publishing a series of well-documented articles on multiple aspects of the topic, including the lexical shift from humanities computing to digital humanities. Moreover, as Cynthia Selfe in an ADE Bulletin from 1988 reminds us, computers have been part of our disciplinary lives for well over two decades now. During this time digital humanities has accumulated a robust professional apparatus that is probably more rooted in English than any other departmental home.",
    "author": [
      {
        "family": "Kirschenbaum",
        "given": "Matthew G."
      }
    ],
    "container-title": "Debates in the digital humanities",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      }
    ],
    "id": "Kirschenbaum2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "3-11",
    "publisher": "University of Minnesota Press",
    "publisher-place": "Minneapolis, MN, USA",
    "title": "What is digital humanities and what’s it doing in English departments?",
    "type": "chapter"
  },
  {
    "URL": "http://dhdebates.gc.cuny.edu/debates/text/48",
    "author": [
      {
        "family": "Kirschenbaum",
        "given": "Matthew G."
      }
    ],
    "container-title": "Debates in the digital humanities",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      }
    ],
    "id": "Kirschenbaum2012a",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "415-428",
    "publisher": "University of Minnesota Press",
    "publisher-place": "Minneapolis, MN, USA",
    "title": "Digital humanities as/is a tactical term",
    "type": "chapter"
  },
  {
    "DOI": "10.1215/10407391-2419997",
    "author": [
      {
        "family": "Kirschenbaum",
        "given": "Matthew G."
      }
    ],
    "container-title": "differences",
    "id": "Kirschenbaum2014",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "46-63",
    "title": "What is “digital humanities,” and why are they saying such terrible things about it?",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "DOI": "10.1145/571727.571736",
    "author": [
      {
        "family": "Kiselyov",
        "given": "Oleg"
      }
    ],
    "container-title": "ACM SIGPLAN Notices",
    "id": "Kiselyov2002",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "page": "52-58",
    "title": "SXML specification",
    "type": "article-journal",
    "volume": "37"
  },
  {
    "DOI": "10.1021/acscatal.5b00538",
    "author": [
      {
        "family": "Kitchin",
        "given": "John R."
      }
    ],
    "container-title": "ACS Catalysis",
    "id": "Kitchin2015",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          2015,
          5
        ]
      ]
    },
    "page": "3894-3899",
    "title": "Examples of effective data sharing in scientific publishing",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.1016/j.datak.2009.07.012",
    "ISSN": "0169023X",
    "abstract": "Enrichment of text documents with semantic metadata reflecting their meaning facilitates document organization, indexing and retrieval. However, most web data remain unstructured because of the difficulty and the cost of manually annotating text. In this work, we present Cerno, a framework for semi-automatic semantic annotation of textual documents according to a domain-specific semantic model. The proposed framework is founded on light-weight techniques and tools intended for legacy code analysis and markup. To illustrate the feasibility of our proposal, we report experimental results of its application to two different domains. These results suggest that light-weight semi-automatic techniques for semantic annotation are feasible, require limited human effort for adaptation to a new domain, and demonstrate markup quality comparable with state-of-the-art methods.",
    "author": [
      {
        "family": "Kiyavitskaya",
        "given": "Nadzeya"
      },
      {
        "family": "Zeni",
        "given": "Nicola"
      },
      {
        "family": "Cordy",
        "given": "James R."
      },
      {
        "family": "Mich",
        "given": "Luisa"
      },
      {
        "family": "Mylopoulos",
        "given": "John"
      }
    ],
    "container-title": "Data & Knowledge Engineering",
    "id": "Kiyavitskaya2009",
    "issue": "12",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "annotation, xml",
    "language": "en-US",
    "page": "1470-1492",
    "title": "Cerno: Light-weight tool support for semantic annotation of textual documents",
    "title-short": "Cerno",
    "type": "article-journal",
    "volume": "68"
  },
  {
    "author": [
      {
        "family": "Klaus",
        "given": "Georg"
      }
    ],
    "id": "Klaus1961",
    "issued": {
      "date-parts": [
        [
          1961
        ]
      ]
    },
    "keyword": "cybernetics",
    "language": "de-DE",
    "publisher": "Dietz",
    "title": "Kybernetik in philosophischer Sicht",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Klaus",
        "given": "Georg"
      },
      {
        "family": "Liebscher",
        "given": "Heinz"
      }
    ],
    "id": "Klaus1966a",
    "issued": {
      "date-parts": [
        [
          1966
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Urania",
    "title": "Was ist, was soll Kybernetik",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Klaus",
        "given": "Georg"
      }
    ],
    "id": "Klaus1966b",
    "issued": {
      "date-parts": [
        [
          1966
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "VEB Deutscher Verlag der Wissenschaften",
    "title": "Kybernetik und Erkenntnistheorie",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Kleimann",
        "given": "Bernd"
      },
      {
        "family": "Wannemacher",
        "given": "Klaus"
      }
    ],
    "genre": "HIS-Kurzinformation",
    "id": "Kleimann2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "number": "B3/2003",
    "publisher": "Hochschul-Informations-System",
    "publisher-place": "Hannover",
    "title": "Nachhaltigkeitsstrategien für E-Learning im Hochschulbereich",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Kleimann",
        "given": "Bernd"
      },
      {
        "family": "Wannemacher",
        "given": "Klaus"
      }
    ],
    "genre": "HIS-Kurzinformation",
    "id": "Kleimann2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "number": "B4/2005",
    "publisher": "Hochschul-Informations-System",
    "publisher-place": "Hannover",
    "title": "E-Learning-Strategien deutscher Universitäten: Fallbeispiele aus der Hochschulpraxis",
    "type": "report"
  },
  {
    "ISSN": "2331-7523",
    "URL": "https://ejournals.unm.edu/index.php/historicalgeography/article/view/2925",
    "author": [
      {
        "family": "Knowles",
        "given": "Anne Kelly"
      }
    ],
    "container-title": "Historical Geography",
    "id": "Knowles2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "digital_humanities, gis",
    "language": "en-US",
    "page": "7-13",
    "title": "Emerging trends in historical GIS",
    "type": "article-journal",
    "volume": "33"
  },
  {
    "URL": "http://www.smithsonianmag.com/history/1-180947921/",
    "abstract": "New technology has given us the chance to re-examine how the Civil War battle was won and lost.",
    "author": [
      {
        "family": "Knowles",
        "given": "Anne K."
      }
    ],
    "container-title": "SMITHSONIAN.COM",
    "id": "Knowles2013a",
    "issued": {
      "date-parts": [
        [
          2013,
          6,
          27
        ]
      ]
    },
    "keyword": "digital_humanities, simulation",
    "language": "en-US",
    "title": "A cutting-edge second look at the Battle of Gettysburg",
    "type": "article-journal"
  },
  {
    "author": [
      {
        "family": "Knuth",
        "given": "Donald E."
      }
    ],
    "id": "Knuth1973",
    "issued": {
      "date-parts": [
        [
          1973
        ]
      ]
    },
    "publisher": "Addison-Wesley",
    "publisher-place": "Reading, MA, USA",
    "title": "The art of computer programming",
    "type": "book",
    "volume": "3 (Sorting and Searching)"
  },
  {
    "ISBN": "9781575865843",
    "abstract": "Donald Knuth’s influence in computer science ranges from the invention of methods for translating and defining programming languages to the creation of the TEX and METAFONT systems for desktop publishing. His award-winning textbooks have become classics that are often given credit for shaping the field; his scientific papers are widely referenced and stand as milestones of development over a wide variety of topics. The present volume, which is the eighth and final book in his series of collected papers, is the one that he has saved up for dessert: It’s a potpourri devoted to recreational aspects of mathematics and computer science, filled with the works that gave him most pleasure during his 50-year career. Here you’ll find puzzles, paradoxes, and appealing patterns: visual, numerical, and musical. Nearly fifty of Knuth’s works are collected in this book, beginning with his famous first paper in MAD Magazine, and containing several similarly delightful spoofs written \"in a jugular vein.\" Knuth’s well-known introduction to the \"dancing links\" algorithm for combinatorial searches is accompanied by several chapters that shed new light on the age-old problem of knight’s tours on a chessboard. There are chapters about word games, computer games, and even basketball, together with topics of modern folk culture such as traffic signs and license plates. Seventeen of these chapters are being published for the first time; fourteen others have appeared only in publications of limited circulation that are difficult to find in libraries. All are found here, together with more than 700 newly created illustrations.",
    "author": [
      {
        "family": "Knuth",
        "given": "Donald E."
      }
    ],
    "collection-title": "CSLI lecture notes",
    "id": "Knuth2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "classic, interactive_fiction",
    "language": "en-US",
    "publisher": "CSLI Publications",
    "publisher-place": "Stanford, CA, USA",
    "title": "Selected papers on fun and games",
    "type": "book",
    "volume": "192"
  },
  {
    "URL": "http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-442",
    "accessed": {
      "date-parts": [
        [
          2009,
          1,
          25
        ]
      ]
    },
    "author": [
      {
        "family": "Knutsson",
        "given": "Ola"
      }
    ],
    "genre": "PhD thesis",
    "id": "Knutsson2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "publisher": "KTH",
    "publisher-place": "Stockholm",
    "title": "Developing and evaluating language tools for writers and learners of swedish",
    "type": "thesis"
  },
  {
    "DOI": "10.1145/1056808.1056965",
    "ISBN": "1-59593-002-7",
    "author": [
      {
        "family": "Ko",
        "given": "Andrew J."
      },
      {
        "family": "Aung",
        "given": "Htet H."
      },
      {
        "family": "Myers",
        "given": "Brad A."
      }
    ],
    "container-title": "CHI ’05: CHI ’05 extended abstracts on human factors in computing systems",
    "id": "Ko2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "1557-1560",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Design requirements for more flexible structured editors from a study of programmers’ text editing",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/996350.996435",
    "ISBN": "1-58113-832-6",
    "abstract": "We report on experience gained from our ongoing multi-year project to produce an Electronic Variorum Edition of Cervantes’ Don Quixote de la Mancha. Initially designed around a custom database representation, the project’s evolution has lead to the adoption of a TEI-based format for information interchange among the project’s major components. We discuss the mechanics of this approach and its benefits.",
    "author": [
      {
        "family": "Kochumman",
        "given": "Rajiv"
      },
      {
        "family": "Monroy",
        "given": "Carlos"
      },
      {
        "family": "Deng",
        "given": "Jie"
      },
      {
        "family": "Furuta",
        "given": "Richard"
      },
      {
        "family": "Urbina",
        "given": "Eduardo"
      }
    ],
    "container-title": "Proceedings of the 4th ACM/IEEE-CS joint conference on digital libraries (JCDL ’04)",
    "id": "Kochumman2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "cultural_heritage, interactive_editing, tei",
    "language": "en-US",
    "page": "368-369",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Tools for a new generation of scholarly edition unified by a TEI-based interchange format",
    "type": "paper-conference"
  },
  {
    "abstract": "Historisches Lernen und Lesen gehen Hand in Hand. Obwohl der Geschichtsunterricht ein ausgesprochenes Lesefach darstellt, sind die Verarbeitungsprozesse, die beim Lesen historischer Quellen und Darstellungen ablaufen, bisher nur unzureichend erforscht. Welchen Einfluss nehmen Identifikationsprozesse beim Lesen? Wie wird das Textverstehen von historischen Urteilen, von Vorwissen und Einstellungen der Schülerinnen und Schüler beeinflusst? Diesen Fragen geht die vorliegende Studie in einem interdisziplinären empirischen Zugriff am thematischen Beispiel Nationalsozialismus nach.",
    "author": [
      {
        "family": "Köster",
        "given": "Manuel"
      }
    ],
    "collection-title": "Geschichtskultur und historisches Lernen",
    "id": "Koester2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "history, uncertainty",
    "language": "de-DE",
    "publisher": "LIT",
    "publisher-place": "Münster",
    "title": "Historisches Textverstehen: Rezeption und Identifikation in der multiethnischen Gesellschaft",
    "title-short": "Historisches Textverstehen",
    "type": "book",
    "volume": "11"
  },
  {
    "DOI": "10.1016/j.ijar.2007.05.003",
    "ISSN": "0888613X",
    "abstract": "Statistical problems were at the origin of the mathematical theory of evidence, or Dempster–Shafer theory. It was also one of the major concerns of Philippe Smets, starting with his PhD dissertation. This subject is reconsidered here, starting with functional models, describing how data is generated in statistical experiments. Inference is based on these models, using probabilistic assumption-based reasoning. It results in posterior belief functions on the unknown parameters. Formally, the information used in the process of inference can be represented by hints. Basic operations on hints are combination, corresponding to Dempster’s rule, and focussing. This leads to an algebra of hints. Applied to functional models, this introduces an algebraic flavor into statistical inference. It emphasizes the view that in statistical inference different pieces of information have to be combined and then focussed onto the question of interest. This theory covers Bayesian and Fisher type inference as two extreme cases of a more general theory of inference.",
    "author": [
      {
        "family": "Kohlas",
        "given": "Jürg"
      },
      {
        "family": "Monney",
        "given": "Paul-André"
      }
    ],
    "container-title": "International Journal of Approximate Reasoning",
    "id": "Kohlas2008",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "378-398",
    "title": "An algebraic theory for statistical information based on the theory of hints",
    "type": "article-journal",
    "volume": "48"
  },
  {
    "DOI": "10.1007/978-3-642-00659-3_6",
    "abstract": "In the previous chapter Information Algebra, an algebraic structure capturing the idea that pieces of information refer to precise questions and that they can be combined and focussed on other questions is presented and discussed. A prototype of such information algebras is relational algebra. But also various kind of logic systems induce information algebras. In this chapter, this framework will be used to study uncertain information. It is often the case that a piece of information is known to be valid under certain assumptions, but it is not altogether sure that these assumptions really hold. Varying the assumptions leads to different information. Given such an uncertain body of information, assumption-based reasoning permits to deduce certain conclusions or to prove certain hypotheses under some assumptions. This kind of assumption-based inference can be carried further if the varying likelihood of different assumptions is described by a probability measure on the assumptions. Then, it is possible to measure the degree of support of a hypothesis by the probability that the assumptions supporting the hypothesis hold.",
    "author": [
      {
        "family": "Kohlas",
        "given": "Jürg"
      },
      {
        "family": "Eichenberger",
        "given": "Christian"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Formal theories of information",
    "editor": [
      {
        "family": "Sommaruga",
        "given": "Giovanni"
      }
    ],
    "id": "Kohlas2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "information_theory, uncertainty",
    "language": "en-US",
    "page": "128-160",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Uncertain information",
    "type": "chapter",
    "volume": "5363"
  },
  {
    "DOI": "10.3115/1073445.1073463",
    "abstract": "In this paper, we introduce a generative probabilistic optical character recognition (OCR) model that describes an end-to-end process in the noisy channel framework, progressing from generation of true text through its transformation into the noisy output of an OCR system. The model is designed for use in error correction, with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks. We present an implementation of the model based on finite-state models, demonstrate the model’s ability to significantly reduce character and word error rate, and provide evaluation results involving automatic extraction of translation lexicons from printed text.",
    "author": [
      {
        "family": "Kolak",
        "given": "Okan"
      },
      {
        "family": "Byrne",
        "given": "William"
      },
      {
        "family": "Resnik",
        "given": "Philip"
      }
    ],
    "collection-title": "NAACL ’03",
    "container-title": "Proceedings of the 2003 conference of the north american chapter of the association for computational linguistics on human language technology - volume 1",
    "id": "Kolak2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "ocr, spelling_correction",
    "language": "en-US",
    "page": "55-62",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "A generative probabilistic OCR model for NLP applications",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.linguatools.de/disco/disco_en.html",
    "author": [
      {
        "family": "Kolb",
        "given": "Peter"
      }
    ],
    "container-title": "KONVENS 2008 – ergänzungsband: Textressourcen und lexikalisches wissen",
    "editor": [
      {
        "family": "Storrer",
        "given": "Angelika"
      },
      {
        "family": "Geyken",
        "given": "Alexander"
      },
      {
        "family": "Siebert",
        "given": "Alexander"
      },
      {
        "family": "Würzner",
        "given": "Kay-Michael"
      }
    ],
    "id": "Kolb2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "computational_linguistics",
    "language": "en-US",
    "page": "37-44",
    "title": "DISCO: A multilingual database of distributionally similar words",
    "title-short": "DISCO",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1023/A:1025071200644",
    "abstract": "Abstract. The computation of the optimal phonetic alignment and the phonetic similarity between words is an important step in many applications in computational phonology, including dialectometry. After discussing several related algorithms, I present a novel approach to the problem that employs a scoring scheme for computing phonetic similarity between phonetic segments on the basis of multivalued articulatory phonetic features. The scheme incorporates the key concept of feature salience, which is necessary to properly balance the importance of various features. The new algorithm combines several techniques developed for sequence comparison: an extended set of edit operations, local and semiglobal modes of alignment, and the capability of retrieving a set of near-optimal alignments. On a set of 82 cognate pairs, it performs better than comparable algorithms reported in the literature.",
    "author": [
      {
        "family": "Kondrak",
        "given": "Grzegorz"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Kondrak2003",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "approximate_matching, cultural_heritage, phonetics",
    "language": "en-US",
    "page": "273-291",
    "title": "Phonetic alignment and similarity",
    "type": "article-journal",
    "volume": "37"
  },
  {
    "URL": "http://atala.org/IMG/pdf/TAL-2009-50-2-08-Kondrak.pdf",
    "abstract": "Identification of cognates and recurrent sound correspondences is a component of two principal tasks of historical linguistics: demonstrating the relatedness of languages, and reconstructing the histories of language families. We propose methods for detecting and quantifying three characteristics of cognates: recurrent sound correspondences, phonetic similarity, and semantic affinity. The ultimate goal is to identify cognates and correspondences directly from lists of words representing pairs of languages that are known to be related. The proposed solutions are language independent, and are evaluated against authentic linguistic data. The results of evaluation experiments involving the Indo-European, Algonquian, and Totonac language families indicate that our methods are more accurate than comparable programs, and achieve high precision and recall on various test sets. The results also suggest that combining various types of evidence substantially increases cognate identification accuracy.",
    "author": [
      {
        "family": "Kondrak",
        "given": "Grzegorz"
      }
    ],
    "container-title": "Traitement Automatique des Langues",
    "id": "Kondrak2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "page": "201-235",
    "title": "Identification of cognates and recurrent sound correspondences in word lists",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=1642049.1642051",
    "abstract": "In this paper we describe an authoring environment for the creation of cultural-domain ontologies and the associated linguistic and profile annotations, for dynamically generating adaptable natural-language descriptions of the cultural objects in the ontology. Adaptation is achieved at the expense of considerable authoring effort, since it relies on providing numerical parameters for each ontological entity. To assist the authoring process, we provide an intelligent authoring back-end that completes manually authored models by inferring missing values. This intelligent authoring support facility, combined with immediate previews, can considerably reduce the effort required to create a fully functional model as the author can iterate through cycles of providing information, previewing the generated text, and only elaborating the model where the text is unsatisfactory.",
    "author": [
      {
        "family": "Konstantopoulos",
        "given": "Stasinos"
      },
      {
        "family": "Karkaletsis",
        "given": "Vangelis"
      },
      {
        "family": "Bilidas",
        "given": "Dimitris"
      }
    ],
    "container-title": "LaTeCH-SHELTR ’09: Proceedings of the EACL 2009 workshop on language technology and resources for cultural heritage, social sciences, humanities, and education",
    "id": "Konstantonopoulos2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "page": "10-17",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Morristown, NJ, USA",
    "title": "An intelligent authoring environment for abstract semantic representations of cultural object descriptions",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/11735106_36",
    "abstract": "Our cultural heritage, as preserved in libraries, archives and museums, is made up of documents written many centuries ago. Large-scale digitization initiatives make these documents available to non-expert users through digital libraries and vertical search engines. For a user, querying a historic document collection may be a disappointing experience: queries involving modern words may not be very effective for retrieving documents that contain many historic terms. We propose a cross-language approach to historic document retrieval, and investigate (1) the automatic construction of translation resources for historic languages, and (2) the retrieval of historic documents using cross-language information retrieval techniques. Our experimental evidence is based on a collection of 17th century Dutch documents and a set of 25 known-item topics in modern Dutch. Our main findings are as follows: First, we are able to automatically construct rules for modernizing historic language based on comparing (a) phonetic sequence similarity, (b) the relative frequency of consonant and vowel sequences, and (c) the relative frequency of character n-gram sequences, of historic and modern corpora. Second, modern queries are not very effective for retrieving historic documents, but the historic language tools lead to a substantial improvement in retrieval effectiveness. The improvements are above and beyond the improvement due to using a modern stemming algorithm (whose effectiveness actually goes up when the historic language is modernized).",
    "author": [
      {
        "family": "Koolen",
        "given": "Marijn"
      },
      {
        "family": "Adriaans",
        "given": "Frans"
      },
      {
        "family": "Kamps",
        "given": "Jaap"
      },
      {
        "dropping-particle": "de",
        "family": "Rijke",
        "given": "Maarten"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Advances in information retrieval",
    "editor": [
      {
        "family": "Lalmas",
        "given": "Mounia"
      },
      {
        "family": "MacFarlane",
        "given": "Andy"
      },
      {
        "family": "Rüger",
        "given": "Stefan"
      },
      {
        "family": "Tombros",
        "given": "Anastasios"
      },
      {
        "family": "Tsikrika",
        "given": "Theodora"
      },
      {
        "family": "Yavlinsky",
        "given": "Alexei"
      }
    ],
    "id": "Koolen2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage, dutch, ir",
    "language": "en-US",
    "page": "407-419",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "A cross-language approach to historic document retrieval",
    "type": "chapter",
    "volume": "3936"
  },
  {
    "DOI": "10.1093/llc/fqx016",
    "author": [
      {
        "family": "Koolen",
        "given": "Corina"
      },
      {
        "dropping-particle": "van",
        "family": "Cranenburgh",
        "given": "Andreas"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Koolen2018",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "page": "59-71",
    "title": "Blue eyes and porcelain cheeks: Computational extraction of physical descriptions from Dutch chick lit and literary novels",
    "title-short": "Blue eyes and porcelain cheeks",
    "type": "article-journal",
    "volume": "33"
  },
  {
    "abstract": "By computer-aided instruction (CAI) we mean the use of a computer application for instructing a specific subject of a domain. The essential question is what CAI applications are good and why. In order to be able to evaluate a CAI application it is necessary to fix the domain. The domain of this work is computer science at the university level. Evaluation of CAI applications is a complex process in which several perspectives have to be considered. We developed evaluation criteria that consist of four parts, namely, domain-based criteria, instructional criteria, user interface criteria, and pragmatic criteria. Our domain-based criteria focus on evaluating the course contents and their relevancy to the instructional aims of the CAI course. We examined several human learning theories in order to find the most appropriate one to be the basis of our instructional criteria, which concentrate on evaluating the educational support. User interface criteria and pragmatic criteria focus on evaluating the implementation of the user interface and the practical matters, such as hardware, software and human resources, respectively. We designed an analysis method based on our criteria for testing the evaluation criteria in practice. The analysis method was applied to a collection of CAI courses on computer science, called COSTOC (COmputer Supported Teaching Of Computer science). The analysis indicated that our criteria worked properly. The main results were that the contents of a CAI course have to be designed by an expert of the domain and that the instructional support should be based on human learning. Further, authoring tools should support CAI authors by offering instructional advice. In addition, authoring CAI courses on computer science requires some special properties of the authoring tool, such as possibilities to speak mathematical language, to animate abstract structures, and to write and execute pseudo or programming code.",
    "author": [
      {
        "family": "Kopponen",
        "given": "Maria"
      }
    ],
    "genre": "PhD thesis",
    "id": "Kopponen1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "University of Joensuu",
    "publisher-place": "Joensuu, Finland",
    "title": "CAI in CS",
    "type": "thesis"
  },
  {
    "ISBN": "059610121X",
    "abstract": "Fundamentally, computers just deal with numbers. They store letters and other characters by assigning a number for each one. There are hundreds of different encoding systems for mapping characters to numbers, but Unicode promises a single mapping. Unicode enables a single software product or website to be targeted across multiple platforms, languages and countries without re-engineering. It’s no wonder that industry giants like Apple, Hewlett-Packard, IBM and Microsoft have all adopted Unicode. Containing everything you need to understand Unicode, this comprehensive reference from O’Reilly takes you on a detailed guide through the complex character world. For starters, it explains how to identify and classify characters - whether they’re common, uncommon, or exotic. It then shows you how to type them, utilize their properties, and process character data in a robust manner. The book is broken up into three distinct parts. The first few chapters provide you with a tutorial presentation of Unicode and character data. It gives you a firm grasp of the terminology you need to reference various components, including character sets, fonts and encodings, glyphs and character repertoires. The middle section offers more detailed information about using Unicode and other character codes. It explains the principles and methods of defining character codes, describes some of the widely used codes, and presents code conversion techniques. It also discusses properties of characters, collation and sorting, line breaking rules and Unicode encodings. The final four chapters cover more advanced material, such as programming to support Unicode. You simply can’t afford to be without the nuggets of valuable information detailed in Unicode Explained.",
    "author": [
      {
        "family": "Korpela",
        "given": "Jukka K."
      }
    ],
    "id": "Korpela2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "unicode",
    "language": "en-US",
    "publisher": "O’Reilly",
    "publisher-place": "Sebastopol, CA, USA",
    "title": "Unicode explained",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Koskenniemi",
        "given": "Kimmo"
      },
      {
        "family": "Haapalainen",
        "given": "Mariikka"
      }
    ],
    "chapter-number": "11",
    "container-title": "Linguistische verifikation. Dokumentation zur ersten morpholympics 1994",
    "editor": [
      {
        "family": "Hausser",
        "given": "Roland"
      }
    ],
    "id": "Koskenniemi1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "morphology, nlp",
    "language": "en-US",
    "page": "121-140",
    "publisher": "Niemeyer",
    "publisher-place": "Tübingen",
    "title": "GERTWOL – Lingsoft Oy",
    "type": "chapter"
  },
  {
    "editor": [
      {
        "family": "Kovalchick",
        "given": "Ann"
      },
      {
        "family": "Dawson",
        "given": "Kara"
      }
    ],
    "id": "Kovalchick2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "publisher": "ABC-CLIO",
    "publisher-place": "Santa Barbara, CA, USA",
    "title": "Education and technology: An encyclopedia",
    "title-short": "Education and technology",
    "type": "book"
  },
  {
    "URL": "https://ewic.bcs.org/content/ConWebDoc/50996",
    "abstract": "Historic time is inherently uncertain, with missing, disputed, doubtful or ill-defined dates for objects and events. Yet digital timeline visualisations tend to represent it as exact and of undisputed confidence. This is partly due to a lack of awareness, partly due to a lack of support for ambiguities in digital data types and modes of representation. We outline some of the issues around uncertainty in timeline visualisation and ways of addressing them.",
    "author": [
      {
        "family": "Kräutli",
        "given": "Florian"
      },
      {
        "family": "Boyd Davis",
        "given": "Stephen"
      }
    ],
    "container-title": "Proceedings of electronic visualisation and the arts 2013",
    "id": "Kraeutli2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities, uncertainty",
    "language": "en-US",
    "page": "61-68",
    "publisher": "British Computer Society",
    "title": "Known unknowns: Representing uncertainty in historical time",
    "title-short": "Known unknowns",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1186/gb-2008-9-s2-s8",
    "ISSN": "1465-6906",
    "PMCID": "PMC2559992",
    "PMID": "18834499",
    "abstract": "Efficient access to information contained in online scientific literature collections is essential for life science research, playing a crucial role from the initial stage of experiment planning to the final interpretation and communication of the results. The biological literature also constitutes the main information source for manual literature curation used by expert-curated databases. Following the increasing popularity of web-based applications for analyzing biological data, new text-mining and information extraction strategies are being implemented. These systems exploit existing regularities in natural language to extract biologically relevant information from electronic texts automatically. The aim of the BioCreative challenge is to promote the development of such tools and to provide insight into their performance. This review presents a general introduction to the main characteristics and applications of currently available text-mining systems for life sciences in terms of the following: the type of biological information demands being addressed; the level of information granularity of both user queries and results; and the features and methods commonly exploited by these applications. The current trend in biomedical text mining points toward an increasing diversification in terms of application types and techniques, together with integration of domain-specific resources such as ontologies. Additional descriptions of some of the systems discussed here are available on the internet http://zope.bioinfo.cnio.es/bionlp_tools/.",
    "author": [
      {
        "family": "Krallinger",
        "given": "Martin"
      },
      {
        "family": "Valencia",
        "given": "Alfonso"
      },
      {
        "family": "Hirschman",
        "given": "Lynette"
      }
    ],
    "container-title": "Genome Biology",
    "id": "Krallinger2008",
    "issue": "Suppl 2",
    "issued": {
      "date-parts": [
        [
          2008,
          9,
          1
        ]
      ]
    },
    "keyword": "bionlp, ir",
    "language": "en-US",
    "page": "S8.1-S8.14",
    "publisher": "BioMed Central Ltd",
    "title": "Linking genes to literature: Text mining, information extraction, and retrieval applications for biology",
    "title-short": "Linking genes to literature",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "DOI": "10.2307/1315198",
    "ISSN": "07425562",
    "author": [
      {
        "family": "Krause",
        "given": "Steven D."
      }
    ],
    "container-title": "The Journal of the Midwest Modern Language Association",
    "id": "Krause2000",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "interactive_editing, writing_research",
    "language": "en-US",
    "page": "6-16",
    "publisher": "Midwest Modern Language Association",
    "title": "\"Among the greatest benefactors of mankind\": What the success of chalkboards tells us about the future of computers in the classroom",
    "title-short": "\"Among the greatest benefactors of mankind\"",
    "type": "article-journal",
    "volume": "33"
  },
  {
    "DOI": "10.1093/llc/fqu057",
    "ISSN": "2055-7671",
    "abstract": "This article is concerned with the data structures, properties of query languages, and visualization facilities required for the generic representation of richly annotated, heterogeneous linguistic corpora. We propose that above and beyond a general graph-based data model, which is becoming increasingly popular in many complex annotation formats, a well-defined concept of multiple, potentially conflicting segmentation layers must be introduced to deal with different sources and applications of corpus data flexibly. We also propose a generic solution for specialized corpus visualizations in a Web interface using annotation-triggered style sheets, which leverage the power of modern browsers and CSS for multiple and highly customizable views of primary data. We offer an implementation and evaluation of our architecture in ANNIS3, an open-source browser-based architecture for corpus search and visualization. We present three case studies to test the coverage of the system, encompassing core linguistic and digital humanities use-cases including richly annotated newspaper treebanks, multilingual diplomatic and normalized manuscript materials edited in TEI, and analysis of multimodal recordings of spoken language.",
    "author": [
      {
        "family": "Krause",
        "given": "Thomas"
      },
      {
        "family": "Zeldes",
        "given": "Amir"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Krause2016",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "annotation, corpus_linguistics, database, digital_edition, digital_humanities, tei",
    "language": "en-US",
    "page": "118-139",
    "title": "ANNIS3: A new architecture for generic corpus query and visualization",
    "title-short": "ANNIS3",
    "type": "article-journal",
    "volume": "31"
  },
  {
    "URL": "http://elda.org/article48.html",
    "abstract": "In this article we give a brief overview of ELSNET and its activities, show where ELSNET’s and ELRA’s interests intersect, and sketch a possible joint action in the field of language resources, entitled BLARK, to be initiated under the European Commission’s Fifth Framework Programme.",
    "author": [
      {
        "family": "Krauwer",
        "given": "Steven"
      }
    ],
    "container-title": "ELRA Newsletter",
    "id": "Krauwer1998",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "blark, nlp",
    "language": "en-US",
    "title": "ELSNET and ELRA: A common past and a common future",
    "title-short": "ELSNET and ELRA",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "URL": "http://www.elsnet.org/dox/krauwer-specom2003.pdf",
    "abstract": "The problem addressed by this paper is the fact that many languages, especially the languages of little or no commercial interest are lagging behind with respect to the development and the use of language and speech technology. The definition, adoption and implementation of a standard Basic Language Resource Kit (BLARK) for all languages, irrespective of their size or importance, should help to create better starting conditions for research, education and development in language and speech technology. A coordinated effort in this direction should help reducing the burden of arriving at a definition of a minimal set of resources required to do any work at all for each individual language, it should facilitate porting of insights and expertise between languages, and it should help ensuring interoperability and interconnectivity.",
    "author": [
      {
        "family": "Krauwer",
        "given": "Steven"
      }
    ],
    "container-title": "Proceedings of the 2003 international workshop speech and computer (SPECOM 2003)",
    "id": "Krauwer2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "blark, less-resourced_languages",
    "language": "en-US",
    "page": "8-15",
    "publisher": "Moscow State Linguistic University",
    "title": "The Basic Language Resource Kit (BLARK) as the first milestone for the language resources roadmap",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.iawf.unibe.ch/aae/documents/methodik/mc_anl_med.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Krebs",
        "given": "René"
      }
    ],
    "id": "Krebs2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "publisher": "Universität Bern",
    "publisher-place": "Bern",
    "title": "Anleitung zur Herstellung von MC-Fragen und MC-Prüfungen für die ärztliche Ausbildung",
    "type": "book"
  },
  {
    "DOI": "10.7717/peerj-cs.112",
    "ISSN": "2376-5992",
    "author": [
      {
        "family": "Krewinkel",
        "given": "Albert"
      },
      {
        "family": "Winkler",
        "given": "Robert"
      }
    ],
    "container-title": "PeerJ Computer Science",
    "id": "Krewinkel2017",
    "issued": {
      "date-parts": [
        [
          2017,
          5
        ]
      ]
    },
    "page": "e112",
    "title": "Formatting Open Science: Agilely creating multiple document formats for academic manuscripts with Pandoc Scholar",
    "title-short": "Formatting Open Science",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.1007/978-3-540-85845-4_31",
    "ISBN": "978-3-540-85844-7",
    "ISSN": "0302-9743",
    "abstract": "Representing temporally-changing information becomes increasingly important for reasoning & query services defined on top of RDF and OWL, and for the Semantic Web/Web 2.0 in general. Extending binary OWL properties or RDF triples with a further temporal argument either lead to additional objects or to reification, as [1] have shown.",
    "author": [
      {
        "family": "Krieger",
        "given": "Hans-Ulrich"
      }
    ],
    "chapter-number": "31",
    "collection-title": "Lecture notes in computer science",
    "container-title": "KI 2008: Advances in artificial intelligence",
    "editor": [
      {
        "family": "Dengel",
        "given": "Andreas R."
      },
      {
        "family": "Berns",
        "given": "Karsten"
      },
      {
        "family": "Breuel",
        "given": "Thomas M."
      },
      {
        "family": "Bomarius",
        "given": "Frank"
      },
      {
        "family": "Roth-Berghofer",
        "given": "Thomas R."
      }
    ],
    "id": "Krieger2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "rdf, temporal_data",
    "language": "en-US",
    "page": "249-257",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Where temporal description logics fail: Representing Temporally-Changing relationships",
    "title-short": "Where temporal description logics fail",
    "type": "chapter",
    "volume": "5243"
  },
  {
    "URL": "http://ceur-ws.org/Vol-1399/paper16.pdf",
    "abstract": "Representing time-dependent information has become increasingly important for reasoning and querying services defined on top of RDF and OWL. In particular, addressing this task properly is vital for practical applications such as modern biographical information systems, but also for the Semantic Web/Web 2.0/Social Web in general. Extending binary relation instances with temporal information often translates into a massive proliferation of useless container objects when trying to keep the underlying RDF model. In this paper, we argue for directly extending RDF triples with further arguments in order to easily represent time-dependent factual knowledge and to allow for practical forms of reasoning. We also report on a freely available lightweight OWL ontology for representing biographical knowledge that models entities of interest via a tri-partite structure of the pairwise disjoint classes Abstract, Object, and Happening. Even though the ontology was manually developed utilizing the Prote ́ge ́ ontology editor, and thus sticking to the triple model of RDF, the meta-modelling facilities allowed us to cross-classify all properties as being either synchronic or diachronic. When viewing the temporal arguments as “extra” arguments that only apply to relation instances, universal biographical knowledge from the ontology can still be described as if there is no time.",
    "author": [
      {
        "family": "Krieger",
        "given": "Hans-Ulrich"
      },
      {
        "family": "Declerck",
        "given": "Thierry"
      }
    ],
    "container-title": "Biographical data in a digital world 2015. Proceedings of the first conference",
    "editor": [
      {
        "family": "Braake",
        "given": "Serge T."
      },
      {
        "family": "Fokkens",
        "given": "Antske"
      },
      {
        "family": "Sluijter",
        "given": "Ronald"
      },
      {
        "family": "Declerck",
        "given": "Thierry"
      },
      {
        "family": "Wandl-Vogt",
        "given": "Eveline"
      }
    ],
    "id": "Krieger2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities, temporal_data",
    "language": "en-US",
    "page": "101-110",
    "title": "An OWL ontology for biographical knowledge. Representing time-dependent factual knowledge",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Krinke",
        "given": "Jens"
      },
      {
        "family": "Störzer",
        "given": "Maximilian"
      },
      {
        "family": "Zeller",
        "given": "Andreas"
      }
    ],
    "container-title": "Proceedings des workshop neue medien in der informatik-lehre",
    "id": "Krinke2002",
    "issued": {
      "date-parts": [
        [
          2002,
          10
        ]
      ]
    },
    "publisher-place": "Dortmund",
    "title": "Web-basierte programmierpraktika mit praktomat",
    "type": "paper-conference"
  },
  {
    "URL": "http://eprints.ncrm.ac.uk/783/",
    "abstract": "This paper examines disciplines and disciplinarity through the lenses of certain academic disciplines including philosophy, anthropology, sociology, history, management and education.",
    "author": [
      {
        "family": "Krishnan",
        "given": "Armin"
      }
    ],
    "genre": "NCRM Working Paper",
    "id": "Krishnan2009",
    "issued": {
      "date-parts": [
        [
          2009,
          1
        ]
      ]
    },
    "keyword": "philosophy_of_science",
    "language": "en-US",
    "number": "03/09",
    "publisher": "ESRC National Centre for Research Methods",
    "publisher-place": "Southampton, UK",
    "title": "What are academic disciplines? Some observations on the disciplinarity vs. Interdisciplinarity debate",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Kroeber",
        "given": "Karl"
      }
    ],
    "chapter-number": "13",
    "container-title": "Computers in humanistic research: Readings and perspectives",
    "editor": [
      {
        "family": "Bowles",
        "given": "Edmund A."
      }
    ],
    "id": "Kroeber1967",
    "issued": {
      "date-parts": [
        [
          1967
        ]
      ]
    },
    "language": "en-US",
    "page": "135-142",
    "publisher": "Prentice-Hall",
    "publisher-place": "Englewood Cliffs, NJ, USA",
    "title": "Computers and research in literary analysis",
    "type": "chapter"
  },
  {
    "URL": "http://www.ifib.de/publikationsdateien/MMKH_Endbericht_2004-05-26.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Kubicek",
        "given": "Herbert"
      },
      {
        "family": "Breiter",
        "given": "Andreas"
      },
      {
        "family": "Fischer",
        "given": "Arne"
      },
      {
        "family": "Wiedwald",
        "given": "Christian"
      }
    ],
    "id": "Kubicek2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "publisher": "Institut für Informationsmanagement",
    "publisher-place": "Bremen, Germany",
    "title": "Organisatorische Einbettung von E-Learning an deutschen Hochschulen",
    "type": "report"
  },
  {
    "URL": "http://jones.ling.indiana.edu/\\~{}skuebler/papers/acidca.ps",
    "abstract": "In this paper, we investigate the role of sub-optimality in training data for part-of-speech tagging. In particular, we examine to what extent the size of the training corpus and certain types of errors in it affect the performance of the tagger. We distinguish four types of errors: If a word is assigned a wrong tag, this tag can belong to the ambiguity class of the word (i.e. to the set of possible tags for that word) or not; furthermore, the major syntactic category (e.g. “N” or “V”) can be correctly assigned (e.g. if a finite verb is classified as an infinitive) or not (e.g. if a verb is classified as a noun). We empirically explore the decrease of performance that each of these error types causes for different sizes of the training set. Our results show that those types of errors that are easier to eliminate have a particularly negative effect on the performance. Thus, it is worthwhile concentrating on the elimination of these types of errors, especially if the training corpus is large.",
    "author": [
      {
        "family": "Kübler",
        "given": "Sandra"
      },
      {
        "family": "Wagner",
        "given": "Andreas"
      }
    ],
    "id": "Kuebler2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "german, pos_tagging",
    "language": "en-US",
    "title": "Evaluating POS tagging under sub-optimal conditions. Or: Does meticulousness pay?",
    "title-short": "Evaluating POS tagging under sub-optimal conditions. Or",
    "type": "article-journal"
  },
  {
    "URL": "http://aclweb.org/anthology/W08-1008",
    "accessed": {
      "date-parts": [
        [
          2009,
          1,
          26
        ]
      ]
    },
    "author": [
      {
        "family": "Kübler",
        "given": "Sandra"
      }
    ],
    "container-title": "Proceedings of the workshop on parsing german",
    "id": "Kuebler2008",
    "issued": {
      "date-parts": [
        [
          2008,
          6
        ]
      ]
    },
    "page": "55-63",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Columbus, OH, USA",
    "title": "The PaGe 2008 shared task on parsing German",
    "type": "paper-conference"
  },
  {
    "URL": "http://lotos.library.uu.nl/publish/articles/000259/bookpart.pdf",
    "accessed": {
      "date-parts": [
        [
          2009,
          1,
          25
        ]
      ]
    },
    "author": [
      {
        "family": "Kübler",
        "given": "Sandra"
      },
      {
        "family": "Rehbein",
        "given": "Ines"
      },
      {
        "dropping-particle": "van",
        "family": "Genabith",
        "given": "Josef"
      }
    ],
    "container-title": "Proceedings of the 7<sup>th</sup> international workshop on treebanks and linguistic theories (TLT 7)",
    "id": "Kuebler2009",
    "issued": {
      "date-parts": [
        [
          2009,
          1
        ]
      ]
    },
    "page": "15-28",
    "publisher-place": "Groningen, The Netherlands",
    "title": "TePaCoC – a testsuite for testing parser performance on complex German grammatical constructions",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Kuhn",
        "given": "Tobias"
      },
      {
        "family": "Schwitter",
        "given": "Rolf"
      }
    ],
    "container-title": "Proceedings of ALTA 2008",
    "editor": [
      {
        "family": "Powers",
        "given": "David"
      },
      {
        "family": "Stockes",
        "given": "Nicola"
      }
    ],
    "id": "Kuhn2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "page": "46-54",
    "publisher-place": "Tasmania, Australia",
    "title": "Writing support for controlled natural languages",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-38288-8_33",
    "URL": "https://arxiv.org/abs/1303.2446",
    "abstract": "In this paper, we present an approach for extending the existing concept of nanopublications—tiny entities of scientific results in RDF representation—to broaden their application range. The proposed extension uses English sentences to represent informal and underspecified scientific claims. These sentences follow a syntactic and semantic scheme that we call AIDA (Atomic, Independent, Declarative, Absolute), which provides a uniform and succinct representation of scientific assertions. Such AIDA nanopublications are compatible with the existing nanopublication concept and enjoy most of its advantages such as information sharing, interlinking of scientific findings, and detailed attribution, while being more flexible and applicable to a much wider range of scientific results. We show that users are able to create AIDA sentences for given scientific results quickly and at high quality, and that it is feasible to automatically extract and interlink AIDA nano publications from existing unstructured data sources. To demonstrate our approach, a web-based interface is introduced, which also exemplifies the use of nano publications for non-scientific content, including meta-nanopublications that describe other nanopublications.",
    "author": [
      {
        "family": "Kuhn",
        "given": "Tobias"
      },
      {
        "family": "Barbano",
        "given": "Paolo E."
      },
      {
        "family": "Nagy",
        "given": "Mate L."
      },
      {
        "family": "Krauthammer",
        "given": "Michael"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Proceedings of the 10<sup>th</sup> extended semantic web conference (ESWC 2013)",
    "id": "Kuhn2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "nanopublications, rdf, semantic_web",
    "language": "en-US",
    "page": "487-501",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Broadening the scope of nanopublications",
    "type": "paper-conference",
    "volume": "7882"
  },
  {
    "DOI": "10.1007/978-3-319-07443-6_27",
    "abstract": "To make digital resources on the web verifiable, immutable, and permanent, we propose a technique to include cryptographic hash values in URIs. We call them trusty URIs and we show how they can be used for approaches like nanopublications to make not only specific resources but their entire reference trees verifiable. Digital artifacts can be identified not only on the byte level but on more abstract levels such as RDF graphs, which means that resources keep their hash values even when presented in a different format. Our approach sticks to the core principles of the web, namely openness and decentralized architecture, is fully compatible with existing standards and protocols, and can therefore be used right away. Evaluation of our reference implementations shows that these desired properties are indeed accomplished by our approach, and that it remains practical even for very large files.",
    "author": [
      {
        "family": "Kuhn",
        "given": "Tobias"
      },
      {
        "family": "Dumontier",
        "given": "Michel"
      }
    ],
    "collection-number": "8465",
    "collection-title": "Lecture notes in computer science",
    "container-title": "The semantic web: Trends and challenges. 11th international conference, ESWC 2014, anissaras, crete, greece, may 25–29, 2014, proceedings",
    "editor": [
      {
        "family": "Presutti",
        "given": "Valentina"
      },
      {
        "family": "Amato",
        "given": "Claudia",
        "non-dropping-particle": "d’"
      },
      {
        "family": "Gandon",
        "given": "Fabien"
      },
      {
        "family": "Aquin",
        "given": "Mathieu",
        "non-dropping-particle": "d’"
      },
      {
        "family": "Staab",
        "given": "Steffen"
      },
      {
        "family": "Tordai",
        "given": "Anna"
      }
    ],
    "id": "Kuhn2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "nanopublications, semantic_web",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Trusty URIs: Verifiable, immutable, and permanent digital artifacts for linked data",
    "title-short": "Trusty URIs",
    "type": "paper-conference"
  },
  {
    "URL": "http://tinyurl.com/pl74co4",
    "abstract": "This position paper advocates joint efforts towards establishing a modular architecture for ” deep” analytical approaches to DH collections such as text corpora. The core idea is to augment a typical DH project agenda with an element of project-independent and interdisciplinary bootstrapping, aiming to identify analytical subtasks beyond standard processing steps, which can be used as building blocks across different complex DH modeling processes (adopting McCarty’s (2005) concept of modeling as a transition through temporary, provisional stages). Our discussion focuses on text-oriented DH projects.",
    "author": [
      {
        "family": "Kuhn",
        "given": "Jonas"
      },
      {
        "family": "Reiter",
        "given": "Nils"
      }
    ],
    "container-title": "Proceeding of digital humanities 2015",
    "id": "Kuhn2015",
    "issued": {
      "date-parts": [
        [
          2015,
          6
        ]
      ]
    },
    "keyword": "creta, cretapubs, digital_humanities",
    "language": "en-US",
    "title": "A plea for a Method-Driven agenda in the digital humanities",
    "type": "paper-conference"
  },
  {
    "DOI": "10.3233/ds-170010",
    "ISSN": "2451-8492",
    "abstract": "Various approaches and systems have been presented in the context of scholarly communication for what has been called semantic publishing. Closer inspection, however, reveals that these approaches are mostly not about publishing semantic representations, as the name seems to suggest. Rather, they take the processes and outcomes of the current narrative-based publishing system for granted and only work with already published papers. This includes approaches involving semantic annotations, semantic interlinking, semantic integration, and semantic discovery, but with the semantics coming into play only after the publication of the original article. While these are interesting and important approaches, they fall short of providing a vision to transcend the current publishing paradigm. We argue here for taking the term semantic publishing literally and work towards a vision of genuine semantic publishing, where computational tools and algorithms can help us with dealing with the wealth of human knowledge by letting researchers capture their research results with formal semantics from the start, as integral components of their publications. We argue that these semantic components should furthermore cover at least the main claims of the work, that they should originate from the authors themselves, and that they should be fine-grained and light-weight for optimized re-usability and minimized publication overhead. This paper is in fact not just advocating our concept, but is itself a genuine semantic publication, thereby demonstrating and illustrating our points.",
    "author": [
      {
        "family": "Kuhn",
        "given": "Tobias"
      },
      {
        "family": "Dumontier",
        "given": "Michel"
      }
    ],
    "container-title": "Data Science",
    "id": "Kuhn2017",
    "issue": "1–2",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "nanopublications, scientific_publishing, semantic_web",
    "language": "en-US",
    "page": "139-154",
    "title": "Genuine semantic publishing",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.1145/146370.146380",
    "ISSN": "0360-0300",
    "abstract": "Research aimed at correcting words in text has focused on three progressively more difficult problems:(1) nonword error detection; (2) isolated-word error correction; and (3) context-dependent work correction. In response to the first problem, efficient pattern-matching and n-gram analysis techniques have been developed for detecting strings that do not appear in a given word list. In response to the second problem, a variety of general and application-specific spelling correction techniques have been developed. Some of them were based on detailed studies of spelling error patterns. In response to the third problem, a few experiments using natural-language-processing tools or statistical-language models have been carried out. This article surveys documented findings on spelling error patterns, provides descriptions of various nonword detection and isolated-word error correction techniques, reviews the state of the art of context-dependent word correction techniques, and discusses research issues related to all three areas of automatic error correction in text.",
    "author": [
      {
        "family": "Kukich",
        "given": "Karen"
      }
    ],
    "container-title": "ACM Computing Surveys",
    "id": "Kukich1992",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "classic, spelling_correction",
    "language": "en-US",
    "page": "377-439",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Techniques for automatically correcting words in text",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "DOI": "10.2307/1170294",
    "abstract": "This review used Glass’ (1976) meta-analytic techniques to integrate findings from 59 independent evaluations of computer-based college teaching. The meta-analysis showed that computer-based instruction made small but significant contributions to the course achievement of college students and also produced positive, but again small, effects on the attitudes of students toward instruction and toward the subject matter they were studying. Computer-assisted instruction also reduced substantially the amount of time needed for instruction. In general, the meta-analysis found little relationship between study findings and design features of the experiments, settings for the studies, or manner and date of publication of the findings.",
    "author": [
      {
        "family": "Kulik",
        "given": "James A."
      },
      {
        "family": "Chen"
      },
      {
        "family": "Cohen",
        "given": "Peter A."
      }
    ],
    "container-title": "Review of Educational Research",
    "id": "Kulik1980",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "keyword": "e-learning, plato_system, ticcit",
    "language": "en-US",
    "page": "525-544",
    "publisher": "American Educational Research Association",
    "title": "Effectiveness of computer-based college teaching: A meta-analysis of findings",
    "title-short": "Effectiveness of computer-based college teaching",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "URL": "http://trec.nist.gov/pubs/trec16/papers/ursinus.legal.final.pdf",
    "abstract": "This paper describes our participation in the TREC Legal experiments in 2007. We have applied novel normal- ization techniques that are designed to slightly favor longer documents instead of assuming that all documents should have equal weight. We have also developed a new method for reformulating query text when background information is provided with an information request. We have also experimented with using enhanced OCR error detection to reduce the size of the term list and remove noise in the data. In this article, we discuss the impact of these effects on the TREC 2007 data sets. We show that the use of simple normalization methods significantly outperforms cosine normalization in the legal domain.",
    "author": [
      {
        "family": "Kulp",
        "given": "Scott"
      },
      {
        "family": "Kontostathis",
        "given": "April"
      }
    ],
    "collection-number": "500-274",
    "collection-title": "Special publication",
    "container-title": "Proceedings of the sixteenth text REtrieval conference (TREC 2007)",
    "editor": [
      {
        "family": "Voorhees",
        "given": "Ellen M."
      },
      {
        "family": "Buckland",
        "given": "Lori P."
      }
    ],
    "id": "Kulp2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "ir, ocr, spelling_correction",
    "language": "en-US",
    "publisher": "National Institute of Standards and Technology (NIST)",
    "publisher-place": "Gaithersburg, MD, USA",
    "title": "On retrieving legal files: Shortening documents and weeding out garbage",
    "title-short": "On retrieving legal files",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/800107.803472",
    "author": [
      {
        "family": "Kumar",
        "given": "V. K."
      },
      {
        "family": "Rogers",
        "given": "James L."
      }
    ],
    "container-title": "Proceedings of the ACM SIGCSE-SIGCUE technical symposium on computer science and education",
    "id": "Kumar1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "page": "189-191",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Instructional uses of the Olin experimental classroom",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Kunze",
        "given": "Claudia"
      },
      {
        "family": "Lemnitzer",
        "given": "Lothar"
      }
    ],
    "container-title": "Proceedings of the third international conference on language resources and evaluation (LREC)",
    "id": "Kunze2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "page": "29-31",
    "publisher-place": "Las Palmas, Canary Islands, Spain",
    "title": "GermaNet – representation, visualization, application",
    "type": "paper-conference"
  },
  {
    "URL": "http://eprints.pascal-network.org/archive/00004303/",
    "abstract": "The goal of Morpho Challenge 2008 was to find and evaluate unsupervised algorithms that provide morpheme analyses for words in different languages. Especially in morphologically complex languages, such as Finnish, Turkish and Arabic, morpheme analysis is important for lexica l modeling of words in speech recognition, information retrieval and machine translation. The evaluation in Morpho Challenge competitions consisted of both a linguistic and an application oriented performance analysis. This paper describes an evaluation where the competition entries were compared to a linguistic morpheme analysis gold standard. Because the morpheme labels in an unsupervised analysis can be arbitrary, the evaluation is based on matching the morpheme-shar ing words between the proposed and the gold standard analyses. In addition to Finnish, Turkish, German and English evaluations performed in Morpho Challenge 2007, the competition this year h ad an additional evaluation in Arabic. The results in 2008 show that although the level of precision and recall varies substantially between the tasks in different la nguages, the best methods seem to manage all the tested languages quite well. The Morpho Challenge was part of the EU Network of Excellence PASCAL Challenge Program and organized in collaboration with CLEF.",
    "author": [
      {
        "family": "Kurimo",
        "given": "Mikko"
      },
      {
        "family": "Varjokallio",
        "given": "Matti"
      }
    ],
    "container-title": "Workshop of the cross-language evaluation forum, CLEF 2008",
    "id": "Kurimo2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "morphology",
    "language": "en-US",
    "title": "Unsupervised morpheme analysis evaluation by a comparison to a linguistic gold standard – Morpho Challenge 2008",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/2169095.2169101",
    "ISBN": "978-1-4503-1188-5",
    "abstract": "Recently, large-scale knowledge bases have been constructed by automatically extracting relational facts from text. Unfortunately, most of the current knowledge bases focus on static facts and ignore the temporal dimension. However, the vast majority of facts are evolving with time or are valid only during a particular time period. Thus, time is a significant dimension that should be included in knowledge bases. In this paper, we introduce a complete information extraction framework that harvests temporal facts and events from semi-structured data and free text of Wikipedia articles to create a temporal ontology. First, we extend a temporal data representation model by making it aware of events. Second, we develop an information extraction method which harvests temporal facts and events from Wikipedia infoboxes, categories, lists, and article titles in order to build a temporal knowledge base. Third, we show how the system can use its extracted knowledge for further growing the knowledge base. We demonstrate the effectiveness of our proposed methods through several experiments. We extracted more than one million temporal facts with precision over 90% for extraction from semi-structured data and almost 70% for extraction from text.",
    "author": [
      {
        "family": "Kuzey",
        "given": "Erdal"
      },
      {
        "family": "Weikum",
        "given": "Gerhard"
      }
    ],
    "container-title": "Proceedings of the 2<sup>nd</sup> temporal web analytics workshop (TempWeb ’12)",
    "id": "Kuzey2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "rdf, spatio-temporal_annotation, temporal_data, wikipedia",
    "language": "en-US",
    "page": "25-32",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Extraction of temporal facts and events from Wikipedia",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-35722-0_19",
    "abstract": "We introduce a justification logic with a novel constructor for evidence terms, according to which the new information itself serves as evidence for believing it. We provide a sound and complete axiomatization for belief expansion and minimal change and explain how the minimality can be graded according to the strength of reasoning. We also provide an evidential analog of the Ramsey axiom.",
    "author": [
      {
        "family": "Kuznets",
        "given": "Roman"
      },
      {
        "family": "Studer",
        "given": "Thomas"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Logical foundations of computer science. Proceedings of LFCS 2013",
    "editor": [
      {
        "family": "Artemov",
        "given": "Sergei N."
      },
      {
        "family": "Nerode",
        "given": "Anil"
      }
    ],
    "id": "Kuznets2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "logic, uncertainty",
    "language": "en-US",
    "page": "266-279",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Update as evidence: Belief expansion",
    "title-short": "Update as evidence",
    "type": "paper-conference",
    "volume": "7734"
  },
  {
    "DOI": "10.1145/1215942.1215943",
    "ISSN": "0095-2737",
    "author": [
      {
        "family": "Kuznetsov",
        "given": "Stacey"
      }
    ],
    "container-title": "ACM SIGCAS Computers and Society",
    "id": "Kuznetsov2006",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2006,
          6
        ]
      ]
    },
    "keyword": "interactive_editing, wiki",
    "language": "en-US",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Motivations of contributors to Wikipedia",
    "type": "article-journal",
    "volume": "36"
  },
  {
    "DOI": "10.1007/978-1-4020-2824-3_4",
    "abstract": "In the Western academic world, the making of the notion of the “human sciences” has gone through a long and intricate path. As far as terminology is concerned, there were debates as to what would be the most suitable designation for the broad but vague field of humanistic studies. In English, there are notions such as liberal arts, humanities, social sciences, human sciences etc. In German, besides the central notion of Geisteswissenschaft, other terms such as Kulturwissenschaft or Geschichtswissenschaft have been used. Recently, even the ambiguous term Humanwissenschaft is being considered.",
    "author": [
      {
        "family": "Kwan",
        "given": "Tze-wan"
      }
    ],
    "collection-title": "Contributions to phenomenology",
    "container-title": "Space, time, and culture",
    "editor": [
      {
        "family": "Carr",
        "given": "David"
      },
      {
        "family": "Chan-Fai",
        "given": "Cheung"
      }
    ],
    "id": "Kwan2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "page": "31-55",
    "publisher": "Kluwer",
    "publisher-place": "Dordrecht",
    "title": "The human sciences and historicality: Heidegger and the self-positioning of the Western humanistic tradition",
    "title-short": "The human sciences and historicality",
    "type": "chapter",
    "volume": "51"
  },
  {
    "URL": "http://icame.uib.no/hc/",
    "abstract": "The Helsinki Corpus of English Texts: Diachronic and Dialectal is a computerized collection of extracts of continuous text. It is the result of a project commenced in 1984 and directed by Matti Rissanen and Ossi Ihalainen at the University of Helsinki. The Corpus contains a diachronic part covering the period from c. 750 to c. 1700 and a dialect part based on transcripts of interviews with speakers of British rural dialects from the 1970’s. The aim of the Corpus is to promote and facilitate the diachronic and dialectal study of English as well as to offer computerized material to those interested in the development and varieties of language. The material is intended for both mainframe and microcomputer use. The aim of this guide is to help the users of the text files included in the diachronic part of the Corpus. It contains a key to coding conventions, and lists of source references and abbreviated titles to the extracts included. A volume by Rissanen et al. (1993, see Note 3) discusses the principles of compilation and offers a number of pilot studies illustrating the use of the material.",
    "author": [
      {
        "family": "Kytö",
        "given": "Merja"
      }
    ],
    "edition": "3<sup>rd</sup>",
    "id": "Kyto1993",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, english",
    "language": "en-US",
    "publisher": "University of Helsinki",
    "publisher-place": "Helsinki, Finland",
    "title": "Manual to the diachronic part of the helsinki corpus of english texts. Coding conventions and lists of source texts",
    "type": "book"
  },
  {
    "URL": "http://aclweb.org/anthology/W07-09",
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "dropping-particle": "van den",
        "family": "Bosch",
        "given": "Antal"
      },
      {
        "family": "Grover",
        "given": "Claire"
      }
    ],
    "id": "LATECH2007",
    "issued": {
      "date-parts": [
        [
          2007,
          6
        ]
      ]
    },
    "publisher": "Association for Computational Linguistics",
    "title": "Proceedings of the workshop on language technology for cultural heritage data (LaTeCH 2007)",
    "type": "book"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/workshops/W22_Proceedings.pdf",
    "editor": [
      {
        "family": "Ribarov",
        "given": "Kiril"
      },
      {
        "family": "Sporleder",
        "given": "Caroline"
      }
    ],
    "id": "LATECH2008",
    "issued": {
      "date-parts": [
        [
          2008,
          6
        ]
      ]
    },
    "title": "Proceedings of the LREC 2008 workshop on language technology for cultural heritage data (LaTeCH 2008)",
    "type": "book"
  },
  {
    "URL": "http://aclweb.org/anthology/W09-03",
    "editor": [
      {
        "family": "Borin",
        "given": "Lars"
      },
      {
        "family": "Lendvai",
        "given": "Piroska"
      }
    ],
    "id": "LATECH2009",
    "issued": {
      "date-parts": [
        [
          2009,
          3
        ]
      ]
    },
    "publisher": "Association for Computational Linguistics",
    "title": "Proceedings of the EACL 2009 workshop on language technology and resources for cultural heritage, social sciences, humanities, and education (LaTeCH – SHELT&r 2009)",
    "type": "book"
  },
  {
    "URL": "http://ilk.uvt.nl/LaTeCH2010/LPF/ws16.pdf",
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      }
    ],
    "id": "LATECH2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "title": "Proceedings of the ECAI 2010 workshop on language technology for cultural heritage, social sciences, and humanities (LaTeCH 2010)",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Laakso",
        "given": "Mikko"
      },
      {
        "family": "Salakoski",
        "given": "Tapio"
      },
      {
        "family": "Korhonen",
        "given": "Ari"
      },
      {
        "family": "Malmi",
        "given": "Lauri"
      }
    ],
    "container-title": "Proceedings of the 4<sup>th</sup> finnish/baltic sea conference on computer science education, october 1-3, 2004, koli, finland",
    "id": "Laakso2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "28-36",
    "title": "Automatic assessment of exercises for algorithms and data structures – a case study with TRAKLA2",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Laakso",
        "given": "Mikko-Jussi"
      },
      {
        "family": "Salakoski",
        "given": "Tapio"
      },
      {
        "family": "Korhonen",
        "given": "Ari"
      }
    ],
    "container-title": "Proceedings of cognition and exploratory learning in digital age (CELDA 2005)",
    "id": "Laakso2005",
    "issued": {
      "date-parts": [
        [
          2005,
          12
        ]
      ]
    },
    "page": "113-122",
    "publisher": "IEEE Technical Committee on Learning Technology and Japanese Society of Information and Systems in Education",
    "title": "The feasibility of automatic assessment and feedback",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1871840.1871853",
    "ISBN": "978-1-4503-0376-7",
    "abstract": "The automatic processing of microblogging messages may be problematic, even in the case of very elementary operations such as tokenization. The problems arise from the use of non-standard language, including media-specific words (e.g. \"2day\", \"gr8\", \"tl;dr\", \"loool\"), emoticons (e.g. \"(ò_ó)\", \"(=^-^=)\"), non-standard letter casing (e.g. \"dr. Fred\") and unusual punctuation (e.g. \".... ..\", \"!??!!!?\", \",,,\"). Additionally, spelling errors are abundant (e.g. \"I;m\"), and we can frequently find more than one language (with different tokenization requirements) in the same short message. For being efficient in such environment, manually-developed rule-based tokenizer systems have to deal with many conditions and exceptions, which makes them difficult to build and maintain. We present a text classification approach for tokenizing Twitter messages, which address complex cases successfully and which is relatively simple to set up and maintain. For that, we created a corpus consisting of 2500 manually tokenized Twitter messages – a task that is simple for human annotators – and we trained an SVM classifier for separating tokens at certain discontinuity characters. For comparison, we created a baseline rule-based system designed specifically for dealing with typical problematic situations. Results show that we can achieve F-measures of 96% with the classification-based approach, much above the performance obtained by the baseline rule-based tokenizer (85%). Also, subsequent analysis allowed us to identify typical tokenization errors, which we show that can be partially solved by adding some additional descriptive examples to the training corpus and re-training the classifier.",
    "author": [
      {
        "family": "Laboreiro",
        "given": "Gustavo"
      },
      {
        "family": "Sarmento",
        "given": "Luís"
      },
      {
        "family": "Teixeira",
        "given": "Jorge"
      },
      {
        "family": "Oliveira",
        "given": "Eugénio"
      }
    ],
    "collection-title": "AND ’10",
    "container-title": "Proceedings of the fourth workshop on analytics for noisy unstructured text data (AND ’10)",
    "id": "Laboreiro2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "microtext, spelling_normalization",
    "language": "en-US",
    "page": "81-88",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Tokenizing micro-blogging messages using a text classification approach",
    "type": "paper-conference"
  },
  {
    "URL": "http://larc.unt.edu/ian/Cruise2007/madgdcse2007.pdf#page=65",
    "abstract": "Computer game development entices, entertains, and engages students of the \"Nintendo\" generation. Many instructors have taken this to mean that students must develop games as graphically intensive as those recently released by Nintendo. The current work describes the use of text adventure games to engage students at much lower cost. The focus on text means students work with standard C++ and the fundamentals of computer science rather than being distracted producing eye candy. Student enthusiasm, evaluations, and outcomes attest to the success of our approach.",
    "author": [
      {
        "family": "Ladd",
        "given": "Brian C."
      }
    ],
    "container-title": "Proceedings of the 2<sup>nd</sup> annual microsoft academic days on game development in computer science education",
    "id": "Ladd2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "interactive_fiction",
    "language": "en-US",
    "page": "64-69",
    "title": "XYZZY: Finding new magic in text adventure games",
    "title-short": "XYZZY",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s00799-005-0130-3",
    "ISSN": "1432-5012",
    "abstract": "The Fedora architecture is an extensible framework for the storage, management, and dissemination of complex objects and the relationships among them. Fedora accommodates the aggregation of local and distributed content into digital objects and the association of services with objects. This allows an object to have several accessible representations, some of them dynamically produced. The architecture includes a generic Resource Description Framework (RDF)-based relationship model that represents relationships among objects and their components. Queries against these relationships are supported by an RDF triple store. The architecture is implemented as a web service, with all aspects of the complex object architecture and related management functions exposed through REST and SOAP interfaces. The implementation is available as open-source software, providing the foundation for a variety of end-user applications for digital libraries, archives, institutional repositories, and learning object systems.",
    "author": [
      {
        "family": "Lagoze",
        "given": "Carl"
      },
      {
        "family": "Payette",
        "given": "Sandy"
      },
      {
        "family": "Shin",
        "given": "Edwin"
      },
      {
        "family": "Wilper",
        "given": "Chris"
      }
    ],
    "container-title": "International Journal on Digital Libraries",
    "id": "Lagoze2005",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2006,
          4,
          8
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "language": "en-US",
    "page": "124-138",
    "title": "Fedora: An architecture for complex objects and their relationships",
    "title-short": "Fedora",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "DOI": "10.18352/lq.10112",
    "ISSN": "2213-056X",
    "abstract": "This article analyses the publication trends of history in early modern Britain and North-America, 1470-1800, based on the English Short-Title Catalogue (ESTC) data. Its most important contribution is a demonstration how digitized library catalogues can become a crucial tool for scholarship and part of reproducible research. The article also suggests a novel way how to make a quantitative analysis of a particular trend in book production, namely publishing of history. This study is also our first experiment with the analysis of paper consumption in early modern book production. It demonstrates in practice the importance of open science principles for library and information science. The article studies three main research questions: 1) who wrote history 2) where was history published, and 3) how did publishing of history change over time in early modern Britain and North America. As the main findings, we demonstrate that the average book size for history publications becomes smaller over time and that the octavo-sized book is the rising vehicle of history in the eighteenth century that tells us factually about widening audiences. The article will also compare different aspects of most popular authors on history such as Edmund Burke and David Hume. While focusing on history publishing, these findings may reflect more widespread trends in publication in the early modern era, and this article illustrates how some of the key questions in this field can be assessed by statistical analysis of large-scale bibliographic data collections.",
    "author": [
      {
        "family": "Lahti",
        "given": "Leo"
      },
      {
        "family": "Ilomäki",
        "given": "Niko"
      },
      {
        "family": "Tolonen",
        "given": "Mikko"
      }
    ],
    "container-title": "LIBER Quarterly",
    "id": "Lahti2015",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "87+",
    "title": "A quantitative study of history in the English Short-Title Catalogue (ESTC), 1470–1800",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "URL": "http://www2.ivlos.uu.nl/ictexpertisecentrum/downloads/CvB\\%20brief_definitieve\\%20versie.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Lam",
        "given": "Ineke"
      },
      {
        "family": "Rubens",
        "given": "Wilfred"
      },
      {
        "family": "Simons",
        "given": "Robert-Jan"
      },
      {
        "dropping-particle": "van den",
        "family": "Berg",
        "given": "Jörgen"
      },
      {
        "dropping-particle": "de",
        "family": "Groot",
        "given": "Esther"
      },
      {
        "dropping-particle": "van der",
        "family": "Kooij",
        "given": "Siebren"
      },
      {
        "family": "Kooistra",
        "given": "Jan"
      }
    ],
    "id": "Lam2006",
    "issued": {
      "date-parts": [
        [
          2006,
          6
        ]
      ]
    },
    "language": "nl-NL",
    "publisher": "Universiteit Utrecht, expertisecentrum ICT in het onderwijs",
    "title": "Advies voor één ELO aan de UU",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Lamport",
        "given": "Leslie"
      }
    ],
    "edition": "2",
    "id": "Lamport1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Addison-Wesley",
    "publisher-place": "Reading, MA, USA",
    "title": "LaTeX: A document preparation system",
    "title-short": "LaTeX",
    "type": "book"
  },
  {
    "DOI": "10.1093/llc/fqy006",
    "author": [
      {
        "family": "Lang",
        "given": "Sabine"
      },
      {
        "family": "Ommer",
        "given": "Björn"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Lang2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "page": "fqy006",
    "title": "Attesting similarity: Supporting the organization and study of art image collections with computer vision",
    "title-short": "Attesting similarity",
    "type": "article-journal"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2010/pdf/293_Paper.pdf",
    "abstract": "In this paper, we present a simple protocol to evaluate word aligners on bilingual lexicon induction tasks from parallel corpora. Rather than resorting to gold standards, it relies on a comparison of the outputs of word aligners against a reference bilingual lexicon. The quality of this reference bilingual lexicon does not need to be particularly high, because evaluation quality is ensured by systematically filtering this reference lexicon with the parallel corpus the word aligners are trained on. We perform a comparison of three freely available word aligners on numerous language pairs from the Bible parallel corpus (Resnik et al., 1999): MGIZA++ (Gao and Vogel, 2008), BerkeleyAligner (Liang et al., 2006), and Anymalign (Lardilleux and Lepage, 2009). We then select the most appropriate one to produce bilingual lexicons for all language pairs of this corpus. These involve Cebuano, Chinese, Danish, English, Finnish, French, Greek, Indonesian, Latin, Spanish, Swedish, and Vietnamese. The 66 resulting lexicons are made freely available.",
    "author": [
      {
        "family": "Lardilleux",
        "given": "Adrien"
      },
      {
        "family": "Gosme",
        "given": "Julien"
      },
      {
        "family": "Lepage",
        "given": "Yves"
      }
    ],
    "container-title": "Proceedings of the seventh international conference on language resources and evaluation (LREC’10)",
    "editor": [
      {
        "family": "Calzolari",
        "given": "Nicoletta"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Maegaard",
        "given": "Bente"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Odijk",
        "given": "Jan"
      },
      {
        "family": "Piperidis",
        "given": "Stelios"
      },
      {
        "family": "Rosner",
        "given": "Michael"
      },
      {
        "family": "Tapias",
        "given": "Daniel"
      }
    ],
    "id": "Lardilleux2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "less-resourced_languages, nlp",
    "language": "en-US",
    "page": "252-256",
    "publisher": "European Language Resources Association (ELRA)",
    "title": "Bilingual lexicon induction: Effortless evaluation of word alignment tools and production of resources for improbable language pairs",
    "title-short": "Bilingual lexicon induction",
    "type": "paper-conference"
  },
  {
    "URL": "http://hdl.handle.net/10062/17290",
    "author": [
      {
        "family": "Borin",
        "given": "Lars"
      },
      {
        "family": "Ahlberger",
        "given": "Christer"
      }
    ],
    "id": "LarsBorin2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_humanities, swedish",
    "language": "en-US",
    "page": "58-65",
    "title": "Semantic search in literature as an e-Humanities research tool: CONPLISIT – consumption patterns and Life-Style in 19th century swedish literature",
    "title-short": "Semantic search in literature as an e-Humanities research tool",
    "type": "article-journal"
  },
  {
    "URL": "http://hdl.handle.net/2142/416",
    "abstract": "Digital library (DL) projects are beginning to create very large-scale repositories of digital information on a wide range of topics. As with traditional print libraries, this information can be indexed and retrieved in a variety of ways, ranging from purely descriptive cataloging of items in the database and topical analysis of content, to more specialized methods of classification and description that exploit the characteristics of digital information. This paper will examine the problems and prospects of a class of retrieval and indexing methods that are particularly suited to digital library materials with geographic content or associations. The characteristics of spatial queries and some of the problems of uncertainty and approximation in spatial and geographic information retrieval are discussed. Requirements and a methodologyfor automatic indexing and georeferencing of text documents are then examined. A \"state-of-the-art\" examination of network access to georeferenced information is provided, and a specification language and tool for development of graphical interfaces to support geographic information retrieval and spatial browsing is also described. In conclusion, general issues and characteristics of georeferenced multimedia information systems are discussed.",
    "author": [
      {
        "family": "Larson",
        "given": "Ray R."
      }
    ],
    "container-title": "Geographic information systems and libraries: Patrons, maps, and spatial information",
    "editor": [
      {
        "family": "Smith"
      },
      {
        "family": "Myke Gluck",
        "given": "Linda C."
      }
    ],
    "id": "Larson1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "digital_library, geo_ir",
    "page": "81-124",
    "publisher": "University of Illinois at Urbana-Champaign",
    "publisher-place": "Urbana-Champaign, IL, USA",
    "title": "Geographic information retrieval and spatial browsing",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1016/0004-3702(89)90078-7",
    "ISSN": "00043702",
    "abstract": "A formal equivalence is demonstrated between Shafer-Dempster belief theory and assumption-based truth maintenance with a probability calculus on the assumptions. This equivalence means that any Shafer-Dempster inference network can be represented as a set of ATMS justifications with probabilities attached to assumptions. A proposition’s belief is equal to the probability of its label conditioned on label consistency. An algorithm is given for computing these beliefs. When the ATMS is used to manage beliefs, non-independencies between nodes are automatically and correctly accounted for. The approach described here unifies symbolic and numeric approaches to uncertainty management, thus facilitating dynamic construction of quantitative belief arguments, explanation of beliefs, and resolution of conflicts.",
    "author": [
      {
        "family": "Laskey",
        "given": "Kathryn B."
      },
      {
        "family": "Lehner",
        "given": "Paul E."
      }
    ],
    "container-title": "Artificial Intelligence",
    "id": "Laskey1989",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "65-77",
    "title": "Assumptions, beliefs and probabilities",
    "type": "article-journal",
    "volume": "41"
  },
  {
    "DOI": "10.1109/3468.487959",
    "ISSN": "10834427",
    "abstract": "A model is a representation of a system that can be used to answer questions about the system. In many situations in which models are used, there exists no set of universally accepted modeling assumptions. The term model uncertainty commonly refers to uncertainty about a model’s structure, as distinguished from uncertainty about parameters. This paper presents alternative formal approaches to treating model uncertainty, discusses methods for using data to reduce model uncertainty, presents approaches for diagnosing inadequate models, and discusses appropriate use of models that are subject to model uncertainty.",
    "author": [
      {
        "family": "Laskey",
        "given": "Kathryn B."
      }
    ],
    "container-title": "IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",
    "id": "Laskey1996",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "340-348",
    "title": "Model uncertainty: Theory and practical implications",
    "title-short": "Model uncertainty",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "DOI": "10.1016/j.ijar.2009.05.011",
    "ISSN": "0888613X",
    "abstract": "Geospatial reasoning has been an essential aspect of military planning since the invention of cartography. Although maps have always been a focal point for developing situational awareness, the dawning era of network-centric operations brings the promise of unprecedented battlefield advantage due to improved geospatial situational awareness. Geographic information systems (GIS) and GIS-based decision support systems are ubiquitous within current military forces, as well as civil and humanitarian organizations. Understanding the quality of geospatial data is essential to using it intelligently. A systematic approach to data quality requires: estimating and describing the quality of data as they are collected; recording the data quality as metadata; propagating uncertainty through models for data processing; exploiting uncertainty appropriately in decision support tools; and communicating to the user the uncertainty in the final product. There are shortcomings in the state-of-the-practice in GIS applications in dealing with uncertainty. No single point solution can fully address the problem. Rather, a system-wide approach is necessary. Bayesian reasoning provides a principled and coherent framework for representing knowledge about data quality, drawing inferences from data of varying quality, and assessing the impact of data quality on modeled effects. Use of a Bayesian approach also drives a requirement for appropriate probabilistic information in geospatial data quality metadata. This paper describes our research on data quality for military applications of geospatial reasoning, and describes model views appropriate for model builders, analysts, and end users.",
    "author": [
      {
        "family": "Laskey",
        "given": "Kathryn B."
      },
      {
        "family": "Wright",
        "given": "Edward J."
      },
      {
        "dropping-particle": "da",
        "family": "Costa",
        "given": "Paulo C. G."
      }
    ],
    "container-title": "International Journal of Approximate Reasoning",
    "id": "Laskey2010",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "209-223",
    "title": "Envisioning uncertainty in geospatial information",
    "type": "article-journal",
    "volume": "51"
  },
  {
    "URL": "http://connect.educause.edu/Library/EDUCAUSE+Quarterly/CreatingFlexibleELearning/39885?time=1213375570",
    "abstract": "The University System of Georgia deconstructs existing online courses to create separate files of reusable content",
    "author": [
      {
        "family": "Lasseter",
        "given": "Marie"
      },
      {
        "family": "Rogers",
        "given": "Michael"
      }
    ],
    "container-title": "EDUCAUSE Quarterly",
    "id": "Lasseter2004",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "language": "en-US",
    "page": "72-74",
    "title": "Creating flexible E-Learning through the use of learning objects",
    "type": "article-journal",
    "volume": "27"
  },
  {
    "URL": "http://jstor.org/stable/24147050",
    "abstract": "The use of computers for research and instruction in Classics has a long and proud history, now going back more than 50 years. Over this time classicists have often been on the cutting edge of development, starting with mainframe applications, migrating later to microcomputer, and continuing on through multimedia CD-ROMs, Web-based research tools, and the construction of virtual Rome on line. This article describes many of the most significant development efforts in this field and discuss some of the trends that are revealed by this history.",
    "author": [
      {
        "family": "Latousek",
        "given": "Rob"
      }
    ],
    "container-title": "CALICO Journal",
    "id": "Latousek2001",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_humanities, greek, latin",
    "language": "en-US",
    "page": "211-222",
    "title": "Fifty years of classical computing: A progress report",
    "title-short": "Fifty years of classical computing",
    "type": "article-journal",
    "volume": "18"
  },
  {
    "URL": "http://derstandard.at/2000012110707",
    "abstract": "Bisher wurden informatische Methoden vor allem in den Naturwissenschaften angewendet. In den Digital Humanities kommen sie auch in den Geisteswissenschaften zum Einsatz",
    "author": [
      {
        "family": "Lau",
        "given": "Johannes"
      }
    ],
    "container-title": "Der Standard",
    "id": "Lau2015",
    "issued": {
      "date-parts": [
        [
          2015,
          2
        ]
      ]
    },
    "keyword": "digital_humanities, in_the_media",
    "language": "de-DE",
    "title": "Wittgensteins Enkel im digitalen Wandel",
    "type": "article-journal"
  },
  {
    "URL": "http://jstor.org/stable/2345762",
    "abstract": "A causal network is used in a number of areas as a depiction of patterns of “influence” among sets of variables. In expert systems it is common to perform “inference” by means of local computations on such large but sparse networks. In general, non-probabilistic methods are used to handle uncertainty when propagating the effects of evidence, and it has appeared that exact probabilistic methods are not computationally feasible. Motivated by an application in electromyography, we counter this claim by exploiting a range of local representations for the joint probability distribution, combined with topological changes to the original network termed “marrying” and “filling-in”. The resulting structure allows efficient algorithms for transfer between representations, providing rapid absorption and propagation of evidence. The scheme is first illustrated on a small, fictitious but challenging example, and the underlying theory and computational aspects are then discussed.",
    "author": [
      {
        "family": "Lauritzen",
        "given": "Steffen L."
      },
      {
        "family": "Spiegelhalter",
        "given": "David J."
      }
    ],
    "container-title": "Journal of the Royal Statistical Society. Series B (Methodological)",
    "id": "Lauritzen1988",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "157-224",
    "title": "Local computations with probabilities on graphical structures and their application to expert systems",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "URL": "http://assessment.cetis.ac.uk/minutes",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Lay",
        "given": "Steve"
      }
    ],
    "id": "Lay2005a",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "publisher": "Presentation at the 15<sup>th</sup> CETIS Assessment SIG meeting, University of York",
    "title": "What’s new in IMS QTI v2.0?",
    "type": ""
  },
  {
    "URL": "http://www.ledonline.it/ledonline/JADT-2010/allegati/JADT-2010-1045-1056_106-Lay.pdf",
    "abstract": "In regard to its annotated corpus exploration capabilities, AnaLog is a textometric tool. It allows linear and synthetic reading of annotated texts, inventory and counting of linguistic units, and their simultaneous contextualization on the paradigmatic and syntagmatic dimensions. To meet the needs of humanist users and linguists, AnaLog was designed to explore, at the same time, selected raw contents (text corpora), and their corresponding model-dependent representations (annotation resources), and then to dynamically analyze the composition of these two dimensions: annotated corpora. AnaLog is also singular by its unique navigation mode within data, results and parameters, using simple tables that can be sorted, filtered and modified to refine the current analysis.",
    "author": [
      {
        "family": "Lay",
        "given": "Marie-Hélène"
      },
      {
        "family": "Pincemin",
        "given": "Bénédicte"
      }
    ],
    "container-title": "Proceedings of the 10<sup>th</sup> International Conference on Statistical Analysis of Textual Data (JADT 2010)",
    "editor": [
      {
        "family": "Bolasco",
        "given": "Sergio"
      },
      {
        "family": "Chiari",
        "given": "Isabella"
      },
      {
        "family": "Giuliano",
        "given": "Luca"
      }
    ],
    "id": "Lay2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "corpus_linguistics, spelling_normalization",
    "language": "fr-FR",
    "page": "1045-1056",
    "publisher": "Edizioni Universitarie di Lettere Economia Diritto",
    "publisher-place": "Milan, Italy",
    "title": "Pour une exploration humaniste des textes : AnaLog",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1504/IJIL.2009.022809",
    "author": [
      {
        "family": "Lazarinis",
        "given": "Fotis"
      },
      {
        "family": "Green",
        "given": "Steve"
      },
      {
        "family": "Pearson",
        "given": "Elaine"
      }
    ],
    "container-title": "International Journal of Innovation and Learning",
    "id": "Lazarinis2009a",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "page": "127-146",
    "title": "Measuring the conformance of hypermedia assessment tools to QTI",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "DOI": "10.1504/IJAMC.2009.027012",
    "abstract": "In this paper we present the development of a multimedia assessment tool conforming to the latest version of IMS QTI learning standard. Initially, a presentation of the development of IMS QTI is given and relevant research papers are analysed. Then, the requirements analysis and the development decisions for the assessment tool are discussed. At the end of the paper the assessment tool is evaluated and the method of validating its output is presented.",
    "author": [
      {
        "family": "Lazarinis",
        "given": "Fotis"
      },
      {
        "family": "Green",
        "given": "Steve"
      },
      {
        "family": "Pearson",
        "given": "Elaine"
      }
    ],
    "container-title": "International Journal of Advanced Media and Communication",
    "id": "Lazarinis2009b",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "page": "260-276",
    "title": "Engineering an interoperable multimedia assessment authoring and run-time environment conforming to IMS QTI",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.2307/747503",
    "ISSN": "00340553",
    "abstract": "This study was designed to measure the influence of highly variable spelling upon the reading efficiency of skilled readers of Modern English as that influence was reflected by several measures of performance. Caxton’s preface to the Eneydos was variously modified to provide 5 experimental conditions: a control condition, 2 conditions measuring the influence of the visibility of the line of print, 1 condition measuring the influence of highly variable spelling, and 1 condition measuring the combined influence of highly variable spelling and an archaic syntactic-semantic system. These 5 experimental conditions were employed in 3 experiments. Experiment A measured silent reading performance; Experiment B measured oral reading performance; and Experiment C measured eye movements via the Reading Eye Camera II. The subjects employed in this study were 150 skilled readers of Modern English who were randomly selected without replacement from a population of 400 college students. Ten subjects were randomly assigned to each of the 5 experimental conditions in each of the 3 experiments. The results of these 3 experiments suggest 1) that the efficiency of the skilled reader of Modern English is decreased more by highly variable spelling than by a reduction in the visibility of the line of print and 2) that the skilled reader’s efficiency is reduced even more when he is deprived not only of standard spelling but also of a familiar syntactic-semantic system. In brief, the results of this study support the hypothesis that the skilled reader of Modern English relies upon the orthographic and syntactic-semantic information that is stored within his brain in order to read efficiently. Thus, this study reinforces the assumption that reading is a highly cognitive process.",
    "author": [
      {
        "family": "Hunt Lazerson",
        "given": "Barbara"
      }
    ],
    "container-title": "Reading Research Quarterly",
    "id": "Lazerson1975",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1975
        ]
      ]
    },
    "keyword": "orthography",
    "language": "en-US",
    "publisher": "International Reading Association",
    "title": "The influence of highly variable spelling upon the reading performance of skilled readers of Modern English",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "ISSN": "1433-2833",
    "URL": "http://portal.acm.org/citation.cfm?id=1237487",
    "abstract": "DEBORA (Digital AccEss to BOoks of the RenAissance) is a multidisciplinary European project aiming at digitizing and thus making rare sixteenth century books more accessible. End-users, librarians, historians, researchers in book history and computer scientists participated in the development of remote and collaborative access to digitized Renaissance books, necessary because of the reduced accessibility to digital libraries in image mode through the Internet. The size of files for the storage of images, the lack of a standard file format exchange suitable for progressive transmission, and limited querying possibilities currently limit remote access to digital libraries. To improve accessibility, historical documents must be digitized and retro-converted to extract a detailed description of the image contents suited to users’ needs. Specialists of the Renaissance have described the metadata generally required by end-users and the ideal functionalities of the digital library. The retro-conversion of historical documents is a complex process that includes image capture, metadata extraction, image storage and indexing, automatic conversion in a reusable electronic form, publication on the Internet, and data compression for faster remote access. The steps of this process cannot be developed independently. DEBORA proposes a global approach to retro-conversion from the digitization to the final functionalities of the digital library centered on users’ needs. The retro-conversion process is mainly based on a document image analysis system that simultaneously extracts the metadata and compresses the images. We also propose a file format to describe compressed books as heterogeneous data (images/text/links/ annotation/physical layout and logical structure) suitable for progressive transmission, editing, and annotation. DEBORA is an exploratory project that aims at demonstrating the feasibility of the concepts by developing prototypes tested by end-users.",
    "author": [
      {
        "family": "Le Bourgeois",
        "given": "Frank"
      },
      {
        "family": "Emptoz",
        "given": "Hubert"
      }
    ],
    "container-title": "International Journal on Document Analysis and Recognition",
    "id": "LeBourgeois2007",
    "issue": "2–4",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage, document_analysis",
    "language": "en-US",
    "page": "193-221",
    "title": "DEBORA: Digital AccEss to BOoks of the RenAissance",
    "title-short": "DEBORA",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "URL": "http://www.intelligence-complexite.org/inserts/ouvrages/0609tsgtm.pdf",
    "author": [
      {
        "family": "Le Moigne",
        "given": "Jean-Louis"
      }
    ],
    "edition": "4",
    "id": "LeMoigne1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "formal_models",
    "language": "fr-FR",
    "original-date": {
      "date-parts": [
        [
          1977
        ]
      ]
    },
    "publisher": "PUF",
    "publisher-place": "Paris",
    "title": "La théorie du système général: Théorie de la modélisation",
    "title-short": "La théorie du système général",
    "type": "book"
  },
  {
    "ISBN": "9782100043828",
    "abstract": "« Nous ne raisonnons que sur des modèles » rappelait Paul Valéry. Mais comment construisons-nous les modèles des situations dans lesquelles nous raisonnons ? De trop nombreux échecs nous ont appris ce qu’il en coûte de traiter simplement des questions complexes, c’est-à-dire de réduire à des modèles simplistes la foisonnante complexité de la vie, de la société, de la connaissance. Nous savons aujourd’hui que la complexité peut être source de sagesse puisqu’elle active nos intelligences : plutôt que de la simplifier en la mutilant, ne pouvons-nous commencer par l’assumer en la modélisant ? Écrit pour tous ceux qui ont à gérer des « situations complexes » - responsables d’organisations, ingénieurs, consultants, chercheurs en sciences humaines... - ce livre montre comment, dans toutes les disciplines, le développement de la science des systèmes permet de construire les modèles sur lesquels exercer notre intelligence de la complexité. Au carrefour d’exigences multiples, la science des systèmes contribue à l’émergence de nouvelles représentations de la réalité, et offre des voies de recherche et de réflexion parmi les plus prometteuses. L’ambition des ouvrages de cette série est de présenter les textes les plus significatifs s’inscrivant dans cette perspective.",
    "author": [
      {
        "family": "Le Moigne",
        "given": "Jean-Louis"
      }
    ],
    "id": "LeMoigne1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "complex_systems, formal_models",
    "language": "fr-FR",
    "publisher": "Dunod",
    "publisher-place": "Paris",
    "title": "La Modélisation des systèmes complexes",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Le Moigne",
        "given": "Jean-Louis"
      }
    ],
    "id": "LeMoigne2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "L’Harmattan",
    "publisher-place": "Paris",
    "title": "Le constructivisme: Modéliser pour comprendre",
    "title-short": "Le constructivisme",
    "type": "book",
    "volume": "3"
  },
  {
    "DOI": "10.3917/dunod.lebar.2015.01.0003",
    "ISBN": "9782100703845",
    "abstract": "L’analyse des données, ou plus précisément l’analyse géométrique des données (AGD) est l’approche de la statistique multivariée, développée par J.-P. Benzécri autour de l’analyse des correspondances, dans laquelle les données sont représentées sous forme de nuages de points et l’interprétation se fait de manière privilégiée sur les nuages de points. Dans ce chapitre, nous évoquons d’abord le champ de la statistique et ses sous-champs (§1.1). Puis nous introduisons l’agd avec les trois paradigmes, les trois idées–clefs et un bref historique (§1.2). Ensuite, nous examinons les points forts de la méthodologie de l’AGD (§1.3). Enfin, nous abordons succinctement la mise en œuvre informatique des méthodes d’AGD (§1.4).",
    "author": [
      {
        "family": "Le Roux",
        "given": "Brigitte"
      },
      {
        "family": "Lebaron",
        "given": "Frédéric"
      }
    ],
    "chapter-number": "1",
    "container-title": "La méthodologie de Pierre Bourdieu en action",
    "editor": [
      {
        "family": "Le Roux",
        "given": "Brigitte"
      },
      {
        "family": "Lebaron",
        "given": "Frédéric"
      }
    ],
    "id": "LeRoux2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "philosophy_of_science",
    "language": "fr-FR",
    "page": "3-20",
    "publisher": "Dunod",
    "publisher-place": "Paris",
    "title": "Idées–clefs de l’analyse géométrique des données",
    "type": "chapter"
  },
  {
    "DOI": "10.2200/s00275ed1v01y201006hlt009",
    "abstract": "It has been estimated that over a billion people are using or learning English as a second or foreign language, and the numbers are growing not only for English but for other languages as well. These language learners provide a burgeoning market for tools that help identify and correct learners’ writing errors. Unfortunately, the errors targeted by typical commercial proofreading tools do not include those aspects of a second language that are hardest to learn. This volume describes the types of constructions English language learners find most difficult – constructions containing prepositions, articles, and collocations. It provides an overview of the automated approaches that have been developed to identify and correct these and other classes of learner errors in a number of languages. Error annotation and system evaluation are particularly important topics in grammatical error detection because there are no commonly accepted standards. Chapters in the book describe the options available to researchers, recommend best practices for reporting results, and present annotation and evaluation schemes. The final chapters explore recent innovative work that opens new directions for research. It is the authors’ hope that this volume will contribute to the growing interest in grammatical error detection by encouraging researchers to take a closer look at the field and its many challenging problems. Table of Contents: Introduction / History of Automated Grammatical Error Detection / Special Problems of Language Learners / Language Learner Data / Evaluating Error Detection Systems / Article and Preposition Errors / Collocation Errors / Different Approaches for Different Errors / Annotating Learner Errors / New Directions / Conclusion",
    "author": [
      {
        "family": "Leacock",
        "given": "Claudia"
      },
      {
        "family": "Chodorow",
        "given": "Martin"
      },
      {
        "family": "Gamon",
        "given": "Michael"
      },
      {
        "family": "Tetreault",
        "given": "Joel"
      }
    ],
    "collection-title": "Synthesis lectures on human language technologies",
    "id": "Leacock2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "call, grammar_checking, learner_corpora",
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "publisher-place": "San Rafael, CA, USA",
    "title": "Automated grammatical error detection for language learners",
    "type": "book",
    "volume": "9"
  },
  {
    "abstract": "Cet article présente une démarche méthodologique pour l’analyse d’un phénomène linguistique dans un corpus de textes. Plus spécifiquement, nous présentons la démarche élaborée dans notre thèse d’obédience générativiste où nous nous intéressions aux propriétés morphologiques et syntaxiques du morphème par en ancien français (Lebel, 2003).",
    "author": [
      {
        "family": "Lebel",
        "given": "Marie-Élaine"
      }
    ],
    "container-title": "Revue québécoise de linguistique",
    "id": "Lebel2009",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, french, nlp",
    "language": "en-US",
    "page": "88-108",
    "title": "Traitement assisté par ordinateur : À l’assaut du morphème par dans un corpus d’ancien français !",
    "title-short": "Traitement assisté par ordinateur ",
    "type": "article-journal",
    "volume": "33"
  },
  {
    "DOI": "10.1109/mc.1979.1658697",
    "ISSN": "0018-9162",
    "abstract": "Is magic real? Do swords glow if the enemy is nearby? In the demonic world of Zork, a simulated universe entices the player into a new form of problem solving.",
    "author": [
      {
        "family": "Lebling",
        "given": "P. David"
      },
      {
        "family": "Blank",
        "given": "Marc S."
      },
      {
        "family": "Anderson",
        "given": "Timothy A."
      }
    ],
    "container-title": "Computer",
    "id": "Lebling1979",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1979
        ]
      ]
    },
    "keyword": "interactive_fiction",
    "language": "en-US",
    "page": "51-59",
    "publisher": "IEEE",
    "publisher-place": "Piscataway, NJ, USA",
    "title": "Zork: A computerized fantasy simulation game",
    "title-short": "Zork",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "DOI": "2065/11882",
    "author": [
      {
        "family": "Lee",
        "given": "Kiyong"
      }
    ],
    "container-title": "Language, information and computation: Proceedings of the 10<sup>th</sup> pacific asia conference",
    "id": "Lee1995",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "page": "215-224",
    "publisher": "City University of Hong Kong",
    "publisher-place": "Hong Kong",
    "title": "Recursion problems in concatenation: A case of Korean morphology",
    "title-short": "Recursion problems in concatenation",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Lee",
        "given": "Kiyong"
      }
    ],
    "id": "Lee2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "note": "In Korean",
    "publisher": "Korea University Press",
    "title": "Korean computational morphology",
    "type": "book"
  },
  {
    "URL": "http://academic.research.microsoft.com/Publication/5013289/a-nearest-neighbor-approach-to-the-automatic-analysis-of-ancient-greek-morphology",
    "abstract": "We propose a data-driven method for automatically analyzing the morphology of ancient Greek. This method improves on existing ancient Greek analyzers in two ways. First, through the use of a nearest-neighbor machine learning framework, the analyzer requires no hand-crafted rules. Second, it is able to predict novel roots, and to rerank its predictions by exploiting a large, unlabelled corpus of ancient Greek.",
    "author": [
      {
        "family": "Lee",
        "given": "John"
      }
    ],
    "id": "Lee2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, greek",
    "language": "en-US",
    "page": "127-134",
    "publisher": "ACL",
    "publisher-place": "Stroudsburg, PA",
    "title": "A nearest-neighbor approach to the automatic analysis of ancient Greek morphology",
    "type": "article-journal"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2010/pdf/631_Paper.pdf",
    "abstract": "We have recently converted a dependency treebank, consisting of ancient Greek and Latin texts, from one annotation scheme to another that was independently designed. This paper makes two observations about this conversion process. First, we show that, despite significant surface differences between the two treebanks, a number of straightforward transformation rules yield a substantial level of compatibility between them, giving evidence for their sound design and high quality of annotation. Second, we analyze some linguistic annotations that require further disambiguation, proposing some simple yet effective machine learning methods.",
    "author": [
      {
        "family": "Lee",
        "given": "John"
      },
      {
        "family": "Haug",
        "given": "Dag"
      }
    ],
    "container-title": "Proceedings of the seventh international conference on language resources and evaluation (LREC’10)",
    "editor": [
      {
        "family": "Calzolari",
        "given": "Nicoletta"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Maegaard",
        "given": "Bente"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Odijk",
        "given": "Jan"
      },
      {
        "family": "Piperidis",
        "given": "Stelios"
      },
      {
        "family": "Rosner",
        "given": "Mike"
      },
      {
        "family": "Tapias",
        "given": "Daniel"
      }
    ],
    "id": "Lee2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, greek, latin",
    "language": "en-US",
    "page": "1918-1924",
    "publisher": "European Language Resources Association (ELRA)",
    "title": "Porting an Ancient Greek and Latin treebank",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/W12-1011",
    "abstract": "We introduce a corpus of classical Chinese poems that has been word segmented and tagged with parts-of- speech (POS). Due to the ill-defined concept of a “word” in Chinese, previous Chinese corpora suffer from a lack of standardization in word segmentation, resulting in inconsistencies in POS tags, therefore hindering interoperability among corpora. We address this problem with nested POS tags, which accommodates different theories of wordhood and facilitates research objectives requiring annotations of the “word” at different levels of granularity.",
    "author": [
      {
        "family": "Lee",
        "given": "John"
      }
    ],
    "container-title": "Proceedings of the 6<sup>th</sup> workshop on language technology for cultural heritage, social sciences, and humanities (LaTeCH 2012)",
    "editor": [
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      },
      {
        "dropping-particle": "van den",
        "family": "Bosch",
        "given": "Antal"
      }
    ],
    "id": "Lee2012a",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "chinese, corpus_linguistics, cultural_heritage",
    "language": "en-US",
    "page": "75-84",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "A Classical Chinese corpus with nested part-of-speech tags",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/N12-1020",
    "abstract": "As interest grows in the use of linguistically annotated corpora in research and teaching of foreign languages and literature, treebanks of various historical texts have been developed. We introduce the first large-scale dependency treebank for Classical Chinese literature. Derived from the Stanford dependency types, it consists of over 32K characters drawn from a collection of poems written in the 8th century CE. We report on the design of new dependency relations, discuss aspects of the annotation process and evaluation, and illustrate its use in a study of parallelism in Classical Chinese poetry.",
    "author": [
      {
        "family": "Lee",
        "given": "John"
      },
      {
        "family": "Kong",
        "given": "Yin H."
      }
    ],
    "container-title": "Proceedings of the 2012 conference of the north american chapter of the association for computational linguistics: Human language technologies",
    "id": "Lee2012b",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "chinese, corpus_linguistics, cultural_heritage",
    "language": "en-US",
    "page": "191-199",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "A dependency treebank of Classical Chinese poems",
    "type": "paper-conference"
  },
  {
    "DOI": "10.7551/mitpress/11180.001.0001",
    "ISBN": "978-0262536424",
    "author": [
      {
        "family": "Lee",
        "given": "Edward A."
      }
    ],
    "id": "Lee2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Plato and the nerd: The creative partnership of humans and technology",
    "title-short": "Plato and the nerd",
    "type": "book"
  },
  {
    "DOI": "10.1145/3231590",
    "ISSN": "1557-7317",
    "author": [
      {
        "family": "Lee",
        "given": "Edward A."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Lee2019",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "35--36",
    "title": "Modeling in engineering and science",
    "type": "article-journal",
    "volume": "62"
  },
  {
    "DOI": "10.1016/0004-3702(92)90041-u",
    "ISSN": "00043702",
    "abstract": "This paper presents a logical approach to nonmonotonic reasoning based on the notion of a nonmonotonic consequence relation. A conditional knowledge base, consisting of a set of conditional assertions of the type if … then …, represents the explicit defeasible knowledge an agent has about the way the world generally behaves. We look for a plausible definition of the set of all conditional assertions entailed by a conditional knowledge base. In a previous paper, Kraus and the authors defined and studied preferential consequence relations. They noticed that not all preferential relations could be considered as reasonable inference procedures. This paper studies a more restricted class of consequence relations, rational relations. It is argued that any reasonable nonmonotonic inference procedure should define a rational relation. It is shown that the rational relations are exactly those that may be represented by a ranked preferential model, or by a (nonstandard) probabilistic model. The rational closure of a conditional knowledge base is defined and shown to provide an attractive answer to the question of the title. Global properties of this closure operation are proved: it is a cumulative operation. It is also computationally tractable. This paper assumes the underlying language is propositional.",
    "author": [
      {
        "family": "Lehmann",
        "given": "Daniel"
      },
      {
        "family": "Magidor",
        "given": "Menachem"
      }
    ],
    "container-title": "Artificial Intelligence",
    "id": "Lehmann1992",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "logic, uncertainty",
    "language": "en-US",
    "page": "1-60",
    "title": "What does a conditional knowledge base entail?",
    "type": "article-journal",
    "volume": "55"
  },
  {
    "abstract": "The ZEN corpus covers early English newspapers published between 1661 and 1791. It documents newspapers as an emerging genre from the early issues of The London Gazette up to the period of the first publication of The Times. The corpus consists of 349 complete newspaper issues containing 1.6 million words that were keyed in manually.",
    "author": [
      {
        "family": "Lehmann",
        "given": "Hans M."
      },
      {
        "dropping-particle": "auf dem",
        "family": "Keller",
        "given": "Caren"
      },
      {
        "family": "Ruef",
        "given": "Beni"
      }
    ],
    "container-title": "Corpus-based studies of diachronic english",
    "editor": [
      {
        "family": "Facchinetti",
        "given": "Roberta"
      },
      {
        "family": "Rissanen",
        "given": "Matti"
      }
    ],
    "id": "Lehmann2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage, english",
    "language": "en-US",
    "page": "135-155",
    "publisher": "Peter Lang",
    "publisher-place": "Bern, Switzerland",
    "title": "ZEN Corpus 1.0",
    "type": "chapter"
  },
  {
    "abstract": "Is science the only path to knowledge? In this sparkling and provocative book, Jonah Lehrer explains that when it comes to understanding the brain, art got there first. Taking a group of celebrated writers, painters and composers, Lehrer shows us how artists have discovered truths about the human mind - real, tangible truths - that science is only now rediscovering. We learn, for example, how Proust first revealed the fallibility of memory; how George Eliot understood the brain’s malleability; how the French chef Escoffier intuited umami (the fifth taste); how Cezanne worked out the subtleties of vision; and how Virginia Woolf pierced the mysteries of consciousness. It’s a riveting tale of art trumping science again and again.",
    "author": [
      {
        "family": "Lehrer",
        "given": "Jonah"
      }
    ],
    "id": "Lehrer2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "publisher": "Canongate",
    "publisher-place": "Edinburgh, UK",
    "title": "Proust was a neuroscientist",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Leidner",
        "given": "Jochen L."
      }
    ],
    "genre": "Magisterarbeit",
    "id": "Leidner1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
    "title": "Links-assoziative Morphologie des Englischen mit stochastischer Disambiguierung",
    "type": "thesis"
  },
  {
    "URL": "http://www.iccs.inf.ed.ac.uk/~jleidner/documents/leidner-2004-fyrsigir.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Leidner",
        "given": "Jochen L."
      }
    ],
    "container-title": "Proceedings of the the 27th annual international ACM SIGIR conference (SIGIR 2004)",
    "id": "Leidner2004",
    "note": "Abstract, Doctoral Consortium",
    "publisher-place": "Sheffield, UK",
    "title": "Toponym resolution in text: “Which sheffield ist it?”",
    "title-short": "Toponym resolution in text",
    "type": "paper-conference"
  },
  {
    "URL": "http://hdl.handle.net/1842/1849",
    "author": [
      {
        "family": "Leidner",
        "given": "Jochen L."
      }
    ],
    "genre": "PhD thesis",
    "id": "Leidner2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "geocoding, toponym_resolution",
    "language": "en-US",
    "publisher": "University of Edinburgh",
    "publisher-place": "Edinburgh",
    "title": "Toponym resolution in text",
    "type": "thesis"
  },
  {
    "author": [
      {
        "family": "Leidner",
        "given": "Jochen L."
      }
    ],
    "id": "Leidner2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "publisher": "Universal",
    "publisher-place": "Boca Raton, FL, USA",
    "title": "Toponym resolution in text: Annotation, evaluation and applications of spatial grounding of place names",
    "title-short": "Toponym resolution in text",
    "type": "book"
  },
  {
    "DOI": "10.1145/2682571.2797097",
    "ISBN": "978-1-4503-3307-8",
    "abstract": "Madoko is a novel authoring system for writing complex documents. It is especially well suited for complex academic or industrial documents, like scientific articles, reference manuals, or math-heavy presentations. It started out as a project to take a fresh look at how we write academic articles.",
    "author": [
      {
        "family": "Leijen",
        "given": "Daan"
      }
    ],
    "container-title": "Proceedings of the 2015 ACM symposium on document engineering",
    "id": "Leijen2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "authoring, document_engineering, scientific_publishing",
    "language": "en-US",
    "page": "129-132",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Madoko: Scholarly documents for the Web",
    "title-short": "Madoko",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Lenders",
        "given": "Winfried"
      },
      {
        "family": "Bátori",
        "given": "I."
      },
      {
        "family": "Dogil",
        "given": "G."
      },
      {
        "family": "Görz",
        "given": "G."
      },
      {
        "family": "Seewald",
        "given": "U."
      }
    ],
    "container-title": "Linguistische verifikation. Dokumentation zur ersten morpholympics 1994",
    "editor": [
      {
        "family": "Hausser",
        "given": "Roland"
      }
    ],
    "id": "Lenders1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "morphology",
    "language": "en-US",
    "page": "15-24",
    "publisher": "Niemeyer",
    "publisher-place": "Tübingen",
    "title": "Stellungnahme der Jury für die Morpholympics 94",
    "type": "chapter"
  },
  {
    "URL": "http://www.newmine.org/NewMinE_ePaper2.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Lepori",
        "given": "Benedetto"
      },
      {
        "family": "Succi",
        "given": "Chiara"
      }
    ],
    "genre": "{2\\textsuperscript{nd} Report of the Educational\n                  Management in the Swiss Virtual Campus Mandate\n                  (EDUM)}",
    "id": "Lepori2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "publisher": "ICeF – NewMinE",
    "title": "eLearning in Higher Education: Prospects for Swiss Universities",
    "type": "report"
  },
  {
    "accessed": {
      "date-parts": [
        [
          2010,
          1,
          14
        ]
      ]
    },
    "author": [
      {
        "family": "Lesbegueries",
        "given": "Julien"
      },
      {
        "family": "Sallaberry",
        "given": "Christian"
      },
      {
        "family": "Gaio",
        "given": "Mauro"
      }
    ],
    "container-title": "Proceedings of the 3<sup>rd</sup> workshop on geographic information retrieval (GIR 2006)",
    "editor": [
      {
        "family": "Purves",
        "given": "Ross"
      },
      {
        "family": "Jones",
        "given": "Chris"
      }
    ],
    "id": "Lesbegueries2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "title": "Associating spatial patterns to text-units for summarizing geographic information",
    "type": "paper-conference"
  },
  {
    "URL": "http://plan9.bell-labs.com/7thEdMan/",
    "author": [
      {
        "family": "Lesk",
        "given": "Michael E."
      }
    ],
    "genre": "Computing Science Technical Report",
    "id": "Lesk1978",
    "issued": {
      "date-parts": [
        [
          1978,
          6
        ]
      ]
    },
    "number": "69",
    "publisher": "Bell Laboratories",
    "publisher-place": "Murray Hill, NJ",
    "title": "Some applications of inverted indexes on the UNIX system",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Leuf",
        "given": "Bo"
      },
      {
        "family": "Cunningham",
        "given": "Ward"
      }
    ],
    "id": "Leuf2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "publisher": "Addison-Wesley",
    "publisher-place": "Harlow, UK",
    "title": "The wiki way: Quick collaboration on the web",
    "title-short": "The wiki way",
    "type": "book"
  },
  {
    "DOI": "10.1080/13658810701626244",
    "abstract": "Metonymically used location names (toponyms) refer to other, related entities and thus possess a meaning different from their literal, geographic sense. Metonymic uses are to be treated differently to improve the performance of geographic information retrieval (GIR). Statistics on toponym senses show that 75.06% of all location names are used in their literal sense, 17.05% are used metonymically, and 7.89% have a mixed sense. This article presents a method for disambiguating location names in texts between literal and metonymic senses, based on shallow features. The evaluation of this method is two-fold. First, we use a memory-based learner (TiMBL) to train a classifier and determine standard evaluation measures such as F-score and accuracy. The classifier achieved an F-score of 0.842 and an accuracy of 0.846 for identifying toponym senses in a subset of the CoNLL (Conference on Natural Language Learning) data. Second, we perform retrieval experiments based on the GeoCLEF data (newspaper article corpus and queries) from 2005 and 2006. We compare searching location names in a database index containing both their literal and metonymic senses with searching in an index containing their literal senses only. Evaluation results indicate that removing metonymic senses from the index yields a higher mean average precision (MAP) for GIR. In total, we observed a significant gain in MAP: an increase from 0.0704 to 0.0715 MAP for the GeoCLEF 2005 data, and an increase from 0.1944 to 0.2100 MAP for the GeoCLEF 2006 data.",
    "author": [
      {
        "family": "Leveling",
        "given": "Johannes"
      },
      {
        "family": "Hartrumpf",
        "given": "Sven"
      }
    ],
    "container-title": "Int. J. Geogr. Inf. Sci.",
    "id": "Leveling2008",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "geocoding, toponym_resolution",
    "language": "en-US",
    "page": "289-299",
    "publisher": "Taylor & Francis",
    "publisher-place": "Bristol, PA, USA",
    "title": "On metonymy recognition for geographic information retrieval",
    "type": "article-journal",
    "volume": "22"
  },
  {
    "abstract": "Not Available",
    "author": [
      {
        "family": "Levenshtein",
        "given": "Vladimir I."
      }
    ],
    "container-title": "Cybernetics and Control Theory",
    "id": "Levenshtein1966",
    "issue": "8",
    "issued": {
      "date-parts": [
        [
          1966
        ]
      ]
    },
    "keyword": "approximate_matching, classic",
    "language": "en-US",
    "page": "707-710",
    "title": "Binary codes capable of correcting deletions, insertions and reversals",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "DOI": "10.1145/62506.62545",
    "author": [
      {
        "family": "Levy",
        "given": "David M."
      }
    ],
    "container-title": "DOCPROCS ’88: Proceedings of the ACM conference on document processing systems",
    "id": "Levy1988",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "document_research",
    "page": "187-193",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Topics in document research",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/192757.192760",
    "ISBN": "0897916409",
    "author": [
      {
        "family": "Levy",
        "given": "David M."
      }
    ],
    "container-title": "ECHT ’94: Proceedings of the 1994 ACM european conference on hypermedia technology",
    "id": "Levy1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "document_research, hypertext, literature, philosophy",
    "language": "en-US",
    "page": "24-31",
    "publisher": "ACM",
    "title": "Fixed or fluid?: Document stability and new media",
    "title-short": "Fixed or fluid?",
    "type": "paper-conference"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=1858681.1858797",
    "abstract": "This paper presents a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrases given a context. We use a topic model to decompose this conditional probability into two conditional probabilities with latent variables. We propose three different instantiations of the model for solving sense disambiguation problems with different degrees of resource availability. The proposed models are tested on three different tasks: coarse-grained word sense disambiguation, fine-grained word sense disambiguation, and detection of literal vs. non-literal usages of potentially idiomatic expressions. In all three cases, we outperform state-of-the-art systems either quantitatively or statistically significantly.",
    "author": [
      {
        "family": "Li",
        "given": "Linlin"
      },
      {
        "family": "Roth",
        "given": "Benjamin"
      },
      {
        "family": "Sporleder",
        "given": "Caroline"
      }
    ],
    "container-title": "Proceedings of the 48th annual meeting of the association for computational linguistics (ACL ’10)",
    "id": "Li2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "phraseology, topic_modeling, wikipedia",
    "language": "en-US",
    "page": "1138-1147",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Topic models for word sense disambiguation and token-based idiom detection",
    "type": "paper-conference"
  },
  {
    "ISBN": "978-3-642-20160-8",
    "URL": "http://portal.acm.org/citation.cfm?id=1996889.1996972",
    "abstract": "In this paper we propose a general framework for word sense disambiguation using knowledge latent in Wikipedia. Specifically, we exploit the rich and growing Wikipedia corpus in order to achieve a large and robust knowledge repository consisting of keyphrases and their associated candidate topics. Keyphrases are mainly derived from Wikipedia article titles and anchor texts associated with wikilinks. The disambiguation of a given keyphrase is based on both the commonness of a candidate topic and the context-dependent relatedness where unnecessary (and potentially noisy) context information is pruned. With extensive experimental evaluations using different relatedness measures, we show that the proposed technique achieved comparable disambiguation accuracies with respect to state-of-the-art techniques, while incurring orders of magnitude less computation cost.",
    "author": [
      {
        "family": "Li",
        "given": "Chenliang"
      },
      {
        "family": "Sun",
        "given": "Aixin"
      },
      {
        "family": "Datta",
        "given": "Anwitaman"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Proceedings of the 33rd european conference on advances in information retrieval",
    "id": "Li2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "wikipedia, wsd",
    "language": "en-US",
    "page": "653-664",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "A generalized method for word sense disambiguation based on Wikipedia",
    "type": "paper-conference",
    "volume": "6611"
  },
  {
    "URL": "http://memex.org/licklider.pdf",
    "author": [
      {
        "family": "Licklider",
        "given": "J. C. R."
      },
      {
        "family": "Taylor",
        "given": "Robert W."
      }
    ],
    "container-title": "Science and Technology",
    "id": "Licklider1968",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1968
        ]
      ]
    },
    "keyword": "classic, hci",
    "language": "en-US",
    "page": "20-41",
    "title": "The computer as a communication device",
    "type": "article-journal",
    "volume": "76"
  },
  {
    "author": [
      {
        "family": "Lienert",
        "given": "Gustav A."
      },
      {
        "family": "Raatz",
        "given": "Ulrich"
      }
    ],
    "edition": "6",
    "id": "Lienert1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "publisher": "Psychologie Verlags Union",
    "publisher-place": "Weinheim",
    "title": "Testaufbau und Testanalyse",
    "type": "book"
  },
  {
    "URL": "http://www.ascilite.org.au/conferences/melbourne01/pdf/papers/listerr.pdf",
    "abstract": "We describe a minimal XML mark-up for multiple choice exams. In our system, exams may be generated at anytime, by choosing a subset of questions from a pool. Furthermore, the system randomises the order of the choices within each chosen question. Thus a student may sit the exam many times. Our first version of this system has been described elsewhere. In this paper, we discuss the limitations of our first system that led to our current work, and give a description of the new system features, including XML tags for supporting collaborative authoring.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Lister",
        "given": "Raymond"
      },
      {
        "family": "Jerram",
        "given": "Peter"
      }
    ],
    "container-title": "Meeting at the crossroads. Short paper proceedings of the 18th annual conference of the australian society for computers in learning in tertiary education",
    "editor": [
      {
        "family": "Kennedy",
        "given": "G."
      },
      {
        "family": "Keppell",
        "given": "M."
      },
      {
        "family": "Mcnaught",
        "given": "C."
      },
      {
        "family": "Petrovic",
        "given": "T."
      }
    ],
    "id": "Lister2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "assessment, e-learning",
    "language": "en-US",
    "page": "101-104",
    "publisher": "Biomedical Multimedia Unit, The University of Melbourne",
    "publisher-place": "Melbourne, Australia",
    "title": "Minimal mark-up of multiple choice exams using XML",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1108/00197850210697168",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Little",
        "given": "Bob"
      }
    ],
    "container-title": "Industrial and Commercial Training",
    "id": "Little2002",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "language": "en-US",
    "page": "76-79",
    "title": "Harnessing learning technology to succeed in business",
    "type": "article-journal",
    "volume": "34"
  },
  {
    "ISBN": "0415403618",
    "abstract": "Blended and online learning skills are rapidly becoming essential for effective teaching and learning in universities and colleges. Covering theory where useful but maintaining an emphasis on practice, this book provides teachers and lecturers with an accessible introduction to e-learning. Beginning by exploring the meaning of ’e-learning’, it supports tutors in identifying how they plan to use technology to support courses that blend online and face-to-face interactions. Illustrated by a range of case of studies, the book covers: designing quality, appropriate effective and online learning efficient and sustainable e-learning activity providing appropriate feedback to learners devising student activities and sourcing learning resources managing online and offline interactions. Packed with practical advice and ideas, this book provides the core skills and knowledge that teachers in HE and FE need when starting out and further developing their teaching course design for blended and online learning.",
    "author": [
      {
        "family": "Littlejohn",
        "given": "Allison"
      },
      {
        "family": "Pegler",
        "given": "Chris"
      }
    ],
    "edition": "New edition",
    "id": "Littlejohn2007",
    "issued": {
      "date-parts": [
        [
          2007,
          6,
          16
        ]
      ]
    },
    "keyword": "e-learning, pedagogy",
    "language": "en-US",
    "publisher": "Paperback; Routledge",
    "title": "Preparing for blended E-Learning",
    "type": "book"
  },
  {
    "DOI": "10.1109/CCECE.2003.1225995",
    "abstract": "This paper analyses current standards and proposals for e-learning system architecture. Its main objective is to contribute an original proposal for a functional architecture and service architecture for building standard-driven distributed and interoperable learning systems. The functional architecture defines components that make up an e-learning system and the objects that must be moved among these components. We implement the service model with Web Services technology to provide a standard means of communication among different Learning management systems and different content authoring tools. This paper focuses on how to integrate Web Services on the e-learning application domain. We use J2EE as our technical infrastructure to build our components and integrate with Web service.",
    "author": [
      {
        "family": "Liu",
        "given": "Xiaofei"
      },
      {
        "family": "El Saddik",
        "given": "Abdulmotaleb"
      },
      {
        "family": "Georganas",
        "given": "Nicolas D."
      }
    ],
    "container-title": "IEEE canadian conference on electrical and computer engineering, 2003 (IEEE CCECE 2003)",
    "id": "Liu2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "717-720",
    "title": "An implementable architecture of an e-learning system",
    "type": "paper-conference",
    "volume": "2"
  },
  {
    "URL": "http://aclweb.org/anthology/P11-2013",
    "abstract": "Most text message normalization approaches are based on supervised learning and rely on human labeled training data. In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel web-based approach and performed character-level alignment for model training. Experiments on both Twitter and SMS messages show that our system significantly outperformed the state-of-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21.69% and 18.16% over jazzy spell checker on the two test sets respectively).",
    "author": [
      {
        "family": "Liu",
        "given": "Fei"
      },
      {
        "family": "Weng",
        "given": "Fuliang"
      },
      {
        "family": "Wang",
        "given": "Bingqing"
      },
      {
        "family": "Liu",
        "given": "Yang"
      }
    ],
    "container-title": "Proceedings of the 49<sup>th</sup> annual meeting of the association for computational linguistics: Human language technologies",
    "id": "Liu2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "microtext, spelling_normalization",
    "language": "en-US",
    "page": "71-76",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Insertion, deletion, or substitution? Normalizing text messages without pre-categorization nor supervision",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/download/4815/5410",
    "abstract": "In this work, we build a large scale reasoning engine under temporal RDFS semantics using MapReduce. We identify the major challenges of applying MapReduce framework to reason over temporal information, and present our solutions to tackle them.",
    "author": [
      {
        "family": "Liu",
        "given": "Chang"
      },
      {
        "family": "Qi",
        "given": "Guilin"
      },
      {
        "family": "Yu",
        "given": "Yong"
      }
    ],
    "container-title": "Proceedings of the twenty-sixth AAAI conference on artificial intelligence",
    "id": "Liu2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "rdf, semantic_web, temporal_data",
    "language": "en-US",
    "page": "2441-2442",
    "publisher": "AAAI",
    "publisher-place": "Palo Alto, CA, USA",
    "title": "Large scale temporal RDFS reasoning using MapReduce",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1632/pmla.2013.128.2.409",
    "ISSN": "0030-8129",
    "author": [
      {
        "family": "Liu",
        "given": "Alan"
      }
    ],
    "container-title": "Publications of the Modern Language Association of America",
    "id": "Liu2013",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "409-423",
    "title": "The meaning of the digital humanities",
    "type": "article-journal",
    "volume": "128"
  },
  {
    "author": [
      {
        "family": "Liu",
        "given": "Bing"
      }
    ],
    "id": "Liu2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Cambridge University Press",
    "publisher-place": "New York, NY, USA",
    "title": "Sentiment analysis: Mining opinions, sentiments, and emotions",
    "title-short": "Sentiment analysis",
    "type": "book"
  },
  {
    "DOI": "10.1109/ITI.2007.4283829",
    "ISBN": "953-7138-09-7",
    "abstract": "The goal of this paper is to discuss the language identification problem of Croatian, language that even state-of-the-art language identification tools find, hard to distinguish from similar languages, such as Serbian, Slovenian or Slovak language. We developed the tool that implements the list of Croatian most frequent words with the threshold that each document needs to satisfy, we added, the specific characters elimination rule, applied second-order Markov model classification and a, rule of forbidden words. Finally, we built up the tool that, overperforms current tools in discriminating between these similar languages.",
    "author": [
      {
        "family": "Ljubešić",
        "given": "Nikola"
      },
      {
        "family": "Mikelić",
        "given": "Nives"
      },
      {
        "family": "Boras",
        "given": "Damir"
      }
    ],
    "container-title": "Proceedings of the  int. Conf. On information technology interfaces (ITI 2007)",
    "id": "Ljubesic2007",
    "issued": {
      "date-parts": [
        [
          2007,
          6
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "language": "en-US",
    "page": "541-546",
    "publisher": "IEEE",
    "title": "Language indentification: How to distinguish similar languages?",
    "title-short": "Language indentification",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/BF02297052",
    "abstract": "A study of authoring system technology was done to update a previous study done in 1985. While the earlier study focused on software selection methods, the current one emphasizes trends in authoring tool characteristics, vendor approaches to the marketplace, and evaluation methods. It also raises questions about some of the often tacit assumptions underlying the technology’s development and use.",
    "author": [
      {
        "family": "Locatis",
        "given": "Craig"
      },
      {
        "family": "Ullmer",
        "given": "Eldon"
      },
      {
        "family": "Carr",
        "given": "Victor"
      },
      {
        "family": "Banvard",
        "given": "Richard"
      },
      {
        "family": "Le",
        "given": "Quang"
      },
      {
        "family": "Lo",
        "given": "Raulie"
      },
      {
        "family": "Williamson",
        "given": "Matthews"
      }
    ],
    "container-title": "Educational Technology Research and Development",
    "id": "Locatis1992",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "77-82",
    "title": "Authoring systems reassessed",
    "type": "article-journal",
    "volume": "40"
  },
  {
    "DOI": "10.1007/BF02299634",
    "abstract": "Computer systems, interactive technologies, and the software tools for authoring multimedia programs have evolved over the past several decades. The evolution of multimedia technology is discussed and current alternatives for authoring multimedia programs are described. Issues related to analyzing authoring tool requirements and selecting software are presented and trends in the evolution of authoring tools are identified. Many factors affecting the selection and use of authoring software are exogenous to the technology itself. Both technological trends and market forces must be considered.",
    "author": [
      {
        "family": "Locatis",
        "given": "Craig"
      },
      {
        "family": "Al-Nuaim",
        "given": "Hana"
      }
    ],
    "container-title": "Educational Technology Research and Development",
    "id": "Locatis1999",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "63-75",
    "title": "Interactive technology and authoring tools: A historical review and analysis",
    "title-short": "Interactive technology and authoring tools",
    "type": "article-journal",
    "volume": "47"
  },
  {
    "URL": "http://www.tagesspiegel.de/wissen/19721710.html",
    "abstract": "Den Digital Humanities fehlt ein sinnstiftendes Manifest: Es gilt, die Lücke zwischen Datensammlern und Gelehrten zu schließen. Ein Gastbeitrag.",
    "author": [
      {
        "family": "Loescher",
        "given": "Jens"
      }
    ],
    "container-title": "Tagesspiegel",
    "id": "Loescher2017",
    "issue": "23 098",
    "issued": {
      "date-parts": [
        [
          2017,
          4,
          27
        ]
      ]
    },
    "keyword": "digital_humanities, in_the_media",
    "language": "de-DE",
    "page": "19+",
    "title": "Garagenbastler der Geisteswissenschaften",
    "type": "article-journal",
    "volume": "73"
  },
  {
    "DOI": "10.1111/j.1536-7150.1993.tb02536.x",
    "abstract": "The electronic revolution in publishing has altered some of the traditional relationships in the industry. Today, computers are the common link between authors, publishers, and compositors. Consider how computers have revolutionized the way authors prepare their manuscripts. Computers not only make revising and rewriting a lot easier, they also provide a way for an author’s keystrokes to be stored and used by the compositor. Most of the typesetting systems used by today’s compositors can read and translate files from virtually any word-processing program. Some of these typesetting systems can even preserve the author’s formatting–the bold and italic and superscript and subscript that computers allow authors to use.",
    "author": [
      {
        "family": "Long",
        "given": "Fred A."
      }
    ],
    "container-title": "American Journal of Economics and Sociology",
    "id": "Long1993",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "ocr, typesetting",
    "language": "en-US",
    "page": "223-226",
    "publisher": "Blackwell Publishing Ltd",
    "title": "Electronic composition and the typesetter",
    "type": "article-journal",
    "volume": "52"
  },
  {
    "author": [
      {
        "family": "Longrée",
        "given": "Dominique"
      },
      {
        "family": "Purnelle",
        "given": "Gérald"
      },
      {
        "family": "Philippart",
        "given": "Caroline"
      }
    ],
    "collection-title": "Innsbrucker beiträge zur sprachwissenschaft",
    "container-title": "Latin linguistics today. Akten des 15. Internationalem kolloquiums zur lateinischen linguistik",
    "editor": [
      {
        "family": "Anreiter",
        "given": "Peter"
      },
      {
        "family": "Kienpointner",
        "given": "Manfred"
      }
    ],
    "id": "Longree2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, latin",
    "language": "en-US",
    "page": "677-685",
    "title": "Subordinate clause boundaries and word order in Latin: The contribution of the L.A.S.L.A. Syntactic parser project",
    "title-short": "Subordinate clause boundaries and word order in Latin",
    "type": "paper-conference",
    "volume": "137"
  },
  {
    "URL": "https://www.w3.org/2009/12/rdf-ws/papers/ws09",
    "abstract": "While the current mechanism of reification in RDF is without semantics and widely considered inappropriate and cumbersome, some form of reification – speaking about triples themselves – is needed in RDF for many reasonable applications: in particular, reification allows for enhancing triples with annotations relating to provenance, spatio-temporal validity, degrees of trust, fuzzy values and/or other contextual information. In this position paper, we argue that – besides resolving the issue of how to syntactically represent reification in the future (i.e., whether to stick with the current reification mechanism or standardise a different mechanism such as Named Graphs) – it is time to agree on certain core annotations that are widely needed. We summarise existing work and provide a possible direction towards handling reification by means of a general annotation framework that can be instantiated for those major use cases we currently see arising.",
    "author": [
      {
        "family": "Lopes",
        "given": "Nuno"
      },
      {
        "family": "Zimmermann",
        "given": "Antoine"
      },
      {
        "family": "Hogan",
        "given": "Aidan"
      },
      {
        "family": "Lukácsy",
        "given": "Gergely"
      },
      {
        "family": "Polleres",
        "given": "Axel"
      },
      {
        "family": "Straccia",
        "given": "Umberto"
      },
      {
        "family": "Decker",
        "given": "Stefan"
      }
    ],
    "container-title": "W3C RDF next steps workshop",
    "id": "Lopes2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "semantic_web, uncertainty",
    "language": "en-US",
    "title": "RDF needs annotations",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1006/cviu.1996.0502",
    "ISSN": "1077-3142",
    "abstract": "We present experimental results suggesting that between 20 and 50% of the errors caused by a single OCR package can be eliminated by simply scanning a page three times and running a \"consensus sequence\" voting procedure. This technique, which originates from molecular biology, takes exponential time in general, but can be specialized to a fast heuristic guaranteed to be optimal for the cases of interest. The improvement in recognition accuracy is achieved without making a priori assumptions about the distribution of OCR errors (i.e., no \"training\" is required).",
    "author": [
      {
        "family": "Lopresti",
        "given": "Daniel"
      },
      {
        "family": "Jiangying",
        "given": "Zhou"
      }
    ],
    "container-title": "Computer Vision and Image Understanding",
    "id": "Lopresti1997",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "ocr, spelling_correction",
    "language": "en-US",
    "page": "39-47",
    "publisher": "Academic Press",
    "title": "Using consensus sequence voting to correct OCR errors",
    "type": "article-journal",
    "volume": "67"
  },
  {
    "DOI": "10.1145/1390749.1390753",
    "ISBN": "978-1-60558-196-5",
    "abstract": "Errors are unavoidable in advanced computer vision applications such as optical character recognition, and the noise induced by these errors presents a serious challenge to down-stream processes that attempt to make use of such data. In this paper, we apply a new paradigm we have proposed for measuring the impact of recognition errors on the stages of a standard text analysis pipeline: sentence boundary detection, tokenization, and part-of-speech tagging. Our methodology formulates error classification as an optimization problem solvable using a hierarchical dynamic programming approach. Errors and their cascading effects are isolated and analyzed as they travel through the pipeline. We present experimental results based on a large collection of scanned pages to study the varying impact depending on the nature of the error and the character(s) involved. The problem of identifying tabular structures that should not be parsed as sentential text is also discussed.",
    "author": [
      {
        "family": "Lopresti",
        "given": "Daniel"
      }
    ],
    "container-title": "Proceedings of the second workshop on analytics for noisy unstructured text data (AND 2008)",
    "id": "Lopresti2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, nlp, ocr",
    "language": "en-US",
    "page": "9-16",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Optical character recognition errors and their effects on natural language processing",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Lorenz",
        "given": "Oliver"
      }
    ],
    "genre": "Magisterarbeit",
    "id": "Lorenz1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "publisher": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
    "title": "Automatische Wortformerkennung für das Deutsche im Rahmen von Malaga",
    "type": "thesis"
  },
  {
    "author": [
      {
        "family": "Lorenz",
        "given": "Annabell"
      }
    ],
    "container-title": "Lernen im digitalen Zeitalter. GMW09 – 4. Europäische Jahrestagung Gesellschaft für Medien in der Wissenschaft (GMW)",
    "editor": [
      {
        "family": "Apostolopoulos",
        "given": "Nicolas"
      },
      {
        "family": "Schwill",
        "given": "Andreas"
      }
    ],
    "id": "Lorenz2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "GMW",
    "title": "Elchtest in Austria – Umstände eines LMS-Wechsels und seine Folgen – ein Prüfbericht",
    "type": "paper-conference"
  },
  {
    "URL": "https://eric.ed.gov/?q=ED287493",
    "abstract": "Designed to facilitate the understanding of the way scholars work in the context of new organizational and technical capabilities, this annotated bibliography includes 167 sources related to topics such as research methods and communication practice:, in the disciplines of the humanities and social sciences; computer applications in social sciences and humanities research; communication technologies; bibliographic information retrieval systems; and computers and society. The bibliographic citation and a brief description of the contents are included for each source. Following a prefatory discussion of the purpose and contents of the bibliography, sources are listed under the following headings:(1) Computers and Society; (2) Characteristics of Scholarly Research and Communication–General, Humanities, and Social Sciences; and (3) New Technologies in Scholarly Research and Communication–General, Bibliographic Information Retrieval, Data Organization and Analysis, and Electronic Publishing and Telecommunications. Author and title indices are also provided.",
    "author": [
      {
        "family": "Lowry",
        "given": "Anita"
      },
      {
        "family": "Stuveras",
        "given": "Junko"
      }
    ],
    "id": "Lowry1987",
    "issued": {
      "date-parts": [
        [
          1987,
          2
        ]
      ]
    },
    "keyword": "bibliography, digital_humanities",
    "language": "en-US",
    "publisher": "Council on Library Resources",
    "publisher-place": "Washington, D.C., USA",
    "title": "Scholarship in the electronic age. A selected bibliography on research and communication in the humanities and social sciences",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Lucke",
        "given": "Ulrike"
      },
      {
        "family": "Tavangarian",
        "given": "Djamshid"
      },
      {
        "family": "Voigt",
        "given": "Denny"
      }
    ],
    "container-title": "Proceedings of world conference on e-learning in corporate, government, healthcare, and higher education (e-learn) 2003, phoenix, AZ",
    "editor": [
      {
        "family": "Richards",
        "given": "Griff"
      }
    ],
    "id": "Lucke2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "publisher": "AACE",
    "title": "Multidimensional educational multimedia with ",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.center.kva.se/svenska/forskning/NS_147_Program.html",
    "author": [
      {
        "family": "Lüdeling",
        "given": "Anke"
      }
    ],
    "collection-title": "Nobel symposium",
    "container-title": "Going digital. Evolutionary and revolutionary aspects of digitization",
    "editor": [
      {
        "family": "Grandin",
        "given": "Karl"
      }
    ],
    "id": "Luedeling2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "tei, xml",
    "language": "en-US",
    "page": "220-243",
    "publisher": "Science History Publications",
    "publisher-place": "New York, NY, USA",
    "title": "Corpora in linguistics: Sampling and annotation",
    "title-short": "Corpora in linguistics",
    "type": "chapter",
    "volume": "147"
  },
  {
    "DOI": "10.1145/1330598.1330642",
    "author": [
      {
        "family": "Lukashenko",
        "given": "Romans"
      },
      {
        "family": "Graudina",
        "given": "Vita"
      },
      {
        "family": "Grundspenkis",
        "given": "Janis"
      }
    ],
    "container-title": "CompSysTech ’07: Proceedings of the 2007 international conference on computer systems and technologies",
    "editor": [
      {
        "family": "Rachev",
        "given": "Boris"
      },
      {
        "family": "Smrikarov",
        "given": "Angel"
      },
      {
        "family": "Dimov",
        "given": "Dimo"
      }
    ],
    "id": "Lukashenko2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "page": "1-6",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Computer-based plagiarism detection methods and tools: An overview",
    "title-short": "Computer-based plagiarism detection methods and tools",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1096601.1096615",
    "abstract": "The Document Description Framework (DDF) is a representation for variable-data documents. It supports very high flexibility in the type and extent of variation supported, considerably beyond the ’copy-hole’ or flow-based mechanisms of existing formats and tools. DDF is based on holding application data, logical data structure and presentation as well as constructional ’programs’ together within a single document. DDF documents can be merged with other documents, bound to variable values incrementally, combine several types of layout and styling in the same document and support final delivery to different devices and page-ready formats. The framework uses XML syntax and fragments of XSLT to describe ’programmatic construction’ of a bound document. DDF is extensible, especially in the ability to add new types of layout and inter-operability between components in different formats. In this paper we describe the motivation for DDF, the major design choices and how we evaluate a DDF document with specific data values. We show through implemented examples how it can be used to construct high-complexity and variability presentations and how the framework complements and can use many existing XML-based documents formats, such as SVG and XSL-FO.",
    "author": [
      {
        "family": "Lumley",
        "given": "John"
      },
      {
        "family": "Gimson",
        "given": "Roger"
      },
      {
        "family": "Rees",
        "given": "Owen"
      }
    ],
    "container-title": "Proceedings of the 2005 ACM symposium on document engineering",
    "id": "Lumley2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "document_engineering, functional, svg, xml, xslt",
    "language": "en-US",
    "page": "32-41",
    "publisher": "ACM",
    "publisher-place": "Bristol, United Kingdom",
    "title": "A framework for structure, layout & function in documents",
    "type": "paper-conference"
  },
  {
    "URL": "https://eric.ed.gov/?id=ED089663",
    "abstract": "The MITRE Corporation is disseminating computer-assisted instruction (CAI) through a demonstration program funded by the National Science Foundation. The goal is to show that CAI can provide improved, cost-effective instruction in community colleges. Significant products include: two demonstration systems, each with 128 student terminals; four semesters of math and English courses; and a complete package of authoring and delivery software. Some of the major innovations of the Time-Shared, Interactive, Computer-Controlled Information Television (TICCIT) system include: 1) courseware designed to produce mastery, improve learning strategies and student attitudes, and develop responsibility; 2) the use of audio and color television displays (the Digicolor System); 3) minicomputers to power self-contained systems; 4) low system cost of approximately $450,000 for one system or $250,000 for moderate quantities; 5) efficient courseware production procedures; 6) an on-line authoring system which produces quality CAI; 7) a learner control command language which fosters efficient strategies and positive attitudes; and 8) an implementation planning model which promotes faculty acceptance and system integration into colleges.",
    "author": [
      {
        "literal": "MITRE Corporation"
      }
    ],
    "id": "MITRE1974",
    "issued": {
      "date-parts": [
        [
          1974
        ]
      ]
    },
    "keyword": "e-learning, ticcit",
    "language": "en-US",
    "publisher": "MITRE Corporation",
    "publisher-place": "McLean, VA",
    "title": "An overview of the TICCIT program",
    "type": "report"
  },
  {
    "URL": "http://aclweb.org/anthology/W10-07",
    "abstract": "The NAACL-2010 Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk explores applications of crowdsourcing technologies for the creation and study of language data. Recent work has evaluated the effectiveness of using crowdsourcing platforms, such as Amazon’s Mechanical Turk, to create annotated data for natural language processing applications. This workshop further explores this area and these proceedings contain 34 papers and an overview paper that each experiment with applications of Mechanical Turk. The diversity of applications showcases the new possibilities for annotating speech and text, and has the potential to dramatically change how we create data for human language technologies.",
    "editor": [
      {
        "family": "Callison-Burch",
        "given": "Chris"
      },
      {
        "family": "Dredze",
        "given": "Mark"
      }
    ],
    "id": "MTURK2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "crowdsourcing, nlp",
    "language": "en-US",
    "publisher": "Association for Computational Linguistics",
    "title": "Proceedings of the NAACL HLT 2010 workshop on creating speech and language data with amazon’s mechanical turk",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-642-39283-2",
    "ISBN": "978-3-642-39282-5",
    "abstract": "This book goes to great depth concerning the fast growing topic of technologies and approaches of fuzzy logic in the Semantic Web. The topics of this book include fuzzy description logics and fuzzy ontologies, queries of fuzzy description logics and fuzzy ontology knowledge bases, extraction of fuzzy description logics and ontologies from fuzzy data models, storage of fuzzy ontology knowledge bases in fuzzy databases, fuzzy Semantic Web ontology mapping, and fuzzy rules and their interchange in the Semantic Web. The book aims to provide a single record of current research in the fuzzy knowledge representation and reasoning for the Semantic Web. The objective of the book is to provide the state of the art information to researchers, practitioners and graduate students of the Web intelligence and at the same time serve the knowledge and data engineering professional faced with non-traditional applications that make the application of conventional approaches difficult or impossible.",
    "author": [
      {
        "family": "Ma",
        "given": "Zongmin"
      },
      {
        "family": "Zhang",
        "given": "Fu"
      },
      {
        "family": "Yan",
        "given": "Li"
      },
      {
        "family": "Cheng",
        "given": "Jingwei"
      }
    ],
    "id": "Ma2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "semantic_web, uncertainty",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Fuzzy knowledge management for the Semantic Web",
    "type": "book",
    "volume": "306"
  },
  {
    "DOI": "10.1080/1355800750120202",
    "abstract": "The purpose of legibility research is not to discover universal truths, but to improve the practice of typography and design. Research should be directed to specific decisions in particular, practical situations. This article suggests a practical research style which makes use of the tacit know-how of typographers and designers. The idea is to preface any testing activities with critical and creative activities. This leads to a three-part cyclical model: criticism, alternatives and tests. Such a model starts with a practical problem and finishes with a decision. This model is illustrated by a critical analysis of some Open University texts.",
    "author": [
      {
        "family": "Macdonald-Ross",
        "given": "Michael"
      },
      {
        "family": "Waller",
        "given": "Robert H. W."
      }
    ],
    "container-title": "Innovations in Education and Teaching International",
    "id": "Macdonald-Ross1975",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1975
        ]
      ]
    },
    "keyword": "typography",
    "language": "en-US",
    "page": "75-83",
    "publisher": "Routledge",
    "title": "Criticism, alternatives and tests: A conceptual framework for improving typography",
    "title-short": "Criticism, alternatives and tests",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "URL": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1095380",
    "abstract": "This paper describes the Writer’s Workbench programs, which analyze English prose and suggest improvements. Some limited data on the use of the Writer’s Workbench and its acceptance are also presented. The Writer’s Workbench incorporates the style and diction programs, described in a previous paper of this TRANSACTIONS, into a more extensive system to help writers improve their writing. The system runs under the UNIX<sup>TM</sup>operating system, and includes programs to: 1) proofread, 2) comment on stylistic features of text, and 3) provide reference information about the English language. Among other writing faults, the programs detect split infinitives, errors in spelling and punctuation, overly long sentences, wordy phrases, and passive sentences.",
    "author": [
      {
        "family": "Macdonald",
        "given": "N."
      },
      {
        "family": "Frase",
        "given": "L."
      },
      {
        "family": "Gingrich",
        "given": "P."
      },
      {
        "family": "Keenan",
        "given": "S."
      }
    ],
    "container-title": "Communications, IEEE Transactions on [legacy, pre - 1988]",
    "id": "Macdonald1982",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "keyword": "authoring, interactive_editing",
    "language": "en-US",
    "page": "105-110",
    "title": "The writer’s workbench: Computer aids for text analysis",
    "title-short": "The writer’s workbench",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "DOI": "10.1108/00242530610667558",
    "ISSN": "0024-2535",
    "abstract": "Purpose – The purpose of the paper is to provide an overview of the collaborative tagging phenomenon and explore some of the reasons for its emergence. Design/methodology/approach – The paper reviews the related literature and discusses some of the problems associated with, and the potential of, collaborative tagging approaches for knowledge organisation and general resource discovery. A definition of controlled vocabularies is proposed and used to assess the efficacy of collaborative tagging. An exposition of the collaborative tagging model is provided and a review of the major contributions to the tagging literature is presented. Findings – There are numerous difficulties with collaborative tagging systems (e.g. low precision, lack of collocation, etc.) that originate from the absence of properties that characterise controlled vocabularies. However, such systems can not be dismissed. Librarians and information professionals have lessons to learn from the interactive and social aspects exemplified by collaborative tagging systems, as well as their success in engaging users with information management. The future co- existence of controlled vocabularies and collaborative tagging is predicted, with each appropriate for use within distinct information contexts: formal and informal. Research limitations/implications – Librarians and information professional researchers should be playing a leading role in research aimed at assessing the efficacy of collaborative tagging in relation to information storage, organisation, and retrieval, and to influence the future development of collaborative tagging systems. Practical implications – The paper indicates clear areas where digital libraries and repositories could innovate in order to better engage users with information. Originality/value – At time of writing there were no literature reviews summarising the main contributions to the collaborative tagging research or debate. Keywords Classification, Controlled languages, Information management, Information retrieval, Knowledge management Paper type General review",
    "author": [
      {
        "family": "Macgregor",
        "given": "George"
      },
      {
        "family": "McCulloch",
        "given": "Emma"
      }
    ],
    "container-title": "Library Review",
    "id": "Macgregor2006",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2006,
          6
        ]
      ]
    },
    "keyword": "collaborative_tagging, terminology",
    "language": "en-US",
    "page": "291-300",
    "title": "Collaborative tagging as a knowledge organisation and resource discovery tool",
    "type": "article-journal",
    "volume": "55"
  },
  {
    "ISBN": "0201144603",
    "author": [
      {
        "family": "Mackenzie",
        "given": "Charles"
      }
    ],
    "id": "Mackenzie1980",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Addison-Wesley",
    "publisher-place": "Reading, MA, USA",
    "title": "Coded character sets: History and development",
    "title-short": "Coded character sets",
    "type": "book"
  },
  {
    "abstract": "SGML is a passive standard. That is, it provides mechanisms through which descriptive markup to be applied to documents but says nothing about how these documents are to be processed. The SGML standard refers frequently to the ” application” but includes no clean mechanism for attaching applications to SGML parsers. The purpose of this paper is to present one such mechanism while maintaining compatibility with the current SGML standard.",
    "author": [
      {
        "family": "Macleod",
        "given": "Ian A."
      },
      {
        "family": "Nordin",
        "given": "Brent"
      },
      {
        "family": "Barnard",
        "given": "David T."
      },
      {
        "family": "Hamilton",
        "given": "Doug"
      }
    ],
    "container-title": "Proceedings of Electronic Publishing 1992 (EP 92)",
    "editor": [
      {
        "family": "Vanoirbeek",
        "given": "Christine"
      },
      {
        "family": "Coray",
        "given": "Giovanni"
      }
    ],
    "id": "Macleod1992",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "sgml",
    "language": "en-US",
    "page": "53-63",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge",
    "title": "A framework for developing SGML applications",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/5254.920602",
    "ISSN": "1541-1672",
    "abstract": "The Semantic Web relies heavily on formal ontologies to structure data for comprehensive and transportable machine understanding. Thus, the proliferation of ontologies factors largely in the Semantic Web’s success. The authors present an ontology learning framework that extends typical ontology engineering environments by using semiautomatic ontology construction tools. The framework encompasses ontology import, extraction, pruning, refinement and evaluation.",
    "author": [
      {
        "family": "Maedche",
        "given": "Alexander"
      },
      {
        "family": "Staab",
        "given": "Steffen"
      }
    ],
    "container-title": "IEEE Intelligent Systems",
    "id": "Maedche2001",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "semantic_web, vocabularies",
    "language": "en-US",
    "page": "72-79",
    "publisher": "IEEE",
    "publisher-place": "Piscataway, NJ, USA",
    "title": "Ontology learning for the Semantic Web",
    "type": "article-journal",
    "volume": "16"
  },
  {
    "DOI": "10.1145/1101149.1101255",
    "ISBN": "1-59593-044-2",
    "abstract": "We report experiments on the use of standard natural language processing (NLP) tools for the analysis of music lyrics. A significant amount of music audio has lyrics. Lyrics encode an important part of the semantics of a song, therefore their analysis complements that of acoustic and cultural metadata and is fundamental for the development of complete music information retrieval systems. Moreover, a textual analysis of a song can generate ground truth data that can be used to validate results from purely acoustic methods. Preliminary results on language identification, structure extraction, categorization and similarity searches suggests that a lot of profit can be gained from the analysis of lyrics.",
    "author": [
      {
        "family": "Mahedero",
        "given": "Jose P. G."
      },
      {
        "family": "Martínez",
        "given": "Álvaro"
      },
      {
        "family": "Cano",
        "given": "Pedro"
      },
      {
        "family": "Koppenberger",
        "given": "Markus"
      },
      {
        "family": "Gouyon",
        "given": "Fabien"
      }
    ],
    "container-title": "MULTIMEDIA ’05: Proceedings of the 13th annual ACM international conference on multimedia",
    "id": "Mahadero2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "ir, language_identification, nlp",
    "language": "en-US",
    "page": "475-478",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Natural language processing of lyrics",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/FIE.1996.567690",
    "abstract": "An important area for World Wide Web (WWW) development is customized educational web pages incorporating interactive features. Unfortunately, it is usually necessary for an engineering instructor to expend a substantial amount of time to learn the peculiarities of programming and debugging web-based interactive applications. To help eliminate this time hurdle a description is given in this paper of a set of software routines which allow an instructor to prepare a set of quiz questions or drill problems in the form of ordinary text files, then to convert the files automatically into hypertext markup language (HTML) with an interactive “forms” interface for inclusion in a web site. The relatively simple examples considered in this paper are intended to serve as templates for those individuals who are interested in probing more deeply into the intricacies of web software. Examples and computer code are also available",
    "author": [
      {
        "family": "Maher",
        "given": "Robert C."
      }
    ],
    "container-title": "Proceedings of FIE ’96, 26th annual conference “frontiers in education”",
    "id": "Maher1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "1035-1038",
    "title": "Simple but useful tools for interactive WWW development",
    "type": "paper-conference",
    "volume": "3"
  },
  {
    "URL": "http://maher.filfre.net/if-book/",
    "author": [
      {
        "family": "Maher",
        "given": "Jimmy"
      }
    ],
    "id": "Maher2006",
    "issued": {
      "date-parts": [
        [
          2006,
          5,
          8
        ]
      ]
    },
    "keyword": "interactive_fiction",
    "language": "en-US",
    "title": "Let’s tell a story together (a history of interactive fiction)",
    "type": ""
  },
  {
    "DOI": "10.1515/9783110891751",
    "ISBN": "9783110891751",
    "abstract": "On the occasion of the 150th anniversary of Gotthelf’s death, the editors began on an historical-critical edition of the author’s non-literary works, representing the first stage in the eventual historical-critical edition of Gotthelf’s entire oeuvre. The editors have engaged in extensive consultations with experienced editing experts and Gotthelf specialists on the objective of editing and commenting on Gotthelf’s complete works in their historical and political context, and on potential avenues to be explored toward the realization of this endeavor. The articles assembled here indicate new approaches to Gotthelf, whose experiences as a committed clergyman, school reformer, journalist, and contributor to religious almanacs all found their way into his narratives.",
    "author": [
      {
        "family": "Mahlmann-Bauer",
        "given": "Barbara"
      },
      {
        "dropping-particle": "von",
        "family": "Zimmermann",
        "given": "Christian"
      }
    ],
    "collection-title": "Beihefte zu editio",
    "id": "Mahlmann2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "De Gruyter",
    "publisher-place": "Berlin",
    "title": "Jeremias Gotthelf – Wege zu einer neuen Ausgabe",
    "type": "book",
    "volume": "24"
  },
  {
    "URL": "http://www.cl.uzh.ch/people/team/mahlow/maggi.online.pdf",
    "accessed": {
      "date-parts": [
        [
          2010,
          10,
          8
        ]
      ]
    },
    "author": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      }
    ],
    "genre": "Magisterarbeit",
    "id": "Mahlow2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "mahlow, malaga, nlp",
    "language": "de-DE",
    "publisher": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
    "title": "Automatische Wortformanalyse für das Spanische",
    "type": "thesis"
  },
  {
    "URL": "http://acl.ldc.upenn.edu/W/W04/W04-1709.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Hess",
        "given": "Michael"
      }
    ],
    "container-title": "COLING 2004 workshop “eLearning for computational linguistics and computational linguistics for eLearning”",
    "editor": [
      {
        "family": "Lothar Lemnitzer",
        "given": "Erhard Hinrichs",
        "suffix": "Detmar Meurers"
      }
    ],
    "id": "Mahlow2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "61-70",
    "publisher": "COLING",
    "publisher-place": "Geneva, Switzerland",
    "title": "Sentence completion tests for training and assessment in a computational linguistics curriculum",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-540-78135-6_54",
    "abstract": "Revising and editing are important parts of the writing process. In fact, multiple revision and editing cycles are crucial for the production of high-quality texts. However, revising and editing are also tedious and error-prone, since changes may introduce new errors. One reason why revising a text is tedious, is that editors and word processors offer few, if any, functions for handling text on the same cognitive level as the author: While the author is thinking in high-level linguistic terms, editors and word processors mostly provide low-level character oriented functions. Mapping the intended outcome to these low-level operations is distracting for the author, who now has to focus for a long time on small parts of the text. This results in a loss of global overview of the text and in typical revision errors (duplicate verbs, extraneous conjunctions, etc.). Grammar checkers, as offered by some word processors, are not a solution. Besides the fact that they are only available for few languages, and regardless of the debatable quality, their conceptual approach is not suitable for experienced writers, who actively create their texts. They neither need nor want critique from an automated system; they do not want to passively receive instructions, but they have well-defined ideas about their texts, which they want to realize. Thus, they need “power tools,” that operate on the same conceptual level as they do.",
    "author": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Computational linguistics and intelligent text processing, 9<sup>th</sup> international conference CICLing 2008",
    "editor": [
      {
        "family": "Gelbukh",
        "given": "Alexander"
      }
    ],
    "id": "Mahlow2008a",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "authoring, interactive_editing, mxp",
    "language": "en-US",
    "page": "631-642",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Linguistic support for revising and editing",
    "type": "paper-conference"
  },
  {
    "abstract": "While software developers have various power tools at their disposal that make the writing of computer programs more efficient, authors of texts do not have the support of such power tools. Text processors still operate on the level of characters and strings rather than on the level of word forms and grammatical constructions. This forces authors to constantly switch between low-level, character oriented, editing operations and high-level, conceptual, verbalisation processes. We suggest the development of language-aware text editing tools that simplify certain frequent, yet complex editing operations by defining them on the level of linguistic units. Pluralizing an entire noun phrase plus the verb forms governed by it would be an ambitious example, swapping the elements of a conjunctive construction a more modest one. We describe a pilot implementation for German where these operations are seamlessly integrated with the standard functions of an existing open-source editor. The operations can be invoked on demand and do not intrude on the authoring process. Changes can be performed locally or globally, thus simplifying the writing process considerably, and making the resulting texts more consistent.",
    "author": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Hess",
        "given": "Michael"
      }
    ],
    "container-title": "LREC 2008 workshop on NLP resources, algorithms and tools for authoring aids",
    "editor": [
      {
        "family": "Dale",
        "given": "Robert"
      },
      {
        "family": "Max",
        "given": "Aurélien"
      },
      {
        "family": "Zock",
        "given": "Michael"
      }
    ],
    "id": "Mahlow2008b",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "emacs, interactive_editing, mxp",
    "language": "en-US",
    "page": "9-13",
    "publisher": "ELRA",
    "publisher-place": "Marrakech, Morrocco",
    "title": "Language-aware text editing",
    "type": "paper-conference"
  },
  {
    "abstract": "In this paper we briefly outline editing functions which are aware of the structures of natural languages by using methods from computational linguistics. Such functions could reduce errors and better support writers in realizing their communicative goals. However, linguistic methods have limits, and there are various aspects software developers have to take into account to avoid creating a solution looking for a problem: Language-aware functions could be powerful tools for writers, but writers must not be forced to adapt to their tools.",
    "author": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Workshop on NLP for reading and writing: Resources, algorithms and tools",
    "editor": [
      {
        "family": "Sofkova Hashemi",
        "given": "Sylvana"
      },
      {
        "family": "Knutsson",
        "given": "Ola"
      },
      {
        "family": "Domeij",
        "given": "Rickard"
      },
      {
        "family": "Johansson Kokkinakis",
        "given": "Sofie"
      }
    ],
    "id": "Mahlow2008c",
    "issued": {
      "date-parts": [
        [
          2008,
          11
        ]
      ]
    },
    "keyword": "interactive_editing, mxp",
    "language": "en-US",
    "publisher": "KTH Royal Institute of Technology; Online",
    "title": "Computational linguistics for word processing: Opportunities and limits",
    "title-short": "Computational linguistics for word processing",
    "type": "paper-conference"
  },
  {
    "URL": "http://hdl.handle.net/10062/8696",
    "abstract": "In this paper we argue that the concept of language awareness, as known from programmer’s editors, can be transferred to writing natural language and word processors. We propose editing functions which use methods from computational linguistics and take the structures of natural languages into consideration. Such functions could reduce errors and better support writers in realizing their communicative goals. We briefly compare characteristics of programming languages and natural languages and their processing tools with respect to their suitability for being used in language-aware functions in editors. However, linguistic methods have limits, and there are various aspects software developers have to take into account to avoid creating a solution looking for a problem: Language-aware functions could be powerful tools for writers, but writers must not be forced to adapt to their tools.",
    "author": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "collection-title": "NEALT proceedings series",
    "container-title": "Proceedings of the workshop on NLP for reading and writing—resources, algorithms and tools",
    "editor": [
      {
        "family": "Domeij",
        "given": "Rickard"
      },
      {
        "family": "Johansson Kokkinakis",
        "given": "Sofie"
      },
      {
        "family": "Knutsson",
        "given": "Ola"
      },
      {
        "family": "Sofkova Hashemi",
        "given": "Sylvana"
      }
    ],
    "id": "Mahlow2009a",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "interactive_editing, mxp",
    "language": "en-US",
    "page": "14-18",
    "publisher": "Northern European Association for Language Technology (NEALT); Tartu University Library",
    "publisher-place": "Tartu, Estonia",
    "title": "Opportunities and limits for language awareness in text editors",
    "type": "paper-conference",
    "volume": "3"
  },
  {
    "URL": "http://www.gelbukh.com/polibits/2009_39/39_06.pdf",
    "abstract": "We present a morphological analyzer for Spanish called SMM. SMM is implemented in the grammar development framework Malaga, which is based on the formalism of Left-Associative Grammar. We briefly present the Malaga framework, describe the implementation decisions for some interesting morphological phenomena of Spanish, and report on the evaluation results from the analysis of corpora. SMM was originally only designed for analyzing word forms; in this article we outline two approaches for using SMM and the facilities provided by Malaga to also generate verbal paradigms. SMM can also be embedded into applications by making use of the Malaga programming interface; we briefly discuss some application scenarios.",
    "author": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Polibits",
    "id": "Mahlow2009b",
    "issue": "39",
    "issued": {
      "date-parts": [
        [
          2009,
          6
        ]
      ]
    },
    "keyword": "morphology",
    "page": "41-48",
    "title": "SMM: Detailed, structured morphological analysis for Spanish",
    "title-short": "SMM",
    "type": "article-journal"
  },
  {
    "ISBN": "978-3-86582-955-9",
    "abstract": "In this paper we present an evaluation of rule-based morphological components for German for use in an interactive editing environment. The criteria for the evaluation are deduced from the intended use of these components, namely availability, performance, programming interfaces, and analysis quality. We evaluated systems developed and maintained since decades as well as new systems. However, we note serious general shortcomings when looking closer at recent implementations and come to the conclusion that the oldest system is the only one that satisfies our requirements.",
    "author": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Searching answers – festschrift in honour of michael hess on the occasion of his 60th birthday",
    "editor": [
      {
        "family": "Clematide",
        "given": "Simon"
      },
      {
        "family": "Klenner",
        "given": "Manfred"
      },
      {
        "family": "Volk",
        "given": "Martin"
      }
    ],
    "id": "Mahlow2009g",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "morphology, mxp, nlp",
    "language": "en-US",
    "page": "85-99",
    "publisher": "MV-Verlag",
    "publisher-place": "Münster",
    "title": "A target-driven evaluation of morphological components for German",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      }
    ],
    "container-title": "Learning management systems technologies and software solutions for online teaching: Tools and applications",
    "editor": [
      {
        "family": "Kats",
        "given": "Yefim"
      }
    ],
    "id": "Mahlow2010a",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "publisher": "IGI",
    "publisher-place": "Hershey, PA, USA",
    "title": "Choosing the appropriate e-learning system for a university",
    "type": "chapter"
  },
  {
    "URL": "http://nbn-resolving.de/urn:nbn:de:bsz:291-universaar-124",
    "abstract": "Effective authoring aids, whether for novice, second-language, or experienced writers, require linguistic knowledge. With respect to depth of analysis, authoring aids that aim to support revising and editing go beyond POS-tagging but cannot work on complete, mostly well-formed sentences to perform deep syntactic analysis, since a text undergoing revision is in a constant state of flux. In order to cope with incomplete and changing text, authoring aids for revising and editing thus have to use shallow analyses, which are fast and robust. In this paper, we discuss noun phrase chunking for German as resource for language-aware editing functions as developed in the LingURed project. We will identify requirements for resources with respect to availability, interactivity, performance and quality of results. From our experiments we also provide some information concerning ambiguity of German noun phrases.",
    "author": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Semantic approaches in natural language processing: Proceedings of the conference on natural language processing 2010 (KONVENS)",
    "editor": [
      {
        "family": "Pinkal",
        "given": "Manfred"
      },
      {
        "family": "Rehbein",
        "given": "Ines"
      },
      {
        "dropping-particle": "Schulte im",
        "family": "Walde",
        "given": "Sabine"
      },
      {
        "family": "Storrer",
        "given": "Angelika"
      }
    ],
    "id": "Mahlow2010c",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "language": "en-US",
    "page": "57-65",
    "publisher": "Universaar",
    "publisher-place": "Saarbrücken, Germany",
    "title": "Noun phrase chunking and categorization for authoring aids",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-23138-4",
    "ISBN": "3642231373",
    "abstract": "This book constitutes the refereed proceedings of the Second International Workshop on Systems and Frameworks for Computational Morphology, SFCM 2011, held in Zurich, Switzerland in August 2011. The eight revised full papers presented together with one invited paper were carefully reviewed and selected from 13 submissions. The papers address various topics in computational morphology and the relevance of morphology to computational linguistics more broadly.",
    "collection-title": "Communications in computer and information science",
    "editor": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "Mahlow2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "computational_linguistics, morphology, mxp",
    "language": "en-US",
    "publisher": "Paperback; Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Systems and frameworks for computational morphology: Second international workshop, SFCM 2011, zurich, switzerland, august 26, 2011, proceedings",
    "title-short": "Systems and frameworks for computational morphology",
    "type": "book",
    "volume": "100"
  },
  {
    "URL": "https://www.princeton.edu/~hos/Mahoney/articles/models/models.html",
    "author": [
      {
        "family": "Mahoney",
        "given": "Michael S."
      }
    ],
    "container-title": "XIII<sup>th</sup> DHS-DLMPS joint conference on “scientific models: Their historical and philosophical relevance”",
    "id": "Mahoney2000c",
    "issued": {
      "date-parts": [
        [
          2000,
          10
        ]
      ]
    },
    "keyword": "formal_models, models_in_general",
    "title": "Historical perspectives on models and modeling",
    "type": "paper-conference"
  },
  {
    "DOI": "10.6092/issn.2532-8816/7253",
    "URL": "https://umanisticadigitale.unibo.it/article/view/7253",
    "abstract": "How can we study, present and teach complex cultural phenomena such as the Russian philological culture of the 1920s? To achieve this goal, we elaborated a knowledge representation that facilitates scientific collaboration, enables distant reading, improves the navigation of scholarly literature, links classical texts to rich international scholarship, and provides a basis of effective visualization. Digital Humanities offer an ideal framework for the intense human-computer collaboration required to carry out such a project. We focus on the network of relations both within and between three key communities of the early Soviet philological milieu – the Formalists, the Marrists and the Bakhtinists – approaching them through the optics of two major philological romans à clef of the period. To this end, we (1) prepared a collection of primary texts; (2) built a repository of secondary literature; (3) using this research literature, enriched primary texts with both general and ad locum annotations; (4) adapted the nano-publications method as a comprehensive approach for representing this scholarly knowledge in the Semantic Web. We make use of the quantitative methods toolkit of the VicoGlossia system, which was developed as part of an international and inter-institutional collaboration.",
    "author": [
      {
        "family": "Maiatsky",
        "given": "Michail"
      },
      {
        "family": "Boyarsky",
        "given": "Alexey"
      },
      {
        "family": "Boyarskaya",
        "given": "Natalia"
      },
      {
        "family": "Velmezova",
        "given": "Ekaterina"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Umanistica Digitale",
    "id": "Maiatsky2018",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "keyword": "digital_edition, digital_humanities, mxp, nanopublications",
    "language": "en-US",
    "title": "VicoGlossia: Annotatable and commentable library as a bridge between reader and scholar (a proof of concept study: Early Soviet philological culture)",
    "title-short": "VicoGlossia",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.3233/sw-2012-0049",
    "abstract": "This paper presents the CultureSampo system from the viewpoint of publishing heterogeneous linked data as a service. Discussed are the problems of converting legacy data into linked data, as well as the challenge of making the massively heterogeneous yet interlinked cultural heritage content interoperable on a semantic level. In the approach described, the data is published not only for human use, but also as intelligent services for other computer systems that can then provide interfaces of their own for the linked data. As a concrete use case of using CultureSampo as a service, the BookSampo system for publishing Finnish fiction literature on the semantic web is presented.",
    "author": [
      {
        "family": "Mäkelä",
        "given": "Eetu"
      },
      {
        "family": "Hyvönen",
        "given": "Eero"
      },
      {
        "family": "Ruotsalo",
        "given": "Tuukka"
      }
    ],
    "container-title": "Semantic Web",
    "id": "Makela2012",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "cultural_heritage, semantic_web",
    "language": "en-US",
    "page": "85-109",
    "title": "How to deal with massively heterogeneous cultural heritage data – lessons learned in CultureSampo",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.1016/j.iheduc.2006.06.006",
    "abstract": "Course management systems (CMSs) have become a common resource for resident courses at colleges and universities. Researchers have analyzed which CMS features faculty members use most primarily by asking them which features are used. The study described builds on previous research by counting the number of CMS features a faculty member used and by analyzing how three external factors are related to the use of CMS features. The external factors are (a) the college in which a course was offered, (b) class size, and (c) the level of a class–such as 100 or 200. The only external factor showing a statistically significant relationship to the use of CMS features was the college in which a course was offered. Another finding was that CMSs are primarily used to transmit information to students. Implications are described for using external factors to increase effective use of more complex CMS features.",
    "author": [
      {
        "family": "Malikowski",
        "given": "Steven R."
      },
      {
        "family": "Thompson",
        "given": "Merton E."
      },
      {
        "family": "Theis",
        "given": "John G."
      }
    ],
    "container-title": "The Internet and Higher Education",
    "id": "Malikowski2006",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "163-174",
    "title": "External factors associated with adopting a CMS in resident college courses",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "DOI": "10.2190/1002-1T50-27G2-H3V7",
    "abstract": "Course management systems (CMSs), such as Blackboard, Desire2Learn, or WebCT, have become a common resource at universities, colleges, and distance learning organizations. Research into how these systems are used for learning is in an early state. Currently, this research focuses on technical features in a CMS more than research about how people learn. This article recommends a model for CMS research that equally considers technical features and research about how people learn. Technical features and learning research are diverse topics. The model was developed by reviewing literature from each topic and should provide a conceptual middle ground. Findings from current CMS research are presented using the model, to show its relevance and adaptability. This model should also ease the process of synthesizing research in CMSs created by different vendors, which contain similar features but label them differently. Implications for developing learning activities in a CMS are also described.",
    "author": [
      {
        "family": "Malikowski",
        "given": "Steven R."
      },
      {
        "family": "Thompson",
        "given": "Merton E."
      },
      {
        "family": "Theis",
        "given": "John G."
      }
    ],
    "container-title": "Journal of Educational Computing Research",
    "id": "Malikowski2007",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "149-173",
    "title": "A model for research into course management systems: Bridging technology and learning theory",
    "title-short": "A model for research into course management systems",
    "type": "article-journal",
    "volume": "36"
  },
  {
    "DOI": "10.1016/j.iheduc.2008.03.003",
    "abstract": "A unique resource in course management systems (CMSs) is that they offer faculty members convenient access to a variety of integrated features. Some featurs allow faculty members to provide information to students, and others allow students to interact with each other or a computer. This diverse set of features can be used to help meet the variety of learning goals that are part of college classes. Currently, most CMS research has analyzed how and why individual CMS features are used, instead of analyzing how and why multiple features are used. The study described here reports how and why faculty members use multiple CMS features, in resident college classes. Results show that nearly half of faculty members use one feature or less. Those who use multiple features are significantly more likely to have experience with interactive technologies. Implications for using and encouraging the use of multiple CMS features are provided.",
    "author": [
      {
        "family": "Malikowski",
        "given": "Steven R."
      }
    ],
    "container-title": "The Internet and Higher Education",
    "id": "Malikowski2008",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "81-86",
    "title": "Factors relted  to breadth of use in course management systems",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "author": [
      {
        "family": "Malmi",
        "given": "Lauri"
      },
      {
        "family": "Korhonen",
        "given": "Ari"
      },
      {
        "family": "Saikkonen",
        "given": "Riku"
      }
    ],
    "container-title": "Proceedings of the 7<sup>th</sup> annual conference on innovation and technology in computer science education",
    "id": "Malmi2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "page": "55-59",
    "publisher": "ACM",
    "title": "Experiences in automatic assessment on mass courses and issues for designing virtual courses",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/summaries/106.html",
    "abstract": "SpatialML is an annotation scheme for marking up references to places in natural language. It covers both named and nominal references to places, grounding them where possible with geo-coordinates, including both relative and absolute locations, and characterizes relationships among places in terms of a region calculus. A freely available annotation editor has been developed for SpatialML, along with a corpus of annotated documents released by the Linguistic Data Consortium. Inter-annotator agreement on SpatialML extents is 77.0 F-measure on that corpus, and 92.3 F-measure on a ProMED corpus. Disambiguation agreement on geo-coordinates is 71.85 F-measure on the latter corpus. An automatic tagger for SpatialML extents scores 78.5 F-measure. A disambiguator scores 93.0 F-measure. In adapting the extent tagger to new domains, merging the training data from the above corpus with annotated data in the new domain provides the best performance.",
    "author": [
      {
        "family": "Mani",
        "given": "Inderjeet"
      },
      {
        "family": "Hitzeman",
        "given": "Janet"
      },
      {
        "family": "Richer",
        "given": "Justin"
      },
      {
        "family": "Dave Harris",
        "given": "Rob Q."
      },
      {
        "family": "Wellner",
        "given": "Ben"
      }
    ],
    "container-title": "Proceedings of the sixth international language resources and evaluation (LREC’08)",
    "editor": [
      {
        "family": "Calzolari",
        "given": "Nicoletta"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Maegaard",
        "given": "Bente"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Odijk",
        "given": "Jan"
      },
      {
        "family": "Piperidis",
        "given": "Stelios"
      },
      {
        "family": "Tapias",
        "given": "Daniel"
      }
    ],
    "id": "Mani2008",
    "issued": {
      "date-parts": [
        [
          2008,
          5
        ]
      ]
    },
    "keyword": "spatio-temporal_annotation",
    "language": "en-US",
    "publisher": "European Language Resources Association (ELRA)",
    "publisher-place": "Paris, France",
    "title": "SpatialML: Annotation scheme, corpora, and tools",
    "title-short": "SpatialML",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s10579-010-9121-0",
    "ISSN": "1574-020X",
    "abstract": "SpatialML is an annotation scheme for marking up references to places in natural language. It covers both named and nominal references to places, grounding them where possible with geo-coordinates, and characterizes relationships among places in terms of a region calculus. A freely available annotation editor has been developed for SpatialML, along with several annotated corpora. Inter-annotator agreement on SpatialML extents is 91.3 F-measure on a corpus of SpatialML-annotated ACE documents released by the Linguistic Data Consortium. Disambiguation agreement on geo-coordinates on ACE is 87.93 F-measure. An automatic tagger for SpatialML extents scores 86.9 F on ACE, while a disambiguator scores 93.0 F on it. Results are also presented for two other corpora. In adapting the extent tagger to new domains, merging the training data from the ACE corpus with annotated data in the new domain provides the best performance.",
    "author": [
      {
        "family": "Mani",
        "given": "Inderjeet"
      },
      {
        "family": "Doran",
        "given": "Christy"
      },
      {
        "family": "Harris",
        "given": "Dave"
      },
      {
        "family": "Hitzeman",
        "given": "Janet"
      },
      {
        "family": "Quimby",
        "given": "Rob"
      },
      {
        "family": "Richer",
        "given": "Justin"
      },
      {
        "family": "Wellner",
        "given": "Ben"
      },
      {
        "family": "Mardis",
        "given": "Scott"
      },
      {
        "family": "Clancy",
        "given": "Seamus"
      }
    ],
    "container-title": "Language Resources and Evaluation",
    "id": "Mani2010",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "spatio-temporal_annotation",
    "language": "en-US",
    "page": "263-280",
    "publisher": "Springer",
    "title": "SpatialML: Annotation scheme, resources, and evaluation",
    "title-short": "SpatialML",
    "type": "article-journal",
    "volume": "44"
  },
  {
    "abstract": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.",
    "author": [
      {
        "family": "Manning",
        "given": "Christopher D."
      },
      {
        "family": "Schütze",
        "given": "Hinrich"
      }
    ],
    "id": "Manning1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "computational_linguistics, nlp",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Foundations of statistical natural language processing",
    "type": "book"
  },
  {
    "URL": "http://informationretrieval.org/",
    "abstract": "Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.",
    "author": [
      {
        "family": "Manning",
        "given": "Christopher D."
      },
      {
        "family": "Raghavan",
        "given": "Prabhakar"
      },
      {
        "family": "Schütze",
        "given": "Hinrich"
      }
    ],
    "id": "Manning2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ir",
    "language": "en-US",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge, UK",
    "title": "Introduction to information retrieval",
    "type": "book"
  },
  {
    "DOI": "10.1162/coli_a_00239",
    "author": [
      {
        "family": "Manning",
        "given": "Christopher D."
      }
    ],
    "container-title": "Computational Linguistics",
    "id": "Manning2015",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "computational_linguistics",
    "language": "en-US",
    "page": "701-707",
    "publisher": "MIT Press",
    "title": "Computational linguistics and deep learning",
    "type": "article-journal",
    "volume": "41"
  },
  {
    "URL": "http://www.blark.org/fichiers/report.doc",
    "abstract": "The aim of this report is to help define a minimal set of LRs to be made available for as many languages as possible, and map the actual gaps which should be filled in order to meet the needs of the HLT field. The present document aims at providing the basics of a larger initiative in order to determine the BLARK concept more specifically. With the perspective to improve the current overview of the BLARK, ELRA has produced a combined matrix which aims to be implemented online, so that any customer or provider of LR aware of existing LR will be able to complete the cross-linked matrices, pointing to an existing LR. In the future, such an initiative, combined with all ongoing initiatives, should contribute to map and, in the end, fill, if not all, at least a fair number of the gaps that should improve the working material of the HLT community. Expenses on LRs are big enough to take also into consideration their reusability on a long-term, therefore maintenance and updating are rather important issues.",
    "author": [
      {
        "family": "Mapelli",
        "given": "Valérie"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      }
    ],
    "genre": "ENABLER project internal report, Deliverable ",
    "id": "Mapelli2003",
    "issued": {
      "date-parts": [
        [
          2003,
          6
        ]
      ]
    },
    "keyword": "computational_linguistics, language_resources, nlp",
    "language": "en-US",
    "number": "5.1",
    "publisher": "ELDA",
    "publisher-place": "Paris, France",
    "title": "Report on a (minimal) set of LRs to be made available for as many languages as possible, and map of the actual gaps",
    "type": "report"
  },
  {
    "DOI": "10.1145/2330667.2330680",
    "ISSN": "0001-0782",
    "abstract": "Some thoughts about author order in research papers.",
    "author": [
      {
        "family": "Marcos",
        "given": "Esperanza"
      },
      {
        "family": "Vara",
        "given": "Juan M."
      },
      {
        "dropping-particle": "de",
        "family": "Castro",
        "given": "Valeria"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Marcos2012",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "scientific_publishing",
    "language": "en-US",
    "page": "39-41",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Author order: What science can learn from the arts",
    "title-short": "Author order",
    "type": "article-journal",
    "volume": "55"
  },
  {
    "author": [
      {
        "family": "Marczewski",
        "given": "Jean"
      }
    ],
    "collection-number": "35",
    "collection-title": "Travaux de droit, d’économie, de sociologie et des sciences politiques",
    "id": "Marczewski1965",
    "issued": {
      "date-parts": [
        [
          1965
        ]
      ]
    },
    "keyword": "formal_models, history",
    "language": "fr-FR",
    "publisher": "Droz",
    "publisher-place": "Geneva",
    "title": "Introduction à l’histoire quantitative",
    "type": "book"
  },
  {
    "DOI": "10.1145/2507065.2507103",
    "ISBN": "978-1-4503-2131-0",
    "abstract": "Communication design encompasses how information is structured behind the scenes, as much as how the information is shared across networks (Potts & Albers). Information architecture can profoundly alter our perceptions of society and culture (Swarts). Today cultural heritage institutions like libraries, archives, and museums (LAMs) are searching for new ways to engage and educate patrons. This paper examines how linked open data (LOD) can solve the communication design problems that these institutions face and help LAM patrons find new meaning in cultural heritage artifacts.",
    "author": [
      {
        "family": "Marden",
        "given": "Julia"
      },
      {
        "family": "Madeo",
        "given": "Carolyn L."
      },
      {
        "family": "Whysel",
        "given": "Noreen"
      },
      {
        "family": "Edelstein",
        "given": "Jeffrey"
      }
    ],
    "container-title": "Proceedings of the 31<sup>st</sup> ACM international conference on design of communication (SIGDOC ’13)",
    "id": "Marden2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "cultural_heritage, semantic_web",
    "language": "en-US",
    "page": "107-112",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Linked open data for cultural heritage: Evolution of an information technology",
    "title-short": "Linked open data for cultural heritage",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-540-88564-1_17",
    "abstract": "The growing popularity of social tagging systems promises to alleviate the knowledge bottleneck that slows down the full materialization of the Semantic Web since these systems allow ordinary users to create and share knowledge in a simple, cheap, and scalable representation, usually known as folksonomy. However, for the sake of knowledge workflow, one needs to find a compromise between the uncontrolled nature of folksonomies and the controlled and more systematic vocabulary of domain experts. In this paper we propose to address this concern by devising a method that automatically enriches a folksonomy with domain expert knowledge and by introducing a novel algorithm based on frequent itemset mining techniques to efficiently learn an ontology over the enriched folksonomy. In order to quantitatively assess our method, we propose a new benchmark for task-based ontology evaluation where the quality of the ontologies is measured based on how helpful they are for the task of personalized information finding. We conduct experiments on real data and empirically show the effectiveness of our approach.",
    "author": [
      {
        "family": "Balby Marinho",
        "given": "Leandro"
      },
      {
        "family": "Buza",
        "given": "Krisztian"
      },
      {
        "family": "Schmidt-Thieme",
        "given": "Lars"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "The semantic web – ISWC 2008",
    "editor": [
      {
        "family": "Sheth",
        "given": "Amit"
      },
      {
        "family": "Staab",
        "given": "Steffen"
      },
      {
        "family": "Dean",
        "given": "Mike"
      },
      {
        "family": "Paolucci",
        "given": "Massimo"
      },
      {
        "family": "Maynard",
        "given": "Diana"
      },
      {
        "family": "Finin",
        "given": "Timothy"
      },
      {
        "family": "Thirunarayan",
        "given": "Krishnaprasad"
      }
    ],
    "id": "Marinho2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ontologies, semantic_web",
    "language": "en-US",
    "page": "261-276",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Folksonomy-Based collabulary learning",
    "type": "chapter",
    "volume": "5318"
  },
  {
    "DOI": "10.1007/bf00186485",
    "ISSN": "0010-4817",
    "abstract": "A cooperative team of researchers from various Italian universities are collaborating on a project for Latin lexicography. This article describes the linguistic work being done on the Latin language. The various problems â and their solutions â are discussed: allographs, homographs, source material and classification methods, word searching, etc.",
    "author": [
      {
        "family": "Marinone",
        "given": "Nino"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Marinone1990",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "cultural_heritage, latin, morphology",
    "language": "en-US",
    "page": "417-420",
    "publisher": "Springer Netherlands",
    "title": "A project for Latin lexicography: 1. Automatic lemmatization and word-list",
    "title-short": "A project for Latin lexicography",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "ISBN": "0670033820",
    "author": [
      {
        "family": "Markoff",
        "given": "John"
      }
    ],
    "id": "Markoff2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Penguin",
    "publisher-place": "New York, NY, USA",
    "title": "What the dormouse said: How the 60s counterculture shaped the personal computer",
    "title-short": "What the dormouse said",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Marquilhas",
        "given": "Rita"
      }
    ],
    "container-title": "Letter writing in late modern europe",
    "editor": [
      {
        "family": "Dossena",
        "given": "Marina"
      },
      {
        "family": "Del Lungo Camiciotti",
        "given": "Gabriella"
      }
    ],
    "id": "Marquilhas2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "publisher": "John Benjamins",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "A historical digital archive of Portuguese letters",
    "type": "chapter"
  },
  {
    "URL": "http://www.cavi.univ-paris3.fr/lexicometrica/thema/thema7.htm",
    "abstract": "We discuss two projects dealing with the analysis of French texts from a diachronical and dialectal perspective. The project Chevalier au Lion (LFA) presents various manuscripts of Chevalier au Lion, with different modules for their analysis (indexes, lexicon, grammatical database). The Based’analyse verbale, with a FileMaker Pro interface for the Web, is an efficient tool for morphologic analysis,but another program, SATO, is used to execute more complex syntactic and contextual tasks. The project Microvariation et épistolarité en Nouvelle-France presents a corpus of 17th, 18th and 19th century textswritten in vernacular French. The results of automatic lemmatisation programs such as Tree Tagger arepoor due to the high spelling variations. The solution considered is a program which recognizes writingstrategies used by less educated people.",
    "author": [
      {
        "family": "Martineau",
        "given": "France"
      }
    ],
    "container-title": "Actes du colloque “L’analyse de données textuelles : De l’enquête aux corpus littéraires”",
    "id": "Martineau2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "language": "fr-FR",
    "title": "Un corpus de textes français pour l’analyse de la variation diachronique et dialectale",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Martineau",
        "given": "France"
      },
      {
        "family": "Diaconescu",
        "given": "Constanta R."
      },
      {
        "family": "Hirschbühler",
        "given": "Paul"
      }
    ],
    "collection-title": "Zeitschrift für französische Sprache und Literatur – Beihefte. Neue Folge (ZFSL-B)",
    "container-title": "Le Nouveau Corpus d’Amsterdam. Actes de l’atelier de Lauterbad, 23–26 février 2006",
    "editor": [
      {
        "family": "Kunstmann",
        "given": "Pierre"
      },
      {
        "family": "Stein",
        "given": "Achim"
      }
    ],
    "id": "Martineau2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, french",
    "language": "fr-FR",
    "page": "121-142",
    "publisher": "Steiner",
    "publisher-place": "Stuttgart, Germany",
    "title": "Le Corpus Voies du français : de l’élaboration à l’annotation",
    "type": "chapter",
    "volume": "34"
  },
  {
    "URL": "http://corpus.revues.org/index1508.html",
    "abstract": "A corpus for the study of variation and linguistic change This paper is about building corpora specially designed to study variation and morphosyntactic change. The MCVF Corpus (Modéliser le changement : les voies du français / Modeling Change : the Paths of French) presented in this paper has been built to study morphosyntactic variation across time as well as social and regional distribution. The principles which were followed from the selection of the documents to the morphosyntatic annotation are discussed. Then, the relevance of morphosyntactic annotated corpora is illustrated by a few research points on negation phenomena.",
    "author": [
      {
        "family": "Martineau",
        "given": "France"
      }
    ],
    "container-title": "Corpus",
    "id": "Martineau2008",
    "issue": "7",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "language": "en-US",
    "title": "Un corpus pour l’analyse de la variation et du changement linguistique",
    "type": "article-journal"
  },
  {
    "abstract": "Instructional research is synthesized, using categories specific and functional enough to provide guidance for classroom practice. The opening chapter presents a theory for conducting a theory-based meta-analysis of the research on instruction, outlining a theory that involves four elements of human information processing: the self-system, the metacognitive system, the cognitive system, and knowledge. Chapter 2 describes the ways in which these four elements are represented in permanent memory. In chapter 3, the four elements and their subcomponents are described in detail. Chapter 4 describes specific design features of the meta-analysis, and chapters 5 through 8 present the results of the meta-analysis. Chapter 9 contains a general discussion of the findings in terms of classroom practice. In all, the meta-analysis used more than 4,000 effect sizes that involved an estimated 1,237,00 subjects. One observation that results from the meta-analysis is that the self-system appears to be the control center for human behavior, while the metacognitive system is the engine of learning. Three relatively straightforward implications are drawn. The teacher should: (1) identify knowledge and skills that are targets of instruction; (2) identify and use specific instructional techniques for specific instructional goals; and (3) use instructional techniques that apply to all types of instructional goals. (Contains 47 tables, 25 figures, and 398 references.) (SLD)",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Marzano",
        "given": "Robert J."
      }
    ],
    "id": "Marzano1998",
    "issued": {
      "date-parts": [
        [
          1998,
          12
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "Mid-continent Regional Educational Laboratory",
    "publisher-place": "Aurora, CO, USA",
    "title": "A theory-based meta-analysis of research on instruction",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Marzano",
        "given": "Robert J."
      }
    ],
    "id": "Marzano2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "publisher": "Corwin Press",
    "publisher-place": "Thousand Oaks, CA, USA",
    "title": "Designing a new taxonomy of educational objectives",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Marzano",
        "given": "Robert J."
      },
      {
        "family": "Kendall",
        "given": "John S."
      }
    ],
    "edition": "2<sup>nd</sup>",
    "id": "Marzano2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "publisher": "Corwin Press",
    "publisher-place": "Thousand Oaks, CA, USA",
    "title": "The new taxonomy of educational objectives",
    "type": "book"
  },
  {
    "abstract": "_\"Marzano concisely and effectively shows how his three domains of learning and the New Taxonomy can be operationalized for teachers and administrators. This book integrates objectives, instructional approaches, and assessment options so that these critical aspects of teaching are aligned to promote student learning.\"_ -James McMillan, Professor and Chair, Foundations of Education Virginia Commonwealth University _**A hands-on guide for applying the New Taxonomy to develop meaningful and targeted educational objectives and assessments.**_ Translating mandated standards into concrete objectives and then creating appropriate tasks to assess student learning of those objectives can be a challenge for educators. This practical resource provides a step-by-step process that shows readers how to make designing educational objectives and creating appropriate assessment tasks a part of their day-to-day practice. Written as a stand-alone volume, **_Designing and Assessing Educational Objectives_** reviews the framework and basic principles of Marzano’s New Taxonomy and illustrates how educators can utilize Marzano’s model to assess student performance on a broad scale or for a specific unit of instruction or grading period. The book explores objectives and tasks for each of the six levels of mental processing-retrieval, comprehension, analysis, knowledge utilization, metacognition, and self-system thinking-and features: * Benchmark statements that provide a starting point for the process * Step-by-step models, helpful diagrams, and useful charts * Numerous detailed examples from multiple subject areas and grade levels * Application of the taxonomy’s three domains of knowledge: information, mental procedures, and psychomotor procedures Comprehensive and profound, this resource is essential for teachers, school and district administrators, curriculum directors, and assessment specialists seeking to apply standards to curriculum and instruction for measurable results. (20071203)",
    "author": [
      {
        "family": "Marzano",
        "given": "Robert J."
      },
      {
        "family": "Kendall",
        "given": "John S."
      }
    ],
    "id": "Marzano2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "assessment, e-learning",
    "language": "en-US",
    "publisher": "Paperback; Corwin Press",
    "publisher-place": "Thousand Oaks, CA, USA",
    "title": "Designing and assessing educational objectives: Applying the new taxonomy",
    "title-short": "Designing and assessing educational objectives",
    "type": "book"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=800253.807671",
    "abstract": "In a software development project of any appreciable size, the production of usable, accurate documentation may well consume more effort than the production of the software itself. Several years of experience on many Programmer’s Workbench projects have shown that document preparation should not be separated from software development and that the combination of a flexible operating system, a powerful command language, and good text processing facilities permits quick and convenient production of many kinds of documentation which might be otherwise unobtainable, impractical, or very expensive. Our basic approach has been to develop techniques for effective combination of existing UNIX facilities. A number of case histories are given to illustrate the flexibility, convenience, and general usefulness of these techniques.",
    "author": [
      {
        "family": "Mashey",
        "given": "John R."
      },
      {
        "family": "Smith",
        "given": "D. W."
      }
    ],
    "container-title": "ICSE ’76: Proceedings of the 2<sup>nd</sup> international conference on software engineering",
    "id": "Mashey1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "language": "en-US",
    "page": "177-181",
    "publisher": "IEEE",
    "publisher-place": "Los Alamitos, CA, USA",
    "title": "Documentation tools and techniques",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1111/j.1467-8535.2004.00429.x",
    "abstract": "This article considers the various uses of e-portfolios in an educational context and looks at the particular characteristics of the electronic version of portfolios. It then focuses on the application of the e-portfolio as an assessment method. A case is made for the use of the e-portfolio as an appropriate end of course assessment process where learning objects are the basis of the course design. Evaluation data from such a course is presented. This is a post-graduate online course run by the Institute of Educational Technology at the Open University. Conclusions are drawn from the evaluation about the appropriateness of e-portfolios as an end of course assessment method.",
    "author": [
      {
        "family": "Mason",
        "given": "Robin"
      },
      {
        "family": "Pegler",
        "given": "Chris"
      },
      {
        "family": "Weller",
        "given": "Martin"
      }
    ],
    "container-title": "British Journal of Educational Technology",
    "id": "Mason2004",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "e-learning, e-portfolios",
    "language": "en-US",
    "page": "717-727",
    "publisher-place": "Institute of Educational Technology, The Open University",
    "title": "E-portfolios: An assessment tool for online courses",
    "title-short": "E-portfolios",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "author": [
      {
        "dropping-particle": "von",
        "family": "Matt",
        "given": "Urs"
      }
    ],
    "container-title": "SIGCUE Outlook",
    "id": "Matt1994",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "language": "en-US",
    "page": "26-40",
    "title": "Kassandra: The automatic grading system",
    "title-short": "Kassandra",
    "type": "article-journal",
    "volume": "22"
  },
  {
    "DOI": "10.1145/1297144.1297185",
    "author": [
      {
        "family": "Maxwell",
        "given": "John W."
      }
    ],
    "container-title": "Proceedings of the 25th annual ACM international conference on design of communication",
    "id": "Maxwell2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "page": "196-200",
    "publisher": "ACM",
    "publisher-place": "El Paso, Texas, USA",
    "title": "Using wiki as a multi-mode publishing platform",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/W10-1820",
    "abstract": "In this paper, we introduce the NotaBene RDF Annotation Tool free software used to build the Syntactic Reference Cor- pus of Medieval French. It relies on a dependency-based model to manually annotate Old French texts from the Base de Français Médiéval and the Nouveau Corpus d’Amsterdam. NotaBene uses OWL ontologies to frame the terminology used in the annotation, which is displayed in a tree-like view of the annotation. This tree widget allows easy grouping and tagging of words and structures. To increase the quality of the annotation, two annotators work independently on the same texts at the same time and NotaBene can also generate automatic comparisons between both analyses. The RDF format can be used to export the data to several other formats: namely, TigerXML (for querying the data and extracting structures) and graphviz dot format (for quoting syntactic description in research papers).",
    "author": [
      {
        "family": "Mazziotta",
        "given": "Nicolas"
      }
    ],
    "container-title": "Proceedings of the fourth linguistic annotation workshop",
    "id": "Mazziotta2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, french",
    "language": "en-US",
    "page": "142-146",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Building the syntactic reference corpus of medieval French using NotaBene RDF annotation tool",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1023/a:3a1016757315265",
    "abstract": "We articulate a dialectical argumentation framework for qualitative representation of epistemic uncertainty in scientific domains. The framework is grounded in specific philosophies of science and theories of rational mutual discourse. We study the formal properties of our framework and provide it with a game theoretic semantics. With this semantics, we examine the relationship between the snaphots of the debate in the framework and the long run position of the debate, and prove a result directly analogous to the standard (Neyman–Pearson) approach to statistical hypothesis testing. We believe this formalism for representating uncertainty has value in domains with only limited knowledge, where experimental evidence is ambiguous or conflicting, or where agreement between different stakeholders on the quantification of uncertainty is difficult to achieve. All three of these conditions are found in assessments of carcinogenic risk for new chemicals.",
    "author": [
      {
        "family": "McBurney",
        "given": "Peter"
      },
      {
        "family": "Parsons",
        "given": "Simon"
      }
    ],
    "container-title": "Annals of Mathematics and Artificial Intelligence",
    "id": "McBurney2001",
    "issue": "1–4",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "argumentation, uncertainty",
    "language": "en-US",
    "page": "125-169",
    "title": "Representing epistemic uncertainty by means of dialectical argumentation",
    "type": "article-journal",
    "volume": "32"
  },
  {
    "DOI": "10.1145/367177.367199",
    "author": [
      {
        "family": "McCarthy",
        "given": "John"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "McCarthy1960",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1960,
          4
        ]
      ]
    },
    "keyword": "lisp",
    "page": "184-195",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Recursive functions of symbolic expressions and their computation by machine, part I",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "URL": "http://www.mccarty.org.uk/essays/McCarty,\\%20Humanities\\%20computing.pdf",
    "author": [
      {
        "family": "McCarty",
        "given": "Willard"
      }
    ],
    "container-title": "Encyclopedia of library and information science",
    "edition": "Second",
    "editor": [
      {
        "family": "Drake",
        "given": "Miriam A."
      }
    ],
    "id": "McCarty2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "en-US",
    "page": "1224-1235",
    "publisher": "Marcel Dekker",
    "publisher-place": "New York, NY, USA",
    "title": "Humanities computing",
    "type": "chapter"
  },
  {
    "DOI": "10.1002/9780470999875.ch19",
    "ISBN": "9780470999875",
    "abstract": "The question of modeling arises naturally for humanities computing from the prior question of what its practitioners across the disciplines have in common. What are they all doing with their computers that we might find in their diverse activities indications of a coherent or cohesible practice? How do we make the best, most productive sense of what we observe? There are, of course, many answers: practice varies from person to person, from project to project, and ways of construing it perhaps vary even more. In this chapter I argue for modeling as a model of such a practice. I have three confluent goals: to identify humanities computing with an intellectual ground shared by the older disciplines, so that we may say how and to what extent our field is of as well as in the humanities, how it draws from and adds to them; at the same time to reflect experience with computers \"in the wild\"; and to aim at the most challenging problems, and so the most intellectually rewarding future now imaginable. My primary concern here is, as Confucius almost said, that we use the correct word for the activity we share lest our practice go awry for want of understanding (Analects 13.3). Several words are on offer. By what might be called a moral philology I examine them, arguing for the most popular of these, \"modeling.\" The nominal form, \"model\", is of course very useful and even more popular, but for reasons I will adduce, its primary virtue is that properly defined it defaults to the present participle, its semantic lemma. Before getting to the philology I discuss modeling in the light of the available literature and then consider the strong and learned complaints about the term.",
    "author": [
      {
        "family": "McCarty",
        "given": "Willard"
      }
    ],
    "chapter-number": "19",
    "container-title": "A companion to digital humanities",
    "editor": [
      {
        "family": "Schreibman",
        "given": "Susan"
      },
      {
        "family": "Siemens",
        "given": "Ray"
      },
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "id": "McCarty2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "en-US",
    "page": "254-270",
    "publisher": "Blackwell",
    "publisher-place": "Oxford",
    "title": "Modeling: A study in words and meanings",
    "title-short": "Modeling",
    "type": "chapter"
  },
  {
    "ISBN": "9781137440426",
    "abstract": "Humanities Computing provides a rationale for a computing practice that is of and for as well as in the humanities and the interpretative social sciences. It engages philosophical, historical, ethnographic and critical perspectives to show how computing helps us fulfil the basic mandate of the humane sciences to ask ever better questions of the most challenging kind. It strengthens current practice by stimulating debate on the role of the computer in our intellectual life, and outlines an agenda for the field to which individual scholars across the humanities can contribute.",
    "author": [
      {
        "family": "McCarty",
        "given": "Willard"
      }
    ],
    "id": "McCarty2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "publisher": "Palgrave Macmillan",
    "publisher-place": "Basingstoke",
    "title": "Humanities computing",
    "type": "book"
  },
  {
    "ISBN": "9781137440426",
    "abstract": "Humanities Computing provides a rationale for a computing practice that is of and for as well as in the humanities and the interpretative social sciences. It engages philosophical, historical, ethnographic and critical perspectives to show how computing helps us fulfil the basic mandate of the humane sciences to ask ever better questions of the most challenging kind. It strengthens current practice by stimulating debate on the role of the computer in our intellectual life, and outlines an agenda for the field to which individual scholars across the humanities can contribute.",
    "author": [
      {
        "family": "McCarty",
        "given": "Willard"
      }
    ],
    "edition": "Paperback",
    "id": "McCarty2005-2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "publisher": "Palgrave Macmillan",
    "publisher-place": "Basingstoke",
    "title": "Humanities computing",
    "type": "book"
  },
  {
    "abstract": "When the understanding of scientific models and archetypes comes to be regarded as a reputable part of scientific culture, the gap between the sciences and the humanities will have been partly filled. For exercise of the imagination, with all its promises and its dangers, provides a common ground. The author concentrates on the term “modeling” because that is the predominate term in scientific practice. Then, the author provides an extended example from humanities computing and discusses the epistemological implications. Finally, the author focuses on Jean-Claude Gardin’s very different agenda, what he calls “the logicist programme,” and also focuses on closely allied questions of knowledge representation in artificial intelligence.",
    "author": [
      {
        "family": "McCarty",
        "given": "Willard"
      }
    ],
    "container-title": "A companion to digital literary studies",
    "editor": [
      {
        "family": "Schreibman",
        "given": "Susan"
      },
      {
        "family": "Siemens",
        "given": "Ray"
      }
    ],
    "id": "McCarty2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "en-US",
    "page": "389-401",
    "publisher": "Blackwell",
    "publisher-place": "Oxford",
    "title": "Knowing …: Modeling in literary studies",
    "title-short": "Knowing …",
    "type": "chapter"
  },
  {
    "DOI": "10.1093/llc/fqu022",
    "ISSN": "1477-4615",
    "abstract": "In this slightly modified version of my 2013 Roberto Busa Prize lecture, I look from the first four decades of digital humanities through its present toward a possible future. I find a means to construct this future by paying close attention to the enemy we need in order to grow: the fear that closed down the horizons of imaginative exploration during the years of the Cold War and that re-presents itself now clothed in numerous techno-scientific challenges to the human.",
    "author": [
      {
        "family": "McCarty",
        "given": "Willard"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "McCarty2014",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "283-306",
    "title": "Getting there from here. Remembering the future of digital humanities",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "ISBN": "9781472506092",
    "abstract": "Corpus linguistics has much to offer history, being as both disciplines engage so heavily in analysis of large amounts of textual material. This book demonstrates the opportunities for exploring corpus linguistics as a method in historiography and the humanities and social sciences more generally. Focusing on the topic of prostitution in 17th-century England, it shows how corpus methods can assist in social research, and can be used to deepen our understanding and comprehension. McEnery and Baker draw principally on two sources – the newsbook Mercurius Fumigosis and the Early English Books Online Corpus. This scholarship on prostitution and the sex trade offers insight into the social position of women in history.",
    "author": [
      {
        "family": "McEnery",
        "given": "Tony"
      },
      {
        "family": "Baker",
        "given": "Helen"
      }
    ],
    "id": "McEnery2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "corpus_linguistics, history",
    "language": "en-US",
    "publisher": "Bloomsbury",
    "publisher-place": "London",
    "title": "Corpus linguistics and 17th-century prostitution: Computational linguistics and history",
    "title-short": "Corpus linguistics and 17th-century prostitution",
    "type": "book"
  },
  {
    "URL": "http://aclweb.org/anthology/W09-0306",
    "abstract": "We present a valency lexicon for Latin verbs extracted from the Index Thomisticus Treebank, a syntactically annotated corpus of Medieval Latin texts by Thomas Aquinas. In our corpus-based approach, the lexicon reflects the empirical evidence of the source data. Verbal arguments are induced directly from annotated data.",
    "author": [
      {
        "family": "McGillivray",
        "given": "Barbara"
      },
      {
        "family": "Passarotti",
        "given": "Marco"
      }
    ],
    "container-title": "Proceedings of the EACL 2009 workshop on language technology and resources for cultural heritage, social sciences, humanities, and education (LaTeCH-SHELT&r ’09)",
    "editor": [
      {
        "family": "Borin",
        "given": "Lars"
      },
      {
        "family": "Lendvai",
        "given": "Piroska"
      }
    ],
    "id": "McGillivray2009a",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, latin",
    "page": "43-50",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Morristown, NJ, USA",
    "title": "The development of the Index Thomisticus treebank valency lexicon",
    "type": "paper-conference"
  },
  {
    "URL": "http://atala.org/IMG/pdf/TAL-2009-50-2-04-McGillivray.pdf",
    "abstract": "We present an overview of the Index Thomisticus Treebank project (IT-TB). The IT-TB consists of around 60,000 tokens from the Index Thomisticus by Roberto Busa SJ, an 11-million-token Latin corpus of the texts by Thomas Aquinas. We briefly describe the annotation guidelines, shared with the Latin Dependency Treebank (LDT). The application of data-driven dependency parsers on IT-TB and LDT data is reported on. We present training and parsing results on several datasets and provide evaluation of learning algorithms and techniques. Furthermore, we introduce the IT-TB valency lexicon extracted from the treebank. We report on quantitative data of the lexicon and provide some statistical measures on subcategorisation structures.",
    "author": [
      {
        "family": "McGillivray",
        "given": "Barbara"
      },
      {
        "family": "Passarotti",
        "given": "Marco"
      },
      {
        "family": "Ruffolo",
        "given": "Paolo"
      }
    ],
    "container-title": "Traitement Automatique des Langues",
    "id": "McGillivray2009b",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, latin, corpus_linguistics",
    "page": "103-127",
    "title": "The Index Thomisticus Treebank project: Annotation, parsing and valency lexicon",
    "title-short": "The Index Thomisticus Treebank project",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "DOI": "10.1109/tcom.1982.1095395",
    "ISSN": "0096-2244",
    "URL": "http://dx.doi.org/10.1109/tcom.1982.1095395",
    "abstract": "The word list used by the UNIX spelling checker, SPELL, was developed from many sources over several years. As the spelling checker may be used on minicomputers, it is important to make the list as compact as possible. Stripping prefixes and suffixes reduces the list below one third of its original size, hashing discards 60 percent of the bits that remain, and data compression halves it once again. This paper tells how the spelling checker works, how the words were chosen, how the spelling checker was used to improve itself, and how the (reduced) list of 30000 English words was squeezed into 26000 16-bit machine words.",
    "author": [
      {
        "family": "McIlroy",
        "given": "M. Douglas"
      }
    ],
    "container-title": "IEEE Transactions on Communications",
    "id": "McIlroy1982",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "keyword": "spelling_correction",
    "page": "91-99",
    "title": "Development of a spelling list",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "author": [
      {
        "family": "McLuhan",
        "given": "Marshall"
      }
    ],
    "container-title": "Marshall McLuhan unbound",
    "editor": [
      {
        "family": "McLuhan",
        "given": "Eric"
      },
      {
        "family": "Gordon",
        "given": "W. Terrence"
      }
    ],
    "id": "McLuhan1961-2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1961
        ]
      ]
    },
    "publisher": "Gingko Press",
    "publisher-place": "Corte Madera, CA, USA",
    "title": "The humanities in the electronic age",
    "type": "chapter",
    "volume": "7"
  },
  {
    "author": [
      {
        "family": "McNeill",
        "given": "Patrick"
      },
      {
        "family": "Chapman",
        "given": "Steve"
      }
    ],
    "edition": "3<sup>rd</sup>",
    "id": "Mcneill2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "publisher": "Routledge",
    "publisher-place": "London, UK and New York, NY, USA",
    "title": "Research methods",
    "type": "book"
  },
  {
    "id": "Mcqdesign",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "title": "Designing and managing MCQs, http://web.uct.ac.za/projects/cbe/mcqman/",
    "title-short": "Designing and managing MCQs, http",
    "type": ""
  },
  {
    "URL": "http://www.aaai.org/Papers/Workshops/2008/WS-08-15/WS08-15-004.pdf",
    "abstract": "Wikipedia can be utilized as a controlled vocabulary for identifying the main topics in a document, with article titles serving as index terms and redirect titles as their synonyms. Wikipedia contains over 4M such titles covering the terminology of nearly any document collection. This permits controlled indexing in the absence of manually created vocabularies. We combine state-of-the-art strategies for automatic controlled indexing with Wikipedia’s unique property – a richly hyperlinked encyclopedia. We evaluate the scheme by comparing automatically assigned topics with those chosen manually by human indexers. Analysis of indexing consistency shows that our algorithm performs as well as the average person.",
    "author": [
      {
        "family": "Medelyan",
        "given": "Olena"
      },
      {
        "family": "Witten",
        "given": "Ian H."
      },
      {
        "family": "Milne",
        "given": "David"
      }
    ],
    "editor": [
      {
        "family": "Bunescu",
        "given": "Razvan"
      },
      {
        "family": "Gabrilovich",
        "given": "Evgeniy"
      },
      {
        "family": "Mihalcea",
        "given": "Rada"
      }
    ],
    "id": "Medelyan2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ir, topic_modeling, wikipedia",
    "page": "19-24",
    "publisher": "AAAI",
    "publisher-place": "Menlo Park, CA, USA",
    "title": "Topic indexing with Wikipedia",
    "type": "book"
  },
  {
    "DOI": "10.1016/j.ijhcs.2009.05.004",
    "ISSN": "10715819",
    "abstract": "Wikipedia is a goldmine of information; not just for its many readers, but also for the growing community of researchers who recognize it as a resource of exceptional scale and utility. It represents a vast investment of manual effort and judgment: a huge, constantly evolving tapestry of concepts and relations that is being applied to a host of tasks. This article provides a comprehensive description of this work. It focuses on research that extracts and makes use of the concepts, relations, facts and descriptions found in Wikipedia, and organizes the work into four broad categories: applying Wikipedia to natural language processing; using it to facilitate information retrieval and information extraction; and as a resource for ontology building. The article addresses how Wikipedia is being used as is, how it is being improved and adapted, and how it is being combined with other structures to create entirely new resources. We identify the research groups and individuals involved, and how their work has developed in the last few years. We provide a comprehensive list of the open-source software they have produced.",
    "author": [
      {
        "family": "Medelyan",
        "given": "Olena"
      },
      {
        "family": "Milne",
        "given": "David"
      },
      {
        "family": "Legg",
        "given": "Catherine"
      },
      {
        "family": "Witten",
        "given": "Ian H."
      }
    ],
    "container-title": "International Journal of Human-Computer Studies",
    "id": "Medelyan2009",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "ir, wikipedia",
    "page": "716-754",
    "publisher": "Academic Press, Inc.",
    "publisher-place": "Duluth, MN, USA",
    "title": "Mining meaning from Wikipedia",
    "type": "article-journal",
    "volume": "67"
  },
  {
    "URL": "http://nbn-resolving.de/urn:nbn:de:0168-ssoar-378413",
    "abstract": "In practice the Digital Humanities are methodologically defined by the principle of digital conceptualization of the objects and procedures of research. Who embarks upon Digital Humanities considers the objects of study implicitly as a complex of discrete measurable states, to apply, based upon this, computer based procedures: analytical, symbolizing or modeling. This mode of digital conceptualization of humanistic topics of research can in principle be used within all disciplines, as a digital lingua franca. Before this background we formulate two theses: (1) This methodological theoretical claim of universality has to be relativized by the Digital Humanities community through critical reflection of methodology; digital access does not turn out to be appropriate everywhere, when we make the specifically humanistic drive for knowledge the yardstick of a cost-benefit analysis. (2) The trans-disciplinary nature of the Digital Humanities may be politically ’unbearable’ by tendency from the perspective of traditional Humanities’ disciplines, as it challenges their disciplinary identity. For the Digital Humanities community both of these theses lead to the obligation to engage in a critical self reflexion of their own methods - and open the dialogue with the established humanistic disciplines against its backdrop.",
    "author": [
      {
        "family": "Meister",
        "given": "Jan Christoph"
      }
    ],
    "container-title": "Historical Social Research",
    "id": "Meister2012",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "page": "77-85",
    "title": "DH is us or on the unbearable lightness of a shared methodology",
    "type": "article-journal",
    "volume": "37"
  },
  {
    "URL": "http://jstor.org/stable/1764604",
    "author": [
      {
        "family": "Mendenhall",
        "given": "Thomas C."
      }
    ],
    "container-title": "Science",
    "id": "Mendenhall1887",
    "issue": "214",
    "issued": {
      "date-parts": [
        [
          1887
        ]
      ]
    },
    "keyword": "classic, digital_humanities, literature",
    "page": "237-249",
    "title": "The characteristic curves of composition",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2000/pdf/59.pdf",
    "abstract": "This paper discusses a general approach to the description and encoding of linguistic corpora annotated with hierarchically structured syntactic information. A general format can be motivated by the variety and incompatibility of existing annotation formats. By using XML as a representation format the theoretical and technical problems encountered can be overcome.",
    "author": [
      {
        "family": "Mengel",
        "given": "Andreas"
      },
      {
        "family": "Lezius",
        "given": "Wolfgang"
      }
    ],
    "container-title": "Proceedings of the 2<sup>nd</sup> international conference on language resources and evaluation (LREC 2000)",
    "id": "Mengel2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "nlp, xml",
    "page": "Article 59+",
    "publisher": "European Language Resources Association (ELRA)",
    "publisher-place": "Paris",
    "title": "An XML-based representation format for syntactically annotated corpora",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Merrill",
        "given": "M. David"
      },
      {
        "family": "Tennyson",
        "given": "Robert D."
      }
    ],
    "id": "Merrill1977",
    "issued": {
      "date-parts": [
        [
          1977
        ]
      ]
    },
    "publisher": "Educational Technology Publications",
    "publisher-place": "Englewood Cliffs, NJ",
    "title": "Teaching concepts: An instructional design guide",
    "title-short": "Teaching concepts",
    "type": "book"
  },
  {
    "URL": "http://id2.usu.edu/Papers/Reclaiming.PDF",
    "abstract": "Education and its related disciplines continue to flutter this way and that by every philosophical wind that blows. In an uncertain science and technology, unscientific theories flourish. People are anxious for answers. When answers are slow in coming, uncertain in statement, and difficult to find; then the void is filled with wild speculation and philosophical extremism. This brief statement attempts to make clear our belief that instruction is a science and that instructional design is a technology founded in this science. We attempt to identify some of the assumptions underlying the science-based technology of instructional design, and to clarify its role in the larger context of education and social change.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Merrill",
        "given": "David M."
      },
      {
        "family": "Drake",
        "given": "Leston"
      },
      {
        "family": "Lacy",
        "given": "Mark J."
      },
      {
        "family": "Pratt",
        "given": "Jean A."
      }
    ],
    "container-title": "Educational Technology",
    "id": "Merrill1996",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "e-learning",
    "page": "5-7",
    "title": "Reclaiming instructional design",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "URL": "https://eric.ed.gov/?id=EJ657802",
    "abstract": "I Instructional systems development (ISD) has recently come under attack to suggestions that it may not be an appropriate methodology for developing effective instruction (Gordon & Zemke, 2000). ISD is accused of being too slow and clumsy, of claiming to be a technology when it is not, of producing bad instruction, and of being out of touch with today’s training needs.",
    "author": [
      {
        "family": "Merril",
        "given": "David M."
      }
    ],
    "container-title": "Performance Improvement",
    "id": "Merrill2002",
    "issue": "7",
    "issued": {
      "date-parts": [
        [
          2002,
          8
        ]
      ]
    },
    "keyword": "e-learning",
    "page": "39-44",
    "title": "A Pebble-in-the-Pond model for instructional design.",
    "type": "article-journal",
    "volume": "41"
  },
  {
    "author": [
      {
        "family": "Merritt",
        "given": "Richard L."
      }
    ],
    "chapter-number": "10",
    "container-title": "Computers in humanistic research: Readings and perspectives",
    "editor": [
      {
        "family": "Bowles",
        "given": "Edmund A."
      }
    ],
    "id": "Merritt1967",
    "issued": {
      "date-parts": [
        [
          1967
        ]
      ]
    },
    "language": "en-US",
    "page": "90-107",
    "publisher": "Prentice-Hall",
    "publisher-place": "Englewood Cliffs, NJ, USA",
    "title": "Political science and computer research",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/1099554.1099695",
    "ISBN": "1-59593-140-6",
    "abstract": "Text similarity spans a spectrum, with broad topical similarity near one extreme and document identity at the other. Intermediate levels of similarity – resulting from summarization, paraphrasing, copying, and stronger forms of topical relevance – are useful for applications such as information flow analysis and question-answering tasks. In this paper, we explore mechanisms for measuring such intermediate kinds of similarity, focusing on the task of identifying where a particular piece of information originated. We consider both sentence-to-sentence and document-to-document comparison, and have incorporated these algorithms into RECAP, a prototype information flow analysis tool. Our experimental results with RECAP indicate that new mechanisms such as those we propose are likely to be more appropriate than existing methods for identifying the intermediate forms of similarity.",
    "author": [
      {
        "family": "Metzler",
        "given": "Donald"
      },
      {
        "family": "Bernstein",
        "given": "Yaniv"
      },
      {
        "family": "Croft",
        "given": "W. Bruce"
      },
      {
        "family": "Moffat",
        "given": "Alistair"
      },
      {
        "family": "Zobel",
        "given": "Justin"
      }
    ],
    "container-title": "Proceedings of the 14<sup>th</sup> ACM international conference on information and knowledge management (CIKM ’05)",
    "id": "Metzler2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "ir, plagiarism",
    "page": "517-524",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Similarity measures for tracking information flow",
    "type": "paper-conference"
  },
  {
    "URL": "http://sens-public.org/articles/1121",
    "abstract": "S’inscrivant dans l’univers des technologies informatiques, les humanités dites numériques se présentent comme un nouveau programme de recherche. Le vrai défi de ce programme de recherche ne sera cependant pas dans le traitement numérique qu’il veut appliquer aux objets et opérations des humanités mais dans la modélisation computationnelle qu’il doit leur appliquer. Malheureusement l’utilisation du concept de numérique masque la grand complexité de cette tâche de modélisation qui, elle, devra marier des fonctions computationnelles avec des fonctions interprétatives.",
    "author": [
      {
        "family": "Meunier",
        "given": "Jean-Guy"
      }
    ],
    "container-title": "Sens public",
    "id": "Meunier2014",
    "issued": {
      "date-parts": [
        [
          2014,
          12,
          5
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "fr-FR",
    "title": "Humanités numériques ou computationnelles: Enjeux herméneutiques",
    "title-short": "Humanités numériques ou computationnelles",
    "type": "article-journal"
  },
  {
    "DOI": "10.4000/questionsdecommunication.11040",
    "URL": "https://cairn.info/revue-questions-de-communication-2017-1-page-19.htm",
    "abstract": "Can digital humanities build scientific theories? The pragmatic and cognitive conception of science allows us to see scientific theories as sets of various types of models: be they formal, material, conceptual, etc. It is by working together that the models contribute to the advancement of knowledge. Implicitly or explicitly, digital humanities also use models that allow them to position themselves as a scientific inquiry. But this also limits and constrains their method and application in the field of the humanities. At the same time though, this gives place to hermeneutics. In this sense, digital Humanities create an original bridge between science and hermeneutics.",
    "author": [
      {
        "family": "Meunier",
        "given": "Jean-Guy"
      }
    ],
    "container-title": "Questions de communication",
    "id": "Meunier2017",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "fr-FR",
    "page": "19-48",
    "title": "Humanités numériques et modélisation scientifique",
    "type": "article-journal",
    "volume": "31"
  },
  {
    "URL": "http://www.digitalhumanities.org/dhq/vol/12/1/000362/000362.html",
    "abstract": "The digitization of texts is omnipresent in the digital humanities. It seems to present itself only as a modification of the material medium: from text on paper to digital text. But it does more than that. Digitization also affects the text as a semiotic object. The multiple operations of this technology implement interpretative decisions that are not without their effects on the semiotic text; that is to say, the text that offers itself for reading and analysis. In this sense, the digitization of texts is not neutral. It is an important moment of material hermeneutics.",
    "author": [
      {
        "family": "Meunier",
        "given": "Jean G."
      }
    ],
    "container-title": "Digital Humanities Quarterly",
    "id": "Meunier2018",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "keyword": "digital_humanities, document_research",
    "language": "fr-FR",
    "title": "Le texte numérique: enjeux herméneutiques",
    "title-short": "Le texte numérique",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "DOI": "10.4000/quaderni.1407",
    "author": [
      {
        "family": "Meunier",
        "given": "Jean-Guy"
      }
    ],
    "container-title": "Quaderni",
    "id": "Meunier2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "fr-FR",
    "page": "19-31",
    "title": "Le paradoxe des humanités numériques",
    "type": "article-journal",
    "volume": "98"
  },
  {
    "abstract": "Natural language processing (NLP) has long been used to automatically analyze language produced by language learners, typically aimed at providing individualized feedback and learner modeling in intelligent computer-assisted language learning systems (see Heift & Schulze, 2007). While much interesting research has been reported, it is difficult to determine the state of the art for the automatic analysis of learner language. Which error types and other learner language properties can be detected and diagnosed automatically? How reliably can this be done, for which kind of learner language, resulting from which types of tasks? For sustainable progress on the automatic analysis of learner language it is arguably crucial to answer these questions, to discuss and compare the performance of different analysis methods on real-life learner data sets.",
    "author": [
      {
        "family": "Meurers",
        "given": "Detmar"
      }
    ],
    "container-title": "CALICO Journal",
    "id": "Meurers2009",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "call, learner_corpora, nlp",
    "page": "469-473",
    "title": "On the automatic analysis of learner language: Introduction to the special issue",
    "title-short": "On the automatic analysis of learner language",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "DOI": "10.1145/356887.356889",
    "ISSN": "0360-0300",
    "author": [
      {
        "family": "Meyrowitz",
        "given": "Norman"
      },
      {
        "dropping-particle": "van",
        "family": "Dam",
        "given": "Andries"
      }
    ],
    "container-title": "ACM Computing Surveys",
    "id": "Meyrowitz1982a",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1982,
          9
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "321-352",
    "publisher": "ACM",
    "title": "Interactive editing systems: Part I",
    "title-short": "Interactive editing systems",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "DOI": "10.1145/356887.356890",
    "ISSN": "0360-0300",
    "author": [
      {
        "family": "Meyrowitz",
        "given": "Norman"
      },
      {
        "dropping-particle": "van",
        "family": "Dam",
        "given": "Andries"
      }
    ],
    "container-title": "ACM Computing Surveys",
    "id": "Meyrowitz1982b",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1982,
          9
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "353-415",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Interactive editing systems: Part II",
    "title-short": "Interactive editing systems",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "DOI": "10.1126/science.1199644",
    "ISSN": "1095-9203",
    "PMID": "21163965",
    "abstract": "We constructed a corpus of digitized texts containing about 4% of all books ever printed. Analysis of this corpus enables us to investigate cultural trends quantitatively. We survey the vast terrain of ’culturomics,’ focusing on linguistic and cultural phenomena that were reflected in the English language between 1800 and 2000. We show how this approach can provide insights about fields as diverse as lexicography, the evolution of grammar, collective memory, the adoption of technology, the pursuit of fame, censorship, and historical epidemiology. Culturomics extends the boundaries of rigorous quantitative inquiry to a wide array of new phenomena spanning the social sciences and the humanities.",
    "author": [
      {
        "family": "Michel",
        "given": "Jean-Baptiste"
      },
      {
        "family": "Shen",
        "given": "Yuan K."
      },
      {
        "family": "Aiden",
        "given": "Aviva P."
      },
      {
        "family": "Veres",
        "given": "Adrian"
      },
      {
        "family": "Gray",
        "given": "Matthew K."
      },
      {
        "literal": "The Google Books Team"
      },
      {
        "family": "Pickett",
        "given": "Joseph P."
      },
      {
        "family": "Hoiberg",
        "given": "Dale"
      },
      {
        "family": "Clancy",
        "given": "Dan"
      },
      {
        "family": "Norvig",
        "given": "Peter"
      },
      {
        "family": "Orwant",
        "given": "Jon"
      },
      {
        "family": "Pinker",
        "given": "Steven"
      },
      {
        "family": "Nowak",
        "given": "Martin A."
      },
      {
        "family": "Aiden",
        "given": "Erez L."
      }
    ],
    "container-title": "Science",
    "id": "Michel2011",
    "issue": "6014",
    "issued": {
      "date-parts": [
        [
          2011,
          1,
          14
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_humanities",
    "language": "en-US",
    "page": "176-182",
    "publisher": "American Association for the Advancement of Science",
    "title": "Quantitative analysis of culture using millions of digitized books",
    "type": "article-journal",
    "volume": "331"
  },
  {
    "DOI": "10.1007/s10032-007-0052-2",
    "ISSN": "1433-2833",
    "URL": "http://dx.doi.org/10.1007/s10032-007-0052-2",
    "abstract": "Information extraction from unstructured, ungrammatical data such as classified listings is difficult because traditional structural and grammatical extraction methods do not apply. Previous work has exploited reference sets to aid such extraction, but it did so using supervised machine learning. In this paper, we present an unsupervised approach that both selects the relevant reference set(s) automatically and then uses it for unsupervised extraction. We validate our approach with experimental results that show our unsupervised extraction is competitive with supervised machine learning approaches, including the previous supervised approach that exploits reference sets.",
    "author": [
      {
        "family": "Michelson",
        "given": "Matthew"
      },
      {
        "family": "Knoblock",
        "given": "Craig A."
      }
    ],
    "container-title": "International Journal on Document Analysis and Recognition",
    "id": "Michelson2007",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "ir, noisy_text",
    "page": "211-226",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Unsupervised information extraction from unstructured, ungrammatical data sources on the World Wide Web",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "collection-title": "Lecture notes in computer science",
    "editor": [
      {
        "family": "Miesenberger",
        "given": "Klaus"
      },
      {
        "family": "Klaus",
        "given": "Joachim"
      },
      {
        "family": "Zagler",
        "given": "Wolfgang L."
      },
      {
        "family": "Karshmer",
        "given": "Arthur I."
      }
    ],
    "id": "Miesenberger2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Computers helping people with special needs, 10<sup>th</sup> international conference, ICCHP 2006, linz, austria, july 11–13, 2006, proceedings",
    "type": "book",
    "volume": "4061"
  },
  {
    "edition": "Second",
    "editor": [
      {
        "family": "Milagros Cárcel Ortí",
        "given": "Maria"
      }
    ],
    "id": "Milagros1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "publisher": "Universitat de València",
    "publisher-place": "Valencia, Spain",
    "title": "Vocabulaire international de la diplomatique",
    "type": "book"
  },
  {
    "DOI": "10.1007/bf00188010",
    "ISSN": "0010-4817",
    "URL": "http://jstor.org/stable/30199191",
    "author": [
      {
        "family": "Milic",
        "given": "Louis T."
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Milic1966",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1966
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_humanities, theory, classic",
    "page": "3-6",
    "title": "The next step",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "URL": "http://citeseer.ist.psu.edu/miller03essay.html",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Miller",
        "given": "Tristan"
      }
    ],
    "container-title": "Journal of Educational Computing Research",
    "id": "Miller2003",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "page": "495-512",
    "title": "Essay assessment with latent semantic analysis",
    "type": "article-journal",
    "volume": "28"
  },
  {
    "DOI": "10.1016/j.apal.2013.07.015",
    "ISSN": "01680072",
    "abstract": "In Artemov’s Justification Logic, one can make statements interpreted as “t is evidence for the truth of formula F.” We propose a variant of this logic in which one can say “I have degree r of confidence that t is evidence for the truth of formula F.” After defining both an axiomatic approach and a semantics for this Logic of Uncertain Justifications, we will prove the usual soundness and completeness theorems.",
    "author": [
      {
        "family": "Milnikel",
        "given": "Robert S."
      }
    ],
    "container-title": "Annals of Pure and Applied Logic",
    "id": "Milnikel2014",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "logic, uncertainty",
    "language": "en-US",
    "page": "305-315",
    "title": "The logic of uncertain justifications",
    "type": "article-journal",
    "volume": "165"
  },
  {
    "ISBN": "978-1-932432-62-6",
    "URL": "http://portal.acm.org/citation.cfm?id=1699571.1699627",
    "abstract": "Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.",
    "author": [
      {
        "family": "Mimno",
        "given": "David"
      },
      {
        "family": "Wallach",
        "given": "Hanna M."
      },
      {
        "family": "Naradowsky",
        "given": "Jason"
      },
      {
        "family": "Smith",
        "given": "David A."
      },
      {
        "family": "McCallum",
        "given": "Andrew"
      }
    ],
    "collection-title": "EMNLP ’09",
    "container-title": "Proceedings of the 2009 conference on empirical methods in natural language processing (EMNLP 2009)",
    "id": "Mimno2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "topic_modeling, wikipedia",
    "page": "880-889",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Polylingual topic models",
    "type": "paper-conference",
    "volume": "2"
  },
  {
    "URL": "http://www.dfll.univr.it/documenti/Iniziativa/dall/dall234343.pdf",
    "abstract": "This project of construction of a lexical knowledge-base for Latin language was born with the ambitious target to give a specimen of a Latin semantic network, trying to fill the gap constituted by the absence of such a resource, in order to open the possibilities of implementing new techniques of analysis derived from the studies in Natural Language Processing.",
    "author": [
      {
        "family": "Minozzi",
        "given": "Stefano"
      }
    ],
    "collection-title": "Innsbrucker beiträge zur sprachwissenschaft",
    "container-title": "Latin linguistics today. Akten des 15. Internationalem kolloquiums zur lateinischen linguistik",
    "editor": [
      {
        "family": "Anreiter",
        "given": "Peter"
      },
      {
        "family": "Kienpointner",
        "given": "Manfred"
      }
    ],
    "id": "Minozzi2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, latin",
    "page": "707-716",
    "title": "The Latin WordNet project",
    "type": "paper-conference",
    "volume": "137"
  },
  {
    "ISBN": "0-582-23479-4",
    "URL": "http://eprints.bbk.ac.uk/469/",
    "abstract": "The first half of the book is about spelling, the second about computers. Chapter Two describes how English spelling came to be in the state that it’s in today. In Chapter Three I summarize the debate between those who propose radical change to the system and those who favour keeping it as it is, and I show how computerized correction can be seen as providing at least some of the benefits that have been claimed for spelling reform. Too much of the literature on computerized spellcheckers describes tests based on collections of artificially created errors; Chapter Four looks at the sorts of misspellings that people actually make, to see more clearly the problems that a spellchecker has to face. Chapter Five looks more closely at the errors that people make when they don’t know how to spell a word, and Chapter Six at the errors that people make when they know perfectly well how to spell a word but for some reason write or type something else. Chapter Seven begins the second part of the book with a description of the methods that have been devised over the last thirty years for getting computers to detect and correct spelling errors. Its conclusion is that spellcheckers have some way to go before they can do the job we would like them to do. Chapters Eight to Ten describe a spellchecker that I have designed which attempts to address some of the remaining problems, especially those presented by badly spelt text. In 1982, when I began this research, there were no spellcheckers that would do anything useful with a sentence such as, ’You shud try to rember all ways to youz a lifejacket when yotting.’ That my spellchecker corrects this perfectly (which it does) is less impressive now, I have to admit, than it would have been then, simply because there are now a few spellcheckers on the market which do make a reasonable attempt at errors of that kind. My spellchecker does, however, handle some classes of errors that other spellcheckers do not perform well on, and Chapter Eleven concludes the book with the results of some comparative tests, a few reflections on my spellchecker’s shortcomings and some speculations on possible developments.",
    "author": [
      {
        "family": "Mitton",
        "given": "Roger"
      }
    ],
    "id": "Mitton1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "english, spelling_correction",
    "publisher": "Longman",
    "publisher-place": "Harlow, UK",
    "title": "English spelling and the computer",
    "type": "book"
  },
  {
    "URL": "http://edutech.ch/lms/ev2.php",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Monnard",
        "given": "Jacques"
      },
      {
        "family": "Brugger",
        "given": "Rolf"
      }
    ],
    "id": "Monnard2003",
    "issued": {
      "date-parts": [
        [
          2003,
          3
        ]
      ]
    },
    "publisher": "Edutech mandate of the Swiss Virtual Campus program",
    "publisher-place": "Freiburg, Switzerland",
    "title": "Web based course platforms: Evaluation report",
    "title-short": "Web based course platforms",
    "type": "report"
  },
  {
    "URL": "http://CEUR-WS.org/Vol-523/Mons.pdf",
    "abstract": "The rate of data production in the Life Sciences has now reached such proportions that to consider it irresponsible to fund data generation without proper concomitant funding and infrastructure for storing, analyzing and exchanging the information and knowledge contained in, and extracted from, those data, is not an exaggerated position any longer. The chasm between data production and data handling has become so wide, that many data go unnoticed or at least run the risk of relative obscurity, fail to reveal the information contained in the data set or remains inaccessible due to ambiguity, or financial or legal toll-barriers. As a result, inconsistency, ambiguity and redundancy of data and information on the Web are becoming impediments to the performance of comprehensive information extraction and analysis. This paper attempts a stepwise explanation of the use of richly annotated RDF-statements as carriers of unambiguous, meta-analyzed information in the form of traceable nano-publications.",
    "author": [
      {
        "family": "Mons",
        "given": "Barend"
      },
      {
        "family": "Velterop",
        "given": "Jan"
      }
    ],
    "container-title": "Proceedings of the workshop on semantic web applications in scientific discourse (SWASD 2009)",
    "editor": [
      {
        "family": "Clark",
        "given": "Tim"
      },
      {
        "family": "Luciano",
        "given": "Joanne S."
      },
      {
        "family": "Marshall",
        "given": "M. Scott"
      },
      {
        "family": "Prud’hommeaux",
        "given": "Eric"
      },
      {
        "family": "Stephens",
        "given": "Susie"
      }
    ],
    "id": "Mons2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "nanopublications, rdf, semantic_web",
    "title": "Nano-publication in the e-Science era",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1038/ng0411-281",
    "ISSN": "1061-4036",
    "PMID": "21445068",
    "abstract": "Data citation and the derivation of semantic constructs directly from datasets have now both found their place in scientific communication. The social challenge facing us is to maintain the value of traditional narrative publications and their relationship to the datasets they report upon while at the same time developing appropriate metrics for citation of data and data constructs.",
    "author": [
      {
        "family": "Mons",
        "given": "Barend"
      },
      {
        "dropping-particle": "van",
        "family": "Haagen",
        "given": "Herman"
      },
      {
        "family": "Chichester",
        "given": "Christine"
      },
      {
        "family": "Hoen",
        "given": "Peter-Bram"
      },
      {
        "dropping-particle": "den",
        "family": "Dunnen",
        "given": "Johan T."
      },
      {
        "dropping-particle": "van",
        "family": "Ommen",
        "given": "Gertjan"
      },
      {
        "dropping-particle": "van",
        "family": "Mulligen",
        "given": "Erik"
      },
      {
        "family": "Singh",
        "given": "Bharat"
      },
      {
        "family": "Hooft",
        "given": "Rob"
      },
      {
        "family": "Roos",
        "given": "Marco"
      },
      {
        "family": "Hammond",
        "given": "Joel"
      },
      {
        "family": "Kiesel",
        "given": "Bruce"
      },
      {
        "family": "Giardine",
        "given": "Belinda"
      },
      {
        "family": "Velterop",
        "given": "Jan"
      },
      {
        "family": "Groth",
        "given": "Paul"
      },
      {
        "family": "Schultes",
        "given": "Erik"
      }
    ],
    "container-title": "Nature Genetics",
    "id": "Mons2011",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "nanopublications, semantic_web",
    "page": "281-283",
    "publisher": "Nature Publishing Group",
    "title": "The value of data",
    "type": "article-journal",
    "volume": "43"
  },
  {
    "abstract": "This paper presents a new automatic tool for assessing the linguistic quality of scientific papers written in English. A set of complex lexical and syntactic surface-level rules compute more than 80 style-related variables. Their combination defines the score of a text in the four dimensions of style assessment for scientific papers: clarity, variety, conciseness and conviction. The software has been tested on 60 published articles and incorporates an animated agent that acts as a personal assistant and explains the results to the writer and how to improve them.",
    "author": [
      {
        "family": "Montero",
        "given": "Juan M."
      },
      {
        "family": "Duque",
        "given": "Mar M."
      }
    ],
    "container-title": "Proceedings of the corpus linguistics 2003 conference",
    "editor": [
      {
        "family": "Archer",
        "given": "Dawn"
      },
      {
        "family": "Rayson",
        "given": "Paul"
      },
      {
        "family": "Wilson",
        "given": "Andrew"
      },
      {
        "family": "McEnery",
        "given": "Tony"
      }
    ],
    "id": "Montero2003",
    "issued": {
      "date-parts": [
        [
          2003,
          3
        ]
      ]
    },
    "keyword": "interactive_editing, post-writing_tools",
    "page": "544-551",
    "publisher": "Lancaster University",
    "title": "ANESTTE: A writer’s assistant for a specific purpose language",
    "title-short": "ANESTTE",
    "type": "paper-conference"
  },
  {
    "ISBN": "9780262633185",
    "abstract": "Interactive fiction—the best-known form of which is the text game or text adventure—has not received as much critical attention as have such other forms of electronic literature as hypertext fiction and the conversational programs known as chatterbots. Twisty Little Passages (the title refers to a maze in Adventure, the first interactive fiction) is the first book-length consideration of this form, examining it from gaming and literary perspectives. Nick Montfort, an interactive fiction author himself, offers both aficionados and first-time users a way to approach interactive fiction that will lead to a more pleasurable and meaningful experience of it. Twisty Little Passages looks at interactive fiction beginning with its most important literary ancestor, the riddle. Montfort then discusses Adventure and its precursors (including the I Ching and Dungeons and Dragons), and follows this with an examination of mainframe text games developed in response, focusing on the most influential work of that era, Zork. He then considers the introduction of commercial interactive fiction for home computers, particularly that produced by Infocom. Commercial works inspired an independent reaction, and Montfort describes the emergence of independent creators and the development of an online interactive fiction community in the 1990s. Finally, he considers the influence of interactive fiction on other literary and gaming forms. With Twisty Little Passages, Nick Montfort places interactive fiction in its computational and literary contexts, opening up this still-developing form to new consideration.",
    "author": [
      {
        "family": "Montfort",
        "given": "Nick"
      }
    ],
    "id": "Montfort2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "interactive_fiction",
    "language": "en-US",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Twisty little passages: An approach to interactive fiction",
    "title-short": "Twisty little passages",
    "type": "book"
  },
  {
    "ISBN": "3540440429",
    "URL": "http://portal.acm.org/citation.cfm?id=753552",
    "author": [
      {
        "family": "Monz",
        "given": "Christof"
      },
      {
        "dropping-particle": "de",
        "family": "Rijke",
        "given": "Maarten"
      }
    ],
    "container-title": "CLEF ’01: Revised papers from the second workshop of the cross-language evaluation forum on evaluation of cross-language information retrieval systems",
    "id": "Monz2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "ir",
    "page": "262-277",
    "publisher": "Springer-Verlag",
    "publisher-place": "London, UK",
    "title": "Shallow morphological analysis in monolingual information retrieval for dutch, german, and italian",
    "type": "paper-conference"
  },
  {
    "URL": "http://moodle.org/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "id": "Moodle",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "title": "Moodle – a free, open source course management system for online learning",
    "type": ""
  },
  {
    "URL": "http://aclweb.org/anthology/D07-1041",
    "abstract": "We demonstrate an approach for inducing a tagger for historical languages based on existing resources for their modern varieties. Tags from Present Day English source text are projected to Middle English text using alignments on parallel Biblical text. We explore the use of multiple alignment approaches and a bigram tagger to reduce the noise in the projected tags. Finally, we train a maximum entropy tagger on the output of the bigram tagger on the target Biblical text and test it on tagged Middle English text. This leads to tagging accuracy in the low 80’s on Biblical test material and in the 60’s on other Middle English material. Our results suggest that our bootstrapping methods have considerable potential, and could be used to semi-automate an approach based on incremental manual annotation.",
    "author": [
      {
        "family": "Moon",
        "given": "Taesun"
      },
      {
        "family": "Baldridge",
        "given": "Jason"
      }
    ],
    "container-title": "Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL)",
    "id": "Moon2007",
    "issued": {
      "date-parts": [
        [
          2007,
          6
        ]
      ]
    },
    "keyword": "cultural_heritage, english, pos_tagging",
    "page": "390-399",
    "publisher": "Association for Computational Linguistics",
    "title": "Part-of-speech tagging for middle English through alignment and projection of parallel diachronic texts",
    "type": "paper-conference"
  },
  {
    "URL": "https://newleftreview.org/II/1/franco-moretti-conjectures-on-world-literature",
    "abstract": "Nearly two hundred years ago, Goethe announced the imminence of a world literature. Here Franco Moretti offers a set of hypotheses for tracking the birth and fate of the novel in the peripheries of Europe, in Latin America, Arab lands, Turkey, China, Japan, West Africa. For the first time, the prospect of a morphology of global letters?",
    "author": [
      {
        "family": "Moretti",
        "given": "Franco"
      }
    ],
    "container-title": "New Left Review",
    "id": "Moretti2000b",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "digital_humanities, literature",
    "language": "en-US",
    "page": "54-68",
    "title": "Conjectures on world literature",
    "type": "article-journal"
  },
  {
    "URL": "https://newleftreview.org/II/84/franco-moretti-operationalizing",
    "abstract": "’Operationalizing’ must be the ugliest word I’ve ever used, but it is nevertheless the hero of the pages that follow, because it refers to a process which is absolutely central to the new field of computational criticism, or, as it has come to be called, of the digital humanities. Though the word is often used merely as a complicated synonym for ’realizing’ or ’implementing’—the Merriam-Webster online, for instance, mentions ’operationalizing a program’, and adds a quote on ’operationalizing the artistic vision of the organization’—the original root of the term was different, and much more precise; and for once origin is right, this is one of those rare cases when a term has an actual birth date: 1927, when P. W. Bridgman devoted the opening of his Logic of Modern Physics to ’the operational point of view’.",
    "author": [
      {
        "family": "Moretti",
        "given": "Franco"
      }
    ],
    "container-title": "New Left Review",
    "id": "Moretti2013d",
    "issue": "84",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities, literature",
    "language": "en-US",
    "page": "103-119",
    "title": "“Operationalizing” or, the function of measurement in literary theory",
    "type": "article-journal"
  },
  {
    "URL": "http://educause.edu/ir/library/pdf/ers0302/rs/ers0302w.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Morgan",
        "given": "Glenda"
      }
    ],
    "genre": "Research Study",
    "id": "Morgan2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "publisher": "EDUCAUSE Center for Applied Research",
    "publisher-place": "Boulder, CO, USA",
    "title": "Faculty use of course management systems",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Morgan",
        "given": "R. L. “Bob”"
      },
      {
        "family": "Cantor",
        "given": "Scott"
      },
      {
        "family": "Carmody",
        "given": "Steven"
      },
      {
        "family": "Hoehn",
        "given": "Walter"
      },
      {
        "family": "Klingenstein",
        "given": "Ken"
      }
    ],
    "container-title": "EDUCAUSE Quarterly",
    "id": "Morgan2004",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "12-17",
    "title": "Federated security: The Shibboleth approach",
    "title-short": "Federated security",
    "type": "article-journal",
    "volume": "27"
  },
  {
    "DOI": "10.1007/978-3-319-39931-7\\_26",
    "URL": "http://dx.doi.org/10.1007/978-3-319-39931-7\\_26",
    "abstract": "Agent-based models of organizations have traditionally had a single level of agency, whether at the individual or organizational level, but many interesting organizational phenomena, including organizational resilience and turnover, involve agency at multiple organizational levels. We propose an extensible multi-modeling framework, realized in software, to model these phenomena and many more. Two applications will be given to demonstrate the framework’s versatility.",
    "author": [
      {
        "family": "Morgan",
        "given": "Geoffrey P."
      },
      {
        "family": "Carley",
        "given": "Kathleen M."
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Social, cultural, and behavioral modeling. Proceedings of SBP-BRiMS 2016",
    "editor": [
      {
        "family": "Xu",
        "given": "Kevin S."
      },
      {
        "family": "Reitter",
        "given": "David"
      },
      {
        "family": "Lee",
        "given": "Dongwon"
      },
      {
        "family": "Osgood",
        "given": "Nathaniel"
      }
    ],
    "id": "Morgan2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "formal_models",
    "page": "272-281",
    "publisher": "Springer",
    "publisher-place": "Cham",
    "title": "An Agent-Based framework for active multi-level modeling of organizations",
    "type": "paper-conference",
    "volume": "9708"
  },
  {
    "DOI": "10.1109/ICSMC.2001.971946",
    "abstract": "The paper proposes an effective incremental parsing method for such interactive natural language processing systems as real-time dialogue systems, simultaneous machine interpreting systems, etc. This method produces the analysis of the input while it is being received. It can efficiently deal with not only the normal input, which is piecemeal addition to the input from left to right, but also such changes of the input as insertion and deletion. For such changes of the input, this method exploits parts of the previous analyses. We implemented this method on a workstation and conducted an experiment. We confirm that the method can be expected to be useful for an online language processing system",
    "author": [
      {
        "family": "Mori",
        "given": "Daisuke"
      },
      {
        "family": "Matsubara",
        "given": "Shigeki"
      },
      {
        "family": "Inagaki",
        "given": "Yasuyoshi"
      }
    ],
    "container-title": "Systems, man, and cybernetics, 2001 IEEE international conference on",
    "id": "Mori2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "interactive_parsing",
    "page": "2880-2885",
    "title": "Incremental parsing for interactive natural language interface",
    "type": "paper-conference",
    "volume": "5"
  },
  {
    "ISSN": "0891-2017",
    "URL": "http://portal.acm.org/citation.cfm?id=971740",
    "abstract": "In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning. These lexical chains are a direct result of units of text being \"about the same thing,\" and finding text structure involves finding units of text that are about the same thing. Hence, computing the chains is useful, since they will have a correspondence to the structure of the text. Determining the structure of text is an essential step in determining the deep meaning of the text. In this paper, a thesaurus is used as the major knowledge base for computing lexical chains. Correspondences between lexical chains and structural elements are shown to exist. Since the lexical chains are computable, and exist in non-domain-specific text, they provide a valuable indicator of text structure. The lexical chains also provide a semantic context for interpreting words, concepts, and sentences.",
    "author": [
      {
        "family": "Morris",
        "given": "Jane"
      },
      {
        "family": "Hirst",
        "given": "Graeme"
      }
    ],
    "container-title": "Computational Linguistics",
    "id": "Morris1991",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "keyword": "computational_linguistics",
    "page": "21-48",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Lexical cohesion computed by thesaural relations as an indicator of the structure of text",
    "type": "article-journal",
    "volume": "17"
  },
  {
    "URL": "http://www.bath.ac.uk/e-learning/Download/DM20040909.pdf",
    "abstract": "The paper E-Learning Flexible Frameworks and Tools: Is it too late? is based on some of the author’s contributions to Auricle (www.bath.ac.uk/e-learning/auricle.htm), the publicly accessible weblog of the e-Learning team at the University of Bath. The author considers what is likely to happen when the JISC’s vision as promulgated in the various documents and calls supporting both the JISC E-Learning Programme and the JISC Information Environment meets the reality of e-learning infrastructures as already being built at the coalface. The author contends that because key decisions and investments are already being (or have been) made, the widespread adoption by institutions of the current generation of MLE/VLEs is in danger of creating a de facto global e-learning monoculture. Institutional inertia is now extremely high. The irony in all of this is that it’s only now, after a few years of experience, that we are all in a better position to make informed decisions. Whilst it’s gratifying to see the quality of JISC thinking about what’s required it will now be very difficult for institutions to reverse earlier decisions. Why? Some institutions have been doing more than gathering experience and have made a full-blown strategic commitment to products which represent only one way of offering e-learning. How many have thought about exit strategies? How many exit strategies will work? How many will now be willing to allow ’different e-learning tools’ that don’t fit into the licensed, and therefore supported, vendor’s product? Is it possible to think beyond the monolithic VLE model? The author will illustrate his presentation with some of the alternatives to the status quo.",
    "author": [
      {
        "family": "Morrison",
        "given": "Derek"
      }
    ],
    "container-title": "ALT-c 2004, proceedings of 11<sup>th</sup> conference of the association of learning technology (ALT)",
    "id": "Morrison2004a",
    "issued": {
      "date-parts": [
        [
          2004,
          9
        ]
      ]
    },
    "keyword": "e-learning",
    "title": "E-learning flexible frameworks and tools: Is it too late? – the director’s cut",
    "title-short": "E-learning flexible frameworks and tools",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.auricle.org/auriclewp/?p=116",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Morrison",
        "given": "Derek"
      }
    ],
    "id": "Morrison2004b",
    "issued": {
      "date-parts": [
        [
          2004,
          10
        ]
      ]
    },
    "publisher": "Online article",
    "title": "Clark Kent solutions have super-powers – well sort of!",
    "type": ""
  },
  {
    "abstract": "How readable is your writing style? Several public domain/shareware programs for microcomputers can give you fast, helpful insight. If you take the time, several of these programs will also show you how to improve your communications. Microcomputers can fully justify their existence for word processing alone. But besides wordwrap, formatting, fast textediting, and mail merge, word processing offers more to the writer who is concerned with effective communication: for example, thought processors and on-line access to spelling ...",
    "author": [
      {
        "family": "Mortenson",
        "given": "Tom"
      }
    ],
    "container-title": "Computes and Composition",
    "id": "Mortenson1987",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "keyword": "grammar_checking, interactive_editing, spelling_correction",
    "page": "67-77",
    "title": "Writing style/readability checkers to add to your word processing",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "abstract": "In the framework of the colloquia on Machine Processing of Old German Texts held at regular intervals since 1971, the Fifth International Colloquium (March 1997 in Würzburg) concentrated on the use of the new media, corpus-related issues posed by Old German texts, and lexicographic problems in connection with different historical stages of language development. In addition there was discussion of fundamental organizational questions, encoding designs such as the Text Encoding Initiative, and solutions to individuals problems posed by text data processing. The central papers are assembled in this volume.",
    "editor": [
      {
        "family": "Moser",
        "given": "Stephan"
      },
      {
        "family": "Stahl",
        "given": "Peter"
      },
      {
        "family": "Wegstein",
        "given": "Werner"
      },
      {
        "family": "Wolf",
        "given": "Norbert R."
      }
    ],
    "id": "Moser2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "publisher": "De Gruyter",
    "publisher-place": "Berlin/New York",
    "title": "Maschinelle Verarbeitung altdeutscher Texte V. Beiträge zum Fünften Internationalen Symposion, Würzburg 4.–6. März 1997",
    "type": "book"
  },
  {
    "DOI": "10.3200/hmts.41.1.39-64",
    "URL": "http://dx.doi.org/10.3200/hmts.41.1.39-64",
    "abstract": "Historians and other humanists interested in modeling the spatiality of past places should consider the value of digital gazetteers, which are indexes of place names. In contrast to geographic information systems (GIS), gazetteers make it feasible to record uncertainty, textual references, multiple perspectives, and temporal change. Using examples from Chinese history, the author argues that it is feasible to design gazetteers that are consistent with scholarly practices in history and the humanities.",
    "author": [
      {
        "family": "Mostern",
        "given": "Ruth"
      }
    ],
    "container-title": "Historical Methods: A Journal of Quantitative and Interdisciplinary History",
    "id": "Mostern2008",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "page": "39-46",
    "publisher": "Routledge",
    "title": "Historical gazetteers: An experiential perspective, with examples from Chinese history",
    "title-short": "Historical gazetteers",
    "type": "article-journal",
    "volume": "41"
  },
  {
    "ISBN": "2915973318",
    "abstract": "Un des grands paradoxes de la modernité est que, pour la plupart de nos contemporains, la philosophie de la connaissance soit passée à côté d’un fait épistémologique fondamental, à savoir le rôle du processus de modélisation mathématique dans l’intelligence scientifique des mathématiques appliquées. Tout le propos de ce livre est de montrer qu’une compréhension lucide et conséquente de la modélisation mathématique débouche nécessairement sur une nouvelle attitude face à toute connaissance fondée sur un discours.",
    "author": [
      {
        "family": "Mothon",
        "given": "Bernard"
      }
    ],
    "id": "Mothon2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "formal_models",
    "language": "fr-FR",
    "publisher": "Archétype82",
    "publisher-place": "Paris",
    "title": "Modélisation et vérité",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-642-17746-0\\_35",
    "ISBN": "978-3-642-17745-3",
    "URL": "http://dx.doi.org/10.1007/978-3-642-17746-0\\_35",
    "abstract": "RDF(S) and OWL 2 currently support only static ontologies. In practice, however, the truth of statements often changes with time, and Semantic Web applications often need to represent such changes and reason about them. In this paper we present a logic-based approach for representing validity time in RDF and OWL. Unlike the existing proposals, our approach is applicable to entailment relations that are not deterministic, such as the Direct Semantics or the RDF-Based Semantics of OWL 2. We also extend SPARQL to temporal RDF graphs and present a query evaluation algorithm. Finally, we present an optimization of our algorithm that is applicable to entailment relations characterized by a set of deterministic rules, such RDF(S) and OWL 2 RL/RDF entailment.",
    "author": [
      {
        "family": "Motik",
        "given": "Boris"
      }
    ],
    "chapter-number": "35",
    "collection-title": "Lecture notes in computer science",
    "container-title": "The semantic web – ISWC 2010",
    "editor": [
      {
        "family": "Patel-Schneider",
        "given": "Peter F."
      },
      {
        "family": "Pan",
        "given": "Yue"
      },
      {
        "family": "Hitzler",
        "given": "Pascal"
      },
      {
        "family": "Mika",
        "given": "Peter"
      },
      {
        "family": "Zhang",
        "given": "Lei"
      },
      {
        "family": "Pan",
        "given": "Jeff Z."
      },
      {
        "family": "Horrocks",
        "given": "Ian"
      },
      {
        "family": "Glimm",
        "given": "Birte"
      }
    ],
    "id": "Motik2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "rdf, temporal_data",
    "page": "550-565",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Representing and querying validity time in RDF and OWL: A logic-based approach",
    "title-short": "Representing and querying validity time in RDF and OWL",
    "type": "chapter",
    "volume": "6496"
  },
  {
    "author": [
      {
        "family": "Motro",
        "given": "Amihai"
      }
    ],
    "container-title": "Modern database systems: The object model, interoperability, and beyond",
    "editor": [
      {
        "family": "Kim",
        "given": "Won"
      }
    ],
    "id": "Motro1995",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "keyword": "database, uncertainty",
    "language": "en-US",
    "page": "457-476",
    "publisher": "ACM/Addison-Wesley",
    "publisher-place": "New York, NY, USA",
    "title": "Management of uncertainty in database systems",
    "type": "chapter"
  },
  {
    "DOI": "10.4000/jda.3652",
    "ISSN": "1156-0428",
    "abstract": "L’utilisation de l’informatique en sciences humaines et sociales est pratiquée depuis maintenant plus de quarante ans. Plusieurs voies ont été explorées au cours de cette déjà assez longue histoire. La plus récente, qui prend le nom de digital humanities, désigne une intégration intense et à plusieurs niveaux des technologies numériques dans tous les processus de recherche, depuis la collecte de données jusqu’à la publication. Dans ce nouveau contexte d’un travail non plus exceptionnel ni ponctuel mais habituel et courant avec les technologies numériques (constitution de corpus de sources numérisées ou nativement numériques, calculs sta­tistiques, construction de bases de données documentaires, diffusion en ligne des corpus, publications en ligne, pratiques d’écriture collaborative, open peer review), de nouvelles perspectives de recherche se dessinent, mais de nouveaux besoins naissent aussi. C’est le sens en particulier du rapport publié en 2006 aux État-Unis par le Council of American Learned Societies intitulé Our Cultural Commonwealth, qui fait émerger la notion de cyberinfrastructure. Une cyberin­frastructure est, selon les termes du rapport, « un ensemble d’information, d’expertise, de standards, de stratégies, d’outils et de services qui sont partagés largement entre les commu­nautés mais développés spécifiquement pour des usages savants. Une cyberinfrastructure est quelque chose de plus précis que le réseau lui-même, mais de plus général qu’un outil ou une ressource développés pour un projet particulier, ou même, plus largement, pour une discipline particulière ».",
    "author": [
      {
        "family": "Mounier",
        "given": "Pierre"
      }
    ],
    "container-title": "Journal des anthropologues",
    "id": "Mounier2010",
    "issue": "122–123",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "fr-FR",
    "page": "447-452",
    "title": "Manifeste des Digital Humanities",
    "type": "article-journal"
  },
  {
    "DOI": "10.4000/books.editionsmsh.12006",
    "ISBN": "9782735122554",
    "abstract": "Que sont les humanités numériques ? D’abord une rencontre, au lendemain de la Seconde Guerre mondiale. Celle d’un prêtre jésuite soucieux d’analyser la Somme théologique de Thomas d’Aquin avec les ordinateurs d’IBM. Cette collaboration donnera naissance à ce qu’on appellera plus tard les humanités numériques. Porteuses de l’histoire des technologies, marquée par le développement des technosciences et du complexe militaro-industriel, les humanités numériques conduisent à s’interroger en retour sur ce qui fait la spécificité des humanités. L’union des technologies numériques et des humanités conduit-elle à remettre en cause ce qui les dinstingue traditionnellement ? Le numérique pousse-t-il, par les méthodes et modèles qu’il permet de développer dans ce champ de recherche, à placer les humanités sous la domination de modèles scientifiques qui leur sont étrangers ? Quels dangers ces approches comportent-elles, en particulier lorsqu’une part croissante des productions culturelles et des interactions sociales est désormais placée sous l’emprise de sociétés commerciales globalisées qui font un usage massif du numérique ? Dans cet ouvrage, Pierre Mounier nous livre une histoire critique des humanités numériques et propose de redéfinir à la lumière de ces analyses le contrat moral que les humanités peuvent établir avec la société.",
    "author": [
      {
        "family": "Mounier",
        "given": "Pierre"
      }
    ],
    "id": "Mounier2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "fr-FR",
    "publisher": "Éditions de la Maison des sciences de l’homme",
    "title": "Les humanités numériques: Une histoire critique",
    "title-short": "Les humanités numériques",
    "type": "book"
  },
  {
    "URL": "http://i-a-e.org/downloads/doc\\_download/7-introduction-to-problem-solving-in-the-information-age.html",
    "abstract": "This short 99-page book is intended primarily for preservice and inservice teachers of K-12 students, and the teachers of these teachers. In this book, the term problem solving includes posing and solving problems, posing and accomplishing tasks, posing and answering questions, and posing and making decisions. Problem solving is an integral component of every academic discipline. Humans solve problems using their physical and mental capabilities, and tools that they have developed. The Information Age has brought us a wide range of computer-based tools that are powerful aids to problem solving. Now, more than ever, it is important to stress problem solving and higher-order critical thinking throughout our educational system.",
    "author": [
      {
        "family": "Moursund",
        "given": "David"
      }
    ],
    "id": "Moursund2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "computational_thinking, formal_models, pedagogy",
    "publisher": "Information Age Education",
    "publisher-place": "Eugene, OR, USA",
    "title": "Introduction to problem solving in the information age",
    "type": "book"
  },
  {
    "URL": "https://dh2017.adho.org/abstracts/163/163.pdf",
    "abstract": "We propose a tool for analyzing EGO (European History Online) document collection according to the link structure. The analysis tool is pre-step towards building a (semi-)automatic approach for linking EGO articles to each other as well as to other external resources. Our aim is to assist the editorial office in performing the linking task and to increase the inter-connectivity of EGO articles.",
    "author": [
      {
        "family": "Mousselly Sergieh",
        "given": "Hatem"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Gurevych",
        "given": "Iryna"
      }
    ],
    "container-title": "Proceedings of digital humanities 2017 (DH 2017)",
    "id": "MoussellySergieh2017",
    "issued": {
      "date-parts": [
        [
          2017,
          8
        ]
      ]
    },
    "keyword": "digital_humanities, mxp",
    "page": "758-761",
    "publisher": "ADHO",
    "title": "EGOlink: Supporting editors of online historical sources through automatic link discovery",
    "title-short": "EGOlink",
    "type": "paper-conference"
  },
  {
    "URL": "http://hdl.handle.net/2263/10735",
    "abstract": "The literary term \"intertextuality\" was introduced into biblical studies in 1989 and concerns the complex relationships that exist between texts. Not surprisingly, this was of interest to those who study the use of the Old Testament in the New, for old texts appear to be given new meaning by being used in new contexts. In this article, I demonstrate the fruitfulness of this approach by offering a survey of five different \"types\" of intertextuality currently in use today. I conclude that if scholars wish to continue to use the term, they need to clarify which \"type\" of intertextuality they are using, so that readers can know what is being claimed.",
    "author": [
      {
        "family": "Moyise",
        "given": "Steve"
      }
    ],
    "container-title": "Verbum et Ecclesia",
    "id": "Moyise2002",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "intertextuality",
    "page": "418-431",
    "title": "Intertextuality and biblical studies: A review",
    "title-short": "Intertextuality and biblical studies",
    "type": "article-journal",
    "volume": "23"
  },
  {
    "URL": "https://aclanthology.org/W10-0404",
    "abstract": "Web applications have the opportunity to check spelling, style, and grammar using a software service architecture. A software service authoring aid can offer contextual spell checking, detect real word errors, and avoid poor grammar checker suggestions through the use of large language models. Here we present After the Deadline, an open source authoring aid, used in production on Word-Press.com, a blogging platform with over ten million writers. We discuss the benefits of the software service environment and how it affected our choice of algorithms. We summarize our design principles as speed over accuracy, simplicity over complexity, and do what works.",
    "author": [
      {
        "family": "Mudge",
        "given": "Raphael"
      }
    ],
    "container-title": "Proceedings of the NAACL HLT 2010 workshop on computational linguistics and writing: Writing processes and authoring aids (CL&w 2010)",
    "editor": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Dale",
        "given": "Robert"
      }
    ],
    "id": "Mudge2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "authoring, interactive_editing, post-writing_tools, spelling_correction",
    "page": "24-32",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "The design of a proofreading software service",
    "type": "paper-conference"
  },
  {
    "abstract": "This paper reports on a technique used to generate accurate HTML output from GNU Troff. GNU Troff is a typesetting package which reads plain text mixed with formatting commands and produces formatted output. It supports a number of devices and now supports the production of HTML. The paper discusses the design of the HTML device driver grohtml and modifications made to GNU Troff. The front end program troff was modified to maintain a reduced state machine which is examined each time a glyph is passed to the back end device driver (post-grohtml). Any change in system state between the production of two glyphs results in a sequence of events being passed to the device driver. There is a direct correspondence between this technique and creating a script for a next event simulation queue. Furthermore the device driver reconstructs the system state and formats the HTML according to state changes caused when processing the event queue. This technique works well, as it minimises the state information passed from front end to back end device driver whilst still preserving the high level layout of the text. Using this technique GNU Troff effectively translates input source into another mark-up language and thus this technique could be extended to translate GNU Troff documents into any of the OpenOffice supported formats. Troff has been in use for three decades now and is still actively used by authors. Troff’s biggest use, however, is to format manual pages for GNU/Linux and other UNIX like operating systems. Introducing this facility into GNU Troff provides users with the ability to translate legacy documents into HTML and in the future to a format supported by OpenOffice.",
    "author": [
      {
        "family": "Mulley",
        "given": "Gaius"
      },
      {
        "family": "Lemberg",
        "given": "Werner"
      }
    ],
    "container-title": "International Journal of Simulation Systems, Science and Technology",
    "id": "Mulley2005",
    "issue": "7–8",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "typesetting",
    "page": "37-45",
    "title": "Extending GNU Troff to produce HTML through the technique of next event simulation",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "URL": "http://fastcompany.com/online/39/quickstudy.html",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Muoio",
        "given": "Anna"
      }
    ],
    "container-title": "Fast Company",
    "id": "Muoio2000",
    "issue": "39",
    "issued": {
      "date-parts": [
        [
          2000,
          10
        ]
      ]
    },
    "page": "286",
    "title": "Cisco’s quick study",
    "type": "article-journal",
    "volume": " "
  },
  {
    "DOI": "10.1145/963847.963849",
    "author": [
      {
        "family": "Murphy",
        "given": "Richard T."
      },
      {
        "family": "Appeal",
        "given": "Lola Rhea"
      }
    ],
    "container-title": "SIGCUE Outlook",
    "id": "Murphy1978",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1978
        ]
      ]
    },
    "page": "12-28",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Evaluation of the PLATO IV computer-based education system in the community college",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "DOI": "10.1080/09296170500500694",
    "ISSN": "0929-6174",
    "abstract": "There is an increasing need to deal with multi-lingual documents today. If we could segment multi-lingual documents language-wise, it would be very useful both for exploration of linguistic phenomena, such as code-switching and code mixing, and for computational processing of each segment as appropriate. Identification of language from a given small piece of text is therefore an important problem. This paper is about language identification from small text samples. In this paper, language identification is formulated as a generic machine learning problem a supervised classification task in which features extracted from a training corpus are used for classification. Regression is a well established technique for modelling and analysis. Regression can also be used for classification. This paper gives a clear formulation of multiple linear regression for solving a two-class classification problem. Theoretical bases for verifying the adequacy of the model for the task and for analysing the significance of individual features is included. The method has been applied to pair wise language identification among several major Indian languages including Hindi, Bengali, Marathi, Punjabi, Oriya, Telugu, Tamil, Malayalam and Kannada. Some of these languages belong to the Indo-Aryan family while the others come from the Dravidian family of languages. Language identification was so far a largely unexplored problem in the Indian context. Variations within and across language families have been explored. Variations with regard to sizes of test samples have also been explored. Performance is comparable to the best published results for other languages of the world. In most of the published work in language identification so far, bytes have been taken as the fundamental units of text. Indian scripts are primarily syllabic in nature, reflecting phonetic sound units in a more or less direct fashion. The fundamental units of writing are called aksharas. One of the unique characteristics of Indian scripts is the concept of a script grammar. The script grammar, included in this paper, defines the set of valid aksharas. We hypothesize that aksharas are the more appropriate units of text in Indian languages, not characters or bytes. Our experimental results on language identification support this claim.",
    "author": [
      {
        "family": "Murthy",
        "given": "Kavi N."
      },
      {
        "family": "Kumar",
        "given": "G. Bharadwaja"
      }
    ],
    "container-title": "Journal of Quantitative Linguistics",
    "id": "Murthy2006",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "page": "57-80",
    "publisher": "Routledge",
    "title": "Language identification from small text samples",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "URL": "http://www.nbme.org/PDF/ItemWriting_2003/2003IWGwhole.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Case",
        "given": "Susan M."
      },
      {
        "family": "Swanson",
        "given": "David B."
      }
    ],
    "edition": "3<sup>rd</sup>",
    "id": "NBME2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "publisher": "National Board of Medical Examiners",
    "publisher-place": "Philadelphia, PA, USA",
    "title": "Constructing written test questions for the basic and clinical sciences",
    "type": "book"
  },
  {
    "ISBN": "978-1-932432-78-7",
    "URL": "http://portal.acm.org/citation.cfm?id=1870457.1870469",
    "abstract": "This paper describes the use of a pair Hidden Markov Model (pair HMM) system in mining transliteration pairs from noisy Wikipedia data. A pair HMM variant that uses nine transition parameters, and emission parameters associated with single character mappings between source and target language alphabets is identified and used in estimating transliteration similarity. The system resulted in a precision of 78% and recall of 83% when evaluated on a random selection of English-Russian Wikipedia topics.",
    "author": [
      {
        "family": "Nabende",
        "given": "Peter"
      }
    ],
    "container-title": "Proceedings of the 2010 named entities workshop (NEWS ’10)",
    "id": "Nabende2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "proper_names, spelling_normalization, wikipedia",
    "page": "76-80",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Mining transliterations from wikipedia using pair HMMs",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.campustechnology.com/article.aspx?aid=52667",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Nagel",
        "given": "David"
      }
    ],
    "container-title": "Campus Technology",
    "id": "Nagel2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "title": "Louisiana State moves to Moodle",
    "type": "article-journal"
  },
  {
    "URL": "http://www.acten.net/uploads/images/423/e-learning.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Nagy",
        "given": "Attila"
      }
    ],
    "genre": "E-Content Report",
    "id": "Nagy2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "number": "6",
    "publisher": "ACTeN",
    "title": "e-Learning",
    "type": "report"
  },
  {
    "DOI": "10.1007/978-3-540-39964-3_57",
    "abstract": "Time modeling is a crucial feature in many application domains. However, temporal information often is not crisp, but is uncertain, subjective and vague. This is particularly true when representing historical information, as historical accounts are inherently imprecise. Similarly, we conjecture that in the Semantic Web representing uncertain temporal information will be a common requirement. Hence, existing approaches for temporal modeling based on crisp representation of time cannot be applied to these advanced modeling tasks. To overcome these difficulties, in this paper we present fuzzy interval-based temporal model capable of representing imprecise temporal knowledge. Our approach naturally subsumes existing crisp temporal models, i.e. crisp temporal relationships are intuitively represented in our system. Apart from presenting the fuzzy temporal model, we discuss how this model is integrated with the ontology model to allow annotating ontology definitions with time specifications.",
    "author": [
      {
        "family": "Nagypál",
        "given": "Gábor"
      },
      {
        "family": "Motik",
        "given": "Boris"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "On the move to meaningful Internet systems 2003: CoopIS, DOA, and ODBASE",
    "editor": [
      {
        "family": "Meersman",
        "given": "Robert"
      },
      {
        "family": "Tari",
        "given": "Zahir"
      },
      {
        "family": "Schmidt",
        "given": "Douglas C."
      }
    ],
    "id": "Nagypal2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "ontologies, uncertainty",
    "language": "en-US",
    "page": "906-923",
    "publisher": "Springer",
    "publisher-place": "Berlin, Heidelberg",
    "title": "A fuzzy model for representing uncertain, subjective, and vague temporal knowledge in ontologies",
    "type": "paper-conference",
    "volume": "2888"
  },
  {
    "DOI": "10.1145/1860559.1860604",
    "ISBN": "978-1-4503-0231-9",
    "URL": "http://dx.doi.org/10.1145/1860559.1860604",
    "abstract": "This paper presents an OCR method for degraded character recognition applied to a reference number (RN) of 15 printed characters of an invoice document produced by dot-matrix printer. First, the paper deals with the problem of the reference number localization and extraction, in which the characters tops or bottoms are or not touched with a printed reference line of the electrical bill. In case of touched RN, the extracted characters are severely degraded leading to missing parts in the characters tops or bottoms. Secondly, a combined recognition method based on the complementary similarity measure (CSM) method and MLP-based classifier is used. The CSM is used to accept or reject an incoming character. In case of acceptation, the CSM acts as a feature extractor and produces a feature vector of ten component features. The MLP is then trained using these feature vectors. The use of the CSM as a feature extractor tends to make the MLP very powerful and very well suited for rejection. Experimental results on electrical bills show the ability of the model to yield relevant and robust recognition on severely degraded printed characters.",
    "author": [
      {
        "family": "Namane",
        "given": "Abderrahmane"
      },
      {
        "family": "Soubari",
        "given": "El H."
      },
      {
        "family": "Meyrueis",
        "given": "Patrick"
      }
    ],
    "collection-title": "DocEng ’10",
    "container-title": "Proceedings of the 10th ACM symposium on document engineering",
    "id": "Namene2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "ocr",
    "page": "207-210",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Degraded dot matrix character recognition using CSM-based feature extraction",
    "type": "paper-conference"
  },
  {
    "URL": "https://calico.org/a-765-TechWriter\\%20An\\%20Evolving\\%20System\\%20for\\%20Writing\\%20Assistance\\%20for\\%20Advanced\\%20Learners\\%20of\\%20English.html",
    "abstract": "Writing assistance systems, from simple spelling checkers to more complex grammar and readability analyzers, can be helpful aids to nonnative writers of English. However, many writing assistance systems have two disadvantages. First, they are not designed to encourage skills learning and independence in their users; instead, users may begin to use the system as a crutch. Second, they use a \"one-size-fits-all\" approach, treating all writers’ problems as equivalent. In this paper we describe TechWriter, a personalizable writing assistance program for advanced learners of English that encourages skills learning. We describe TechWriter’s basic writing assistance functionalities, how it can be used by writers alone and working together with writing tutors, who it can be personalized for, and how it can help writers acquire better writing skills over time.",
    "author": [
      {
        "family": "Napolitano",
        "given": "Diane M."
      },
      {
        "family": "Stent",
        "given": "Amanda"
      }
    ],
    "container-title": "CALICO Journal",
    "id": "Napolitano2009",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "authoring, learner_corpora, writing_research",
    "page": "611-625",
    "title": "TechWriter: An evolving system for writing assistance for advanced learners of English",
    "title-short": "TechWriter",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "DOI": "10.1117/12.587293",
    "URL": "http://dx.doi.org/10.1117/12.587293",
    "abstract": "We announce the availability of the UNLV/ISRI Analytic Tools for OCR Evaluation together with a large and diverse collection of scanned document images with the associated ground-truth text. This combination of tools and test data will allow anyone to conduct a meaningful test comparing the performance of competing page-reading algorithms. The value of this collection of software tools and test data is enhanced by knowledge of the past performance of several systems using exactly these tools and this data. These performance comparisons were published in previous ISRI Test Reports and are also provided. Another value is that the tools can be used to test the character accuracy of any page-reading OCR system for any language included in the Unicode standard. The paper concludes with a summary of the programs, test data, and documentation that is available and gives the URL where they can be located.",
    "author": [
      {
        "family": "Nartker",
        "given": "Thomas A."
      },
      {
        "family": "Rice",
        "given": "Stephen V."
      },
      {
        "family": "Lumos",
        "given": "Steven E."
      }
    ],
    "container-title": "Proc. IS&t/SPIE 2005 intl. Symp. On electronic imaging science and technology",
    "editor": [
      {
        "family": "Barney Smith",
        "given": "Elisa H."
      },
      {
        "family": "Taghva",
        "given": "Kazem"
      }
    ],
    "id": "Nartker2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "ocr",
    "page": "37-47",
    "publisher": "SPIE",
    "title": "Software tools and test data for research and testing of page-reading OCR systems",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s10032-007-0057-x",
    "ISSN": "1433-2833",
    "URL": "http://dx.doi.org/10.1007/s10032-007-0057-x",
    "abstract": "This paper addresses two types of classification of noisy, unstructured text such as newsgroup messages: (1) spotting messages containing topics of interest, and (2) automatic conceptual organization of messages without prior knowledge of topics of interest. In addition to applying our hidden Markov model methodology to spotting topics of interest in newsgroup messages, we present a robust methodology for rejecting messages which are off-topic . We describe a novel approach for automatically organizing a large, unstructured collection of messages. The approach applies an unsupervised topic clustering procedure to generate a hierarchical tree of topics.",
    "author": [
      {
        "family": "Natarajan",
        "given": "Prem"
      },
      {
        "family": "Prasad",
        "given": "Rohit"
      },
      {
        "family": "Subramanian",
        "given": "Krishna"
      },
      {
        "family": "Saleem",
        "given": "Shirin"
      },
      {
        "family": "Choi",
        "given": "Fred"
      },
      {
        "family": "Schwartz",
        "given": "Rich"
      }
    ],
    "container-title": "International Journal on Document Analysis and Recognition",
    "id": "Natarajan2007",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "clustering, noisy_text, topic_modeling",
    "page": "187-198",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Finding structure in noisy text: Topic classification and unsupervised clustering",
    "title-short": "Finding structure in noisy text",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "DOI": "10.1016/0165-6074(85)90032-8",
    "ISSN": "01656074",
    "abstract": "Some views on programming, taken in a wide sense and regarded as a human activity, are presented. Accepting that programs will not only have to be designed and produced, but also modified so as to cater for changing demands, it is concluded that the proper, primary aim of programming is, not to produce programs, but to have the programmers build theories of the manner in which the problems at hand are solved by program execution. The implications of such a view of programming on matters such as program life and modification, system development methods, and the professional status of programmers, are discussed.",
    "author": [
      {
        "family": "Naur",
        "given": "Peter"
      }
    ],
    "container-title": "Microprocessing and Microprogramming",
    "id": "Naur1985",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "keyword": "classic, formal_models, programming",
    "language": "en-US",
    "page": "253-261",
    "title": "Programming as theory building",
    "type": "article-journal",
    "volume": "15"
  },
  {
    "DOI": "10.1145/375360.375365",
    "ISSN": "0360-0300",
    "abstract": "We survey the current techniques to cope with the problem of string matching that allows errors. This is becoming a more and more relevant issue for many fast growing areas such as information retrieval and computational biology. We focus on online searching and mostly on edit distance, explaining the problem and its relevance, its statistical behavior, its history and current developments, and the central ideas of the algorithms and their complexities. We present a number of experiments to compare the performance of the different algorithms and show which are the best choices. We conclude with some directions for future work and open problems.",
    "author": [
      {
        "family": "Navarro",
        "given": "Gonzalo"
      }
    ],
    "container-title": "ACM Computing Surveys",
    "id": "Navarro2001",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "ir, spelling_correction",
    "page": "31-88",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A guided tour to approximate string matching",
    "type": "article-journal",
    "volume": "33"
  },
  {
    "URL": "https://aclanthology.org/W/W06/W06-0501",
    "abstract": "This paper describes a pattern-based method to automatically enrich a core ontology with the definitions of a domain glossary. We show an application of our methodology to the cultural heritage domain, using the CIDOC CRM core ontology. To enrich the CIDOC, we use available resources such as the AAT art and architecture glossary, WordNet, the Dmoz taxonomy for named entities, and others.",
    "author": [
      {
        "family": "Navigli",
        "given": "Roberto"
      },
      {
        "family": "Velardi",
        "given": "Paola"
      }
    ],
    "container-title": "Proceedings of the 2<sup>nd</sup> workshop on ontology learning and population: Bridging the gap between text and knowledge",
    "id": "Navigli2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "page": "1-9",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Enriching a formal ontology with a thesaurus: An application in the cultural heritage domain",
    "title-short": "Enriching a formal ontology with a thesaurus",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/29933.30866",
    "ISBN": "0-89791-213-6",
    "URL": "http://dx.doi.org/10.1145/29933.30866",
    "abstract": "Syntax-directed editors were created with the intent of aiding in and improving the programming process. Despite their potential, they have not been successful, as evidenced by limited use. In general, they are perceived as being too difficult to use and the benefits of their use are outweighed by the difficulties. We believe that the cognitive styles and skills of the users have been ignored in the design process. In this paper we present some of our initial results which show that cognitive styles vary over a significant spectrum and that their consideration in the design of a syntax-directed editor will result in an intelligent tool that will be right for the cognitive skills and expertise of an individual user. In turn, an approach to design that takes cognitive variation into account would support the construction of syntax-directed editors which are successfully used.",
    "author": [
      {
        "family": "Neal",
        "given": "Lisa R."
      }
    ],
    "container-title": "CHI ’87: Proceedings of the SIGCHI/GI conference on human factors in computing systems and graphics interface",
    "id": "Neal1987",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "keyword": "interactive_editing",
    "page": "99-102",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Cognition-sensitive design and user modeling for syntax-directed editors",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1080/02626667.2016.1183009",
    "ISSN": "0262-6667",
    "URL": "http://dx.doi.org/10.1080/02626667.2016.1183009",
    "abstract": "Uncertainty is an epistemological concept in the sense that any meaningful understanding of uncertainty requires a theory of knowledge. Therefore, uncertainty resulting from scientific endeavors can only be properly understood in the context of a well-defined philosophy of science. Our main message here is that much of the discussion about uncertainty in hydrology has lacked grounding in these foundational concepts, and has resulted in a controversy that is largely the product of logical errors rather than true (axiomatic) disagreement. As an example, we explore the current debate about the appropriate role of probability theory for hydrological uncertainty quantification. Our main messages are: (1) apparent (and/or claimed) limitations of probability theory are not actually consequences of that theory, but rather of deeper underlying epistemological (and ontological) issues; (2) questions about the appropriateness of probability theory are only meaningful if posed as questions about our preferred philosophy of science; and (3) questions about uncertainty may often be better posed as questions about available information and information use efficiency. Our purpose here is to discuss how hydrologists might ask more meaningful questions about uncertainty.",
    "author": [
      {
        "family": "Nearing",
        "given": "Grey S."
      },
      {
        "family": "Tian",
        "given": "Yudong"
      },
      {
        "family": "Gupta",
        "given": "Hoshin V."
      },
      {
        "family": "Clark",
        "given": "Martyn P."
      },
      {
        "family": "Harrison",
        "given": "Kenneth W."
      },
      {
        "family": "Weijs",
        "given": "Steven V."
      }
    ],
    "container-title": "Hydrological Sciences Journal",
    "id": "Nearing2016",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "philosophy_of_science, uncertainty",
    "page": "1666-1678",
    "title": "A philosophical basis for hydrological uncertainty",
    "type": "article-journal",
    "volume": "61"
  },
  {
    "DOI": "10.1524/9783486719796.105",
    "ISBN": "9783486719796",
    "author": [
      {
        "family": "Nebel",
        "given": "Bernhard"
      },
      {
        "family": "Wölfl",
        "given": "Stefan"
      }
    ],
    "container-title": "Handbuch der Künstlichen Intelligenz",
    "editor": [
      {
        "family": "Görz",
        "given": "Günther"
      },
      {
        "family": "Schneeberger",
        "given": "Josef"
      },
      {
        "family": "Schmid",
        "given": "Ute"
      }
    ],
    "id": "Nebel2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "ai, knowledge_representation",
    "language": "de-DE",
    "page": "105-128",
    "publisher": "De Gruyter",
    "publisher-place": "München",
    "title": "Wissensrepräsentation und ",
    "type": "chapter"
  },
  {
    "URL": "http://atala.org/IMG/pdf/TAL-2009-50-2-09-Nederhof.pdf",
    "abstract": "Interlinear text presents a collection of interpretations of a manuscript. Whereas such a form is often compiled by a single author or a single team of scholars, we here consider automatic creation of interlinear text out of independently created linguistic resources. In terms of mathematical structures, we investigate the constraints one may want to impose on the rendering and pair-wise alignment of resources, and present a computer algorithm that solves those constraints, resulting in suitable interlinear text.",
    "author": [
      {
        "family": "Nederhof",
        "given": "Mark-Jan"
      }
    ],
    "container-title": "Traitement Automatique des Langues",
    "id": "Nederhof2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "page": "237-255",
    "title": "Automatic creation of interlinear text for philological purposes",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "DOI": "10.1016/0022-2836(70)90057-4",
    "ISSN": "0022-2836",
    "abstract": "A computer adaptable method for finding similarities in the amino acid sequences of two proteins has been developed. From these findings it is possible to determine whether significant homology exists between the proteins. This information is used to trace their possible evolutionary development. The maximum match is a number dependent upon the similarity of the sequences. One of its definitions is the largest number of amino acids of one protein that can be matched with those of a second protein allowing for all possible interruptions in either of the sequences. While the interruptions give rise to a very large number of comparisons, the method efficiently excludes from consideration those comparisons that cannot contribute to the maximum match. Comparisons are made from the smallest unit of significance, a pair of amino acids, one from each protein. All possible pairs are represented by a two-dimensional array, and all possible comparisons are represented by pathways through the array. For this maximum match only certain of the possible pathways must be evaluated. A numerical value, one in this case, is assigned to every cell in the array representing like amino acids. The maximum match is the largest number that would result from summing the cell values of every pathway.",
    "author": [
      {
        "family": "Needleman",
        "given": "Saul B."
      },
      {
        "family": "Wunsch",
        "given": "Christian D."
      }
    ],
    "container-title": "Journal of Molecular Biology",
    "id": "Needleman1970",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1970
        ]
      ]
    },
    "keyword": "approximate_matching",
    "page": "443-453",
    "title": "A general method applicable to the search for similarities in the amino acid sequence of two proteins",
    "type": "article-journal",
    "volume": "48"
  },
  {
    "ISBN": "0-9713119-0-0",
    "URL": "http://inform-fiction.org/manual/about_dm4.html",
    "abstract": "Inform is a system for creating adventure games, and this is the book to read about it. It translates an author’s textual description into a simulated world which can be explored by readers using almost any computer, with the aid of an ”interpreter” program. Inform is a suite of software, called the ”library”, as well as a compiler. Without the library, it would be a major undertaking to design even the smallest game. The library has two ingredients: the ”parser”, which tries to make sense of the player’s typed commands, and the ”world model”, a complex web of standard rules, such as that people can’t see without a source of light. Given these, the designer only needs to describe places and items, mentioning any exceptional rules that apply. (”There is a bird here, which is a normal item except that you can’t pick it up.”) This manual describes Inform 6.21 (or later), with library 6/9 (or later), but earlier Inform 6 releases are similar.",
    "author": [
      {
        "family": "Nelson",
        "given": "Graham"
      }
    ],
    "edition": "4",
    "id": "Nelson2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "interactive_fiction",
    "language": "en-US",
    "publisher": "The Interactive Fiction Library",
    "publisher-place": "St. Charles, IL, USA",
    "title": "The Inform designer’s manual",
    "type": "book"
  },
  {
    "abstract": "This is an account of theoretical issues that came out, almost unbidden, from a practical test of the following hypothesis: that the natural language in which to write interactive fiction is natural language. IF is a form of creative writing impossible before the development of computing, but whose 30-year history has seen a flourishing of experimentation if not mainstream acceptance (except in an early commercial phase): the author creates an imaginary textual world that can actively be explored by a “reader,” or “player,” directing the actions of a protagonist. Such works have hitherto been created as if computer programs, using specially adapted programming languages (see for instance Nelson (2001)), but the Inform 7 project aims to replace such syntax with natural language: specifically, a subset of English. This change proved far more radical than had initially been expected, and it became clear that semantic analysis and related branches of linguistics were of great relevance to practical issues of how design systems for IF should work.",
    "author": [
      {
        "family": "Nelson",
        "given": "Graham"
      }
    ],
    "container-title": "IF theory reader",
    "editor": [
      {
        "family": "Jackson-Mead",
        "given": "Kevin"
      },
      {
        "family": "Wheeler",
        "given": "J. Robinson"
      }
    ],
    "id": "Nelson2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "interactive_fiction",
    "language": "en-US",
    "page": "141-188",
    "publisher": "Transcript On Press",
    "publisher-place": "Boston, MA, USA",
    "title": "Natural language, semantic analysis, and interactive fiction",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/978-3-319-39931-7\\_5",
    "URL": "http://dx.doi.org/10.1007/978-3-319-39931-7\\_5",
    "abstract": "What is the relationship between an individual’s values and their propensity to trust other people? To explore this question, we built decision trees on the microdata provided by the World Value’s Survey. Our findings confirm the extant literature while also hinting at cultural heterogeneity. We propose that studying nationally-specific decision trees based on survey data allows for easy-to-intuit representations of complex social problems. Moreover, for the sake of pragmatism, decision trees developed in this manner offer researchers a good tool in terms of cost-to-benefits.",
    "author": [
      {
        "family": "Nelson",
        "given": "John B."
      },
      {
        "family": "Kennedy",
        "given": "William G."
      },
      {
        "family": "Krueger",
        "given": "Frank"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Social, cultural, and behavioral modeling. Proceedings of SBP-BRiMS 2016",
    "editor": [
      {
        "family": "Xu",
        "given": "Kevin S."
      },
      {
        "family": "Reitter",
        "given": "David"
      },
      {
        "family": "Lee",
        "given": "Dongwon"
      },
      {
        "family": "Osgood",
        "given": "Nathaniel"
      }
    ],
    "id": "Nelson2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "formal_models, mental_models, uncertainty",
    "page": "42-50",
    "publisher": "Springer",
    "publisher-place": "Cham",
    "title": "Exploratory models of trust with Empirically-Inferred decision trees",
    "type": "paper-conference",
    "volume": "9708"
  },
  {
    "ISBN": "9780262141055",
    "author": [
      {
        "family": "Nersessian",
        "given": "Nancy J."
      }
    ],
    "id": "Nersessian2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Creating scientific concepts",
    "type": "book"
  },
  {
    "DOI": "10.1145/2037342.2037370",
    "ISBN": "978-1-4503-0916-5",
    "abstract": "The paper presents a novel web-based platform for experimental workflow development in historical document digitisation and analysis. The platform has been developed as part of the IMPACT project, providing a range of tools and services for transforming physical documents into digital resources. It explains the main drivers in developing the technical framework and its architecture, how and by whom it can be used and presents some initial results. The main idea lies in setting up an interoperable and distributed infrastructure based on loose coupling of tools via web services that are wrapped in modular workflow templates which can be executed, combined and evaluated in many different ways. As the workflows are registered through a Web 2.0 environment, which is integrated with a workflow management system, users can easily discover, share, rate and tag workflows and thereby support the building of capacity across the whole community. Where ground truth is available, the workflow templates can also be used to compare and evaluate new methods in a transparent and flexible way.",
    "author": [
      {
        "family": "Neudecker",
        "given": "Clemens"
      },
      {
        "family": "Schlarb",
        "given": "Sven"
      },
      {
        "family": "Dogan",
        "given": "Zeki M."
      },
      {
        "family": "Missier",
        "given": "Paolo"
      },
      {
        "family": "Sufi",
        "given": "Shoaib"
      },
      {
        "family": "Williams",
        "given": "Alan"
      },
      {
        "family": "Wolstencroft",
        "given": "Katy"
      }
    ],
    "container-title": "Proceedings of the 2011 workshop on historical document imaging and processing (HIP ’11)",
    "id": "Neudecker2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "page": "161-168",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "An experimental workflow development platform for historical document digitisation and analysis",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.corpora.uni-hamburg.de/gscl2011/downloads/AZM96.pdf",
    "abstract": "Das Paper beschreibt die Entwicklung und den Einsatz von Werkzeugen und Verfahren für die kollaborative Korrektur bei der Erstellung eines rätoromanischen Textkorpus mittels digitaler Tiefenerschließung. Textgrundlage bildet die »Rätoromanische Chrestomathie« von Caspar Decurtins, die 1891-1919 in der Zeitschrift »Romanische Forschungen« erschienen ist. Bei dem hier vorgestellten Ansatz werden manuelle und automatische Korrektur unter Einbeziehung von Angehörigen und Interessierten der rätoromanischen Sprachgemeinschaft über eine kollaborative Arbeitsumgebung kombiniert. In dem von uns entwickelten netzbasierten Editor werden die automatisch gelesenen Texte den Digitalisaten gegenübergestellt. Korrekturen, Kommentare und Verweise können nach Wiki-Prinzipien vorgeschlagen und eingebracht werden. Erstmalig wird so die Sprachgemeinschaft einer Kleinsprache aktiv in den Prozess der Dokumentation und Bewahrung ihres eigenen sprachlichen und kulturellen Erbes eingebunden. In diesem Paper wird die konkrete Umsetzung der kollaborativen Arbeitsumgebung beschrieben, von der architektonischen Grundlage und aktuellen technologischen Umsetzung bis hin zu Weiterentwicklungen und Potentialen. Die Entwicklung erfolgt von Beginn an quelloffen unter http://github.com/spinfo/drc.",
    "author": [
      {
        "family": "Neuefeind",
        "given": "Claes"
      },
      {
        "family": "Rolshoven",
        "given": "Jürgen"
      },
      {
        "family": "Steeg",
        "given": "Fabian"
      }
    ],
    "container-title": "Multilingual Resources and Multilingual Applications: Proceedings of the Conference of the German Society for Computational Linguistics and Language Technology (GSCL 2011)",
    "id": "Neuefeind2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr, rhaeto-romance, spelling_correction",
    "language": "de-DE",
    "page": "163-168",
    "publisher": "GSCL; Universität Hamburg",
    "title": "Die Digitale Rätoromanische Chrestomathie – Werkzeuge und Verfahren für die Korpuserstellung durch kollaborative Volltexterschließung",
    "type": "paper-conference"
  },
  {
    "collection-title": "Jeremias Gotthelf. Historisch-kritische Gesamtausgabe",
    "editor": [
      {
        "dropping-particle": "von",
        "family": "Zimmermann",
        "given": "Christian"
      },
      {
        "family": "Richter",
        "given": "Thomas"
      },
      {
        "family": "Keller",
        "given": "Irene"
      }
    ],
    "id": "NeuerBernerKalender",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Olms",
    "publisher-place": "Hildesheim",
    "title": "Jeremias Gotthelf: Neuer Berner-Kalender",
    "title-short": "Jeremias Gotthelf",
    "type": "book",
    "volume": "D.1"
  },
  {
    "DOI": "10.1145/641007.641067",
    "author": [
      {
        "family": "Neven",
        "given": "Filip"
      },
      {
        "family": "Duval",
        "given": "Erik"
      }
    ],
    "container-title": "MULTIMEDIA ’02: Proceedings of the tenth ACM international conference on multimedia",
    "id": "Neven2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "page": "291-294",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Reusable learning objects: A survey of LOM-based repositories",
    "title-short": "Reusable learning objects",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1816123.1816156",
    "ISBN": "978-1-4503-0085-8",
    "URL": "http://dx.doi.org/10.1145/1816123.1816156",
    "abstract": "Topic models could have a huge impact on improving the ways users find and discover content in digital libraries and search interfaces through their ability to automatically learn and apply subject tags to each and every item in a collection, and their ability to dynamically create virtual collections on the fly. However, much remains to be done to tap this potential, and empirically evaluate the true value of a given topic model to humans. In this work, we sketch out some sub-tasks that we suggest pave the way towards this goal, and present methods for assessing the coherence and interpretability of topics learned by topic models. Our large-scale user study includes over 70 human subjects evaluating and scoring almost 500 topics learned from collections from a wide range of genres and domains. We show how scoring model – based on pointwise mutual information of word-pair using Wikipedia, Google and MEDLINE as external data sources - performs well at predicting human scores. This automated scoring of topics is an important first step to integrating topic modeling into digital libraries",
    "author": [
      {
        "family": "Newman",
        "given": "David"
      },
      {
        "family": "Noh",
        "given": "Youn"
      },
      {
        "family": "Talley",
        "given": "Edmund"
      },
      {
        "family": "Karimi",
        "given": "Sarvnaz"
      },
      {
        "family": "Baldwin",
        "given": "Timothy"
      }
    ],
    "container-title": "Proceedings of the 10th annual joint conference on digital libraries (JCDL ’10)",
    "id": "Newman2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "topic_modeling, wikipedia",
    "page": "215-224",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Evaluating topic models for digital libraries",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Newton",
        "given": "Robert"
      }
    ],
    "container-title": "New Library World",
    "id": "Newton2003",
    "issue": "10",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "page": "412-425",
    "title": "Staff attitudes to the development and delivery of e-learning",
    "type": "article-journal",
    "volume": "104"
  },
  {
    "DOI": "10.1109/64.53180",
    "ISSN": "0885-9000",
    "abstract": "Basic expert system terminology is reviewed, and several uncertainty management paradigms are surveyed. The focus is on subjective probability theory, Dempster-Shafer theory, and possibility theory, although a number of other methods are mentioned. The benefits and limitations of the various schemes are examined, examples of expert systems within each school are presented, and some relevant open problems are discussed.",
    "author": [
      {
        "family": "Ng",
        "given": "Keung-Chi"
      },
      {
        "family": "Abramson",
        "given": "Bruce"
      }
    ],
    "container-title": "IEEE Expert",
    "id": "Ng1990",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "ai, uncertainty",
    "language": "en-US",
    "page": "29-48",
    "publisher": "IEEE",
    "publisher-place": "Piscataway, NJ, USA",
    "title": "Uncertainty management in expert systems",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.1145/1935826.1935887",
    "ISBN": "978-1-4503-0493-1",
    "URL": "http://dx.doi.org/10.1145/1935826.1935887",
    "abstract": "This paper investigates how to effectively do cross lingual text classification by leveraging a large scale and multilingual knowledge base, Wikipedia. Based on the observation that each Wikipedia concept is described by documents of different languages, we adapt existing topic modeling algorithms for mining multilingual topics from this knowledge base. The extracted topics have multiple types of representations, with each type corresponding to one language. In this work, we regard such topics extracted from Wikipedia documents as universal-topics, since each topic corresponds with same semantic information of different languages. Thus new documents of different languages can be represented in a space using a group of universal-topics. We use these universal-topics to do cross lingual text classification. Given the training data labeled for one language, we can train a text classifier to classify the documents of another language by mapping all documents of both languages into the universal-topic space. This approach does not require any additional linguistic resources, like bilingual dictionaries, machine translation tools, or labeling data for the target language. The evaluation results indicate that our topic modeling approach is effective for building cross lingual text classifier.",
    "author": [
      {
        "family": "Ni",
        "given": "Xiaochuan"
      },
      {
        "family": "Sun",
        "given": "Jian T."
      },
      {
        "family": "Hu",
        "given": "Jian"
      },
      {
        "family": "Chen",
        "given": "Zheng"
      }
    ],
    "collection-title": "WSDM ’11",
    "container-title": "Proceedings of the fourth ACM international conference on web search and data mining",
    "id": "Ni2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "topic_modeling, wikipedia",
    "page": "375-384",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Cross lingual text classification by mining multilingual topics from Wikipedia",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.elearningpost.com/articles/archives/lcms_lms_cms_rlos/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Nichani",
        "given": "Maish R."
      }
    ],
    "id": "Nichani2001",
    "issued": {
      "date-parts": [
        [
          2001,
          5
        ]
      ]
    },
    "publisher": "Online article",
    "title": "LCMS = LMS + CMS [RLOs]",
    "type": ""
  },
  {
    "abstract": "SGML and ODA are international standards for the markup and interchange of electronic documents. These standards are incompatible, in the sense that in general a document encoded using SGML cannot be used directly in an ODA-based system, and vice versa. We first describe these two standards, and suggest criteria under which a bridge between the two standards could be evaluated. We then evaluate the Office Document Language (ODL), an SGML application specifically designed for ODA documents, with respect to these criteria. We describe conditions under which reliable automatic translation between SGML and ODA can be achieved, and describe a translation program that converts SGML documents to ODA and back.",
    "author": [
      {
        "family": "Nicholas",
        "given": "Charles K."
      },
      {
        "family": "Welsch",
        "given": "Lawrence A."
      }
    ],
    "container-title": "Electronic Publishing",
    "id": "Nicholas1992",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "document_engineering, sgml",
    "page": "105-130",
    "title": "On the interchangeability of SGML and ODA",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "abstract": "Multimediales Lernen hat sich in der modernen Lehr- und Lernlandschaft durchgesetzt. Neue Technologien schaffen neue Einsatzmöglichkeiten. Wie aber lässt sich die Qualität neuer multimedialer Lehr- und Lernangebote sichern, welche Standards gibt es? Welche psychologisch-didaktischen Ansätze sind Grundlagen effektiven multimedialen Lernens? Welche lehr- und lernpsychologischen Theorien und Befunde sind hilfreich bei der Konzeption? Welche technologischen Fragestellungen sind zu berücksichtigen? Das Kompendium multimediales Lernen ist als Lehr- und Handbuch konzipiert. In 33 Kapiteln werden alle wesentlichen Aspekte multimedialen Lehrens und Lernens dargestellt: Von der Planung und Konzeption auf der Grundlage didaktischer Entwurfsmuster (design patterns) über Interaktivitätsformen, CSCL und Evaluation bis hin zu Softwaresystemen, Werkzeugen wie Authoring Tools und E-Learning-Standards. Das Buch wendet sich an Praktiker in der Aus- und Weiterbildung und E-Learning-Entwickler. Außerdem ist es für den Einsatz in der Lehre geeignet, insbesondere in Lehrveranstaltungen der Erziehungswissenschaften, Informatik und Psychologie.",
    "author": [
      {
        "family": "Niegemann",
        "given": "Helmut M."
      },
      {
        "family": "Domagk",
        "given": "Steffi"
      },
      {
        "family": "Hessel",
        "given": "Silvia"
      },
      {
        "family": "Hein",
        "given": "Alexandra"
      },
      {
        "family": "Hupfer",
        "given": "Matthias"
      },
      {
        "family": "Zobel",
        "given": "Annett"
      }
    ],
    "collection-title": "X.media.press",
    "id": "Niegemann2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "e-learning",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Kompendium multimediales lernen",
    "type": "book"
  },
  {
    "DOI": "10.1145/1121341.1121359",
    "author": [
      {
        "family": "Niezgoda",
        "given": "Sebastian"
      },
      {
        "family": "Way",
        "given": "Thomas P."
      }
    ],
    "container-title": "SIGCSE ’06: Proceedings of the 37th SIGCSE technical symposium on computer science education",
    "id": "Niezgoda2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "page": "51-55",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "SNITCH: A software tool for detecting cut and paste plagiarism",
    "title-short": "SNITCH",
    "type": "paper-conference"
  },
  {
    "abstract": "The trend to digitize (historic) paper-based archives has emerged in the last years. The advantages of digital archives are easy access, searchability and machine readability. These advantages can only be ensured if few or no OCR errors are present. These errors are the result of misrecognized characters during the OCR process. Large archives make it unreasonable to correct errors manually. Therefore, an unsupervised, fully-automatic approach for correcting OCR errors is proposed. The approach combines several methods for retrieving the best correction proposal for a misspelled word: A general spelling correction (Anagram Hash), a new OCR adapted method based on the shape of characters (OCR-Key) and context information (bigrams). A manual evaluation of the approach has been performed on The Times Archive of London, a collection of English newspaper articles spanning from 1785 to 1985. Error reduction rates up to 75% and F-Scores up to 88% could be achieved.",
    "author": [
      {
        "family": "Niklas",
        "given": "Kai"
      }
    ],
    "genre": "Master’s thesis",
    "id": "Niklas2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "ocr, spelling_correction",
    "publisher": "Leibniz-Universität Hannover",
    "publisher-place": "Hannover, Germany",
    "title": "Unsupervised post-correction of OCR errors",
    "type": "thesis"
  },
  {
    "DOI": "10.1515/nor-2017-0213",
    "ISSN": "2001-5119",
    "abstract": "The article discusses formations of fan cultures in terms of intermediality. It examines the construction and consumption of media in three different Finnish fan groups: the fans of Xena: The Warrior Princess (XWP), fans of Ally McBeal and the fans of Finnish TV-host Marco Bjurström. The construction of intermedial relations of each fan group interestingly reveals the institutional and technological spaces of shaping the pleasures of media. The intermedial relations of researched fan groups vary according to a traditional use of media to a use of new media. The intermedial relations in XWP fan culture were produced mainly between television and the Internet. This intermedial connection provided space for fans’ self-definition, community and fan production. Intermedial relations in the case of Ally McBeal were encouraged by the media companies between television and tabloid papers as well as between television and the Internet depicting the show as topical and trendy. However audiences were mostly engaged with only television and Ally McBeal didn’t nurture multiple uses of media. Again in the case of Bjurström, the relation between television and tabloids and the particular portrait type of media coverage constructs this fandom as a traditional star-fan relationship.",
    "author": [
      {
        "family": "Nikunen",
        "given": "Kaarina"
      }
    ],
    "container-title": "Nordicom Review",
    "id": "Nikunen2007",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "intertextuality",
    "title": "The intermedial practises of fandom",
    "type": "article-journal",
    "volume": "28"
  },
  {
    "DOI": "10.1093/llc/fqx065",
    "author": [
      {
        "family": "Nini",
        "given": "Andrea"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Nini2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "page": "fqx065",
    "title": "An authorship analysis of the Jack the Ripper letters",
    "type": "article-journal"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=1613156",
    "abstract": "Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework. However, we also show that it is possible to minimize the number of structures that require non-incremental processing by choosing an optimal parsing algorithm. This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text. When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9%.",
    "author": [
      {
        "family": "Nivre",
        "given": "Joakim"
      }
    ],
    "collection-title": "IncrementParsing ’04",
    "container-title": "Proceedings of the workshop on incremental parsing: Bringing engineering and cognition together (IncrementParsing ’04)",
    "id": "Nivre2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "interactive_parsing",
    "page": "50-57",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Incrementality in deterministic dependency parsing",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1162/coli.07-056-r1-07-027",
    "ISSN": "0891-2017",
    "URL": "http://dx.doi.org/10.1162/coli.07-056-r1-07-027",
    "abstract": "Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.",
    "author": [
      {
        "family": "Nivre",
        "given": "Joakim"
      }
    ],
    "container-title": "Computational Linguistics",
    "id": "Nivre2008",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "interactive_parsing",
    "page": "513-553",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Algorithms for deterministic incremental dependency parsing",
    "type": "article-journal",
    "volume": "34"
  },
  {
    "DOI": "10.1007/11427865_4",
    "abstract": "A large amount of data including HTML documents and Internet mails has been distributed over the Internet. Most of the data on the Internet include geo-referenced descriptions. We have studied a method of converting such descriptions like addresses into their corresponding coordinates, that is, tuples of longitude and latitude. The process of converting descriptions into coordinates is called geocoding. In this paper, we focus on natural route descriptions as a new type of target to geocode. We first explain a core schema of sidewalk network databases on the basis of a characteristic of natural route descriptions, and then propose Formal Route Statement (FRS) to represent and process natural route descriptions by means of a computer. Also, we present our prototype system to geocode natural route descriptions using sidewalk network databases based on our proposed framework.",
    "author": [
      {
        "family": "Noaki",
        "given": "Kouzou"
      },
      {
        "family": "Arikawa",
        "given": "Masatoshi"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Web and wireless geographical information systems. 4th international workshop, W2GIS 2004, goyang, korea, november 26–27, 2004, revised selected papers",
    "editor": [
      {
        "family": "Kwon",
        "given": "Yong-Jin"
      },
      {
        "family": "Bouju",
        "given": "Alain"
      },
      {
        "family": "Claramunt",
        "given": "Christophe"
      }
    ],
    "id": "Noaki2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "geocoding",
    "page": "38-50",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "A geocoding method for natural route descriptions using sidewalk network databases",
    "type": "chapter",
    "volume": "3428"
  },
  {
    "DOI": "10.1016/0920-5489(93)90024-l",
    "ISSN": "09205489",
    "abstract": "The standards ISO 8879 and its related material describes both a text markup scheme and an implementation of a text parser based on that markup scheme. By trying to clarify the relationship between the documents and an implementation, we show that optional SGML features properly belong to separate applications. The result suggests more general and powerful mechanisms which could be obtained.",
    "author": [
      {
        "family": "Nordin",
        "given": "Brent"
      },
      {
        "family": "Barnard",
        "given": "David T."
      },
      {
        "family": "Macleod",
        "given": "Ian A."
      }
    ],
    "container-title": "Computer Standards & Interfaces",
    "id": "Nordin1993",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "markup, sgml, standard",
    "page": "5-19",
    "title": "A review of the Standard Generalized Markup Language (SGML)",
    "type": "article-journal",
    "volume": "15"
  },
  {
    "author": [
      {
        "family": "Norman",
        "given": "Donald A."
      }
    ],
    "id": "Norman1988",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Basic Books",
    "publisher-place": "New York, NY",
    "title": "The psychology of everyday things",
    "type": "book"
  },
  {
    "ISBN": "978-0-465-05065-9",
    "author": [
      {
        "family": "Norman",
        "given": "Donald"
      }
    ],
    "edition": "Revised and expanded edition",
    "id": "Norman2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Basic Books",
    "publisher-place": "New York, NY",
    "title": "The design of everyday things",
    "type": "book"
  },
  {
    "abstract": "This general introduction to the study of Chinese traces the language’s history from its beginnings in the second millennium B.C. to the present, and provides a clear picture of the contemporary language and its sociolinguistic status. Chinese, in its numerous dialects, has more speakers than any other language in the modern world, and this vast extension in time and space brings to its study an exceptional complexity. Nevertheless, Norman’s crisp organization and lucid elegance make this extraordinary range of material easily accessible even to those with an elementary understanding of linguistics. Chinese includes information on the genetic and typological connections of the language, the writing system, the classical and early vernacular tongues, the modern language and non-standard dialects, and the history of linguistic reform in China.",
    "author": [
      {
        "family": "Norman",
        "given": "Jerry"
      }
    ],
    "collection-title": "Cambridge language surveys",
    "id": "NormanJ1988",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "chinese",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge, UK",
    "title": "Chinese",
    "type": "book"
  },
  {
    "DOI": "10.1007/11762256_8",
    "abstract": "The significance of uncertainty representation has become obvious in the Semantic Web community recently. This paper presents our research on uncertainty handling in automatically created ontologies. A new framework for uncertain information processing is proposed. The research is related to OLE (Ontology LEarning) — a project aimed at bottom–up generation and merging of domain–specific ontologies. Formal systems that underlie the uncertainty representation are briefly introduced. We discuss the universal internal format of uncertain conceptual structures in OLE then and offer a utilisation example then. The proposed format serves as a basis for empirical improvement of initial knowledge acquisition methods as well as for general explicit inference tasks.",
    "author": [
      {
        "family": "Nováček",
        "given": "Vít"
      },
      {
        "family": "Smrž",
        "given": "Pavel"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "The Semantic Web: Research and applications (proceedings of ESWC 2006)",
    "editor": [
      {
        "family": "Sure",
        "given": "York"
      },
      {
        "family": "Domingue",
        "given": "John"
      }
    ],
    "id": "Novacek2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "ontologies, semantic_web, uncertainty",
    "language": "en-US",
    "page": "65-79",
    "publisher": "Springer",
    "publisher-place": "Berlin, Heidelberg",
    "title": "Empirical merging of ontologies—a proposal of universal uncertainty representation framework",
    "type": "paper-conference",
    "volume": "4011"
  },
  {
    "URL": "http://journalofdigitalhumanities.org/3-2/on-the-origin-of-hack-and-yack-by-bethany-nowviskie/",
    "author": [
      {
        "family": "Nowviskie",
        "given": "Bethany"
      }
    ],
    "container-title": "Journal of Digital Humanities",
    "id": "Nowviskie2014",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "title": "On the origin of “hack” and “yack”",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "URL": "http://chronicle.com/article/Googles-Book-Search-A/48245/",
    "accessed": {
      "date-parts": [
        [
          2012,
          6,
          4
        ]
      ]
    },
    "author": [
      {
        "family": "Nunberg",
        "given": "Geoffrey"
      }
    ],
    "container-title": "The Chronicle of Higher Education",
    "id": "Nunberg2009",
    "issued": {
      "date-parts": [
        [
          2009,
          8,
          1
        ]
      ]
    },
    "title": "Google’s book search: A disaster for scholars",
    "title-short": "Google’s book search",
    "type": "article-journal"
  },
  {
    "DOI": "10.1093/llc/fqt044",
    "ISSN": "2055-768X",
    "abstract": "This article demonstrates that the history of computing in the humanities is an almost uncharted research topic. It argues that this oversight must be remedied as a matter of urgency so that the evolutionary model of progress that currently dominates the field can be countered. We describe the ’Hidden Histories’ pilot project and explore the origins and practice of oral history; in the corresponding issue of Digital Humanities Quarterly, five oral history interviews that we carried out during the project are presented. We conclude that the selection of interviews presented here demonstrate that oral history is an important and productive methodology in such research. The five oral history interviews form primary sources, which can be used in the writing of a history of computing in the humanities; furthermore, they contain new information and interpretations, which cannot be gleaned from published scholarly articles, for example, information about the varied entry routes into the field that have existed and the interrelationship between myth and history in the narratives we create about the emergence of digital humanities.",
    "author": [
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Flinn",
        "given": "Andrew"
      },
      {
        "family": "Welsh",
        "given": "Anne"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Nyhan2013",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "en-US",
    "page": "71-85",
    "title": "Oral history and the Hidden Histories project: Towards histories of computing in the humanities",
    "title-short": "Oral history and the Hidden Histories project",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "DOI": "10.1007/978-3-319-20170-2",
    "author": [
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Flinn",
        "given": "Andrew"
      }
    ],
    "collection-title": "Springer series on cultural computing",
    "id": "Nyhan2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Computation and the humanities: Towards an oral history of digital humanities",
    "title-short": "Computation and the humanities",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-642-18472-7\\_8",
    "ISBN": "978-3-642-18471-0",
    "URL": "http://dx.doi.org/10.1007/978-3-642-18472-7\\_8",
    "abstract": "Ontologies are becoming a core technology for supporting the sharing, integration, and management of information sources in Semantic Web applications. As critical as ontologies have become, ontology languages such as OWL typically provide minimal support for modeling the complex temporal information often contained in these sources. As a result, ontologies often cannot fully express the temporal knowledge needed by many applications, forcing users and developers to develop ad hoc solutions. In this paper, we present a methodology and a set of tools for representing and querying temporal information in OWL ontologies. The approach uses a lightweight temporal model to encode the temporal dimension of data. It also uses the OWL-based Semantic Web Rule Language (SWRL) and the SWRL-based OWL query language SQWRL to reason with and query the temporal information represented using our model.",
    "author": [
      {
        "family": "O’Connor",
        "given": "Martin J."
      },
      {
        "family": "Das",
        "given": "Amar K."
      }
    ],
    "chapter-number": "8",
    "collection-title": "Communications in computer and information science",
    "container-title": "Biomedical engineering systems and technologies",
    "editor": [
      {
        "family": "Fred",
        "given": "Ana"
      },
      {
        "family": "Filipe",
        "given": "Joaquim"
      },
      {
        "family": "Gamboa",
        "given": "Hugo"
      }
    ],
    "id": "OConnor2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "rdf, temporal_data",
    "page": "97-110",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "A method for representing and querying temporal information in OWL",
    "type": "chapter",
    "volume": "127"
  },
  {
    "DOI": "10.1007/978-3-642-22218-4\\_32",
    "ISBN": "978-3-642-22217-7",
    "URL": "http://dx.doi.org/10.1007/978-3-642-22218-4\\_32",
    "abstract": "The Knowledge-Based Temporal Abstraction (KBTA) method is a well-established mechanism for representing and reasoning with temporal information. Implementations to date have been somewhat heavyweight, however, and custom tools are typically required to build abstraction knowledge and query the resulting abstractions. To address this shortcoming, we created a lightweight method that allows users to rapidly specify KBTA-based temporal knowledge and to immediately construct complex temporal queries with it. The approach is built on the Web Ontology Language (OWL), and its associated rule and query languages, SWRL and SQWRL. The method is reusable and can serve as the basis of a KBTA implementation in any OWL-based system.",
    "author": [
      {
        "family": "O’Connor",
        "given": "Martin J."
      },
      {
        "family": "Hernandez",
        "given": "Genaro"
      },
      {
        "family": "Das",
        "given": "Amar"
      }
    ],
    "chapter-number": "32",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Artificial intelligence in medicine",
    "editor": [
      {
        "family": "Peleg",
        "given": "Mor"
      },
      {
        "family": "Lavrač",
        "given": "Nada"
      },
      {
        "family": "Combi",
        "given": "Carlo"
      }
    ],
    "id": "OConnor2011a",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "rdf, temporal_data",
    "page": "255-259",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "A Rule-Based method for specifying and querying temporal abstractions",
    "type": "chapter",
    "volume": "6747"
  },
  {
    "URL": "http://informationr.net/ir/2-4/paper22.html",
    "abstract": "There are increasing numbers of historical texts available in machine-readable form. These texts retain the original spelling, which can be very different from the modern-day equivalents due to the natural evolution of a language, and to the fact that the concept of standardisation in spelling is a comparatively modern one. Among medieval vernacular writers, the same word could be spelled in different ways and the same author (or scribe) might even use several alternative spellings in the same passage. For example, the French text used in our experiments (described below) variously gives the name of the chief villain as Hoiaus, Hoiax, Hoiel, and Oiaus. Thus, we do not know, a priori, how many variant forms of a particular word there are in such texts, let alone what these variants might be. Searching on the modern equivalent, or even the commonest historical variant, of a particular word may thus fail to retrieve an appreciable number of occurrences unless the searcher already has an extensive knowledge of the language of the documents. Moreover, even specialist scholars may be unaware of some idiosyncratic variants. Here, we consider the use of computer methods to retrieve variant historical spellings. Staff in the Department of French have been editing twelfth-century French narrative poems and analysing the word-variants in them. Traditionally, this has involved teams of researchers recording and classifying each word occurrence, but this has now become impractical for texts of non-trivial size. Previous work in the Department of Information Studies has investigated the searching of historical English text using algorithms developed for detecting spelling errors (Robertson & Willett, 1993). In this paper we summarise the results of applying these algorithms to medieval French literature; full details of the experiments are provided by O’Rourke (1995).",
    "author": [
      {
        "family": "O’Rourke",
        "given": "Alan J."
      },
      {
        "family": "Robertson",
        "given": "Alexander M."
      },
      {
        "family": "Willett",
        "given": "Peter"
      },
      {
        "family": "Eley",
        "given": "Penny"
      },
      {
        "family": "Simons",
        "given": "Penny"
      }
    ],
    "container-title": "Information Research",
    "id": "ORourke1996",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "cultural_heritage, spelling_correction",
    "title": "Word variant identification in Old French",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "URL": "http://educause.edu/educatingthenetgen",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "editor": [
      {
        "family": "Oblinger",
        "given": "Diana G."
      },
      {
        "family": "Oblinger",
        "given": "James L."
      }
    ],
    "id": "Oblinger2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "publisher": "Educause",
    "title": "Educating the net generation",
    "type": "book"
  },
  {
    "DOI": "10.1162/089120103321337421",
    "ISSN": "0891-2017",
    "abstract": "We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.",
    "author": [
      {
        "family": "Och",
        "given": "Franz J."
      },
      {
        "family": "Ney",
        "given": "Hermann"
      }
    ],
    "container-title": "Computational Linguistics",
    "id": "Och2003",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "machine_translation",
    "page": "19-51",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "A systematic comparison of various statistical alignment models",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "DOI": "10.1007/978-3-662-55665-8\\_52",
    "URL": "http://dx.doi.org/10.1007/978-3-662-55665-8\\_52",
    "abstract": "The importance of logics with approximate conditional probabilities is reflected by the fact that they can model non-monotonic reasoning. We introduce a new logic of this kind, \\mathsf\n                  {CPJ} , which extends justification logic and supports non-monotonic reasoning with and about evidences.",
    "author": [
      {
        "family": "Ognjanović",
        "given": "Zoran"
      },
      {
        "family": "Savić",
        "given": "Nenad"
      },
      {
        "family": "Studer",
        "given": "Thomas"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Logic, rationality, and interaction. LORI 2017",
    "editor": [
      {
        "family": "Baltag",
        "given": "Alexandru"
      },
      {
        "family": "Seligman",
        "given": "Jeremy"
      },
      {
        "family": "Yamada",
        "given": "Tomoyuki"
      }
    ],
    "id": "Ognjanovic2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "uncertainty",
    "page": "681-686",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Justification logic with approximate conditional probabilities",
    "type": "paper-conference",
    "volume": "10455"
  },
  {
    "DOI": "10.1007/11853107\\_20",
    "ISBN": "978-3-540-39586-7",
    "URL": "http://dx.doi.org/10.1007/11853107\\_20",
    "abstract": "The FuTI–library is a collection of classes and methods for representing and manipulating fuzzy time intervals. Fuzzy time intervals are represented as polygons over integer coordinates. FuTI is an open source C++ library with many advanced operations and highly optimised algorithms. Version 1.0 is now available from the URL http://www.pms.ifi.lmu.de/CTTN/FuTI.",
    "author": [
      {
        "family": "Ohlbach",
        "given": "Hans J."
      }
    ],
    "chapter-number": "20",
    "collection-title": "Lecture notes in computer science",
    "editor": [
      {
        "family": "Alferes",
        "given": "Jóse J."
      },
      {
        "family": "Bailey",
        "given": "James"
      },
      {
        "family": "May",
        "given": "Wolfgang"
      },
      {
        "family": "Schwertel",
        "given": "Uta"
      }
    ],
    "id": "Ohlbach2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "temporal_data",
    "page": "257-261",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Fuzzy time intervals system description of the FuTI–library",
    "type": "chapter",
    "volume": "4187"
  },
  {
    "DOI": "10.1007/978-3-540-69912-5\\_17",
    "ISBN": "978-3-540-69911-8",
    "URL": "http://dx.doi.org/10.1007/978-3-540-69912-5\\_17",
    "abstract": "This paper contains a brief overview of the ’Geo-Temporal’ specification language GeTS. The objects which can be described and manipulated with this language are time points, crisp and fuzzy time intervals and labeled partitionings of the time axis. The partitionings are used to represent periodic temporal notions like months, semesters etc. and also whole calendar systems. GeTS is essentially a typed functional language with a few imperative constructs and many built-ins. GeTS can be used to specify and compute with many different kinds of temporal notions, from simple arithmetic operations on time points up to complex fuzzy relations between fuzzy time intervals. A parser, a compiler and an abstract machine for GeTS is implemented.",
    "author": [
      {
        "family": "Ohlbach",
        "given": "Hans J."
      }
    ],
    "chapter-number": "17",
    "collection-title": "Lecture notes in computer science",
    "container-title": "KI 2006: Advances in artificial intelligence",
    "editor": [
      {
        "family": "Freksa",
        "given": "Christian"
      },
      {
        "family": "Kohlhase",
        "given": "Michael"
      },
      {
        "family": "Schill",
        "given": "Kerstin"
      }
    ],
    "id": "Ohlbach2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "temporal_data",
    "page": "214-228",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "GeTS – a specification language for geo-temporal notions",
    "type": "chapter",
    "volume": "4314"
  },
  {
    "URL": "https://www.andrew.cmu.edu/course/60-427/aisd/Museum.pdf",
    "abstract": "With a case study of the digital reconstruction of a village 5,000 years ago, the authors revealed the discovery process behind the scene, such as inverse physics process with decision-making and analogies. It is found that digital reconstruction enhances the understanding of archeological details, spatial relationship, and cross-referencing with field data.",
    "author": [
      {
        "family": "Olsen",
        "given": "Sandra"
      },
      {
        "family": "Brickman",
        "given": "Ashley"
      },
      {
        "family": "Cai",
        "given": "Yang"
      }
    ],
    "container-title": "CHI 2004 workshop on ambient intelligence for scientific discovery",
    "id": "Olsen2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "archeology, digital_humanities, simulation",
    "title": "Discovery by reconstruction: Exploring digital archeology",
    "title-short": "Discovery by reconstruction",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1068/a260499",
    "ISSN": "0308-518X",
    "URL": "http://dx.doi.org/10.1068/a260499",
    "abstract": "Seemingly over the last decade a major computing revolution has started in many of the ’hard’ sciences. Slowly at first, but now at an ever increasing pace, the principal methods used at the research frontiers of science have become increasingly computation dependent. New ways of doing science have been developed in physics, chemistry, biology, engineering, geology, and environmental modelling; indeed major grand challenge areas have been recognised in many sciences which now possess the relatively new adjective ’computational’ (Catlow, 1992). It is important to realise that these changes reach far beyond the hard sciences and have the most profound implications for geography and most social sciences too. The purpose of this commentary is to investigate some of the scientific and technological opportunities in geography that are to be created by the new computational era likely soon to exist. Maybe we are witnessing the beginnings of a new technological and intellectual revolution in science with consequences far beyond what we can currently imagine. Certainly these developments constitute an argument for the development of a computational human geography (CHG). Indeed, there is now a large gap between what most geographers see computers as offering and what is now, or will soon be, computationally feasible. The computing world has started to change in big ways and these changes provide the basis for exciting new computationally dependent ways of doing better geography. So what are these developments?",
    "author": [
      {
        "family": "Openshaw",
        "given": "Stan"
      }
    ],
    "container-title": "Environment and Planning A",
    "id": "Openshaw1994",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1994,
          4
        ]
      ]
    },
    "keyword": "computational_thinking",
    "page": "499-505",
    "title": "Computational human geography: Towards a research agenda",
    "title-short": "Computational human geography",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "author": [
      {
        "family": "Oplatka-Steinlin",
        "given": "Helen"
      }
    ],
    "collection-title": "Rechtshistorische Arbeiten",
    "id": "Oplatka1971",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Juris",
    "publisher-place": "Zurich, Switzerland",
    "title": "Untersuchungen zur neuhochdeutschen Gesetzessprache: Befehlsintensität und Satzstruktur im Schweizerischen Zivilgesetzbuch und im Deutschen Bürgerlichen Gesetzbuch",
    "type": "book",
    "volume": "7"
  },
  {
    "author": [
      {
        "family": "Orlandi",
        "given": "Tito"
      }
    ],
    "id": "Orlandi1990",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "digital_humanities, classic",
    "language": "it-IT",
    "publisher": "La Nuova Italia Scientifica",
    "publisher-place": "Rome",
    "title": "Informatica umanistica",
    "type": "book"
  },
  {
    "URL": "http://www.hd.uib.no/AcoHum/book/fm-chapter-final.html",
    "abstract": "This chapter presents an investigation of some methodological questions related to teaching and learning at humanities faculties in Europe, in particular those arising from the use of digital technologies. We make no attempt to be encyclopaedic in scope, but instead attempt to deal with some important general questions which cut across discipline boundaries. Other, more discipline-specific, issues are addressed elsewhere in this volume. In this chapter we address such questions as: 1. What is the extent to which the nature of humanities scholarship changes due to the incorporation of formal and computational methods? How does the adoption of such methods (e.g. the digitization of language and art, the application of artificial intelligence, and the use of computer simulation methods) have an impact on the humanities, from a philosophy of science viewpoint, and what are the consequences of this development for education? 2. What are the theoretical and practical problems involved in the integration of advanced computing in humanities curricula, across the various disciplines involved? Can a common approach be identified offering formal and computational methods for all humanities students? In which respects should humanities computing be taught differently from any other sort of computing?",
    "author": [
      {
        "family": "Orlandi",
        "given": "Tito"
      },
      {
        "family": "Bell",
        "given": "Joseph N."
      },
      {
        "family": "Burnard",
        "given": "Lou"
      },
      {
        "family": "Buzzetti",
        "given": "Dino"
      },
      {
        "dropping-particle": "de",
        "family": "Smedt",
        "given": "Koenraad"
      },
      {
        "family": "Kropac",
        "given": "Ingo"
      },
      {
        "family": "Souillot",
        "given": "Jacques"
      },
      {
        "family": "Thaller",
        "given": "Manfred"
      }
    ],
    "chapter-number": "2",
    "container-title": "Computing in humanities education: A european perspective",
    "editor": [
      {
        "dropping-particle": "de",
        "family": "Smedt",
        "given": "Koenraad"
      },
      {
        "family": "Gardiner",
        "given": "Hazel"
      },
      {
        "family": "Ore",
        "given": "Espen"
      },
      {
        "family": "Orlandi",
        "given": "Tito"
      },
      {
        "family": "Short",
        "given": "Harold"
      },
      {
        "family": "Souillot",
        "given": "Jacques"
      },
      {
        "family": "Vaughan",
        "given": "William"
      }
    ],
    "id": "Orlandi1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "13-62",
    "publisher": "University of Bergen",
    "publisher-place": "Bergen",
    "title": "European studies on formal methods in the humanities",
    "type": "chapter"
  },
  {
    "DOI": "10.2307/258154",
    "abstract": "Diverse applications of the concept of loose coupling are embodied in five recurring voices that focus separately on causation, typology, effects, compensations, and outcomes. Each has a tendency to drift away from a dialectical interpretation of loose coupling toward a unidimensional interpretation of loose coupling, thereby weakening the explanatory value of the concept. The authors first use the five voices to review the loose coupling literature and then to suggest more precise and more productive uses of the concept.",
    "author": [
      {
        "family": "Orton",
        "given": "Douglas J."
      },
      {
        "family": "Weick",
        "given": "Karl E."
      }
    ],
    "container-title": "The Academy of Management Review",
    "id": "Orton1990",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "loose_coupling",
    "page": "203-223",
    "publisher": "Academy of Management",
    "title": "Loosely coupled systems: A reconceptualization",
    "title-short": "Loosely coupled systems",
    "type": "article-journal",
    "volume": "15"
  },
  {
    "URL": "http://cm.bell-labs.com/cm/cs/cstr/54.ps.gz",
    "author": [
      {
        "family": "Osanna",
        "given": "Joseph F."
      },
      {
        "family": "Kernighan",
        "given": "Brian W."
      }
    ],
    "genre": "CSTR",
    "id": "Osanna1992",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "number": "54",
    "publisher": "Bell Laboratories",
    "publisher-place": "Murray Hill, NJ",
    "title": "Troff user’s manual",
    "type": "report"
  },
  {
    "DOI": "10.5539/cis.v8n3p119",
    "ISSN": "1913-8997",
    "URL": "http://dx.doi.org/10.5539/cis.v8n3p119",
    "author": [
      {
        "family": "Othman",
        "given": "Nermin A."
      },
      {
        "family": "Eldin",
        "given": "Ahmed S."
      },
      {
        "family": "El Zanfaly",
        "given": "Doaa S."
      }
    ],
    "container-title": "Computer and Information Science",
    "id": "Othman2015",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2015,
          7,
          31
        ]
      ]
    },
    "keyword": "database, uncertainty",
    "title": "Handling uncertainty in database: An introduction and brief survey",
    "title-short": "Handling uncertainty in database",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "ISSN": "00104817",
    "URL": "http://jstor.org/stable/30207231",
    "author": [
      {
        "family": "Ott",
        "given": "Wilhelm"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Ott1979",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1979
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_edition",
    "page": "29-35",
    "title": "A text processing system for the preparation of critical editions",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "DOI": "10.1093/llc/15.1.93",
    "ISSN": "1477-4615",
    "abstract": "Modularity, professionality, portability, and integration are the key features of the TUebingen System for TExt Processing programs (TUSTEP), a professional toolbox for those academic fields where texts are the object of research. Its potential is illustrated by two examples: (i) typesetting a TEI-lite encoded text, using the TEI tags as formatting instructions; (ii) preparing a critical edition, starting from automatic collation, then semi-automatically selecting the ’substantial’ variants from the collation results, transforming them into a critical apparatus, and publishing the edition both in print and electronically.",
    "author": [
      {
        "family": "Ott",
        "given": "Wilhelm"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Ott2000",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_edition, tei, typesetting",
    "language": "en-US",
    "page": "93-108",
    "publisher": "Oxford University Press",
    "title": "Strategies and tools for textual scholarship: The Tübingen System of Text Processing Programs (TUSTEP)",
    "title-short": "Strategies and tools for textual scholarship",
    "type": "article-journal",
    "volume": "15"
  },
  {
    "author": [
      {
        "family": "Otto",
        "given": "Mirko"
      }
    ],
    "genre": "Diplomarbeit",
    "id": "Otto2008",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "language": "de-DE",
    "note": "Advisor: Prof. Dr. Dietmar Rösner",
    "publisher": "Fakultät für Informatik, Otto-von-Guericke-Universität",
    "publisher-place": "Magdeburg",
    "title": "Ontologien zur semantischen Suche in einem Bestand von Dokumenten",
    "type": "thesis"
  },
  {
    "DOI": "10.1080/13658810701626236",
    "abstract": "This paper describes the generation of a model capturing information on how placenames co-occur together. The advantages of the co-occurrence model over traditional gazetteers are discussed and the problem of placename disambiguation is presented as a case study. We begin by outlining the problem of ambiguous placenames. We demonstrate how analysis of Wikipedia can be used in the generation of a co-occurrence model. The accuracy of our model is compared to a handcrafted ground truth; then we evaluate alternative methods of applying this model to the disambiguation of placenames in free text (using the GeoCLEF evaluation forum). We conclude by showing how the inclusion of placenames in both the text and geographic parts of a query provides the maximum mean average precision and outline the benefits of a co-occurrence model as a data source for the wider field of geographic information retrieval (GIR).",
    "author": [
      {
        "family": "Overell",
        "given": "Simon"
      },
      {
        "family": "Rüger",
        "given": "Stefan"
      }
    ],
    "container-title": "Int. J. Geogr. Inf. Sci.",
    "id": "Overell2008",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "geocoding, toponym_resolution",
    "page": "265-287",
    "publisher": "Taylor & Francis",
    "publisher-place": "Bristol, PA, USA",
    "title": "Using co-occurrence models for placename disambiguation",
    "type": "article-journal",
    "volume": "22"
  },
  {
    "URL": "http://aclweb.org/anthology/C10-1096",
    "abstract": "This paper presents a simple and efficient algorithm for approximate dictionary matching designed for similarity measures such as cosine, Dice, Jaccard, and overlap coefficients. We propose this algorithm, called CPMerge, for the τ-overlap join of inverted lists. First we show that this task is solvable exactly by a τ-overlap join. Given inverted lists retrieved for a query, the algorithm collects fewer candidate strings and prunes unlikely candidates to efficiently find strings that satisfy the constraint of the τ-overlap join. We conducted experiments of approximate dictionary matching on three large-scale datasets that include person names, biomedical names, and general English words. The algorithm exhibited scalable performance on the datasets. For example, it retrieved strings in 1.1 ms from the string collection of Google Web1T unigrams (with cosine similarity and threshold 0.7).",
    "author": [
      {
        "family": "Okazaki",
        "given": "Naoaki"
      },
      {
        "family": "Tsujii",
        "given": "Jun’ichi"
      }
    ],
    "container-title": "Proceedings of the 23rd international conference on computational linguistics (coling 2010)",
    "id": "Ozaki2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "approximate_matching",
    "page": "851-859",
    "publisher": "Coling 2010 Organizing Committee",
    "title": "Simple and efficient algorithm for approximate dictionary matching",
    "type": "paper-conference"
  },
  {
    "abstract": "Formative Tests können für Lehrende und Lernende gleichermaßen nützlich sein. Webbasierte Multiple-Choice-Tests können helfen, den Aufwand für formative Tests zu senken und somit einen breiteren und häufigeren Einsatz zu ermöglichen. Wir stellen ein Modul für das Content-Management-System Plone vor, das es erlaubt, MC-Tests genau wie andere Ressourcen einzusetzen und zu verwalten. Auf dieseWeise können vor allem in Präsenzveranstaltungen, für die üblicherweise keine Lernplattform verwendet wird, Tests eng mit den anderen online verfügbaren Lehr- und Lernmaterialien (z. B. Vorlesungsskripten oder Aufgabenblättern) verknüpft werden. Das Modul erlaubt auch den Import und Export von Aufgaben gemäß IMS QTI; in diesem Zusammenhang diskutieren wir auch unsere Erfahrungen mit dieser Spezifikation.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "collection-title": "Lecture notes in informatics",
    "container-title": "DeLFI2005: 3. Deutsche e-learning fachtagung informatik der gesellschaft für informatik e.v.",
    "editor": [
      {
        "family": "Haake",
        "given": "Jörg M."
      },
      {
        "family": "Lucke",
        "given": "Ulrike"
      },
      {
        "family": "Tavangarian",
        "given": "Djamshid"
      }
    ],
    "id": "P:delfi2005a",
    "issued": {
      "date-parts": [
        [
          2005,
          9
        ]
      ]
    },
    "page": "129-140",
    "publisher": "GI; GI-Verlag",
    "publisher-place": "Rostock, Germany",
    "title": "Integration von E-Assessment und Content-Management",
    "type": "paper-conference",
    "volume": "P-66"
  },
  {
    "abstract": "Wir berichten über Entwurf, Implementierung und Einsatz des Systems LlsChecker. LlsChecker ist eine in ein Content-Management-System (CMS) für Lehr- und Lernmaterialien integrierte Komponente zur automatischen Überprüfung studentischer Lösungen für Programmieraufgaben in unterschiedlichen funktionalen Programmiersprachen. Das System ist so generisch organisiert, dass die Ausweitung der Dienste auf weitere Sprachen – zumindest für funktionale Programmiersprachen – allein durch eine XML-basierte Deklaration möglich ist.",
    "author": [
      {
        "family": "Rösner",
        "given": "Dietmar"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "collection-title": "Lecture notes in informatics",
    "container-title": "DeLFI2005: 3. Deutsche e-learning fachtagung informatik der gesellschaft für informatik e.v.",
    "editor": [
      {
        "family": "Jörg M. Haake",
        "given": "Djamshid Tavangarian",
        "suffix": "Ulrike Lucke"
      }
    ],
    "id": "P:delfi2005b",
    "issued": {
      "date-parts": [
        [
          2005,
          9
        ]
      ]
    },
    "page": "307-318",
    "publisher": "GI; GI-Verlag",
    "publisher-place": "Rostock, Germany",
    "title": "LlsChecker – ein CAA-System für die Lehre im Bereich Programmiersprachen",
    "type": "paper-conference",
    "volume": "P-66"
  },
  {
    "abstract": "Die Erstellung qualitativ hochwertiger Tests ist aufwändig. Daher ist es wünschenswert, einmal erstellte Tests wieder- und weiterverwenden zu können. Um eine Abhängigkeit von einer einzelnen Testplattform zu vermeiden, werden standardisierte Austauschformate benötigt. In diesem Beitrag formulieren wir Desiderata für derartige Formate und untersuchen den derzeitigen De-Facto-Standard, die IMS Question & Test Interoperability Specification (QTI), auf seine Eignung. Das erklärte Ziel von QTI ist es, den Austausch von Tests zwischen verschiedenen Systemen zu ermöglichen. Nach der Analyse der Spezifikation und aufgrund unserer Erfahrungen bei der Implementierung von QTI im System ECQuiz kommen wir zu dem Schluss, dass QTI jedoch als Austauschformat ungeeignet ist.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Fenske",
        "given": "Wolfram"
      }
    ],
    "collection-title": "Lecture Notes in Informatics",
    "container-title": "DeLFI 2007: 5. e-Learning Fachtagung Informatik",
    "editor": [
      {
        "family": "Eibl",
        "given": "Christian"
      },
      {
        "family": "Magenheim",
        "given": "Johannes"
      },
      {
        "family": "Wessner",
        "given": "Martin"
      }
    ],
    "id": "P:delfi2007",
    "issued": {
      "date-parts": [
        [
          2007,
          9
        ]
      ]
    },
    "language": "de-DE",
    "page": "185-196",
    "publisher": "GI; GI-Verlag",
    "publisher-place": "Siegen, Germany",
    "title": "Interoperabilität von elektronischen Tests",
    "type": "paper-conference"
  },
  {
    "URL": "http://edoc.mpg.de/315523",
    "abstract": "This paper presents our approach for supporting face-to-face courses with software components for e-learning based on a general-purpose content management system (CMS). These components—collectively named eduComponents—can be combined with other modules to create tailor-made, sustainable learning environments, which help to make teaching and learning more efficient and effective. We give a short overview of these components, and we report on our practical experiences with the software in our courses.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Rösner",
        "given": "Dietmar"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      }
    ],
    "container-title": "Proceedings of the german e-science conference (GES 2007)",
    "editor": [
      {
        "family": "Bühler",
        "given": "Wilhelm"
      }
    ],
    "id": "P:ges2007",
    "issued": {
      "date-parts": [
        [
          2007,
          5
        ]
      ]
    },
    "publisher": "Helmholtz-Gemeinschaft, Max-Planck-Gesellschaft, and German Rectors’ Conference (HRK); Max-Planck-Gesellschaft",
    "publisher-place": "Baden-Baden, Germany",
    "title": "A sustainable learning environment based on an open source content management system",
    "type": "paper-conference"
  },
  {
    "abstract": "Übungen sind ein zentrales Element in der Informatiklehre. Ausgehend von didaktischen Überlegungen, wie der Übungsbetrieb durch Komponenten des ELearning, insbesondere durch Formen des Computer-Aided Assessment, intensiviert und effizienter gestaltet werden kann, haben wir die EduComponents entwickelt. Dabei handelt es sich um eine Sammlung von Erweiterungsmodulen, die ein allgemeines CMS (Plone) um E-Learning-Funktionalität ergänzen. Seit mehreren Semestern werden diese frei verfügbaren Module sowohl in allen Lehrveranstaltungen unserer Arbeitsgruppe als auch an anderen Institutionen erfolgreich eingesetzt.",
    "author": [
      {
        "family": "Rösner",
        "given": "Dietmar"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "collection-title": "Lecture notes in informatics (LNI) – proceedings",
    "container-title": "2. GI-fachtagung hochschuldidaktik der informatik",
    "editor": [
      {
        "family": "Forbrig",
        "given": "Peter"
      },
      {
        "family": "Siegel",
        "given": "Günter"
      },
      {
        "family": "Schneider",
        "given": "Markus"
      }
    ],
    "id": "P:hdi2006",
    "issued": {
      "date-parts": [
        [
          2006,
          12
        ]
      ]
    },
    "page": "89-102",
    "publisher": "GI; GI-Verlag",
    "publisher-place": "Munich, Germany",
    "title": "E-Learning-Komponenten zur Intensivierung der Übungen in der Informatik-Lehre – ein Erfahrungsbericht",
    "type": "paper-conference",
    "volume": "P-100"
  },
  {
    "abstract": "Most university e-learning strategies mandate the use of a centralized university-wide learning platform. The learning management systems typically employed in this function are “integrated” platforms, i.e., large-scale systems providing most common e-learning functions in a single application. There are,however, a number of issues with this type of systems: Due to their size and complexity they can be difficult and expensive to operate and administrate; and we feel that they are not flexible enough to allow teachers to make tactical decisions; and, since these systems cannot be used for the management of “normal” Web sites, they separate learning content from other content and duplicate functionality and administration. This paper presents an alternative approach: E-learning components that extend a general-purpose content management system with e-learning functionality, enabling the use of a single platform for learning and non-learning content and the creation of tailor-made e-learning environments.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "container-title": "Proceedings of the IADIS international conference e-learning 2007",
    "id": "P:iadis2007",
    "issued": {
      "date-parts": [
        [
          2007,
          7
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "publisher": "IADIS; IADIS Press",
    "publisher-place": "Lisbon, Portugal",
    "title": "Tactical, document-oriented e-learning components",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1140124.1140150",
    "abstract": "To reduce the workload of teachers and to improve the effectiveness of face-to-face courses, it is desirable to supplement them with Web-based tools. This paper presents our approach for supporting computer science education with software components which support the creation, management, submission, and assessment of assignments and tests, including the automatic assessment of programming exercises. These components are integrated into a general-purpose content management system (CMS) and can combinde with other components to create tailored learning environments. We describe the design and implementation of these components, and we report on our practical experience with deploying the software in our courses.",
    "author": [
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "container-title": "ITiCSE ’06: Proceedings of the 11<sup>th</sup> annual conference on innovation and technology in computer science education",
    "id": "P:iticse2006",
    "issued": {
      "date-parts": [
        [
          2006,
          6
        ]
      ]
    },
    "page": "88-92",
    "publisher": "ACM SIGCSE; ACM",
    "publisher-place": "Bologna, Italy",
    "title": "EduComponents: Experiences in e-assessment in computer science education",
    "title-short": "EduComponents",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1268784.1268923",
    "abstract": "We present the eduComponents, a component-based approach to e-learning system architecture. In contrast to typical “integrated” platforms, the eduComponents are implemented as extension modules for a general-purpose content management system (CMS). The components can be used individually, together, and in combination with other modules. The use of a general-purpose (i.e., not e-learning-specific) CMS means that a single platform can be used for e-learning and other Web content, providing the advantages of a uniform user interface, reduced system administration overhead, and extensive code reuse.",
    "author": [
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "container-title": "ITiCSE ’07: Proceedings of the 12<sup>th</sup> annual SIGCSE conference on innovation and technology in computer science education",
    "id": "P:iticse2007",
    "issued": {
      "date-parts": [
        [
          2007,
          6
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "page": "352-352",
    "publisher": "ACM SIGCSE; ACM",
    "publisher-place": "Dundee, U.K.",
    "title": "eduComponents: A component-based e-learning environment",
    "title-short": "eduComponents",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "container-title": "Open source for knowledge and learning management: Strategies beyond tools",
    "editor": [
      {
        "family": "Lytras",
        "given": "Miltiadis"
      },
      {
        "family": "Naeve",
        "given": "Ambjörn"
      }
    ],
    "id": "P:osklm2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "publisher": "Idea Group",
    "publisher-place": "Hershey, PA, USA",
    "title": "EduComponents: Educational components for Plone",
    "title-short": "EduComponents",
    "type": "chapter"
  },
  {
    "URL": "http://www.cs.cityu.edu.hk/~wbl2007/WBL2007_Proceedings_HTML/WBL2007_Proceedings.pdf",
    "abstract": "Since 2003 we have successively introduced the use of e-learning and computer-assisted assessment (CAA) components into all of our courses, namely online multiple-choice tests, electronic submission of assignments, and automatic testing of programs. We originally did not intend to make major changes to the courses; our primary motivation was just to make them more efficient and more effective by freeing teachers from administrative burdens and by offering more flexibility and interactivity for students. After several semesters of usage we have noticed, however, that the courses have changed much more radically than originally envisaged. The electronic support of face-to-face courses offers many new possibilities, but it also opens up new questions. This paper describes our system and our experience, and discusses some of the questions we have encountered.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "container-title": "Proceedings of workshop on blended learning 2007",
    "editor": [
      {
        "family": "Fong",
        "given": "Joseph"
      },
      {
        "family": "Wang",
        "given": "Fu L."
      }
    ],
    "id": "P:wbl2007",
    "issued": {
      "date-parts": [
        [
          2007,
          8
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "page": "257-266",
    "publisher": "The Hong Kong Web Society",
    "publisher-place": "Edinburgh, U.K.",
    "title": "Large-scale computer-assisted assessment in computer science education: New possibilities, new questions",
    "title-short": "Large-scale computer-assisted assessment in computer science education",
    "type": "paper-conference"
  },
  {
    "abstract": "Zum Erwerb von Programmierfähigkeiten ist neben einem theoretischen Verständnis vor allem praktische Übung notwendig. Die automatische Überprüfung von Programmieraufgaben hilft, Studierenden mehr Übungsmöglichkeiten mit schnellerer Rückmeldung zur Verfügung zu stellen und Lehrende gleichzeitig zu entlasten, so dass sie sich auf inhaltliche und didaktische Fragen konzentrieren können. Wir stellen eine schlanke, dienstebasierte Architektur und ihre Implementierung für die automatische Überprüfung von studentischen Einreichungen vor und berichten über praktische Erfahrungen beim Einsatz dieser Software in unseren Lehrveranstaltungen.",
    "author": [
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "collection-title": "Lecture Notes in Informatics",
    "container-title": "2. Workshop Pervasive University im Rahmen der GI-Jahrestagung 2007",
    "id": "P:wpu2007",
    "issued": {
      "date-parts": [
        [
          2007,
          9
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "language": "de-DE",
    "publisher": "GI; GI-Verlag",
    "publisher-place": "Bremen, Germany",
    "title": "Webbasierte Dienste für das E-Assessment",
    "type": "paper-conference"
  },
  {
    "URL": "http://ec.europa.eu/education/archive/elearning/doc/studies/virtual_models_en.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "PLS Rambøll Management A/S"
      }
    ],
    "genre": "Final Report",
    "id": "PLS2004",
    "issued": {
      "date-parts": [
        [
          2004,
          2
        ]
      ]
    },
    "publisher": "European Commission, DG Education and Culture",
    "publisher-place": "Århus",
    "title": "Studies in the context of the e-learning initiative: Virtual models of European universities (lot 1)",
    "title-short": "Studies in the context of the e-learning initiative",
    "type": "report"
  },
  {
    "URL": "http://ec.europa.eu/education/archive/elearning/doc/studies/virtual_annex_g_en.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "PLS Rambøll Management A/S"
      }
    ],
    "genre": "Final Report",
    "id": "PLS2004.Annex_G",
    "issued": {
      "date-parts": [
        [
          2003,
          12
        ]
      ]
    },
    "publisher": "European Commission, DG Education and Culture",
    "publisher-place": "Århus",
    "title": "Studies in the context of the e-learning initiative: Virtual models of European universities (lot 1), annex G",
    "title-short": "Studies in the context of the e-learning initiative",
    "type": "report"
  },
  {
    "URL": "https://sciences-critiques.fr/les-deux-cultures-ou-la-defaite-des-humanites/",
    "abstract": "Puisque le vivant est désormais computable, pourquoi la culture ne le serait-elle pas ? Bienvenue dans l’ère des humanités numériques, un mouvement qui a pris son essor dans les années 2000 au sein des sciences humaines et sociales, des arts et des lettres, pour les rendre, eux aussi, connectés, numérisés, big datés. Reductio ad numero universelle, dont l’objectif est d’annihiler toute humanité dans la compréhension et le récit du monde.",
    "author": [
      {
        "literal": "Pièces et Main d’Œuvre (PMO)"
      }
    ],
    "container-title": "Sciences Critiques",
    "id": "PMO2016",
    "issued": {
      "date-parts": [
        [
          2016,
          9,
          25
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "fr-FR",
    "title": "“Les deux cultures”, ou la défaite des humanités",
    "type": "article-journal"
  },
  {
    "DOI": "10.1484/J.VIATOR.2.301639",
    "URL": "http://dx.doi.org/10.1484/J.VIATOR.2.301639",
    "author": [
      {
        "family": "Packard",
        "given": "David W."
      }
    ],
    "container-title": "Viator",
    "id": "Packard1973",
    "issued": {
      "date-parts": [
        [
          1973
        ]
      ]
    },
    "keyword": "cultural_heritage, latin",
    "page": "27-32",
    "title": "A note on the computer methods used",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "DOI": "10.1007/s10032-007-0047-z",
    "ISSN": "1433-2833",
    "URL": "http://dx.doi.org/10.1007/s10032-007-0047-z",
    "abstract": "Many organizations provide dialog-based support through contact centers to sell their products, handle customer issues, and address product-and service-related issues. This is usually provided through voice calls—of late, web-chat based support is gaining prominence. In this paper, we consider any conversational text derived from web-chat systems, voice recognition systems etc., and propose a method to identify procedures that are embedded in the text. We discuss here how to use the identified procedures in knowledge authoring and agent prompting. In our experiments, we evaluate the utility of the proposed method for agent prompting. We first cluster the call transcripts to find groups of conversations that deal with a single topic. Then, we find possible procedure-steps within each topic-cluster by clustering the sentences within each of the calls in the topic-cluster. We propose a measure for differentiating between clusters that are procedure-steps and those that are topical sentence collections. Once we identify procedure-steps, we represent the calls as sequences of procedure-steps and perform mining to extract distinct and long frequent sequences which represent the procedures that are typically followed in calls. We show that the extracted procedures are comprehensive enough. We outline an approach for retrieving relevant procedures for a partially completed call and illustrate the utility of distinct collections of sequences in the real-world scenario of agent prompting using the retrieval mechanism.",
    "author": [
      {
        "family": "Padmanabhan",
        "given": "Deepak"
      },
      {
        "family": "Kummamuru",
        "given": "Krishna"
      }
    ],
    "container-title": "International Journal on Document Analysis and Recognition",
    "id": "Padmanabhan2007",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "noisy_text",
    "page": "227-238",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Mining conversational text for procedures with applications in contact centers",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2010/pdf/14_Paper.pdf",
    "abstract": "FreeLing is an open-source multilingual language processing library providing a wide range of language analyzers for several languages. It offers text processing and language annotation facilities to natural language processing application developers, simplifying the task of building those applications. FreeLing is customizable and extensible. Developers can use the default linguistic resources (dictionaries, lexicons, grammars, etc.) directly, or extend them, adapt them to specific domains, or even develop new ones for specific languages. This paper overviews the recent history of this tool, summarizes the improvements and extensions incorporated in the latest version, and depicts the architecture of the library. Special focus is brought to the fact and consequences of the library being open-source: After five years and over 35,000 downloads, a growing user community has extended the initial threelanguages (English, Spanish and Catalan) to eight (adding Galician, Italian, Welsh, Portuguese, and Asturian), proving that the collaborative open model is a productive approach for the development of NLP tools and resources.",
    "author": [
      {
        "family": "Padró",
        "given": "Lluís"
      },
      {
        "family": "Collado",
        "given": "Miquel"
      },
      {
        "family": "Reese",
        "given": "Samuel"
      },
      {
        "family": "Lloberes",
        "given": "Marina"
      },
      {
        "family": "Castellón",
        "given": "Irene"
      }
    ],
    "container-title": "Proceedings of the seventh international conference on language resources and evaluation (LREC’10)",
    "editor": [
      {
        "family": "Calzolari",
        "given": "Nicoletta"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Maegaard",
        "given": "Bente"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Odijk",
        "given": "Jan"
      },
      {
        "family": "Piperidis",
        "given": "Stelios"
      },
      {
        "family": "Rosner",
        "given": "Mike"
      },
      {
        "family": "Tapias",
        "given": "Daniel"
      }
    ],
    "id": "Padro2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "nlp, pos_tagging",
    "page": "931-936",
    "publisher": "European Language Resources Association (ELRA)",
    "title": "FreeLing 2.1: Five years of open-source language processing tools",
    "title-short": "FreeLing 2.1",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/2037661.2037664",
    "ISSN": "1046-8188",
    "URL": "http://dx.doi.org/10.1145/2037661.2037664",
    "abstract": "A novel graph-based language-independent stemming algorithm suitable for information retrieval is proposed in this article. The main features of the algorithm are retrieval effectiveness, generality, and computational efficiency. We test our approach on seven languages (using collections from the TREC, CLEF, and FIRE evaluation platforms) of varying morphological complexity. Significant performance improvement over plain word-based retrieval, three other language-independent morphological normalizers, as well as rule-based stemmers is demonstrated.",
    "author": [
      {
        "family": "Paik",
        "given": "Jiaul H."
      },
      {
        "family": "Mitra",
        "given": "Mandar"
      },
      {
        "family": "Parui",
        "given": "Swapan K."
      },
      {
        "family": "Järvelin",
        "given": "Kalervo"
      }
    ],
    "container-title": "ACM Transactions on Information Systems",
    "id": "Paik2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "ir, morphology, stemming",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "GRAS: An effective and efficient stemming algorithm for information retrieval",
    "title-short": "GRAS",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "abstract": "This paper describes the restructuring process of a large corpus of historical documents and the system architecture that is used for accessing it. The initial challenge of this process was to get the most out of existing material, normalizing the legacy markup and harvesting the inherent information using widely available standards. This resulted in a conceptual and technical restructuring of the formerly existing corpus. The development of the standardized markup and techniques allowed the inclusion of important new materials, such as original 16th and 17th century prints and manuscripts; and enlarged the potential user groups. On the technological side, we were grounded on the premise that open standards are the best way of making sure that the resources will be accessible even after years in an archive. This is a welcomed result in view of the additional consequence of the remodeled corpus concept: it serves as a repository for important historical documents, some of which had been preserved for 500 years in paper format. This very rich material can from now on be handled freely for linguistic research goals.",
    "author": [
      {
        "dropping-particle": "Paixão de",
        "family": "Sousa",
        "given": "Maria C."
      },
      {
        "family": "Trippel",
        "given": "Thorsten"
      }
    ],
    "container-title": "Proceedings of the 5<sup>th</sup> international conference on language resources and evaluation (LREC 2006)",
    "id": "PaixaoDeSousa2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, portuguese",
    "page": "1831-1836",
    "publisher": "European Language Resources Association (ELRA)",
    "title": "Building a historical corpus for Classical Portuguese: Some technological aspects",
    "title-short": "Building a historical corpus for Classical Portuguese",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/800107.803458",
    "author": [
      {
        "family": "Palay",
        "given": "Roger M."
      }
    ],
    "container-title": "Proceedings of the ACM SIGCSE-SIGCUE technical symposium on computer science and education",
    "id": "Palay1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "page": "100-103",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The structure and use of a test generating system designed to facilitate individually paced instruction",
    "type": "paper-conference"
  },
  {
    "ISBN": "0810100274",
    "author": [
      {
        "family": "Palmer",
        "given": "Richard"
      }
    ],
    "id": "Palmer1969",
    "issued": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "keyword": "hermeneutics, theory",
    "language": "en-US",
    "publisher": "Northwestern University Press",
    "publisher-place": "Evanston, IL, USA",
    "title": "Hermeneutics: Interpretation theory in schleiermacher, dilthey, heidegger, and gadamer",
    "title-short": "Hermeneutics",
    "type": "book"
  },
  {
    "URL": "http://aclweb.org/anthology/W07-1528",
    "abstract": "We propose a new XML format for representing interlinearized glossed text (IGT), particularly in the context of the documentation and description of endangered languages. The proposed representation, which we call IGT-XML, builds on previous models but provides a more loosely coupled and flexible representation of different annotation layers. Designed to accommodate both selective manual reannotation of individual layers and semi-automatic extension of annotation, IGT-XML is a first step toward partial automation of the production of IGT.",
    "author": [
      {
        "family": "Palmer",
        "given": "Alexis"
      },
      {
        "family": "Erk",
        "given": "Katrin"
      }
    ],
    "container-title": "Proceedings of the linguistic annotation workshop",
    "id": "Palmer2007",
    "issued": {
      "date-parts": [
        [
          2007,
          6
        ]
      ]
    },
    "keyword": "nlp, xml",
    "page": "176-183",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "IGT-XML: An XML format for interlinearized glossed text",
    "title-short": "IGT-XML",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/ijl/13.1.1",
    "abstract": "The Thesaurus Linguae Graecae (TLG) is a digital library of Greek literature established in 1972 at the University of California, Irvine. The project has collected and digitized virtually all texts from the period between Homer (8th century BC) and AD 600 as well as all scholia, historiographical and lexicographical works from the period between AD 600 and the fall of Byzantium in 1453. Its collection at present includes nearly 10, 000 works from over 3, 000 authors – in excess of 75 million words – disseminated on compact disc. The TLG is a major resource for Classics-related research. The project continues to expand its digital collection through the addition of new works and the updating of older editions. Work is also underway to restructure its extensive data using new methodologies of text encoding and to develop tools to process the textual, bibliographic, and lexical information already residing in the TLG collection",
    "author": [
      {
        "family": "Pantelia",
        "given": "Maria"
      }
    ],
    "container-title": "International Journal of Lexicography",
    "id": "Pantelia2000",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "cultural_heritage, greek",
    "page": "1-11",
    "title": "“Noũs into chaos”: The creation of the Thesaurus of the Greek Language",
    "title-short": "“Noũs into chaos”",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "DOI": "10.1093/llc/fqy080",
    "abstract": "The fluctuating role and status of digital humanists—for example as adjunct technicians, hybrid cross-disciplinary scholars, para-academics, or so-called Alt-Acs—is not solely contingent on disciplinary challenges in the academy. Uncontrollable external factors such as economic instability, political change, and technological disruption can radically change and redefine roles and career trajectories. Therefore, the possibility of having to deal with the consequences of not just constant change but also potentially massively disruptive upheaval needs to be considered seriously on an ongoing basis. To avoid, or mitigate, the destructive aspects of such destabilizing change to the Digital Humanities we apply the futurity technique of scenario analysis. In this analysis, we develop and explore four plausible, contesting, but not mutually exclusive, potential futures and ask three fundamental questions to inform future organizational designs and development plans: What could happen? What would be the impact? What needs to be done to be ready and able to respond effectively (to all scenarios)? We conclude that, to remain relevant and resilient in a world constantly threatened by disruption, the Digital Humanities should adopt more flexible and less hierarchical divisions, open processes, and policies, and embrace flatter organizational structures, incorporating extended and inter-operable networks of communities, sources, and technologies, while employing a blended basket of criteria that will prevent identified schisms from becoming dangerous chasms.",
    "author": [
      {
        "family": "Papadopoulos",
        "given": "Costas"
      },
      {
        "family": "Reilly",
        "given": "Paul"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Papadopoulos2019",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "127-145",
    "title": "The digital humanist: Contested status within contesting futures",
    "title-short": "The digital humanist",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "DOI": "10.1007/978-3-642-04930-9\\_30",
    "URL": "http://dx.doi.org/10.1007/978-3-642-04930-9\\_30",
    "abstract": "An increasing number of scientific communities rely on Semantic Web ontologies to share and interpret data within and across research domains. These common knowledge representation resources are usually developed and maintained manually and essentially co-evolve along with experimental evidence produced by scientists worldwide. Detecting automatically the differences between (two) versions of the same ontology in order to store or visualize their deltas is a challenging task for e-science. In this paper, we focus on languages allowing the formulation of concise and intuitive deltas, which are expressive enough to describe unambiguously any possible change and that can be effectively and efficiently detected. We propose a specific language that provably exhibits those characteristics and provide a change detection algorithm which is sound and complete with respect to the proposed language. Finally, we provide a promising experimental evaluation of our framework using real ontologies from the cultural and bioinformatics domains.",
    "author": [
      {
        "family": "Papavassiliou",
        "given": "Vicky"
      },
      {
        "family": "Flouris",
        "given": "Giorgos"
      },
      {
        "family": "Fundulaki",
        "given": "Irini"
      },
      {
        "family": "Kotzinos",
        "given": "Dimitris"
      },
      {
        "family": "Christophides",
        "given": "Vassilis"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "The semantic web – ISWC 2009",
    "editor": [
      {
        "family": "Bernstein",
        "given": "Abraham"
      },
      {
        "family": "Karger",
        "given": "David R."
      },
      {
        "family": "Heath",
        "given": "Tom"
      },
      {
        "family": "Feigenbaum",
        "given": "Lee"
      },
      {
        "family": "Maynard",
        "given": "Diana"
      },
      {
        "family": "Motta",
        "given": "Enrico"
      },
      {
        "family": "Thirunarayan",
        "given": "Krishnaprasad"
      }
    ],
    "id": "Papavassiliou2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "digital_humanities, rdf, semantic_web",
    "page": "473-488",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "On detecting high-level changes in RDF/S KBs",
    "type": "chapter",
    "volume": "5823"
  },
  {
    "DOI": "10.1109/WISE.2003.1254461",
    "abstract": "Service-oriented computing (SOC) is the computing paradigm that utilizes services as fundamental elements for developing applications/solutions. To build the service model, SOC relies on the service oriented architecture (SOA), which is a way of reorganizing software applications and infrastructure into a set of interacting services. However, the basic SOA does not address overarching concerns such as management, service orchestration, service transaction management and coordination, security, and other concerns that apply to all components in a service architecture. In this paper we introduce an extended service oriented architecture that provides separate tiers for composing and coordinating services and for managing services in an open marketplace by employing grid services.",
    "author": [
      {
        "family": "Papazoglou",
        "given": "Mike P."
      }
    ],
    "container-title": "Proceedings of the fourth international conference on web information systems engineering (WISE’03)",
    "id": "Papazoglou2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "cms, soa",
    "page": "3-12",
    "publisher": "IEEE",
    "title": "Service-oriented computing: Concepts, characteristics and directions",
    "title-short": "Service-oriented computing",
    "type": "paper-conference"
  },
  {
    "DOI": "10.3102/0013189X016001022",
    "abstract": "In the beginning, criticism is simple. Do I like it? My judgment is personal and intuitive. I answer to myself alone, and consider only the immediate object of my attention. Soon, however, something more is needed; taste must be justified. Others challenge our opinions and counter with their own, and even personal development eventually requires us to grapple with our reasons. The LOGO community faces the challenge of finding a voice for public dialogue. Where do we look? There is no shortage of models. The education establishment offers the notion of evaluation. Educational psychologists offer the notion of controlled experiment. The computer magazines have developed the idiom of product review. Philosophical tradition suggests inquiry into the essential nature of computation. Each of these has intellectual value in its proper place. I shall argue that this proper place is a conservative context where change is small, slow, and superficial. The crucial experiment, to take one example, is based on a concept of changing a single factor in a complex situation while keeping everything else the same. I shall argue that this is radically incompatible with the enterprise of rebuilding an education system in which nothing shall be the same. I would like to propose a very different model for thinking about the dialogue between LOGO and the world. This model is a department of thought that adopts the adjective \"critical\" in Webster’s first sense. I am proposing a genre of writing one could call \"computer criticism\" by analogy with such disciplines as literary criticism and social criticism. The name does not imply that such writing would condemn computers any more than literary criticism condemns literature or social criticism condemns society. The purpose of computer criticism is not to condemn but to understand, to explicate, to place in perspective. Of course, understanding does not exclude harsh (perhaps even captious) judgment. The result of understanding may well be to debunk. But critical judgment may also open our eyes to previously unnoticed virtue. And in the end, the critical and the creative processes need each other.",
    "author": [
      {
        "family": "Papert",
        "given": "Seymour"
      }
    ],
    "container-title": "Educational Researcher",
    "id": "Papert1987",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "keyword": "classic, computational_thinking",
    "language": "en-US",
    "page": "22-30",
    "title": "Computer criticism vs. Technocentric thinking",
    "type": "article-journal",
    "volume": "16"
  },
  {
    "DOI": "10.1007/11676904_13",
    "ISBN": "978-3-540-32527-7",
    "abstract": "The development of the IRIS semantic desktop platform has provided illumination of some important issues associated with the collection and manipulation of knowledge assets that are organized by an ontology. We explore those issues related to the personalization of the workspace and of the knowledge assets manipulated by IRIS users. We show that a topic map can provide a necessary mediation between the formal organization provided by an ontology to serve the needs of semantic interoperability between workstations and the individual’s need to personalize the workspace in a just for me fashion.",
    "author": [
      {
        "family": "Park",
        "given": "Jack"
      },
      {
        "family": "Cheyer",
        "given": "Adam"
      }
    ],
    "chapter-number": "13",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Charting the topic maps research and applications landscape",
    "editor": [
      {
        "family": "Maicher",
        "given": "Lutz"
      },
      {
        "family": "Park",
        "given": "Jack"
      }
    ],
    "id": "Park2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "ontologies, terminology, vocabularies",
    "page": "145-159",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Just for me: Topic maps and ontologies",
    "title-short": "Just for me",
    "type": "chapter",
    "volume": "3873"
  },
  {
    "author": [
      {
        "family": "Parrochia",
        "given": "Daniel"
      }
    ],
    "container-title": "La modélisation: confluent des sciences",
    "editor": [
      {
        "family": "Brissaud",
        "given": "Marcel"
      },
      {
        "family": "Forsé",
        "given": "Michel"
      },
      {
        "family": "Zighed",
        "given": "Abdelkader"
      }
    ],
    "id": "Parrochia1990",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "formal_models",
    "language": "fr-FR",
    "page": "215-233",
    "publisher": "CNRS",
    "publisher-place": "Paris",
    "title": "Quelques aspects épistémologiques et historiques des notions de  et ",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/3-540-49426-x_2",
    "abstract": "Many different formal techniques, both numerical and symbolic, have been developed over the past two decades for dealing with incomplete and uncertain information. In this paper we review some of the most important of these formalisms, describing how they work, and in what ways they differ from one another. We also consider heterogeneous approaches which incorporate two or more approximate reasoning mechanisms within a single reasoning system. These have been proposed to address limitations in the use of individual formalisms.",
    "author": [
      {
        "family": "Parsons",
        "given": "Simon"
      },
      {
        "family": "Hunter",
        "given": "Anthony"
      }
    ],
    "collection-title": "Lecture notes in artificial intelligence",
    "container-title": "Applications of uncertainty formalisms",
    "editor": [
      {
        "family": "Hunter",
        "given": "Anthony"
      },
      {
        "family": "Parsons",
        "given": "Simon"
      }
    ],
    "id": "Parsons1998-review",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "8-37",
    "publisher": "Springer",
    "publisher-place": "Berlin, Heidelberg",
    "title": "A review of uncertainty handling formalisms",
    "type": "chapter",
    "volume": "1455"
  },
  {
    "ISBN": "9780262161688",
    "abstract": "In this book Simon Parsons describes qualitative methods for reasoning under uncertainty, \"uncertainty\" being a catch-all term for various types of imperfect information. The advantage of qualitative methods is that they do not require precise numerical information. Instead, they work with abstractions such as interval values and information about how values change. The author does not invent completely new methods for reasoning under uncertainty but provides the means to create qualitative versions of existing methods. To illustrate this, he develops qualitative versions of probability theory, possibility theory, and the Dempster-Shafer theory of evidence. According to Parsons, these theories are best considered complementary rather than exclusive. Thus the book supports the contention that rather than search for the one best method to handle all imperfect information, one should use whichever method best fits the problem. This approach leads naturally to the use of several different methods in the solution of a single problem and to the complexity of integrating the results problem to which qualitative methods provide a solution.",
    "author": [
      {
        "family": "Parsons",
        "given": "Simon"
      }
    ],
    "id": "Parsons2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Qualitative approaches for reasoning under uncertainty",
    "type": "book"
  },
  {
    "DOI": "10.1093/llc/fqt037",
    "ISSN": "2055-768X",
    "abstract": "Structured Prosopography provides a formal model for representing prosopography: a branch of historical research that traditionally has focused on the identification of people that appear in historical sources. Since the 1990s, KCL’s Department of Digital Humanities has been involved in the development of structured prosopographical databases using a general ’factoid-oriented’ model of structure that links people to the information about them via spots in primary sources that assert that information. Recent developments, particularly the World Wide Web, and its related technologies around the Semantic Web, have promoted the possibility to both interconnecting dispersed data, and allowing it to be queried semantically. To the purpose of making available our prosopographical databases on the Semantic Web, in this article we review the principles behind our established factoid-based model and reformulate it using a more interoperable approach, based on knowledge representation principles and formal ontologies. In particular, we are going to focus primarily on a high-level semantic analysis of the factoid notion, on its relation to other cultural heritage standards such as CIDOC-CRM, and on the modularity and extensibility of the proposed solutions.",
    "author": [
      {
        "family": "Pasin",
        "given": "Michele"
      },
      {
        "family": "Bradley",
        "given": "John"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Pasin2013",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities, prosopography, semantic_web",
    "page": "86-97",
    "title": "Factoid-based prosopography and computer ontologies: Towards an integrated approach",
    "title-short": "Factoid-based prosopography and computer ontologies",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "DOI": "10.1515/9783110231922.6-269",
    "author": [
      {
        "family": "Passarotti",
        "given": "Marco"
      }
    ],
    "container-title": "Actes du XXV<sup>e</sup> Congrès International de Linguistique et de Philologie Romanes (CILPR)",
    "editor": [
      {
        "family": "Iliescu",
        "given": "Maria"
      },
      {
        "family": "Siller-Runggaldier",
        "given": "Heidi"
      },
      {
        "family": "Danler",
        "given": "Paul"
      }
    ],
    "id": "Passarotti2010a",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, italian",
    "language": "it-IT",
    "page": "269-278",
    "publisher": "De Gruyter",
    "publisher-place": "Berlin, Germany",
    "title": "Per una <i>treebank</i> dell’italiano antico",
    "type": "paper-conference",
    "volume": "VI"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2010/workshops/W21.pdf",
    "abstract": "Despite its key role in the history of computational linguistics, thanks to the pioneering work by Roberto Busa SJ on the Index Thomisticus, Latin can still be considered as a less-resourced language. Although during the last decades several Latin texts have been digitized, only a few of them have been linguistically tagged, while most still lack linguistic tagging at all. However, while the less-resourced status affects historical languages in general, over the past few years a number of language resources for Latin and other historical languages have been started, among which are several treebanks. Presenting the experience of the Index Thomisticus Treebank project and, particularly, its valency lexicon, this paper reports some general insights about the creation and use of language resources for less-resourced languages, showing that, although creating from scratch a language resource for a less-resourced language still remains a labor-intensive and time-consuming task, today this is simplified by exploiting the results provided by previous similar experiences in language resources development.",
    "author": [
      {
        "family": "Passarotti",
        "given": "Marco"
      }
    ],
    "container-title": "7<sup>th</sup> SaLTMiL workshop on creation and use of basic lexical resources for less-resourced languages",
    "editor": [
      {
        "family": "Sarasola",
        "given": "Kepa"
      },
      {
        "family": "Tyers",
        "given": "Francis M."
      },
      {
        "family": "Forcada",
        "given": "Mikel L."
      }
    ],
    "id": "Passarotti2010b",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, latin",
    "page": "27-32",
    "title": "Leaving behind the less-resourced status. The case of Latin through the experience of the Index Thomisticus Treebank",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1515/9783110942347.1",
    "ISBN": "3484640340",
    "abstract": "The 25th edition of Hermann Paul’s grammar is the first step toward a study grammar that combines the virtues of greater ease of use, concentration on essentials, and a more reliable description of Middle High German. The revised accounts of phonology and morphology have profited in part from the ongoing work on the extensive Middle High German grammar now in the process of completion. The description of noun declination has been thoroughly revised. The syntax section has been extensively reworked and greatly expanded, with a wealth of references to Middle High German prose texts.",
    "author": [
      {
        "family": "Paul",
        "given": "Hermann"
      }
    ],
    "collection-number": "2",
    "collection-title": "Sammlung kurzer grammatiken germanischer dialekte. A: hauptreihe",
    "edition": "25<sup>th</sup>",
    "id": "Paul2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "german, linguistics",
    "publisher": "Niemeyer",
    "publisher-place": "Tübingen, Germany",
    "title": "Mittelhochdeutsche grammatik",
    "type": "book"
  },
  {
    "DOI": "10.1016/j.patrec.2010.04.004",
    "ISSN": "01678655",
    "abstract": "It is well known that high-dimensional nearest neighbor retrieval is very expensive. Dramatic performance gains are obtained using approximate search schemes, such as the popular Locality-Sensitive Hashing (LSH). Several extensions have been proposed to address the limitations of this algorithm, in particular, by choosing more appropriate hash functions to better partition the vector space. All the proposed extensions, however, rely on a structured quantizer for hashing, poorly fitting real data sets, limiting its performance in practice. In this paper, we compare several families of space hashing functions in a real setup, namely when searching for high-dimension SIFT descriptors. The comparison of random projections, lattice quantizers, k-means and hierarchical k-means reveal that unstructured quantizer significantly improves the accuracy of LSH, as it closely fits the data in the feature space. We then compare two querying mechanisms introduced in the literature with the one originally proposed in LSH, and discuss their respective merits and limitations.",
    "author": [
      {
        "family": "Paulevé",
        "given": "Loïc"
      },
      {
        "family": "Jégou",
        "given": "Hervé"
      },
      {
        "family": "Amsaleg",
        "given": "Laurent"
      }
    ],
    "container-title": "Pattern Recognition Letters",
    "id": "Pauleve2010",
    "issue": "11",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "approximate_matching, clustering",
    "page": "1348-1358",
    "title": "Locality sensitive hashing: A comparison of hash function types and querying mechanisms",
    "title-short": "Locality sensitive hashing",
    "type": "article-journal",
    "volume": "31"
  },
  {
    "URL": "http://www.formatex.org/micte2006/pdf/1935-1939.pdf",
    "abstract": "Modularization of digital learning content has been addressed for through the concept of Learning Objects. In this article it is shown this modularized concept can be extended to the Virtual Learning Environment. A general, Service Oriented Architecture (SOA) framework for modularized Virtual Learning Environments was implemented based on the VWE Learning Objects Taxonomy. It is argued that SOA is a suitable approach for modularization, and that the resulting SOA-framework can be used as basis for implementing specialized e-learning services, specified by future standard frameworks and reference models.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          23
        ]
      ]
    },
    "author": [
      {
        "family": "Paulsson",
        "given": "Frederik"
      },
      {
        "family": "Berglund",
        "given": "Mikael"
      }
    ],
    "container-title": "Current developments in technology-assisted education. Proceedings of the fourth international conference on multimedia and information and communication technologies in education (m-icte2006)",
    "editor": [
      {
        "family": "Méndez-Vilas",
        "given": "A."
      },
      {
        "family": "Solano Martín",
        "given": "A."
      },
      {
        "family": "Mesa González",
        "given": "J. A."
      },
      {
        "family": "Mesa González",
        "given": "J."
      }
    ],
    "id": "Paulsson2006",
    "issued": {
      "date-parts": [
        [
          2006,
          11
        ]
      ]
    },
    "keyword": "e-learning, soa",
    "page": "1935-1939",
    "publisher": "FORMATEX",
    "publisher-place": "Badajoz, Spain",
    "title": "A service oriented architecture-framework for modularized virtual learning environments",
    "type": "paper-conference",
    "volume": "III"
  },
  {
    "URL": "http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-4712",
    "abstract": "This thesis explores the role of modularity for achieving a better adaptation of learning technology to pedagogical requirements. In order to examine the interrelations that occur between pedagogy and computer science, a theoretical framework rooted in both fields is applied.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          23
        ]
      ]
    },
    "author": [
      {
        "family": "Paulsson",
        "given": "Fredrik"
      }
    ],
    "genre": "PhD thesis",
    "id": "Paulsson2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "e-learning",
    "publisher": "KTH",
    "publisher-place": "Stockholm, Sweden",
    "title": "Modularization of the learning architecture: Supporting learning theories by learning technologies",
    "title-short": "Modularization of the learning architecture",
    "type": "thesis"
  },
  {
    "DOI": "10.1007/bf00116346",
    "URL": "http://dx.doi.org/10.1007/bf00116346",
    "abstract": "This article describes an intelligent computer-assisted language instruction system that is designed to teach principles of syntactic style to students of English. Unlike conventional style checkers, the system performs a complete syntactic analysis of its input, and takes the student’s stylistic intent into account when providing a diagnosis. Named STASEL for Stylistic Treatment At the Sentence Level, the system is specifically developed for the teaching of style, and makes use of artificial intelligence techniques in natural language processing to analyze free-form input sentences interactively.",
    "author": [
      {
        "family": "Payette",
        "given": "Julie"
      },
      {
        "family": "Hirst",
        "given": "Graeme"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Payette1992",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1992,
          4,
          1
        ]
      ]
    },
    "keyword": "e-learning, interactive_editing, post-writing_tools",
    "page": "87-102",
    "title": "An intelligent computer-assistant for stylistic instruction",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "DOI": "10.2307/3344740",
    "abstract": "To compare college students’ opinions of computer-based melodic dictation instruction and classroom instruction, 75 students enrolled in sophomore music theory who had been exposed to both approaches were surveyed. A 34-item questionnaire evaluated students’ opinions of the hardware, software, and departmental requirements regarding the computer-based instruction and asked for comparisons of this tutelage with the classroom instruction they had subsequently received. While hardware and software received generally favorable reviews, students basically related a negative opinion of the computer-based melodic dictation instruction. This opinion was based on the following three points: The computer program required too much time outside class, too much progress was expected in too little time, and pacing (the increase in difficulty levels) was not consistent throughout the program. On the basis of student comments, several suggestions regarding construction of computer-based instructional materials are presented.",
    "author": [
      {
        "family": "Pembrook",
        "given": "Randall G."
      }
    ],
    "container-title": "Journal of Research in Music Education",
    "id": "Pembrook1986",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1986
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "page": "121-133",
    "publisher": "MENC: The National Association for Music Education",
    "title": "Some implications of students’ attitudes toward a computer-based melodic dictation program",
    "type": "article-journal",
    "volume": "34"
  },
  {
    "DOI": "10.1007/978-3-540-85287-2\\_36",
    "ISBN": "978-3-540-85286-5",
    "ISSN": "0302-9743",
    "URL": "http://dx.doi.org/10.1007/978-3-540-85287-2\\_36",
    "abstract": "In this paper, we study how existing natural language processing tools for Italian perform on ancient texts. The first goal is to understand to what extent such tools can be used ” as they are” for the automatic analysis of old literary works. Indeed, while NLP tools for Italian achieve today good performance, it is not clear if they could be successfully used for the humanities, to support the critical study of historical works. Our analysis will show how tools’ performance systematically vary across different time periods, and within literary movements. As a second goal, we want to verify whether or not simple customization methods can improve the tools performance over the old works.",
    "author": [
      {
        "family": "Pennacchiotti",
        "given": "Marco"
      },
      {
        "family": "Zanzotto",
        "given": "Fabio M."
      }
    ],
    "chapter-number": "36",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Advances in natural language processing",
    "editor": [
      {
        "family": "Nordström",
        "given": "Bengt"
      },
      {
        "family": "Ranta",
        "given": "Aarne"
      }
    ],
    "id": "Pennacchiotti2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, italian",
    "page": "371-382",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Natural language processing across time: An empirical investigation on Italian advances in natural language processing",
    "title-short": "Natural language processing across time",
    "type": "chapter",
    "volume": "5221"
  },
  {
    "DOI": "10.3102/0091732x025001001",
    "ISSN": "0091-732X",
    "abstract": "In summary, Illich (1973) argued that we need tools that allow the individual \"the greatest opportunity to enrich the environment with the fruits of his or her vision\" (p. 21). The programmable modeling tools discussed in this chapter are cognitive tools that support students’ knowledge construction by allowing them to instantiate their thought processes. The outcomes are explanatory physical models that can be examined, evaluated, discussed, and reflected upon.",
    "author": [
      {
        "family": "Penner",
        "given": "David E."
      }
    ],
    "container-title": "Review of Research in Education",
    "id": "Penner2000",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "formal_models, models_in_general, pedagogy",
    "language": "en-US",
    "page": "1-35",
    "title": "Cognition, computers, and synthetic science: Building knowledge and meaning through modeling",
    "title-short": "Cognition, computers, and synthetic science",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "URL": "http://perdisco.com.au/australianUniWp.asp",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "Perdisco Pty Limited"
      }
    ],
    "genre": "Whitepaper",
    "id": "Perdisco2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "publisher": "Perdisco Pty Limited",
    "publisher-place": "Ultimo NSW, Australia",
    "title": "E-learning in Australian universities: Opportunities and challenges",
    "title-short": "E-learning in Australian universities",
    "type": "report"
  },
  {
    "DOI": "10.1145/1600193.1600232",
    "ISBN": "978-1-60558-575-8",
    "author": [
      {
        "family": "Peroni",
        "given": "Silvio"
      },
      {
        "family": "Vitali",
        "given": "Fabio"
      }
    ],
    "collection-title": "DocEng ’09",
    "container-title": "Proceedings of the 9<sup>th</sup> ACM symposium on document engineering",
    "id": "Peroni2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "EARMARK, markup, overlapping markup, OWL, XPointer",
    "page": "171-180",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Annotations with EARMARK for arbitrary, overlapping and out-of order markup",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/2063518.2063533",
    "ISBN": "978-1-4503-0621-8",
    "URL": "http://doi.acm.org/10.1145/2063518.2063533",
    "author": [
      {
        "family": "Peroni",
        "given": "Silvio"
      },
      {
        "family": "Gangemi",
        "given": "Aldo"
      },
      {
        "family": "Vitali",
        "given": "Fabio"
      }
    ],
    "collection-title": "I-semantics ’11",
    "container-title": "Proceedings of the 7<sup>th</sup> international conference on semantic systems",
    "id": "Peroni2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "EARMARK, LMM, linguistic act, markup semantics",
    "page": "111-118",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Dealing with markup semantics",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/2361354.2361396",
    "ISBN": "978-1-4503-1116-8",
    "abstract": "The semantic enhancement of a traditional scientific paper is not a straightforward operation, since it involves many different aspects or facets. In this paper we propose eight different semantic lenses through which these facets may be viewed, and describe and exemplify the ontologies by which these lenses may be implemented.",
    "author": [
      {
        "family": "Peroni",
        "given": "Silvio"
      },
      {
        "family": "Shotton",
        "given": "David"
      },
      {
        "family": "Vitali",
        "given": "Fabio"
      }
    ],
    "container-title": "Proceedings of the 2012 ACM symposium on document engineering (DocEng ’12)",
    "id": "Peroni2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "ontologies, rdf, semantic_web",
    "page": "191-194",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Faceted documents: Describing document characteristics using semantic lenses",
    "title-short": "Faceted documents",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1016/j.websem.2012.08.001",
    "abstract": "Semantic publishing is the use of Web and Semantic Web technologies to enhance the meaning of a published journal article, to facilitate its automated discovery, to enable its linking to semantically related articles, to provide access to data within the article in actionable form, and to facilitate integration of data between articles. Recently, semantic publishing has opened the possibility of a major step forward in the digital publishing world. For this to succeed, new semantic models and visualization tools are required to fully meet the specific needs of authors and publishers. In this article, we introduce the principles and architectures of two new ontologies central to the task of semantic publishing: FaBiO, the FRBR-aligned Bibliographic Ontology, an ontology for recording and publishing bibliographic records of scholarly endeavours on the Semantic Web, and CiTO, the Citation Typing Ontology, an ontology for the characterization of bibliographic citations both factually and rhetorically. We present those two models step by step, in order to emphasise their features and to stress their advantages relative to other pre-existing information models. Finally, we review the uptake of FaBiO and CiTO within the academic and publishing communities.",
    "author": [
      {
        "family": "Peroni",
        "given": "Silvio"
      },
      {
        "family": "Shotton",
        "given": "David"
      }
    ],
    "container-title": "Web Semantics: Science, Services and Agents on the World Wide Web",
    "id": "Peroni2012b",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "intertextuality, ontologies, rdf, semantic_web",
    "page": "33-43",
    "title": "FaBiO and CiTO: Ontologies for describing bibliographic resources and citations",
    "title-short": "FaBiO and CiTO",
    "type": "article-journal",
    "volume": "17"
  },
  {
    "DOI": "10.1007/978-3-319-04777-5_5",
    "abstract": "One of the main research areas in semantic publishing is the development of semantic models that fit the requirements of authors and publishers. Although several models and metadata schemas have been developed in the past, they do not fully comply with the vocabulary used by publishers or they are not adequate for describing specific topics (e.g., characterisation of bibliographic citations, definition of publishing roles, description of publishing workflows, etc.). In this chapter I introduce the Semantic Publishing and Referencing (SPAR) Ontologies, a suite of orthogonal and complementary OWL 2 DL ontology modules for the creation of comprehensive machine-readable RDF metadata for every aspect of semantic publishing and referencing. In particular, I show the characteristics and benefits of all the SPAR ontologies, and support the entire discussion with several examples of Turtle code describing a particular reference of the legal discipline, namely Casanovas et al.’s “OPJK and DILIGENT: ontology modelling in a distributed environment”.",
    "author": [
      {
        "family": "Peroni",
        "given": "Silvio"
      }
    ],
    "collection-title": "Law, governance and technology series",
    "container-title": "Semantic web technologies and legal scholarly publishing",
    "id": "Peroni2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "ontologies, scientific_publishing, semantic_web",
    "page": "121-193",
    "publisher": "Springer",
    "publisher-place": "Cham, Switzerland",
    "title": "The semantic publishing and referencing ontologies",
    "type": "chapter",
    "volume": "15"
  },
  {
    "DOI": "10.1108/jd-12-2013-0166",
    "ISSN": "0022-0418",
    "URL": "http://dx.doi.org/10.1108/jd-12-2013-0166",
    "abstract": "Purpose. Citation data needs to be recognized as a part of the Commons – those works that are freely and legally available for sharing – and placed in an open repository. Design/methodology/approach. The Open Citation Corpus is a new open repository of scholarly citation data, made available under a Creative Commons CC0 1.0 public domain dedication and encoded as Open Linked Data using the SPAR Ontologies. Findings. The Open Citation Corpus presently provides open access to reference lists from 204,637 articles from the Open Access Subset of PubMed Central (OA-PMC), containing 6,325,178 individual references to 3,373,961 unique papers. Research limitations/implications. We need tools, such as the CiTO Reference Annotation Tools and CiTalO, to facilitate the semantic enhancement of the references in scholarly papers according to Semantic Publishing models and technologies. Originality/value. Scholars, publishers and institutions may freely build upon, enhance and reuse the open citation data for any purpose, without restriction under copyright or database law. Keywords: Open Citation Corpus, SPAR Ontologies, CiTO, CiTalO, CiTO Reference Annotation Tool, citations, references, semantic publishing, open access",
    "author": [
      {
        "family": "Peroni",
        "given": "Silvio"
      },
      {
        "family": "Dutton",
        "given": "Alexander"
      },
      {
        "family": "Gray",
        "given": "Tanya"
      },
      {
        "family": "Shotton",
        "given": "David"
      }
    ],
    "container-title": "Journal of Documentation",
    "id": "Peroni2015",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "bibliography, ontologies, scientific_publishing",
    "page": "253-277",
    "title": "Setting our bibliographic references free: Towards open citation data",
    "title-short": "Setting our bibliographic references free",
    "type": "article-journal",
    "volume": "71"
  },
  {
    "author": [
      {
        "family": "Perrin",
        "given": "Daniel"
      }
    ],
    "container-title": "Text – Verstehen. Grammatik und darüber hinaus",
    "editor": [
      {
        "family": "Blühdorn",
        "given": "Hardarik"
      },
      {
        "family": "Breindl",
        "given": "Eva"
      },
      {
        "family": "Waßner",
        "given": "Ulrich Herrmann"
      }
    ],
    "id": "Perrin2006b",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "language": "de-DE",
    "page": "332-350",
    "publisher": "de Gruyter",
    "title": "Verstanden werden. Vom doppelten Interesse an theoriebasierter, praxisgerichteter Textberatung",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/978-3-540-76876-0\\_15",
    "ISBN": "978-3-540-76875-3",
    "URL": "http://dx.doi.org/10.1007/978-3-540-76876-0\\_15",
    "abstract": "Spatial and temporal data are critical components in many applications. This is especially true in analytical domains such as national security and criminal investigation. Often, the analytical process requires uncovering and analyzing complex thematic relationships between disparate people, places and events. Fundamentally new query operators based on the graph structure of Semantic Web data models, such as semantic associations, are proving useful for this purpose. However, these analysis mechanisms are primarily intended for thematic relationships. In this paper, we describe a framework built around the RDF metadata model for analysis of thematic, spatial and temporal relationships between named entities. We discuss modeling issues and present a set of semantic query operators. We also describe an efficient implementation in Oracle DBMS and demonstrate the scalability of our approach with a performance study using a large synthetic dataset from the national security domain.",
    "author": [
      {
        "family": "Perry",
        "given": "Matthew"
      },
      {
        "family": "Sheth",
        "given": "Amit P."
      },
      {
        "family": "Hakimpour",
        "given": "Farshad"
      },
      {
        "family": "Jain",
        "given": "Prateek"
      }
    ],
    "chapter-number": "15",
    "collection-title": "Lecture notes in computer science",
    "container-title": "GeoSpatial semantics",
    "editor": [
      {
        "family": "Fonseca",
        "given": "Frederico"
      },
      {
        "family": "Rodríguez",
        "given": "M. Andrea"
      },
      {
        "family": "Levashkin",
        "given": "Sergei"
      }
    ],
    "id": "Perry2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "rdf, temporal_data",
    "page": "228-246",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Supporting complex thematic, spatial and temporal queries over semantic web data",
    "type": "chapter",
    "volume": "4853"
  },
  {
    "URL": "http://scholarship.law.cornell.edu/ijli/vol37/iss3/4/",
    "abstract": "In the actual multilingual and multicultural environment there is a significant need, in the academic world, in the legal profession, in business settings as well as in the context of public administration services to citizens, of common understanding and exchange of legal concepts of the various legal systems. At the same time, there is a strong pressure for the reservation of their basic sense and value. Both requirements are quite difficult to meet, and they are complicated by the complexity of legal language and by the variety of modalities used to express law within the various legal systems. Unlike a number of technical and scientific disciplines where a fair correspondence exists between concepts across languages, serious difficulties arise in interpreting law across countries and languages. This is largely due to the system-bound nature of legal terminology. This paper focuses on crosslanguage retrieval systems’ ability to facilitate access to legal information across different languages and legal orders. As such, issues are addressed relating to linguistics and translation theory, comparative law, theory of law, as well as natural language processing techniques, while some recommendations are provided with the aim to contribute to cross-language retrieval of law.",
    "author": [
      {
        "family": "Peruginelli",
        "given": "Ginevra"
      }
    ],
    "container-title": "International Journal of Legal Information",
    "id": "Peruginelli2009",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "ir, legal, terminology",
    "page": "Article 4+",
    "title": "Accessing legal information across boundaries: A new challenge",
    "title-short": "Accessing legal information across boundaries",
    "type": "article-journal",
    "volume": "37"
  },
  {
    "DOI": "10.1145/359038.359041",
    "ISSN": "0001-0782",
    "URL": "http://dx.doi.org/10.1145/359038.359041",
    "abstract": "With the increase in word and text processing computer systems, programs which check and correct spelling will become more and more common. Peterson investigates the basic structure of several such existing programs and their approaches to solving the problems which arise when this type of program is created. The basic framework and background necessary to write a spelling checker or corrector are provided.",
    "author": [
      {
        "family": "Peterson",
        "given": "James L."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Peterson1980",
    "issue": "12",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "keyword": "spelling_correction",
    "page": "676-687",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Computer programs for detecting and correcting spelling errors",
    "type": "article-journal",
    "volume": "23"
  },
  {
    "DOI": "10.1145/6138.6146",
    "ISSN": "0001-0782",
    "URL": "http://dx.doi.org/10.1145/6138.6146",
    "abstract": "Although human proofreading is still necessary, small, topic-specific word lists in spelling programs will minimize the occurrence of undetected typing errors.",
    "author": [
      {
        "family": "Peterson",
        "given": "James L."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Peterson1986",
    "issue": "7",
    "issued": {
      "date-parts": [
        [
          1986
        ]
      ]
    },
    "keyword": "spelling_correction",
    "page": "633-637",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A note on undetected typing errors",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "URL": "http://muse.jhu.edu/journals/technology_and_culture/v045/45.2petrina.html",
    "abstract": "Rather than dismiss the case of Sidney Pressey and the Automatic Teacher as just another failure in the history of educational technology and psychology, this article makes an argument for paying attention to the automation of the professions. Produced in the 1920s, the Automatic Teacher automated clerical and diagnostic skills and redefined productivity in terms of individualization. While standardizing education with teaching machines and tests, Pressey claimed to liberate teachers from burdens of drilling and testing and students from conformity in mass education. He nonetheless found out the hard way that his project of individualizing the masses was more easily automated than commercialized. The Welch Manufacturing Company produced 250 Automatic Teachers but shelved the project after selling only about 160.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Petrina",
        "given": "Stephen"
      }
    ],
    "container-title": "Technology and Culture",
    "id": "Petrina2004",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2004,
          4
        ]
      ]
    },
    "keyword": "e-learning",
    "page": "305-330",
    "title": "Sidney Pressey and the automation of education, 1924–1934",
    "type": "article-journal",
    "volume": "45"
  },
  {
    "URL": "http://atala.org/IMG/pdf/TAL-2009-50-2-02-Petrova.pdf",
    "abstract": "The present paper reports on the development and evaluation of a historical corpus designed to support detailed empirical studies on the interaction of information structure and syntax in Old High German (OHG). The creation and exploration of this corpus are part of a more general investigation concerning the role of information-structural factors in the explanation of word order variation and change in the Germanic languages. The paper also describes corpus design principles, methodologies, relevant formats and speciﬁcations, and the technical infrastructure employed during the creation of the corpus, as well as its accessibility by means of the linguistic database of information structure ANNIS.",
    "author": [
      {
        "family": "Petrova",
        "given": "Svetlana"
      },
      {
        "family": "Solf",
        "given": "Michael"
      },
      {
        "family": "Ritz",
        "given": "Julia"
      },
      {
        "family": "Chiarcos",
        "given": "Christian"
      },
      {
        "family": "Zeldes",
        "given": "Amir"
      }
    ],
    "container-title": "Traitement Automatique des Langues",
    "id": "Petrova2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, german",
    "page": "47-71",
    "title": "Building and using a richly annotated interlinear diachronic corpus: The case of Old High German Tatian",
    "title-short": "Building and using a richly annotated interlinear diachronic corpus",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "URL": "http://aclweb.org/anthology/W11-1512",
    "abstract": "Even though historical texts reveal a lot of interesting information on culture and social structure in the past, information access is limited and in most cases the only way to find the information you are looking for is to manually go through large volumes of text, searching for interesting text segments. In this paper we will explore the idea of facilitating this time-consuming manual effort, using existing natural language processing techniques. Attention is focused on automatically identifying verbs in early modern Swedish texts (1550–1800). The results indicate that it is possible to identify linguistic categories such as verbs in texts from this period with a high level of precision and recall, using morphological tools developed for present-day Swedish, if the text is normalised into a more modern spelling before the morphological tools are applied.",
    "author": [
      {
        "family": "Pettersson",
        "given": "Eva"
      },
      {
        "family": "Nivre",
        "given": "Joakim"
      }
    ],
    "container-title": "Proceedings of the 5<sup>th</sup> ACL-HLT workshop on language technology for cultural heritage, social sciences, and humanities (LaTeCH 2011)",
    "id": "Pettersson2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "computational_linguistics, cultural_heritage, swedish",
    "page": "87-95",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Morristown, NJ, USA",
    "title": "Automatic verb extraction from historical Swedish texts",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/ISSPA.2003.1224752",
    "ISBN": "0-7803-7946-2",
    "abstract": "Humans can recognize different types of written languages by their grammars and vocabularies. However, computers see everything as numbers. We present a computational algorithm for machine classification of written languages using the method of vector quantization. For a language document, each word is converted to a sequence of numbers and forms as a vector of numerical values according to its characters. This collection of vectors is then represented by a codebook that contains a number of template vectors for classification. The proposed method is more effective for machine learning than the n-gram based method, which has been widely used for written language identification. Experimental results of classifying a set of five closely roman-typed scripts show the promising application of the proposed method.",
    "author": [
      {
        "family": "Pham",
        "given": "Tuan"
      },
      {
        "family": "Tran",
        "given": "Dat"
      }
    ],
    "container-title": "Proceedings of the seventh international symposium on signal processing and its applications, 2003",
    "id": "Pham2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "page": "513-516",
    "publisher": "IEEE",
    "title": "VQ-based written language identification",
    "type": "paper-conference",
    "volume": "1"
  },
  {
    "ISSN": "1075-2838",
    "URL": "http://drdobbs.com/184401251",
    "abstract": "Finding words or names that might be spelled differently than the search string that was typed in presents an aggravated case of this problem. English is not only well known for having maddeningly irrational spelling practices, but in America we have also accumulated names from all over the world. The first algorithm to deal with this problem, Soundex, is gratifyingly simple, but it is not at all an adequate solution. It often fails to do the job of returning alternatives that are pronounced similarly to the search string. Soundex and Metaphone belong to a class of algorithms usually known as \"phonetic encoding\" or \"sound alike\" algorithms — a heuristic type of fuzzy matching. They input a word or name, and return an encoded key, which should be the same for any words that are pronounced similarly — allowing for a reasonable amount of fuzziness. For these particular algorithms, only the first four consonant sounds are encoded, unless the first letter is a vowel. Therefore, Metaphone encodes \"Stephan\" as STFN. Since \"Steven\" and \"Stefan\" are pronounced similarly, you should be able to find those spellings as well. But Soundex doesn’t know much about English spelling peculiarities, so it won’t!",
    "author": [
      {
        "family": "Philips",
        "given": "Lawrence"
      }
    ],
    "container-title": "C/C++ Users Journal",
    "id": "Phillips2000",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "approximate_matching",
    "page": "38-43",
    "publisher": "CMP Media, Inc.",
    "publisher-place": "USA",
    "title": "The Double Metaphone search algorithm",
    "type": "article-journal",
    "volume": "18"
  },
  {
    "URL": "http://multiwordnet.fbk.eu/paper/MWN-India-published.pdf",
    "abstract": "This paper illustrates the MultiWordNet project, aimed at producing an Italian WordNet strongly aligned with the Princeton WordNet. The main conceptual differences between the MultiWordNet and the EuroWordNet conceptual models are presented first. Then two automatic procedures capable of speeding up the work of lexicographers are described. Finally, we give some details about the adopted data model and we present a graphical user interface that can be used to browse and update the aligned database.",
    "author": [
      {
        "family": "Pianta",
        "given": "Emanuele"
      },
      {
        "family": "Bentivogli",
        "given": "Luisa"
      },
      {
        "family": "Girardi",
        "given": "Christian"
      }
    ],
    "container-title": "Proceedings of the first international conference on global WordNet",
    "id": "Pianta2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "computational_linguistics",
    "page": "21-25",
    "title": "MultiWordNet: Developing an aligned multilingual database",
    "title-short": "MultiWordNet",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqt049",
    "ISSN": "0268-1145",
    "abstract": "This contribution discusses some of the challenges involved in building an ontology for research about the philosopher Ludwig Wittgenstein. It pays special attention to different ontological conceptions (event based versus object based). It also discusses how best to model, within the ontology, conflicting views emerging in both Wittgenstein’s work and Wittgenstein scholarship. The contribution presents relevant work in progress at the Wittgenstein Archives at the University of Bergen, which has a special focus on Wittgenstein’s Nachlass, his philosophical estate.",
    "author": [
      {
        "family": "Pichler",
        "given": "Alois"
      },
      {
        "family": "Zollner-Weber",
        "given": "Amélie"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Pichler2013",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_edition, intertextuality, ontologies, rdf, tei",
    "page": "700-707",
    "title": "Sharing and debating Wittgenstein by using an ontology",
    "type": "article-journal",
    "volume": "28"
  },
  {
    "DOI": "10.1093/llc/fql020",
    "ISSN": "0268-1145",
    "abstract": "In this article, we describe our interdisciplinary project   Rule-based search in text databases with nonstandard orthography (RSNSR)  in support of the conservation of cultural heritage, especially for the German reception of the philosopher Nietzsche. We present a rule-based fuzzy search engine that allows users to retrieve text data independently of its orthographical realization. The rules used are derived from statistical analyses, historical publications, linguistic principles, and expert knowledge. Our Web-based tool is intended for experts as well as interested amateurs. Along with its present features, further functions are currently worked out. Among them are automatic rule derivation and finer result classification through a generalized Levenshtein similarity measure. Our work is associated with the recently launched project Deutsch Diachron Digital (DDD) to build a complete diachronic corpus of German for the first time with texts from the ninth century (Old High German) to the present (Modern German).",
    "author": [
      {
        "family": "Pilz",
        "given": "Thomas"
      },
      {
        "family": "Luther",
        "given": "Wolfram"
      },
      {
        "family": "Fuhr",
        "given": "Norbert"
      },
      {
        "family": "Ammon",
        "given": "Ulrich"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Pilz2006",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2006,
          6
        ]
      ]
    },
    "keyword": "cultural_heritage, ir",
    "page": "179-186",
    "publisher": "Oxford University Press",
    "title": "Rule-based search in text databases with nonstandard orthography",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "DOI": "10.1093/llc/fqm044",
    "URL": "http://dx.doi.org/10.1093/llc/fqm044",
    "abstract": "In this article, we describe the respective approaches we have taken when addressing issues of spelling variation in German and English historical texts. More specifically, we describe an experiment to evaluate automatic techniques for the development of letter replacement heuristics against manually created gold standards of known letter replacements rules. As will become clear, the motivation for the research differs according to the team of researchers: the German researchers are seeking to develop a search engine for historical texts; the English researchers want to improve the results obtained when applying corpus linguistic techniques (developed for modern language) to historical data. However, the respective teams do share a longer term goal of assessing whether it is possible to develop a generic spelling detection tool for Indo-European languages.",
    "author": [
      {
        "family": "Pilz",
        "given": "Thomas"
      },
      {
        "family": "Ernst-Gerlach",
        "given": "Andrea"
      },
      {
        "family": "Kempken",
        "given": "Sebastian"
      },
      {
        "family": "Rayson",
        "given": "Paul"
      },
      {
        "family": "Archer",
        "given": "Dawn"
      }
    ],
    "container-title": "Lit Linguist Computing",
    "id": "Pilz2008LLC",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2008,
          4,
          1
        ]
      ]
    },
    "keyword": "cultural_heritage, ir",
    "page": "65-72",
    "title": "The identification of spelling variants in english and german historical texts: Manual or automatic?",
    "title-short": "The identification of spelling variants in english and german historical texts",
    "type": "article-journal",
    "volume": "23"
  },
  {
    "URL": "http://www.ling.helsinki.fi/sky/julkaisut/SKY2008/SKY_JoL_21_2008_final.pdf",
    "author": [
      {
        "family": "Pilz",
        "given": "Thomas"
      },
      {
        "family": "Luther",
        "given": "Wolfram"
      },
      {
        "family": "Ammon",
        "given": "Ulrich"
      }
    ],
    "container-title": "SKY Journal of Linguistics",
    "id": "Pilz2008SKY",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, ir",
    "page": "155-200",
    "title": "Retrieval of spelling variants in nonstandard texts – automated support and visualization",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "DOI": "10.1515/9783110216141.211",
    "abstract": "The interdisciplinary project on rule-based search in text databases with nonstandard orthography develops mechanisms to facilitate working with documents containing spelling variants and recognition errors. Even though the quality of optical character recognition has greatly increased in recent years, suboptimal sources like old documents, poor quality texts or texts in historical fonts still severely complicate successful queries. Proper results are the consequence of several intermediate stages of processing: the collection of data (evidences of nonstandard spelling and related standard spelling), the training of specific search modules and, of course, the actual search task. This paper concentrates on automatic support methods that allow for a constant reduction of human intervention. It describes the inherent problems, the process of evidence collection, the underlying methods, their value for linguistic research and how these methods can be implemented in an interface for automatic user support.",
    "author": [
      {
        "family": "Pilz",
        "given": "Thomas"
      },
      {
        "family": "Luther",
        "given": "Wolfram"
      }
    ],
    "collection-title": "Studies in generative grammar",
    "container-title": "The fruits of empirical linguistics i",
    "editor": [
      {
        "family": "Featherston",
        "given": "Sam"
      },
      {
        "family": "Winkler",
        "given": "Susanne"
      }
    ],
    "id": "Pilz2009",
    "issued": {
      "date-parts": [
        [
          2010,
          5,
          17
        ]
      ]
    },
    "keyword": "cultural_heritage, ir",
    "page": "211-228",
    "publisher": "Mouton de Gruyter",
    "title": "Automated support for evidence retrieval in documents with nonstandard orthography",
    "type": "chapter",
    "volume": "101"
  },
  {
    "URL": "http://www.scg.inf.uni-due.de/fileadmin/Veroeffentlichungen/Pilz-Nichtstandardisierte_Rechtschreibung(2009).pdf",
    "abstract": "Zahlreiche Dokumente kulturhistorischen Wertes sind heute im Internet verfügbar. Auch wenn sie im Volltext vorliegen, können historische Schreibvarianten, Zeichenerkennungs- oder Schreibfehler Suchanfragen auf diesen Texten scheitern lassen. Das Konzept der unscharfen Suche soll helfen, auch solche Wörter zu finden, deren exakte, im Text enthaltene Schreibweise nicht bekannt ist. Die vorliegende Arbeit beschreibt die Konzeption, Implementierung und Evaluation eines trainierbaren Moduls zur unscharfen Suche. Sie präsentiert die grundlegenden Verfahren unscharfer Suche sowie die Regularien heutiger Rechtschreibung und stellt umfassend die strukturellen Unterschiede zwischen Standardschreibungen und Schreibvarianten in ihren möglichen Variationsformen dar. Ein tiefgreifendes Verständnis dieser Aspekte ist vonnöten um eine Textsuchfunktion erfolgreich zu implementieren. Es wird abschließend gezeigt, dass das Training probabilistischer Abstandsmaße auf Wortpaaren aus Standardschreibung und Schreibvariante erlaubt, beliebige Formen zeichenbasierter Variation abzubilden.",
    "author": [
      {
        "family": "Pilz",
        "given": "Thomas"
      }
    ],
    "genre": "PhD thesis",
    "id": "Pilz2009b",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, german, spelling_normalization",
    "publisher": "Universität Duisburg-Essen",
    "title": "Nichtstandardisierte Rechtschreibung: Variationsmodellierung und rechnergestützte Variationsverarbeitung",
    "type": "thesis"
  },
  {
    "URL": "http://citeseer.ist.psu.edu/438814.html",
    "abstract": "Although information retrieval systems mainly deal with natural language, linguistic methods are rarely used. Most systems only use stemming, i.e., the mechanical cutting off of inflectional and derivational suffixes to better match index terms to query terms. Since most research on information retrieval is done for English, which has a relatively weak morphology, this is seldom regarded as problematic. Some researchers even consider stemming as completely unnecessary. There is, however, considerable evidence that stemming and more linguistically motivated methods do have a positive impact on retrieval performance for languages such as Dutch, German, Italian, or Slovene, which are morphologically richer than English. Morphologic phenomena like compounds and changes of the stem are still not handled by conventional stemmers. As German, for example, makes extensive use of these morphologic processes (consider compounds like Bundesverfassungsgericht, and changes of the stem like in Häuser, the plural of Haus), the application of full morphologic analysis to the information retrieval task intuitively seems to be promising. This thesis sets out to determine the usefulness of morphologic analysis in information retrieval systems, particularly for the retrieval of German-language documents. An experimental retrieval system called IRF/1 was developed as a test bed. It is described in this thesis. IRF/1 is used to compare the retrieval effectiveness of different text processing methods for a test collection of about 300 magazine articles. The evaluated methods are: 1. stemming (as a baseline), 2. base form reduction using morphologic analysis 3. same as (2) but compounds are split into the base forms of their constituents, and 4. same as (3) but the base forms of compounds are kept along with their parts. Using the standard information retrieval measures of recall and precision, the comparison finds morphologic analysis to be generally more effective than stemming. While morphologic base form reduction only provides relatively little improvement over stemming, decomposition of compounds results in a decisive increase in retrieval effectiveness for German. It can be concluded that morphologic analysis with decomposition of compounds is a very promising approach to improving information retrieval for German and should be further investigated.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "genre": "Master’s thesis",
    "id": "Piotrowski1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "ir, mxp, nlp",
    "publisher": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
    "title": "NLP-supported full-text retrieval",
    "type": "thesis"
  },
  {
    "abstract": "Formative Tests können für Lehrende und Lernende gleichermaßen nützlich sein. Webbasierte Multiple-Choice-Tests können helfen, den Aufwand für formative Tests zu senken und somit einen breiteren und häufigeren Einsatz zu ermöglichen. Wir stellen ein Modul für das Content-Management-System Plone vor, das es erlaubt, MC-Tests genau wie andere Ressourcen einzusetzen und zu verwalten. Auf dieseWeise können vor allem in Präsenzveranstaltungen, für die üblicherweise keine Lernplattform verwendet wird, Tests eng mit den anderen online verfügbaren Lehr- und Lernmaterialien (z. B. Vorlesungsskripten oder Aufgabenblättern) verknüpft werden. Das Modul erlaubt auch den Import und Export von Aufgaben gemäß IMS QTI; in diesem Zusammenhang diskutieren wir auch unsere Erfahrungen mit dieser Spezifikation.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "collection-title": "Lecture notes in informatics",
    "container-title": "DeLFI2005: 3. Deutsche e-learning fachtagung informatik der gesellschaft für informatik e.v.",
    "editor": [
      {
        "family": "Haake",
        "given": "Jörg M."
      },
      {
        "family": "Lucke",
        "given": "Ulrike"
      },
      {
        "family": "Tavangarian",
        "given": "Djamshid"
      }
    ],
    "id": "Piotrowski2005",
    "issued": {
      "date-parts": [
        [
          2005,
          9
        ]
      ]
    },
    "page": "129-140",
    "publisher": "GI-Verlag",
    "publisher-place": "Rostock, Germany",
    "title": "Integration von E-Assessment und Content-Management",
    "type": "paper-conference",
    "volume": "P-66"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "container-title": "Open source for knowledge and learning management: Strategies beyond tools",
    "editor": [
      {
        "family": "Lytras",
        "given": "Miltiadis"
      },
      {
        "family": "Naeve",
        "given": "Ambjörn"
      }
    ],
    "id": "Piotrowski2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "publisher": "Idea Group",
    "publisher-place": "Hershey, PA, USA",
    "title": "EduComponents: Educational components for Plone",
    "title-short": "EduComponents",
    "type": "chapter"
  },
  {
    "abstract": "Most university e-learning strategies mandate the use of a centralized university-wide learning platform. The learning management systems typically employed in this function are ?integrated? platforms, i.e., large-scale systems providing most common e-learning functions in a single application. There are,however, a number of issues with this type of systems: Due to their size and complexity they can be difficult and expensive to operate and administrate; and we feel that they are not flexible enough to allow teachers to make tactical decisions; and, since these systems cannot be used for the management of ?normal? Web sites, they separate learning content from other content and duplicate functionality and administration. This paper presents an alternative approach: E-learning components that extend a general-purpose content management system with e-learning functionality, enabling the use of a single platform for learning and non-learning content and the creation of tailor-made e-learning environments.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "container-title": "Proceedings of the IADIS international conference e-learning 2007",
    "id": "Piotrowski2007a",
    "issued": {
      "date-parts": [
        [
          2007,
          7
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "publisher": "IADIS; IADIS Press",
    "publisher-place": "Lisbon, Portugal",
    "title": "Tactical, document-oriented e-learning components",
    "type": "paper-conference"
  },
  {
    "abstract": "Die Erstellung qualitativ hochwertiger Tests ist aufwändig. Daher ist es wünschenswert, einmal erstellte Tests wieder- und weiterverwenden zu können. Um eine Abhängigkeit von einer einzelnen Testplattform zu vermeiden, werden standardisierte Austauschformate benötigt. In diesem Beitrag formulieren wir Desiderata für derartige Formate und untersuchen den derzeitigen De-Facto-Standard, die IMS Question & Test Interoperability Specification (QTI), auf seine Eignung. Das erklärte Ziel von QTI ist es, den Austausch von Tests zwischen verschiedenen Systemen zu ermöglichen. Nach der Analyse der Spezifikation und aufgrund unserer Erfahrungen bei der Implementierung von QTI im System ECQuiz kommen wir zu dem Schluss, dass QTI jedoch als Austauschformat ungeeignet ist.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Fenske",
        "given": "Wolfram"
      }
    ],
    "collection-title": "Lecture Notes in Informatics",
    "container-title": "DeLFI 2007: 5. e-Learning Fachtagung Informatik",
    "editor": [
      {
        "family": "Eibl",
        "given": "Christian"
      },
      {
        "family": "Magenheim",
        "given": "Johannes"
      },
      {
        "family": "Wessner",
        "given": "Martin"
      }
    ],
    "id": "Piotrowski2007b",
    "issued": {
      "date-parts": [
        [
          2007,
          9
        ]
      ]
    },
    "language": "de-DE",
    "page": "185-196",
    "publisher": "GI-Verlag",
    "publisher-place": "Siegen, Germany",
    "title": "Interoperabilität von elektronischen Tests",
    "type": "paper-conference"
  },
  {
    "URL": "https://d-nb.info/996257799/34",
    "abstract": "This dissertation questions the common assumption that e-learning requires a learning management system (LMS) such as Moodle or Blackboard. Based on an analysis of the current state of the art in LMSs, we come to the conclusion that the functionality of conventional e-learning platforms consists of basic content management and communications facilities (such as forums, chats, wikis, etc.) and functionality for assessment (such as quizzes). However, only assessment functionality is actually specific to e-learning. Furthermore, the content management and communication functionality in e-learning platforms is typically restricted and often inferior when compared with the more general implementations available in Web content management systems.Since content management systems (CMS) offer more general and more robust functions for managing content, we argue that e-learning platforms should be based on content management systems. Only assessment functions are actually specific to e-learning and need to be added to a CMS; this requires the architecture of the CMS to be modular.As a proof of concept, we have designed and implemented the eduComponents, a component-based e-learning system architecture, realized as software components extending a general-purpose content management system with facilities for course management and assessment.The eduComponents have been successfully used since several semesters at Otto von Guericke University and other institutions. The experience with the eduComponents gives practical evidence for the theses we have put forward in this dissertation and of the feasibility of the eduComponents approach.The research done for this dissertation has also resulted in practical definitions for e-learning and e-learning platform, terms which are notoriously ill-defined. Based on these definitions, we have developed an innovative way to assess and to visualize the areas of functionality of e-learning environments.",
    "accessed": {
      "date-parts": [
        [
          2016,
          3,
          11
        ]
      ]
    },
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "genre": "PhD thesis",
    "id": "Piotrowski2009a",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "language": "en-US",
    "publisher": "Otto-von-Guericke-Universität",
    "publisher-place": "Magdeburg, Germany",
    "title": "Document-oriented e-learning components",
    "type": "thesis"
  },
  {
    "DOI": "10.1145/1600193.1600240",
    "URL": "http://dx.doi.org/10.1145/1600193.1600240",
    "abstract": "Unlike programmers, authors only get very little support from their writing tools, i.e., their word processors and editors. Current editors are unaware of the objects and structures of natural languages and only offer character-based operations for manipulating text. Writers thus have to execute complex sequences of low-level functions to achieve their rhetoric or stylistic goals while composing. Software requiring long and complex sequences of operations causes users to make slips. In the case of editing and revising, these slips result in typical revision errors, such as sentences without a verb, agreement errors, or incorrect word order. In the LingURed project, we are developing language-aware editing functions to prevent errors. These functions operate on linguistic elements, not characters, thus shortening the command sequences writers have to execute. This paper describes the motivation and background of the LingURed project and shows some prototypical language-aware functions.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Mahlow",
        "given": "Cerstin"
      }
    ],
    "container-title": "DocEng’09: Proceedings of the 2009 ACM symposium on document engineering",
    "editor": [
      {
        "family": "Borghoff",
        "given": "Uwe M."
      },
      {
        "family": "Chidlovskii",
        "given": "Boris"
      }
    ],
    "id": "Piotrowski2009b",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "emacs, interactive_editing, mxp, nlp",
    "page": "214-217",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Linguistic editing support",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1722080.1722083",
    "ISBN": "978-1-60558-826-1",
    "URL": "http://dx.doi.org/10.1145/1722080.1722083",
    "abstract": "We describe a corpus of historic mountaineering accounts and ongoing work on geocoding toponyms and route descriptions in these accounts. Mountaineering accounts contain a wealth of geographic information but its extraction for purposes of geographic information retrieval poses specific challenges, in particular the distinction between toponyms pertinent to route descriptions and those mentioned in descriptions of panoramas. We describe some preliminary considerations for natural language cues to distinguish between these two types of occurrences.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Läubli",
        "given": "Samuel"
      },
      {
        "family": "Volk",
        "given": "Martin"
      }
    ],
    "container-title": "Proceedings of the 6th workshop on geographic information retrieval",
    "editor": [
      {
        "family": "Purves",
        "given": "Ross"
      },
      {
        "family": "Clough",
        "given": "Paul"
      },
      {
        "family": "Jones",
        "given": "Chris"
      }
    ],
    "id": "Piotrowski2010a",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "geo_ir, mxp, nlp",
    "page": "15-16",
    "publisher": "ACM Press",
    "publisher-place": "New York, NY, USA",
    "title": "Towards mapping of alpine route descriptions",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1722080.1722102",
    "ISBN": "978-1-60558-826-1",
    "URL": "http://dx.doi.org/10.1145/1722080.1722102",
    "abstract": "We describe ongoing work on detecting toponyms in back-of-the-book indices to geocode historical documents not available in full text; the goal is specifically to provide spatial browsing for the Collection of Swiss Law Sources. We discuss some of the peculiarities of handcrafted indices and approaches for coping with them.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Proceedings of the 6th workshop on geographic information retrieval",
    "editor": [
      {
        "family": "Purves",
        "given": "Ross"
      },
      {
        "family": "Clough",
        "given": "Paul"
      },
      {
        "family": "Jones",
        "given": "Chris"
      }
    ],
    "id": "Piotrowski2010b",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "geo_ir, geocoding, mxp",
    "page": "89-90",
    "publisher": "ACM Press",
    "publisher-place": "New York, NY, USA",
    "title": "Leveraging Back-Of-The-Book indices to enable spatial browsing of a historical document collection",
    "type": "paper-conference"
  },
  {
    "URL": "http://ilk.uvt.nl/LaTeCH2010/LPF/ws16.pdf",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Proceedings of the ECAI 2010 workshop on language technology for cultural heritage, social sciences, and humanities (LaTeCH 2010)",
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      }
    ],
    "id": "Piotrowski2010e",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, mxp",
    "page": "67-71",
    "title": "From law sources to language resources",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1860559.1860608",
    "ISBN": "978-1-4503-0231-9",
    "URL": "http://dx.doi.org/10.1145/1860559.1860608",
    "abstract": "Many large-scale digitization projects are currently under way that intend to preserve the cultural heritage contained in paper documents (in particular books) and make it available on the Web. Typically OCR is used to produce searchable electronic texts from books. For newer books, approximately from the late 1980s onwards,digital text may already exist in the form of typesetting data. For applications that require a higher level of accuracy than OCR can deliver, the conversion of typesetting data can thus be an alternative to manual keying. In this paper, we describe a tool for converting typesetting data in FrameMaker format to XHTML+CSS developed for a collection of source editions of medieval and early modern documents. Even though the books of the Collection are typeset in good quality and in modern typefaces, OCR is unusable,s ince the text is in various historical forms of German, French, Italian, Rhaeto-Romanic, and Latin. The conversion of typesetting data produces fully reliable text free from OCR errors and thus also provides a basis for the construction of language resources for the processing of historical texts.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "DocEng ’10: Proceedings of the 10th ACM symposium on document engineering",
    "id": "Piotrowski2010f",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, mxp",
    "page": "223-226",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Document conversion for cultural heritage texts: FrameMaker to HTML revisited",
    "title-short": "Document conversion for cultural heritage texts",
    "type": "paper-conference"
  },
  {
    "DOI": "10.4018/978-1-61692-789-9.ch004",
    "URL": "http://dx.doi.org/10.4018/978-1-61692-789-9.ch004",
    "abstract": "The creation of good tests is time-consuming and expensive. Tests should therefore be reusable to ensure sustainability and to preserve investments and intellectual assets. This requires a standard, platform-neutral, vendor independent interchange file format for tests. IMS Question and Test Interoperability (QTI) aims to be this standard. Almost a decade after the publication of the first version of QTI, even the interchange of simple multiple-choice tests between different systems remains problematic. In this chapter, we present a critical analysis of QTI. Our conclusion is that QTI has failed to provide interoperability of questions and tests due to serious problems in its design.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Handbook of research on e-learning standards and interoperability: Frameworks and issues",
    "editor": [
      {
        "family": "Lazarinis",
        "given": "Fotis"
      },
      {
        "family": "Green",
        "given": "Steve"
      },
      {
        "family": "Pearson",
        "given": "Elaine"
      }
    ],
    "id": "Piotrowski2011a",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "page": "59-82",
    "publisher": "IGI Global",
    "publisher-place": "Hershey, PA, USA",
    "title": "QTI: A failed e-learning standard?",
    "title-short": "QTI",
    "type": "chapter"
  },
  {
    "abstract": "We describe the construction of two corpora in the domain of Swiss legal texts: The DS21 corpus is based on the Collection of Swiss Law Sources and contains historical legal texts from the early Middle Ages up to 1798; the Swiss Legislation Corpus (SLC) is based on the Classified Compilation of Swiss Federal Legislation and contains all current Swiss federal laws. The paper summarizes the key properties of both corpora, discusses issues encountered while building them, and outlines some applications.",
    "author": [
      {
        "family": "Höfler",
        "given": "Stefan"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Journal for Language Technology and Computational Linguistics",
    "id": "Piotrowski2011b",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, french, german",
    "page": "77-89",
    "title": "Building corpora for the philological study of Swiss legal texts",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "DOI": "10.2200/S00436ED1V01Y201207HLT017",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "collection-number": "17",
    "collection-title": "Synthesis lectures on human language technologies",
    "id": "Piotrowski2012a",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "publisher-place": "San Rafael, CA, USA",
    "title": "Natural language processing for historical texts",
    "type": "book"
  },
  {
    "URL": "http://aclweb.org/anthology/W12-1005",
    "abstract": "We describe ongoing work aiming at deriving a multilingual controlled vocabulary (German, French, Italian) from the combined subject indices from 22 volumes of a large-scale critical edition of historical documents. The controlled vocabulary is intended to support editors in assigning descriptors to new documents and to support users in retrieving documents of interest regardless of the spelling or language variety used in the documents.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Senn",
        "given": "Cathrin"
      }
    ],
    "container-title": "Proceedings of the EACL 2012 workshop on language technology for cultural heritage, social sciences, and humanities (LaTeCH 2012)",
    "editor": [
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      },
      {
        "dropping-particle": "van den",
        "family": "Bosch",
        "given": "Antal"
      }
    ],
    "id": "Piotrowski2012b",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "cultural_heritage, mxp",
    "page": "24-29",
    "publisher": "ACL",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Harvesting indices to grow a controlled vocabulary: Towards improved access to historical legal texts",
    "title-short": "Harvesting indices to grow a controlled vocabulary",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Colavizza",
        "given": "Giovanni"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "genre": "{DARIAH-DE Working Papers}",
    "id": "Piotrowski2013",
    "issued": {
      "date-parts": []
    },
    "publisher": "SUB",
    "publisher-place": "Göttingen, Germany",
    "status": "forthcoming",
    "title": "Controlled vocabularies for research in the humanities",
    "type": "report"
  },
  {
    "DOI": "10.1145/2657480.2657482",
    "abstract": "Shared controlled vocabularies are a prerequisite for collaborative annotation and semantic interchange. The creation and maintenance of such vocabularies is, however, time-consuming and expensive. The diversity of research questions in the humanities makes it virtually impossible to create shared controlled vocabularies that cover a wide range of potential applications and satisfy the needs of diverse stakeholders. In this paper we present a novel conceptual approach for mitigating these problems. We propose that projects define their own vocabularies as needed and link the vocabulary terms to one or more concepts in a reference thesaurus, so that the project-specific term effectively serves as a “label” for a set of shared concepts. We also describe the implementation of this approach in the Labeling System. The Labeling System is a Web application that allows users to easily import concepts or create SKOS vocabularies and link the vocabulary terms to concepts from one or more reference thesauri.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Colavizza",
        "given": "Giovanni"
      },
      {
        "family": "Thiery",
        "given": "Florian"
      },
      {
        "family": "Bruhn",
        "given": "Kai-Christian"
      }
    ],
    "container-title": "DH-CASE II: Collaborative annotations on shared environments: Metadata, tools and techniques in the digital humanities",
    "editor": [
      {
        "family": "Schmitz",
        "given": "Patrick"
      },
      {
        "family": "Pearce",
        "given": "Laurie"
      },
      {
        "family": "Dombrowski",
        "given": "Quinn"
      }
    ],
    "id": "Piotrowski2014a",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "cultural_heritage, ontologies, rdf",
    "page": "A1/1-6",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The Labeling System: A new approach to overcome the vocabulary bottleneck",
    "title-short": "The Labeling System",
    "type": "paper-conference"
  },
  {
    "URL": "http://dhd-wp.hab.de/?q=content/informatik-\\%E2\\%80\\%93-technischer-hilfsdienst-der-digital-humanities",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "DHd Workshop ",
    "editor": [
      {
        "family": "Görz",
        "given": "Günther"
      },
      {
        "family": "Henrich",
        "given": "Andreas"
      },
      {
        "family": "Heyer",
        "given": "Gerhard"
      },
      {
        "family": "Thaller",
        "given": "Manfred"
      },
      {
        "family": "Warnke",
        "given": "Martin"
      }
    ],
    "id": "Piotrowski2015a",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "de-DE",
    "page": "B.9",
    "publisher": "DHd",
    "title": "Informatik – technischer Hilfsdienst der Digital Humanities?",
    "type": "paper-conference"
  },
  {
    "URL": "http://dm2e.eu/files/D4.6_app4_Peace-Treaties_final.pdf",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "genre": "Final Report",
    "id": "Piotrowski2015c",
    "issued": {
      "date-parts": [
        [
          2015,
          3
        ]
      ]
    },
    "language": "en-US",
    "publisher": "DM2E Open Humanities Award",
    "title": "Early modern European peace treaties online",
    "type": "report"
  },
  {
    "DOI": "10.1145/2960811.2967170",
    "abstract": "The familiar PDF-based scholarly publishing workflow—which emulates even earlier paper-based workflows—has been surprisingly resistent to change. However, it is becoming increasingly clear that it no longer meets the requirements of a quickly evolving scholarly, technical, and political environment, which includes the trend towards open access publishing, reproducible research, mobile devices, linked open data, and many other developments. This workshop approaches scholarly publishing from a document engineering perspective and focuses on the question of document formats for submission, review, publication, and archival of scholarly publications. We will discuss the current state of scholarly publishing from a document engineering point of view, with the explicit goal of identifying potential alternatives to the current workflow.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Proceedings of the 2016 ACM symposium on document engineering (DocEng ’16)",
    "id": "Piotrowski2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "document_engineering, scientific_publishing",
    "language": "en-US",
    "page": "7-8",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Future publishing formats",
    "type": "paper-conference"
  },
  {
    "DOI": "10.18420/infdh2018-07",
    "abstract": "The relationship between computer science and digital humanities and the potential contributions of computer science to digital humanities cannot be reasonably discussed without defining what we mean by “digital humanities.” With a view to enabling this important discussion, we propose a concise definition of digital humanities centered around the construction of formal models.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Proceedings of INF-DH 2018",
    "editor": [
      {
        "family": "Burghardt",
        "given": "Manuel"
      },
      {
        "family": "Müller-Birn",
        "given": "Claudia"
      }
    ],
    "event-date": {
      "date-parts": [
        [
          2018,
          9,
          25
        ]
      ]
    },
    "id": "Piotrowski2018a",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Gesellschaft für Informatik",
    "title": "Digital humanities: An explication",
    "title-short": "Digital humanities",
    "type": "paper-conference"
  },
  {
    "DOI": "10.17175/sb004_006a",
    "URL": "http://www.zfdg.de/sb004_006",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Zeitschrift für digitale Geisteswissenschaften",
    "editor": [
      {
        "family": "Kuczera",
        "given": "Andreas"
      },
      {
        "family": "Wübbena",
        "given": "Thorsten"
      },
      {
        "family": "Kollatz",
        "given": "Thomas"
      }
    ],
    "id": "Piotrowski2018b",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "de-DE",
    "title": "Accepting and Modeling Uncertainty",
    "type": "article-journal"
  },
  {
    "DOI": "10.21825/jeps.v4i1.10226",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Journal of European Periodical Studies",
    "id": "Piotrowski2018c",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "8-18",
    "title": "Historical models and serial sources",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "DOI": "10.4000/revuehn.381",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Xanthos",
        "given": "Aris"
      }
    ],
    "container-title": "Humanités numériques",
    "id": "Piotrowski2018d",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "fr-FR",
    "title": "Décomposer les humanités numériques",
    "type": "article-journal"
  },
  {
    "URL": "http://ceur-ws.org/Vol-2314/",
    "editor": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "event": "Workshop on computational methods in the humanities 2018",
    "event-date": {
      "date-parts": [
        [
          2018,
          6,
          4
        ],
        [
          2018,
          6,
          5
        ]
      ]
    },
    "event-place": "Lausanne",
    "id": "Piotrowski2018f",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "CEUR Workshop Proceedings",
    "title": "Proceedings of the Workshop on Computational Methods in the Humanities (COMHUM 2018)",
    "type": "book"
  },
  {
    "URL": "http://ceur-ws.org/Vol-2314/abstract3.pdf",
    "author": [
      {
        "family": "Thaller",
        "given": "Manfred"
      }
    ],
    "container-title": "Proceedings of the Workshop on Computational Methods in the Humanities (COMHUM 2018)",
    "editor": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "event": "Workshop on computational methods in the humanities 2018",
    "event-date": {
      "date-parts": [
        [
          2018,
          6,
          4
        ],
        [
          2018,
          6,
          5
        ]
      ]
    },
    "event-place": "Lausanne",
    "id": "Thaller2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "CEUR Workshop Proceedings",
    "title": "Decoding what the sender did not want to transmit. Information technology and historical data; or something",
    "type": "paper-conference"
  },
  {
    "URL": "http://archive.xmlprague.cz/2019/files/xmlprague-2019-proceedings.pdf#page=335",
    "abstract": "It is clear that the heydays of XML are over, and XML is now confronted with competitors that represent—in some respects—steps backwards. In this paper we argue that at this point it is of particular importance to study the history of markup in order to avoid reinventing the wheel, but also to reflect on and possibly re-evaluate design choices in order to advance the state of the art.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Proceedings of XML prague 2019",
    "editor": [
      {
        "family": "Kosek",
        "given": "Jiří"
      }
    ],
    "id": "Piotrowski2019a",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "keyword": "mxp, sgml, xml",
    "page": "323-333",
    "title": "History and the future of markup",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1632/pmla.2017.132.3.651",
    "ISSN": "0030-8129",
    "URL": "http://dx.doi.org/10.1632/pmla.2017.132.3.651",
    "abstract": "In this essay, I use the idea of the literary model to introduce a new way of thinking about traversing scales of critical analysis. Rather than rely on proper names as placeholders or on the visual icons of graphs, maps, or trees, models return us to the process—the tools, techniques, and practices—through which we construct our knowledge of phenomena that exceed our direct observation. Much of the early discourse surrounding the computational understanding of literature has inevitably focused on notions of distance or bigness, on a vocabulary of transcendence or the macrocosm. Like computing culture (Davis) or literary studies (Wellmon) during their beginnings, the nascent field of data-driven literary studies has prioritized a sense of communion with something greater than ourselves.",
    "author": [
      {
        "family": "Piper",
        "given": "Andrew"
      }
    ],
    "container-title": "PMLA",
    "id": "Piper2017",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models, literature",
    "page": "651-658",
    "title": "Think small: On literary modeling",
    "title-short": "Think small",
    "type": "article-journal",
    "volume": "132"
  },
  {
    "URL": "http://informationr.net/ir/7-2/paper126",
    "abstract": "We present a novel n-gram based string matching technique, which we call the targeted s-gram matching technique. In the technique, n-grams are classified into categories on the basis of character contiguity in words. The categories are then utilized in matching. The technique was compared with the conventional n-gram technique using adjacent characters as n-grams. Several types of words and word pairs were studied. English, German, and Swedish query keys were matched against their Finnish spelling variants and Finnish morphological variants using a target word list of 119 000 Finnish words. In all cross-lingual tests done, the targeted s-gram matching technique outperformed the conventional n-gram matching technique. The technique was highly effective also for monolingual word form variants. The effects of query key length and the length of the longest common subsequence (LCS) of the variants on the performance of s-grams were analyzed.",
    "author": [
      {
        "family": "Pirkola",
        "given": "Ari"
      },
      {
        "family": "Keskustalo",
        "given": "Heikki"
      },
      {
        "family": "Leppänen",
        "given": "Erkka"
      },
      {
        "family": "Känsälä",
        "given": "Antti-Pekka"
      },
      {
        "family": "Järvelin",
        "given": "Kalervo"
      }
    ],
    "container-title": "Information Research",
    "id": "Pirkola2002",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "approximate_matching",
    "title": "Targeted s-gram matching: A novel n-gram matching technique for cross- and monolingual word form variants",
    "title-short": "Targeted s-gram matching",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=1567551",
    "abstract": "The paper presents two techniques for lemmatization of Polish person names. First, we apply a rule-based approach which relies on linguistic information and heuristics. Then, we investigate an alternative knowledge-poor method which employs string distance measures. We provide an evaluation of the adopted techniques using a set of newspaper texts.",
    "author": [
      {
        "family": "Piskorski",
        "given": "Jakub"
      },
      {
        "family": "Sydow",
        "given": "Marcin"
      },
      {
        "family": "Kupść",
        "given": "Anna"
      }
    ],
    "container-title": "Proceedings of the workshop on balto-slavonic natural language processing: Information extraction and enabling technologies",
    "id": "Piskorski2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "approximate_matching, proper_names",
    "page": "27-34",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Lemmatization of Polish person names",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-04235-5_36",
    "ISBN": "978-3-642-04234-8",
    "abstract": "This paper presents the results of recent experiments on application of string distance metrics to the problem of named entity lemmatisation in Polish. It extends of our work in [1] by introducing new results for organisation names. Furthermore, the results presented here and in [2,3] centering around the same topic were used to make a comparative study of the average usefulness of the numerous examined string distance metrics to lemmatisation of Polish named-entities of various types. In particular, we focus on lemmatisation of country names, organisation names and person names.",
    "author": [
      {
        "family": "Piskorski",
        "given": "Jakub"
      },
      {
        "family": "Sydow",
        "given": "Marcin"
      },
      {
        "family": "Wieloch",
        "given": "Karol"
      }
    ],
    "chapter-number": "36",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Human language technology. Challenges of the information society",
    "editor": [
      {
        "family": "Vetulani",
        "given": "Zygmunt"
      },
      {
        "family": "Uszkoreit",
        "given": "Hans"
      }
    ],
    "id": "Piskorski2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "approximate_matching, proper_names",
    "page": "413-427",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Comparison of string distance metrics for lemmatisation of named entities in Polish human language technology",
    "type": "chapter",
    "volume": "5603"
  },
  {
    "DOI": "10.1109/34.824821",
    "ISSN": "01628828",
    "abstract": "Handwriting has continued to persist as a means of communication and recording information in day-to-day life even with the introduction of new technologies. Given its ubiquity in human transactions, machine recognition of handwriting has practical significance, as in reading handwritten notes in a PDA, in postal addresses on envelopes, in amounts in bank checks, in handwritten fields in forms, etc. This overview describes the nature of handwritten language, how it is transduced into electronic data, and the basic concepts behind written language recognition algorithms. Both the online case (which pertains to the availability of trajectory data during writing) and the off-line case (which pertains to scanned images) are considered. Algorithms for preprocessing, character and word recognition, and performance with practical systems are indicated. Other fields of application, like signature verification, writer authentification, handwriting learning tools are also considered",
    "author": [
      {
        "family": "Plamondon",
        "given": "Réjean"
      },
      {
        "family": "Srihari",
        "given": "Sargur N."
      }
    ],
    "container-title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "id": "Plamondon2000",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2000,
          1
        ]
      ]
    },
    "keyword": "handwriting_recognition, ocr",
    "page": "63-84",
    "publisher": "IEEE",
    "publisher-place": "Washington, DC, USA",
    "title": "Online and off-line handwriting recognition: A comprehensive survey",
    "title-short": "Online and off-line handwriting recognition",
    "type": "article-journal",
    "volume": "22"
  },
  {
    "URL": "http://aclweb.org/anthology/D10-1025",
    "abstract": "Representing documents by vectors that are independent of language enhances machine translation and multilingual text categorization. We use discriminative training to create a projection of documents from multiple languages into a single translingual vector space. We explore two variants to create these projections: Oriented Principal Component Analysis (OPCA) and Coupled Probabilistic Latent Semantic Analysis (CPLSA). Both of these variants start with a basic model of documents (PCA and PLSA). Each model is then made discriminative by encouraging comparable document pairs to have similar vector representations. We evaluate these algorithms on two tasks: parallel document retrieval for Wikipedia and Europarl documents, and cross-lingual text classification on Reuters. The two discriminative variants, OPCA and CPLSA, significantly outperform their corresponding baselines. The largest differences in performance are observed on the task of retrieval when the documents are only comparable and not parallel. The OPCA method is shown to perform best.",
    "author": [
      {
        "family": "Platt",
        "given": "John C."
      },
      {
        "family": "Toutanova",
        "given": "Kristina"
      },
      {
        "family": "Yih",
        "given": "Wen T."
      }
    ],
    "container-title": "Proceedings of the 2010 conference on empirical methods in natural language processing (EMNLP ’10)",
    "id": "Platt2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "topic_modeling",
    "page": "251-261",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Translingual document representations from discriminative projections",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1002/(sici)1097-4571(199808)49:10\\%3C888::aid-asi5\\%3E3.0.co;2-z",
    "URL": "http://dx.doi.org/10.1002/(sici)1097-4571(199808)49:10\\%3C888::aid-asi5\\%3E3.0.co;2-z",
    "abstract": "In this article, we describe and test a two-stage algorithm based on a lexical collocation technique which maps from the lexical clues contained in a document representation into a controlled vocabulary list of subject headings. Using a collection of 4,626 INSPEC documents, we create a ” dictionary” of associations between the lexical items contained in the titles, authors, and abstracts, and controlled vocabulary subject headings assigned to those records by human indexers using a likelihood ratio statistic as the measure of association. In the deployment stage, we use the dictionary to predict which of the controlled vocabulary subject headings best describe new documents when they are presented to the system. Our evaluation of this algorithm, in which we compare the automatically assigned subject headings to the subject headings assigned to the test documents by human catalogers, shows that we can obtain results comparable to, and consistent with, human cataloging. In effect, we have cast this as a classic partial match information retrieval problem. We consider the problem to be one of ” retrieving” (or assigning) the most probably ” relevant” (or correct) controlled vocabulary subject headings to a document based on the clues contained in that document. © 1998 John Wiley & Sons, Inc.",
    "author": [
      {
        "family": "Plaunt",
        "given": "Christian"
      },
      {
        "family": "Norgard",
        "given": "Barbara A."
      }
    ],
    "container-title": "Journal of the American Society for Information Science and Technology",
    "id": "Plaunt1998",
    "issue": "10",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "ir",
    "page": "888-902",
    "publisher": "Wiley",
    "title": "An association-based method for automatic indexing with a controlled vocabulary",
    "type": "article-journal",
    "volume": "49"
  },
  {
    "DOI": "10.1111/1467-9671.00121",
    "ISSN": "1361-1682",
    "URL": "http://dx.doi.org/10.1111/1467-9671.00121",
    "abstract": "While the presence of uncertainty in the geometric and attribute aspects of geographic information is well known, it is also present in temporal information. In spatiotemporal GIS databases and other formal representations, uncertainty in all three aspects of geography (space, time, and theme) must often be modeled, but a good data model must first be based on a sound theoretical understanding of spatiotemporal uncertainty. The nature of both uncertainty inherent in a phenomenon (often termed indeterminacy) and uncertainty in assertions of that phenomenon can be better understood through the Uncertain Temporal Entity Model, which characterizes the cause, type, and form of uncertainties in the spatial, temporal, and attribute aspects of geographic information. These uncertainties are the result of complexities and problems in two processes: the process of conceptualization, by which humans make sense of an infinitely complex reality, and measurement, by which we create formal representations (e.g. GIS) of those conceptual models of reality. Based on this framework, the nature and form of uncertainty is remarkably consistent across various situations, and is approximately equivalent in the three aspects, which will enable consistent solutions for representation and processing of spatiotemporal data.",
    "author": [
      {
        "family": "Plewe",
        "given": "Brandon"
      }
    ],
    "container-title": "Transactions in GIS",
    "id": "Plewe2002",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "gis, spatio-temporal_annotation, uncertainty",
    "page": "431-456",
    "title": "The nature of uncertainty in historical geographic information",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "URL": "http://www.plone.org",
    "accessed": {
      "date-parts": [
        [
          2000,
          8,
          6
        ]
      ]
    },
    "author": [
      {
        "literal": "Plone Foundation"
      }
    ],
    "id": "Plone",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "title": "The Plone Open Source Content Management System",
    "type": ""
  },
  {
    "URL": "http://aclweb.org/anthology/W11-1508",
    "abstract": "Most existing HLT pipelines assume the input is pure text or, at most, HTML and either ignore (logical) document structure or remove it. We argue that identifying the structure of documents is essential in digital library and other types of applications, and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved.",
    "author": [
      {
        "family": "Poesio",
        "given": "Massimo"
      },
      {
        "family": "Barbu",
        "given": "Eduard"
      },
      {
        "family": "Stemle",
        "given": "Egon"
      },
      {
        "family": "Girardi",
        "given": "Christian"
      }
    ],
    "container-title": "Proceedings of the 5th ACL-HLT workshop on language technology for cultural heritage, social sciences, and humanities (LaTeCH 2011)",
    "id": "Poesio2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, nlp, xml",
    "page": "54-62",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Structure-preserving pipelines for digital libraries",
    "type": "paper-conference"
  },
  {
    "ISBN": "9780521781886",
    "abstract": "Is the world warming due to the Greenhouse Effect? Can nuclear weapon arsenals be relied upon without periodic testing? Is the world running out of oil? What action should be taken against an outbreak of foot-and-mouth or BSE? Why can’t scientists provide certain answers to these and many other questions? The uncertainty of science is puzzling. It arises when scientists have more than one answer to a problem or disagree amongst themselves. In this engaging book, Henry Pollack guides the reader through the maze of contradiction and uncertainty, acquainting them with the ways that uncertainty arises in science, how scientists accommodate and make use of uncertainty, and how in the face of uncertainty they reach their conclusions. Taking examples from recent science headlines and every day life, Uncertain Science … Uncertain World enables the reader to evaluate uncertainty from their own perspectives, and find out more about how science actually works.",
    "author": [
      {
        "family": "Pollack",
        "given": "Henry N."
      }
    ],
    "id": "Pollack2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge",
    "title": "Uncertain science … uncertain world",
    "type": "book"
  },
  {
    "DOI": "10.1108/eb026733",
    "ISSN": "0022-0418",
    "abstract": "Not only does the problem of correcting spelling errors by computer have a long history, it is evidently of considerable current interest as papers and letters on the topic continue to appear rapidly. This is not surprising, since techniques useful in detecting and correcting mis-spellings normally have other important applications. Moreover, both the power of small computers and the routine production of machine-readable text have increased enormously over the last decade to the point where automatic spelling error detection/correction has become not only feasible but highly desirable.",
    "author": [
      {
        "family": "Pollock",
        "given": "Joseph J."
      }
    ],
    "container-title": "Journal of Documentation",
    "id": "Pollock1982",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "keyword": "spelling_correction",
    "page": "282-291",
    "title": "Spelling error detection and correction by computer: Some notes and a bibliography",
    "title-short": "Spelling error detection and correction by computer",
    "type": "article-journal",
    "volume": "38"
  },
  {
    "DOI": "10.1145/358027.358048",
    "ISSN": "0001-0782",
    "abstract": "An automatic spelling correcting algorithm corrects most of the 50,000 misspellings culled from 25,000,000 words of text from seven scientific and scholarly databases. It uses a similarity key to identify words in a large dictionary that are most similar to a particular misspelling, and then an error-reversal test to select from these the most plausible correction(s).",
    "author": [
      {
        "family": "Pollock",
        "given": "Joseph J."
      },
      {
        "family": "Zamora",
        "given": "Antonio"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Pollock1984",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1984
        ]
      ]
    },
    "keyword": "approximate_matching, ir, spelling_correction",
    "page": "358-368",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Automatic spelling correction in scientific and scholarly text",
    "type": "article-journal",
    "volume": "27"
  },
  {
    "URL": "http://jodi.tamu.edu/Articles/v03/i04/Polsani/",
    "abstract": "The term Learning Object, first popularized by Wayne Hodgins in 1994 when he named the CedMA working group \"Learning Architectures, APIs and Learning Objects\", has become the Holy Grail of content creation and aggregation in the computer-mediated learning field. The terms Learning Objects (LOs) and Reusable Learning Objects are frequently employed in uncritical ways, thereby reducing them to mere slogans. The serious lack of conceptual clarity and reflection is evident in the multitude of definitions and uses of LOs. The objectives of this paper are to assess current definitions of the term Learning Object, to articulate the foundational principles for developing a concept of LOs, and to provide a methodology and broad set of guidelines for creating LOs.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Polsani",
        "given": "Pithamber R."
      }
    ],
    "container-title": "Journal of Digital Information",
    "id": "Polsani2003",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "title": "Use and abuse of reusable learning objects",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.1145/1600193.1600237",
    "ISBN": "978-1-60558-575-8",
    "abstract": "In a large-scale book scanning operation, material can vary widely in language, script, genre, domain, print quality, and other factors, giving rise to a corresponding variability in the OCRed text. It is often desirable to automatically detect errorful and otherwise anomalous text segments, so that they can be filtered out or appropriately flagged, for such applications as indexing, mining, analyzing, displaying, and selectively re-processing such data. Moreover, it is advantageous to require that the automated detector be independent of the underlying OCR engine (or engines), that it work over a broad range of languages, that it seamlessly handle mixed-language material, and that it accommodate documents that contain domain-specific and otherwise rare terminology. A technique is presented that satisfies these requirements, using an adaptive mixture of character-level N-gram language models. Its design, training, implementation, and evaluation are described within the context of high-volume book scanning.",
    "author": [
      {
        "family": "Popat",
        "given": "Ashok C."
      }
    ],
    "container-title": "Proceedings of the 9<sup>th</sup> ACM symposium on document engineering (DocEng ’09)",
    "id": "Popat2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr, spelling_correction",
    "page": "201-204",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A panlingual anomalous text detector",
    "type": "paper-conference"
  },
  {
    "URL": "https://aclanthology.org/H/H05/H05-1043",
    "author": [
      {
        "family": "Popescu",
        "given": "Ana-Maria"
      },
      {
        "family": "Etzioni",
        "given": "Oren"
      }
    ],
    "container-title": "Proceedings of human language technology conference and conference on empirical methods in natural language processing",
    "id": "Popescu2005",
    "issued": {
      "date-parts": [
        [
          2005,
          10
        ]
      ]
    },
    "language": "en-US",
    "page": "339-346",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Vancouver, British Columbia, Canada",
    "title": "Extracting product features and opinions from reviews",
    "type": "paper-conference"
  },
  {
    "URL": "https://aclanthology.org/W14-5905",
    "author": [
      {
        "family": "Poria",
        "given": "Soujanya"
      },
      {
        "family": "Cambria",
        "given": "Erik"
      },
      {
        "family": "Ku",
        "given": "Lun-Wei"
      },
      {
        "family": "Gui",
        "given": "Chen"
      },
      {
        "family": "Gelbukh",
        "given": "Alexander"
      }
    ],
    "container-title": "Proceedings of the second workshop on natural language processing for social media (SocialNLP)",
    "id": "Poria2014",
    "issued": {
      "date-parts": [
        [
          2014,
          8
        ]
      ]
    },
    "language": "en-US",
    "page": "28-37",
    "publisher": "Association for Computational Linguistics and Dublin City University",
    "publisher-place": "Dublin, Ireland",
    "title": "A rule-based approach to aspect extraction from product reviews",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-540-78646-7_51",
    "ISBN": "3-540-78645-7, 978-3-540-78645-0",
    "abstract": "This paper introduces CL-ESA, a new multilingual retrieval model for the analysis of cross-language similarity. The retrieval model exploits the multilingual alignment of Wikipedia: [...]",
    "author": [
      {
        "family": "Potthast",
        "given": "Martin"
      },
      {
        "family": "Stein",
        "given": "Benno"
      },
      {
        "family": "Anderka",
        "given": "Maik"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Advances in information retrieval: 30<sup>th</sup> european conference on IR research, ECIR 2008",
    "editor": [
      {
        "family": "Macdonald",
        "given": "Craig"
      },
      {
        "family": "Ounis",
        "given": "Iadh"
      },
      {
        "family": "Plachouras",
        "given": "Vassilis"
      },
      {
        "family": "Ruthven",
        "given": "Ian"
      },
      {
        "family": "White",
        "given": "Ryen W."
      }
    ],
    "id": "Potthast2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ir, wikipedia",
    "page": "522-530",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "A Wikipedia-based multilingual retrieval model",
    "type": "paper-conference",
    "volume": "4956"
  },
  {
    "URL": "http://www.atala.org/Variations-langagieres-et",
    "abstract": "This paper assesses the performance of three taggers (MBT, TnT and TreeTagger) when used for the morphosyntactic annotation of classical Latin texts. With this aim in view, we selected the training corpora, as well as the samples used for tests, from the texts of the LASLA database. The texts were chosen according to their ability to allow testing of the taggers sensitivity to stylistic, diachronic, generic or discursive variations. On the one hand, this research pinpoints the achievements of each tagger according to the various corpora. On the other hand, the paper proves that these taggers can be used as true heuristic instruments and can help to improve significantly the description of the corpus.",
    "author": [
      {
        "family": "Poudat",
        "given": "Céline"
      },
      {
        "family": "Longrée",
        "given": "Dominique"
      }
    ],
    "container-title": "Traitement Automatique des Langues",
    "id": "Poudat2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, latin, nlp, pos_tagging",
    "page": "129-148",
    "title": "Variations langagières et annotation morphosyntaxique du latin classique",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "URL": "https://arxiv.org/abs/cs/0609065v1",
    "abstract": "We are presenting a method to recognise geographical references in free text.Our tool must work on various languages with a minimum of language-dependentresources, except a gazetteer. The main difficulty is to disambiguate theseplace names by distinguishing places from persons and by selecting the mostlikely place out of a list of homographic place names world-wide. The systemuses a number of language-independent clues and heuristics to disambiguateplace name homographs. The final aim is to index texts with the countries andcities they mention and to automatically visualise this information ongeographical maps using various tools.",
    "author": [
      {
        "family": "Pouliquen",
        "given": "Bruno"
      },
      {
        "family": "Kimler",
        "given": "Marco"
      },
      {
        "family": "Steinberger",
        "given": "Ralf"
      },
      {
        "family": "Ignat",
        "given": "Camelia"
      },
      {
        "family": "Oellinger",
        "given": "Tamara"
      },
      {
        "family": "Blackler",
        "given": "Ken"
      },
      {
        "family": "Fuart",
        "given": "Flavio"
      },
      {
        "family": "Zaghouani",
        "given": "Wajdi"
      },
      {
        "family": "Widiger",
        "given": "Anna"
      },
      {
        "family": "Forslund",
        "given": "Ann-Charlotte"
      },
      {
        "family": "Best",
        "given": "Clive"
      }
    ],
    "container-title": "Proceedings of the 5<sup>th</sup> international conference on language resources and evaluation (LREC 2006)",
    "id": "Pouliquen2006",
    "issued": {
      "date-parts": [
        [
          2006,
          9
        ]
      ]
    },
    "keyword": "geocoding, nlp",
    "page": "53-58",
    "title": "Geocoding multilingual texts: Recognition, disambiguation and visualisation",
    "title-short": "Geocoding multilingual texts",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1016/j.ijar.2012.08.004",
    "ISSN": "0888613X",
    "abstract": "This paper develops a new uncertainty measure for the theory of hints that complies with the established semantics of statistical information theory and further satisfies all classical requirements for such a measure imposed in the literature. The proposed functional decomposes into conversant uncertainty measures and therefore discloses a new interpretation of the latters as well. By abstracting to equivalence classes of hints we transport the new measure to mass functions in Dempster-Shafer theory and analyse its relationship with the aggregate uncertainty, which currently is the only known functional for the Dempster-Shafer theory of evidence that satisfies the same set of properties. Moreover, the perspective of hints reveals that the standard independence notion in Dempster-Shafer theory called non-interactivity corresponds to an amalgamation of probabilistic independence and qualitative independence between frames of discernment. All results in this paper are developed for arbitrary families of compatible frames generalizing the very specialized multi-variate systems that are usually studied in information theory.",
    "author": [
      {
        "family": "Pouly",
        "given": "Marc"
      },
      {
        "family": "Kohlas",
        "given": "Jürg"
      },
      {
        "family": "Ryan",
        "given": "Peter Y. A."
      }
    ],
    "container-title": "International Journal of Approximate Reasoning",
    "id": "Pouly2013",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "228-251",
    "title": "Generalized information theory for hints",
    "type": "article-journal",
    "volume": "54"
  },
  {
    "URL": "http://www.xs4all.nl/\\~{}ajwp/langident.pdf",
    "abstract": "In this paper, we identify two major stages in language identification systems: the language modeling stage, where the distinctive features of languages are learned and stored in models, and the classification stage, in which a model is formed of the (partial) input document, and this model is compared to the language models. The language model most similar to the input document represents the language of the document. We describe all major modeling and classification techniques known in literature, and identify one disadvantage in them: the need to create a model of the entire document, even though the language can be identified with a small number of features. To avoid this, we introduce a new language identification technique that is based on Monte Carlo sampling. We show that, by determining the language of a large enough number of random features, we can determine the document language to be the language which result most often from these features. Whether the amount of samples is sufficiently large can be determined by calculating the standard error of the samples. Finally, we discuss some pilot experiments where we compare this new technique with others.",
    "author": [
      {
        "family": "Poutsma",
        "given": "Arjen"
      }
    ],
    "collection-title": "Language and computers: Studies in practical linguistics",
    "container-title": "Computational linguistics in the netherlands 2001. Selected papers from the twelfth CLIN meeting",
    "editor": [
      {
        "family": "Theune",
        "given": "Mariet"
      },
      {
        "family": "Nijholt",
        "given": "Anton"
      },
      {
        "family": "Hondorp",
        "given": "Hendri"
      }
    ],
    "id": "Poutsma2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "page": "179-189",
    "publisher": "Rodopi",
    "publisher-place": "Amsterdam/New York",
    "title": "Applying Monte Carlo techniques to language identification",
    "type": "paper-conference",
    "volume": "45"
  },
  {
    "author": [
      {
        "family": "Pouyioutas",
        "given": "Philippos"
      },
      {
        "family": "Poveda",
        "given": "Maria"
      },
      {
        "family": "Kalogerou",
        "given": "Victoria"
      },
      {
        "family": "Apraxine",
        "given": "Dmitri"
      }
    ],
    "container-title": "Proceedings of the 4<sup>th</sup> IASTED international conference on Web-based education (WBE 2005), february 21–23, 2005, grindelwald, switzerland",
    "id": "Pouyioutas2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "page": "436-441",
    "publisher": "IASTED; ACTA Press",
    "title": "The Intertest multiple-choice Web-based software",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1045/march2009-powell",
    "author": [
      {
        "family": "Powell",
        "given": "Tracy"
      },
      {
        "family": "Paynter",
        "given": "Gordon"
      }
    ],
    "container-title": "D-Lib Magazine",
    "id": "Powell2009",
    "issue": "3/4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "title": "Going grey? Comparing the OCR accuracy levels of bitonal and greyscale images",
    "type": "article-journal",
    "volume": "15"
  },
  {
    "DOI": "10.1007/bfb0000116",
    "ISBN": "3-540-12310-5",
    "abstract": "In H-graph semantics, graph grammars play a central role as the mechanism for definition of the data types used in a program or programming language. This paper provides an introduction to H-graph semantics, with an emphasis on the role of graph grammars in the semantic models.",
    "author": [
      {
        "family": "Pratt",
        "given": "Terrence W."
      }
    ],
    "chapter-number": "22",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Graph-grammars and their application to computer science",
    "editor": [
      {
        "family": "Ehrig",
        "given": "Hartmut"
      },
      {
        "family": "Nagl",
        "given": "Manfred"
      },
      {
        "family": "Rozenberg",
        "given": "Grzegorz"
      }
    ],
    "id": "Pratt1983",
    "issued": {
      "date-parts": [
        [
          1983
        ]
      ]
    },
    "keyword": "graphs",
    "language": "en-US",
    "page": "314-332",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Formal specification of software using H-graph semantics",
    "type": "chapter",
    "volume": "153"
  },
  {
    "URL": "http://halshs.archives-ouvertes.fr/halshs-00087747/fr/",
    "abstract": "Les linguistes qui s’intéressent à des états de langue contemporains peuvent choisir de recourir ou non au « corpus », comme source principale de leurs données ou comme complément de leur intuition de locuteur ou de celle de leurs informateurs. Les linguistes qui travaillent sur des états de langues disparus, en l’occurrence le français médiéval, n’ont pas ce choix : le corpus est indispensable, puisqu’il conditionne l’existence même de l’objet à décrire. Cette situation n’est pas nouvelle, mais elle n’a pas pour autant suscité, de manière précoce, un questionnement véritable de l’utilisation du corpus : la réflexion liée à la langue ancienne s’est esquissée dans la mouvance de la réflexion générale, née de l’intérêt croissant que la linguistique descriptive a porté, depuis une bonne vingtaine d’années, au travail sur les corpus (désormais numérisés). Elle a cependant suivi en partie sa propre voie, car travailler sur le français médiéval soulève des difficultés proprement liées à cet objet d’étude. Le présent chapitre se penche donc sur les précautions, les contraintes et les spécificités liées à la constitution et à l’exploitation d’un corpus de français médiéval dans le cadre d’une étude de linguistique historique, que ce soit dans une perspective synchronique ou diachronique, tout en envisageant, à l’aide de quelques exemples, en quoi et comment le travail sur corpus permet d’enrichir – voire de modifier – la connaissance d’un état de langue dont nous n’avons pas la compétence.",
    "author": [
      {
        "family": "Prévost",
        "given": "Sophie"
      }
    ],
    "chapter-number": "4",
    "container-title": "Sémantique et corpus",
    "editor": [
      {
        "family": "Condamines",
        "given": "Anne"
      }
    ],
    "id": "Prevost2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, french",
    "page": "147-176",
    "publisher": "Hermes",
    "publisher-place": "London, UK",
    "title": "Constitution et exploitation d’un corpus de français médiéval : Enjeux, spécificités et apports",
    "title-short": "Constitution et exploitation d’un corpus de français médiéval ",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/276675.276694",
    "ISBN": "0897919653",
    "abstract": "The International Standard for the Standard Generalized Markup Language (SGML) published in 1986 is now seen as a mature language for expressing document structure and is accepted as the basis for major projects such as the Text Encoding Initiative and important hypertext languages such as HTML and XML. The historical origin of SGML as a technique for adding marks to texts has left a legacy of complexities and difficulties which hinder its wide acceptance. A key difficulty is the dual role that SGML documents currently play: they are both a representation for interchange and a human readable presentation. We examine possible document markup techniques in a post-SGML 86 world with emphasis on the framework architecture. The novel ideas include the generalization of the notion of a ” character” to a much broader token which is strongly typed to differentiate text, markup, images and other component types.",
    "author": [
      {
        "family": "Price",
        "given": "Roger"
      }
    ],
    "container-title": "Proceedings of the Third ACM Conference on Digital Libraries (DL ’98)",
    "id": "Price1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "markup, sgml",
    "language": "en-US",
    "page": "172-181",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Beyond SGML",
    "type": "paper-conference"
  },
  {
    "DOI": "10.3406/hism.1989.881",
    "ISSN": "0982-1783",
    "URL": "http://dx.doi.org/10.3406/hism.1989.881",
    "abstract": "The study of the population of Orléans in 1911 has been made on the basis of sampling, according to the density of each area. The geographical origin of the population, the social integration, the social differentiation of the areas and the family structures are studied.",
    "author": [
      {
        "family": "Prost",
        "given": "Antoine"
      }
    ],
    "container-title": "Histoire & Mesure",
    "id": "Prost1989",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "digital_humanities, history",
    "language": "fr-FR",
    "page": "121-146",
    "title": "La population d’Orléans en 1911 : une enquête d’histoire sociale informatisée",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "URL": "http://prost.jeanphilippe.free.fr/bib/prostLafourcade2011-cslp.pdf",
    "abstract": "In this paper we investigate the possibility of a syntax-semantics inferface between a framework for Model-Theoretic Syntax on one hand and a semantic network on the other hand. We focus on exploring the ability of such a pairing to solve a collection of grammar checking problems, with an emphasis on cases of missing words. We discuss a solution where constraint violations are interpreted as grammar errors and yield the re-generation of new candidate parses (partially unrealised) through tree operations. Follows a surface realisation phase, where missing words are filled through semantic network exploration.",
    "author": [
      {
        "family": "Prost",
        "given": "Jean-Philippe"
      },
      {
        "family": "Lafourcade",
        "given": "Mathieu"
      }
    ],
    "container-title": "Proceedings of the 6<sup>th</sup> international workshop on constraints and language processing (CSLP@context’11)",
    "id": "Prost2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "authoring, computational_linguistics, french, grammar_checking, parsing",
    "title": "Pairing Model-Theoretic syntax and semantic network for writing assistance",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Provos",
        "given": "Niels"
      }
    ],
    "container-title": "Proceedings of the 12<sup>th</sup> USENIX security symposium, washington, DC",
    "id": "Provos2003",
    "issued": {
      "date-parts": [
        [
          2003,
          8
        ]
      ]
    },
    "title": "Improving host security with system call policies",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.aclweb.org/anthology-new/W04/W04-0312.bib",
    "abstract": "Standard grammar formalisms are defined without reflection of the incremental and serial nature of language processing, and incrementality must therefore be reflected by independently defined parsing and/or generation techniques. We argue that this leads to a poor setup for modelling dialogue, with its rich speaker-hearer interaction, and instead propose context-based parsing and generation models defined in terms of an inherently incremental grammar formalism (Dynamic Syntax), which allow a straightforward model of otherwise problematic dialogue phenomena such as shared utterances, ellipsis and alignment.",
    "author": [
      {
        "family": "Purver",
        "given": "Matthew"
      },
      {
        "family": "Kempson",
        "given": "Ruth"
      }
    ],
    "container-title": "Proceedings of the ACL workshop incremental parsing: Bringing engineering and cognition together",
    "editor": [
      {
        "family": "Keller",
        "given": "Frank"
      },
      {
        "family": "Clark",
        "given": "Stephen"
      },
      {
        "family": "Crocker",
        "given": "Matthew"
      },
      {
        "family": "Steedman",
        "given": "Mark"
      }
    ],
    "id": "Purver2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "interactive_parsing",
    "page": "74-81",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Barcelona, Spain",
    "title": "Incremental parsing, or incremental grammar?",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2010/pdf/55_Paper.pdf",
    "abstract": "In this paper, we present ISO-TimeML, a revised and interoperable version of the temporal markup language, TimeML. We describe the changes and enrichments made, while framing the effort in a more general methodology of semantic annotation. In particular, we assume a principled distinction between the annotation of an expression and the representation which that annotation denotes. This involves not only the specification of an annotation language for a particular phenomenon, but also the development of a meta-model that allows one to interpret the syntactic expressions of the specification semantically.",
    "author": [
      {
        "family": "Pustejovsky",
        "given": "James"
      },
      {
        "family": "Lee",
        "given": "Kiyong"
      },
      {
        "family": "Bunt",
        "given": "Harry"
      },
      {
        "family": "Romary",
        "given": "Laurent"
      }
    ],
    "container-title": "Proceedings of the seventh international conference on language resources and evaluation (LREC’10)",
    "editor": [
      {
        "family": "Calzolari",
        "given": "Nicoletta"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Maegaard",
        "given": "Bente"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Odijk",
        "given": "Jan"
      },
      {
        "family": "Piperidis",
        "given": "Stelios"
      },
      {
        "family": "Rosner",
        "given": "Mike"
      },
      {
        "family": "Tapias",
        "given": "Daniel"
      }
    ],
    "id": "Pustejovsky2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "nlp, spatio-temporal_annotation, xml",
    "publisher": "European Language Resources Association (ELRA)",
    "publisher-place": "Paris",
    "title": "ISO-TimeML: An international standard for semantic annotation",
    "title-short": "ISO-TimeML",
    "type": "paper-conference"
  },
  {
    "id": "Python",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "title": "Python programming language, http://www.python.org",
    "title-short": "Python programming language, http",
    "type": ""
  },
  {
    "DOI": "10.1145/1989323.1989431",
    "ISBN": "978-1-4503-0661-4",
    "URL": "http://dx.doi.org/10.1145/1989323.1989431",
    "abstract": "Given a query string Q, an edit similarity search finds all strings in a database whose edit distance with Q is no more than a given threshold t. Most existing method answering edit similarity queries rely on a signature scheme to generate candidates given the query string. We observe that the number of signatures generated by existing methods is far greater than the lower bound, and this results in high query time and index space complexities. In this paper, we show that the minimum signature size lower bound is t +1. We then propose asymmetric signature schemes that achieve this lower bound. We develop efficient query processing algorithms based on the new scheme. Several dynamic programming-based candidate pruning methods are also developed to further speed up the performance. We have conducted a comprehensive experimental study involving nine state-of-the-art algorithms. The experiment results clearly demonstrate the efficiency of our methods.",
    "author": [
      {
        "family": "Qin",
        "given": "Jianbin"
      },
      {
        "family": "Wang",
        "given": "Wei"
      },
      {
        "family": "Lu",
        "given": "Yifei"
      },
      {
        "family": "Xiao",
        "given": "Chuan"
      },
      {
        "family": "Lin",
        "given": "Xuemin"
      }
    ],
    "container-title": "Proceedings of the 2011 international conference on management of data (SIGMOD ’11)",
    "id": "Qin2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "approximate_matching",
    "page": "1033-1044",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Efficient exact edit similarity query processing with the asymmetric signature scheme",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-319-49046-5\\_6",
    "URL": "http://dx.doi.org/10.1007/978-3-319-49046-5\\_6",
    "abstract": "A new theory of modeling the uncertainty associated with vague concepts is introduced. We consider the problem of quantifying an agents uncertainty concerning which labels are appropriate to describe a given observation. This can be regarded as a simplified model of natural language communication. Semantic meaning conveyed by high-level knowledge representation is often inherently uncertain. Such uncertainty is referred to semantic uncertainty and dominated by fuzzy modeling. In this framework, from an epistemic point of view, labels are precise and uncertainty comes from the undecidable boundary between labels in agents conceptual space. In this framework the boundary is regarded as a random variable and it can be modeled by a probability distribution. We also propose a functional calculus to measure how appropriate of using a certain label to describe an observation. In this way, a vague concept can be represented by a distribution on the labels. The new theory is verified by applying it to the vague category game.",
    "author": [
      {
        "family": "Qin",
        "given": "Zengchang"
      },
      {
        "family": "Wan",
        "given": "Tao"
      },
      {
        "family": "Zhao",
        "given": "Hanqing"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Integrated uncertainty in knowledge modelling and decision making (proceedings of IUKM 2016)",
    "editor": [
      {
        "family": "Huynh",
        "given": "Van-Nam"
      },
      {
        "family": "Inuiguchi",
        "given": "Masahiro"
      },
      {
        "family": "Le",
        "given": "Bac"
      },
      {
        "family": "Le",
        "given": "Bao N."
      },
      {
        "family": "Denoeux",
        "given": "Thierry"
      }
    ],
    "id": "Qin2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "uncertainty",
    "page": "64-75",
    "publisher": "Springer",
    "publisher-place": "Cham",
    "title": "A theory of modeling semantic uncertainty in label representation",
    "type": "paper-conference",
    "volume": "9978"
  },
  {
    "URL": "http://www.iiisci.org/Journal/SCI/Abstract.asp?var=;id=P371402",
    "abstract": "Message-based Web Service architecture provides a unified approach to applications and Web Services that incorporates the flexibility of messaging and distributed components. We propose SMMV and MMMV collaboration as the general architecture of collaboration based on a Web service model, which accommodates both instructor-led learning and participatory learning. This approach derives from our message-based Model-View-Controller (M-MVC) architecture of Web applications, comprises an event-driven Publish/Subscribe scheme, and provides effective collaboration with high interactivity of rich Web content for diverse clients over heterogeneous network environments.",
    "author": [
      {
        "family": "Qiu",
        "given": "Xiaohong"
      },
      {
        "family": "Jooloor",
        "given": "Anumit"
      }
    ],
    "container-title": "Journal of Systemics, Cybernetics and Informatics",
    "id": "Qiu2006",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "e-learning, soa",
    "page": "92-101",
    "title": "Web service architecture for e-Learning",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "id": "Qtilite",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "title": "IMS Question & Test Interoperability – QTILite specification, http://www.imsglobal.org/question/qtiv1p2/",
    "title-short": "IMS Question & Test Interoperability – QTILite specification, http",
    "type": ""
  },
  {
    "URL": "http://hal.inria.fr/inria-00075314/en/",
    "abstract": "This article compares methods and techniques used in software engineering with the ones used for handling electronic documents. It shows the common features in both domains, but also the differences and it proposes an approach which extends the field of document manipulation to document engineering. It shows also in what respect document engineering is different from software engineering. Therefore specific techniques must be developed for building integrated environments for document engineering.",
    "author": [
      {
        "family": "Quint",
        "given": "Vincent"
      },
      {
        "family": "Nanard",
        "given": "Marc"
      },
      {
        "family": "André",
        "given": "Jacques"
      }
    ],
    "container-title": "EP 90: Proceedings of the international conference on electronic publishing, document manipulation & typography",
    "editor": [
      {
        "family": "Furuta",
        "given": "Richard"
      }
    ],
    "id": "Quint1990",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "document_research",
    "page": "17-29",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge, UK",
    "title": "Towards document engineering",
    "type": "paper-conference"
  },
  {
    "URL": "http://hal.inria.fr/inria-00075314/en/",
    "abstract": "This article compares methods and techniques used in software engineering with the ones used for handling electronic documents. It shows the common features in both domains, but also the differences and it proposes an approach which extends the field of document manipulation to document engineering. It shows also in what respect document engineering is different from software engineering. Therefore specific techniques must be developped for building integrated environments for document engineering.",
    "accessed": {
      "date-parts": [
        [
          2009,
          3,
          2
        ]
      ]
    },
    "author": [
      {
        "family": "Quint",
        "given": "Vincent"
      },
      {
        "family": "Nanard",
        "given": "Marc"
      },
      {
        "family": "André",
        "given": "Jacques"
      }
    ],
    "genre": "Research Report",
    "id": "Quint1990a",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "language": "en-US",
    "number": "RR-1244",
    "publisher": "INRIA",
    "title": "Towards document engineering",
    "type": "report"
  },
  {
    "DOI": "10.1145/1030397.1030422",
    "URL": "http://dx.doi.org/10.1145/1030397.1030422",
    "abstract": "This paper reviews the main innovations of XML and considers their impact on the editing techniques for structured documents. Namespaces open the way to compound documents; well-formedness brings more freedom in the editing task; CSS allows style to be associated easily with structured documents. In addition to these innovative features, the wide deployment of XML introduces structured documents in many new applications, including applications where text is not the dominant content type. In languages such as SVG or SMIL, for instance, XML is used to represent vector graphics or multimedia presentations. This is a challenging situation for authoring tools. Traditional methods for editing structured documents are not sufficient to address the new requirements. New techniques must be developed or adapted to allow more users to efficiently create advanced XML documents. These techniques include multiple views, semantic-driven editing, direct manipulation, concurrent manipulation of style and structure, and integrated multi-language editing. They have been implemented and experimented in the Amaya editor and in some other tools.",
    "author": [
      {
        "family": "Quint",
        "given": "Vincent"
      },
      {
        "family": "Vatton",
        "given": "Irène"
      }
    ],
    "collection-title": "DocEng ’04",
    "container-title": "Proceedings of the 2004 ACM symposium on document engineering",
    "id": "Quint2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "authoring, markup, xml",
    "page": "115-123",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Techniques for authoring complex XML documents",
    "type": "paper-conference"
  },
  {
    "URL": "http://www3.iath.virginia.edu/wnr4c/Raabe.Era.UTC.Diss.pdf",
    "author": [
      {
        "family": "Raabe",
        "given": "Wesley N."
      }
    ],
    "genre": "PhD thesis",
    "id": "Raabe2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "digital_edition, digital_humanities",
    "publisher": "University of Virginia",
    "publisher-place": "Charlottesville, VA, USA",
    "title": "Harriet beecher stowe’s <i>uncle tom’s cabin</i>: An electronic edition of the <i>national era</i> version",
    "title-short": "Harriet beecher stowe’s <i>uncle tom’s cabin</i>",
    "type": "thesis"
  },
  {
    "URL": "http://www.w3.org/MarkUp/htmlplus_paper/htmlplus.html",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Raggett",
        "given": "Dave"
      }
    ],
    "id": "Raggett1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "publisher": "World Wide Web Consortium",
    "title": "A review of the HTML+ document format",
    "type": "report"
  },
  {
    "URL": "http://ismir2007.ismir.net/proceedings/ISMIR2007_p417_raimond.pdf",
    "abstract": "In this paper, we overview some Semantic Web technologies and describe the Music Ontology: a formal framework for dealing with music-related information on the Semantic Web, including editorial, cultural and acoustic information. We detail how this ontology can act as a grounding for more domain-specific knowledge representation. In addition, we describe current projects involving the Music Ontology and interlinked repositories of music-related knowledge.",
    "author": [
      {
        "family": "Raimond",
        "given": "Yves"
      },
      {
        "family": "Abdallah",
        "given": "Samer"
      },
      {
        "family": "Sandler",
        "given": "Mark"
      },
      {
        "family": "Giasson",
        "given": "Frederick"
      }
    ],
    "container-title": "Proceedings of the 8<sup>th</sup> international conference on music information retrieval",
    "editor": [
      {
        "family": "Dixon",
        "given": "Simon"
      },
      {
        "family": "Bainbridge",
        "given": "David"
      },
      {
        "family": "Typke",
        "given": "Rainer"
      }
    ],
    "id": "Raimond2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "music, ontologies, semantic_web",
    "language": "en-US",
    "page": "417-422",
    "title": "The Music Ontology",
    "type": "paper-conference"
  },
  {
    "DOI": "10.7202/1005505ar",
    "ISSN": "1705-8546",
    "URL": "http://dx.doi.org/10.7202/1005505ar",
    "abstract": "The concept of intermediality has come to be part of a fixed critical inventory in the debate on literature and the other arts and media. Although there is relative agreement with respect to a definition of intermediality in a broad sense, the research spectrum becomes much more complex, and often contradictory, when precise formal distinctions and the specification and definition of any one particular concept of intermediality are needed. Based on the current state of the question, the present essay specifies one particular approach to intermediality, introducing three more narrowly conceived subcategories, medial transposition, media combination, and intermedial references. By comparison with the genealogical ” remediation” concept of Bolter and Grusin, it is shown with respect to which objects and specific research objectives this subdivided concept gains heuristic and practical value. This is particularly the case when detailed analyses of specific medial configurations, their respective meaning-constitutional strategies, and their overall signification are considered.",
    "author": [
      {
        "family": "Rajewsky",
        "given": "Irina O."
      }
    ],
    "container-title": "Intermédialités: Histoire et théorie des arts, des lettres et des techniques",
    "id": "Rajewsky2005",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "intertextuality",
    "page": "43-64",
    "title": "Intermediality, intertextuality, and remediation: A literary perspective on intermediality",
    "title-short": "Intermediality, intertextuality, and remediation",
    "type": "article-journal"
  },
  {
    "DOI": "10.1109/DIAL.2006.2",
    "ISBN": "0-7695-2531-8",
    "abstract": "In this paper, we describe how meta-data of indexation can be extracted from historical document images using an interactive process with a software called AGORA. The algorithms involved in AGORA use two maps to segment noisy images: a shape map that focuses on connected components and a background map that provides information on white areas corresponding to block separations in the page. Using a first segmentation result obtained by using these two maps, meta-data can be extracted according to scenarios produced by the users. These scenarios are defined very simply during an interactive stage. The user is able to make processing sequences adapted to the different kinds of images he is likely to meet and according to the desired meta-data. Finally, we describe different experimentations that have been done during the BVH project to test the usability and the performances of AGORA software.",
    "author": [
      {
        "family": "Ramel",
        "given": "Jean-Yves"
      },
      {
        "family": "Busson",
        "given": "Sébastien"
      },
      {
        "family": "Demonet",
        "given": "Marie-Luce"
      }
    ],
    "container-title": "Second International Workshop on Document Image Analysis for Libraries (DIAL 2006)",
    "id": "Ramel2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage, document_analysis",
    "page": "145-155",
    "publisher": "IEEE Computer Society",
    "publisher-place": "Los Alamitos, CA, USA",
    "title": "AGORA: The interactive document image analysis tool of the BVH project",
    "title-short": "AGORA",
    "type": "article-journal"
  },
  {
    "DOI": "10.1145/1860559.1860607",
    "ISBN": "978-1-4503-0231-9",
    "abstract": "The amount of digitized legacy documents has been rising dramatically over the last years due mainly to the increasing number of on-line digital libraries publishing this kind of documents, waiting to be classified and finally transcribed into a textual electronic format (such as ASCII or PDF). Nevertheless, most of the available fully-automatic applications addressing this task are far from being perfect and heavy and inefficient human intervention is often required to check and correct the results of such systems. In contrast, multimodal interactive-predictive approaches may allow the users to participate in the process helping the system to improve the overall performance. With this in mind, two sets of recent advances are introduced in this work: a novel interactive method for text block detection and two multimodal interactive handwritten text transcription systems which use active learning and interactive-predictive technologies in the recognition process.",
    "author": [
      {
        "family": "Terrades",
        "given": "Oriol R."
      },
      {
        "family": "Toselli",
        "given": "Alejandro H."
      },
      {
        "family": "Serrano",
        "given": "Nicolas"
      },
      {
        "family": "Romero",
        "given": "Verónica"
      },
      {
        "family": "Vidal",
        "given": "Enrique"
      },
      {
        "family": "Juan",
        "given": "Alfons"
      }
    ],
    "collection-title": "DocEng ’10",
    "container-title": "Proceedings of the 10th ACM symposium on document engineering",
    "id": "Ramos2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "page": "219-222",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Interactive layout analysis and transcription systems for historic handwritten documents",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Ramsay",
        "given": "Stephen"
      }
    ],
    "chapter-number": "13",
    "container-title": "Defining digital humanities",
    "editor": [
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Vanhoutte",
        "given": "Edward"
      }
    ],
    "id": "Ramsay2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "239-241",
    "publisher": "Ashgate",
    "publisher-place": "Farnham",
    "title": "Who’s in and who’s out",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Ramsay",
        "given": "Stephen"
      }
    ],
    "chapter-number": "14",
    "container-title": "Defining digital humanities",
    "editor": [
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Vanhoutte",
        "given": "Edward"
      }
    ],
    "id": "Ramsay2013a",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "243-245",
    "publisher": "Ashgate",
    "publisher-place": "Farnham",
    "title": "On building",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Ramsden",
        "given": "Paul"
      }
    ],
    "edition": "2",
    "id": "Ramsden2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "publisher": "Routeledge",
    "publisher-place": "London",
    "title": "Learning to teach in higher education",
    "type": "book"
  },
  {
    "URL": "http://www.ecti-thailand.org/assets/papers/111_pub_6.pdf",
    "abstract": "Identifying the language of an unknown text is not a new problem but what is new is the task of identifying close languages. Malay and Indonesian as many other languages are very similar, and therefore it is a real difficulty to search, retrieve, classify,and above all translate texts written in one of the two languages. We have built a language identifier to determine whether the text is written in Malay or Indonesian which could be used in any similar situation. It uses the frequency and rank of trigrams of characters, the lists of exclusive words, and the format of numbers. The trigrams are derived from the most frequent words in each language. The current program contains as language models: Malay/Indonesian (661 trigrams), Dutch (826 trigrams), English (652 trigrams), French (579 trigrams), and German (482 trigrams). The trigrams of an unknown text are searched in each language model. The language of the input text is the language having the highest ratio in \"number of shared trigrams / total number of trigrams\" and \"number of winner trigrams / number of shared trigrams\". If the language found at trigram search level is ’Malay or Indonesian’, the text is then scanned by searching the format of numbers and of some exclusive words.",
    "author": [
      {
        "family": "Ranaivo-Malancon",
        "given": "Bali"
      }
    ],
    "container-title": "ECTI Transactions on Computer and Information Technology",
    "id": "Ranaivo2006",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2006,
          11
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "page": "126-134",
    "title": "Automatic identification of close languages – case study: Malay and indonesian",
    "title-short": "Automatic identification of close languages – case study",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.1515/9783110231144.201",
    "author": [
      {
        "family": "Rapp",
        "given": "Andrea"
      }
    ],
    "collection-title": "Beihefte zu editio",
    "container-title": "Digitale edition zwischen experiment und standardisierung",
    "editor": [
      {
        "family": "Stadler",
        "given": "Peter"
      },
      {
        "family": "Veit",
        "given": "Joachim"
      }
    ],
    "id": "Rapp2009",
    "issued": {
      "date-parts": [
        [
          2010,
          7
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_edition",
    "page": "201-206",
    "publisher": "Walter de Gruyter",
    "title": "Einige Anmerkungen zu Retrodigitalisierungs-Verfahren und Perspektiven digitaler Briefeditionen",
    "type": "chapter",
    "volume": "31"
  },
  {
    "DOI": "10.1145/1008992.1009056",
    "abstract": "Many museum and library archives are digitizing their large collections of handwritten historical manuscripts to enable public access to them. These collections are only available in image formats and require expensive manual annotation work for access to them. Current handwriting recognizers have word error rates in excess of 50% and therefore cannot be used for such material. We describe two statistical models for retrieval in large collections of handwritten manuscripts given a text query. Both use a set of transcribed page images to learn a joint probability distribution between features computed from word images and their transcriptions. The models can then be used to retrieve unlabeled images of handwritten documents given a text query. We show experiments with a training set of 100 transcribed pages and a test set of 987 handwritten page images from the George Washington collection. Experiments show that the precision at 20 documents is about 0.4 to 0.5 depending on the model. To the best of our knowledge, this is the first automatic retrieval system for historical manuscripts using text queries, without manual transcription of the original corpus.",
    "author": [
      {
        "family": "Rath",
        "given": "Toni M."
      },
      {
        "family": "Manmatha",
        "given": "R."
      },
      {
        "family": "Lavrenko",
        "given": "Victor"
      }
    ],
    "container-title": "SIGIR ’04: Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval",
    "id": "Rath2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "cultural_heritage, ir",
    "page": "369-376",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A search engine for historical manuscript images",
    "type": "paper-conference"
  },
  {
    "URL": "http://jstor.org/stable/4621727",
    "abstract": "The force of uncertainty is central to every major research tradition in the study of international relations. Yet uncertainty has multiple meanings, and each paradigm has a somewhat unique understanding of it. More often than not, these meanings are implicit. I argue that realists define uncertainty as fear induced by anarchy and the possibility of predation; rationalists as ignorance (in a nonpejorative sense) endemic to bargaining games of incomplete information and enforcement; cognitivists as the confusion (again nonpejoratively) of decision making in a complex international environment; and constructivists as the indeterminacy of a largely socially constructed world that lacks meaning without norms and identities. I demonstrate how these different understandings are what provide the necessary microfoundations for the paradigms’ definitions of learning, their contrasting expectations about signaling,and the functions provided by international organizations. This has conceptual, methodological, and theoretical payoffs. Understanding uncertainty is necessary for grasping the logic of each paradigm, for distinguishing them from each other, and promoting interparadigmatic communication.",
    "author": [
      {
        "family": "Rathbun",
        "given": "Brian C."
      }
    ],
    "container-title": "International Studies Quarterly",
    "id": "Rathbun2007",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "533-557",
    "title": "Uncertain about uncertainty: Understanding the multiple meanings of a crucial concept in international relations theory",
    "title-short": "Uncertain about uncertainty",
    "type": "article-journal",
    "volume": "51"
  },
  {
    "author": [
      {
        "family": "Ratinov",
        "given": "Lev"
      },
      {
        "family": "Roth",
        "given": "Dan"
      }
    ],
    "container-title": "CoNLL ’09: Proceedings of the thirteenth conference on computational natural language learning",
    "id": "Ratinov2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "page": "147-155",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Morristown, NJ, USA",
    "title": "Design challenges and misconceptions in named entity recognition",
    "type": "paper-conference"
  },
  {
    "DOI": "10.3115/1119394.1119402",
    "abstract": "We describe a purely confidence-based geographic term disambiguation system that crucially relies on the notion of \"positive\" and \"negative\" context and methods for combining confidence-based disambiguation with measures of relevance to a user’s query.",
    "author": [
      {
        "family": "Rauch",
        "given": "Erik"
      },
      {
        "family": "Bukatin",
        "given": "Michael"
      },
      {
        "family": "Baker",
        "given": "Kenneth"
      }
    ],
    "container-title": "Proceedings of the HLT-NAACL 2003 workshop on analysis of geographic references",
    "id": "Rauch2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "toponym_resolution",
    "page": "50-54",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Morristown, NJ, USA",
    "title": "A confidence-based framework for disambiguating geographic terms",
    "type": "paper-conference"
  },
  {
    "abstract": "We describe some of the implications of markup for document management systems. Markup’s properties are inherited from text, since it is embedded in text. These properties are most advantageous when document structure is reducible to substrings of characters, and when the update characteristics of the structure are similar to the update characteristics of the text. We describe situations in which these characteristics are disadvantageous. Markup is not a data model, but one of several possible techniques for representing structure. For this reason it should not be the foundation of document management systems.",
    "author": [
      {
        "family": "Raymond",
        "given": "Darrell"
      },
      {
        "family": "Tompa",
        "given": "Frank"
      },
      {
        "family": "Wood",
        "given": "Derrick"
      }
    ],
    "genre": "Technical Report",
    "id": "Raymond1993",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "document_engineering, markup, sgml",
    "note": "Presented at the First International Workshop on the Principles of Document Processing, Washinton DC, October 21–23 1992; an earlier version was circulated privately as  in the late 1980s.",
    "number": "356",
    "publisher": "Department of Computer Science, The University of Western Ontario",
    "title": "Markup reconsidered",
    "type": "report"
  },
  {
    "DOI": "10.1016/0920-5489(96)00033-5",
    "abstract": "The ISO SGML standard for document markup has had a tremendous impact on the electronic document community’s understanding of its data. The notion of a public, common representation for text suggests a level of data sharing and systems interoperability that was previously unknown. By focusing attention on the structure of a document rather than its appearance, SGML popularized a new approach to document management, one that treats documents as databases, rather than artifacts whose sole function is to be displayed. By shifting documents from display-centered media to database data, however, SGML has raised new problems that the standard itself was not intended to solve. These problems arise because a printed document is fundamentally different from a database. A printed document is essentially a closed world with a single product. A database, on the other hand, is an open world, with many potential products, and most importantly, with the possibility of change. In this paper we neither appraise the semantics of previous proposals, nor introduce our own. Instead, we discuss some issues that are “metasemantic”: issues that lie behind every semantics, but that are peculiar to none. We discuss three issues–equivalence, redundancy, and operations–that are implicit in current systems and approaches. By making these issues explicit, we hope to provide the beginnings of a framework for comparison of the semantics of existing systems, and the development of more advanced ones. Before considering meta-semantic issues, it will be useful to briefly recount the differing experiences of the document and data processing communities.",
    "author": [
      {
        "family": "Raymond",
        "given": "Darrell"
      },
      {
        "family": "Tompa",
        "given": "Frank"
      },
      {
        "family": "Wood",
        "given": "Derick"
      }
    ],
    "container-title": "Computer Standards & Interfaces",
    "id": "Raymond1996",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "page": "25-36",
    "title": "From data representation to data model: Meta-semantic issues in the evolution of SGML",
    "title-short": "From data representation to data model",
    "type": "article-journal",
    "volume": "18"
  },
  {
    "abstract": "Unix ranks among the great engineering accomplishments of the last half of the twentieth century, and its heir–Linux–seems already imposing and still on its way to achieving its full potential. Eric S. Raymond argues in <I>The Art of UNIX Programming</I> that the excellence of Unix derives as much from the fact that it was (and continues to be) a community effort as from the fact that a lot of smart people have worked to design and build it. Raymond, best known as the author of the open-source manifesto <I>The Cathedral and the Bazaar</I>, says in his preface that this is a \"why-to\" book, rather than a \"how-to\" book. It aims to show new Unix programmers why they should work under the old \"hacker ethic\"–embracing the principles of good software design for its own sake and of code-sharing. <p> That said, a great deal of valuable practical information appears in this book. Very little of it is in the form of code; most of the practical material takes the form of case studies and discussions of aspects of Unix, all aimed at determining why particular design characteristics are good. In many cases, the people who did the work in the first place make guest appearances and explain their thinking–an invaluable resource. This book is for the deep-thinking software developer in Unix (and perhaps Linux in particular). It shows how to fit into the long and noble tradition, and how to make the software work <I>right</I>. <I>–David Wall</I><p> <B>Topics covered</B>: Why Unix (the term being defined to include Linux) is the way it is, and the people who made it that way. Commentary from Ken Thompson, Steve Johnson, Brian Kernighan, and David Korn enables readers to understand the thought processes of the creators of Unix.",
    "author": [
      {
        "family": "Raymond",
        "given": "Eric S."
      }
    ],
    "id": "Raymond2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "programming, typesetting, xml",
    "publisher": "Addison-Wesley",
    "publisher-place": "Boston, MA, USA",
    "title": "The art of Unix programming",
    "type": "book"
  },
  {
    "URL": "http://comp.eprints.lancs.ac.uk/922/",
    "abstract": "The UCREL semantic analysis system (USAS) is a software tool for undertaking the automatic semantic analysis of English spoken and written data. This paper describes the software system, and the hierarchical semantic tag set containing 21 major discourse fields and 232 fine-grained semantic field tags. We discuss the manually constructed lexical resources on which the system relies, and the seven disambiguation methods including part-of-speech tagging, general likelihood ranking, multi-word-expression extraction, domain of discourse identification, and contextual rules. We report an evaluation of the accuracy of the system compared to a manually tagged test corpus on which the USAS software obtained a precision value of 91%. Finally, we make reference to the applications of the system in corpus linguistics, content analysis, software engineering, and electronic dictionaries.",
    "author": [
      {
        "family": "Rayson",
        "given": "Paul"
      },
      {
        "family": "Archer",
        "given": "Dawn"
      },
      {
        "family": "Piao",
        "given": "Scott"
      },
      {
        "family": "McEnery",
        "given": "Tony"
      }
    ],
    "container-title": "Proceedings of the LREC 2004 workshop on beyond named entity recognition semantic labelling for NLP tasks",
    "id": "Rayson2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "nlp, semantics",
    "page": "7-12",
    "publisher": "European Language Resources Association (ELRA)",
    "publisher-place": "Paris",
    "title": "The UCREL semantic analysis system",
    "type": "paper-conference"
  },
  {
    "abstract": "Analysis of English historical texts poses a number of obstacles for standard corpus analysis and annotation techniques. In addition to non-standard spellings and contractions, there are difficulties at the morphological, phonetic and syntactic levels. Our response has been to develop a VARiant Detector (VARD). We trained VARD on 16th-19th century data, specifically, the Nameless Shakespeare and a selection of texts taken from Chadwyck-Healey’s Eighteenth and Nineteenth Century Fiction collection. We have chosen to explore data from these centuries as, even though variant usage remains an issue up to the present day (because of the use of dialectal forms/ongoing standardisation), it falls substantially in the 18th-19th centuries. This paper reports on experiments to test the utility of VARD. The experiments compared VARD’s performance on unseen data with that of spell checkers for modern English (MS-Word and Aspell). Our hypothesis is that, as these spell checkers are not intended to work on historical data, VARD will be superior at both recognising variants and suggesting modern forms. VARD includes modern equivalents via an XML <reg> tag rather than removing the original variants.",
    "author": [
      {
        "family": "Rayson",
        "given": "Paul"
      },
      {
        "family": "Archer",
        "given": "Dawn"
      },
      {
        "family": "Smith",
        "given": "Nicholas"
      }
    ],
    "container-title": "Proceedings of corpus linguistics 2005",
    "id": "Rayson2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "cultural_heritage, english, spelling_normalization",
    "title": "VARD versus Word: A comparison of the UCREL variant detector and modern spell checkers on English historical corpora",
    "title-short": "VARD versus Word",
    "type": "paper-conference"
  },
  {
    "URL": "http://ucrel.lancs.ac.uk/publications/CL2007/paper/192_Paper.pdf",
    "abstract": "In this paper we focus on automatic part-of-speech (POS) annotation, in the context of historical English texts. Techniques that were originally developed for modern English have been applied to numerous other languages over recent years. Despite this diversification, it is still almost invariably the case that the texts being analysed are from contemporary rather than historical sources. Although there is some recognition among historical linguists of the advantages of annotation for the retrieval of lexical, grammatical and other linguistic phenomena, the implementation of such forms of annotation by automatic methods is problematic. For example, changes in grammar over time will lead to a mismatch between probabilistic language models derived from, say, Present-day English and Middle English. Similarly, variability and changes in spelling can cause problems for POS taggers with fixed lexicons and rulebases. To determine the extent of the problem, and develop possible solutions, we decided to evaluate the accuracy of existing POS taggers, trained on modern English, when they are applied to Early Modern English (EModE) datasets. We focus here on the CLAWS POS tagger, a hybrid rule-based and statistical tool for English, and use as experimental data the Shakespeare First Folio and the Lampeter Corpus. First, using a manually post-edited test set, we evaluate the accuracy of CLAWS when no modifications are made either to its grammatical model or to its lexicon. We then compare this output with CLAWS ’ performance when using a pre-processor that detects spelling variants and matches them to modern equivalents. This experiment highlights (i) the extent to which the handling of orthographic variants is sufficient for the tagging accuracy of EModE data to approximate to the levels attained on modernday text(s), and (ii) in turn, whether revisions to the lexical resources and language models of POS taggers need to be made.",
    "author": [
      {
        "family": "Rayson",
        "given": "Paul"
      },
      {
        "family": "Archer",
        "given": "Dawn"
      },
      {
        "family": "Baron",
        "given": "Alistair"
      },
      {
        "family": "Culpeper",
        "given": "Jonathan"
      },
      {
        "family": "Smith",
        "given": "Nicholas"
      }
    ],
    "container-title": "Corpus linguistics conference (CL2007)",
    "editor": [
      {
        "family": "Davies",
        "given": "Matthew"
      },
      {
        "family": "Rayson",
        "given": "Paul"
      },
      {
        "family": "Hunston",
        "given": "Susan"
      },
      {
        "family": "Danielsson",
        "given": "Pernilla"
      }
    ],
    "id": "Rayson2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage, english, pos_tagging",
    "publisher": "University of Birmingham",
    "publisher-place": "Birmingham, UK",
    "title": "Tagging the bard: Evaluating the accuracy of a modern POS tagger on Early Modern English Corpora",
    "title-short": "Tagging the bard",
    "type": "paper-conference"
  },
  {
    "ISBN": "9781435455061",
    "abstract": "Over the past three decades, interactive fiction—text-based computer games—has evolved into a unique and vibrant medium. While the mainstream game industry has pursued the latest graphics and multimedia technology, the IF movement has focused on becoming more like literature, telling stories that not only talk back, but listen, too. Creating Interactive Fiction with Inform 7 is a step-by-step guide to creating these dynamic stories. Inform is a free, multiplatform interactive fiction authoring environment that uses an intuitive natural language syntax that reads like English. A tool focused on writers, not programmers, Inform allows users to construct complex, rich storytelling worlds by writing sentences as simple as: Tom is a person. …or as complicated as: Instead of attacking Tom when something lethal is held, now every nearby watchdog owned by Tom hates the player. Chapter by chapter you’ll create a full example game while getting comfortable with the features, functions, and vocabulary of Inform and gaining a skill set you can use for future projects. The book doesn’t attempt to cover every aspect of Inform, instead focusing on what you need to know to get started. Tips and tricks throughout the book will guide you as you become more familiar with Inform, and chapter exercises help you practice and test the information you have learned. Creating Interactive Fiction with Inform 7 will quickly have you on your way to creating your own unique, complex, and participatory storytelling world. Now available as a 448-page softcover. Includes a foreword by Don Woods, co-creator of Adventure, and a closing thought by Richard Bartle, inventor of the MUD.",
    "author": [
      {
        "family": "Reed",
        "given": "Aaron"
      }
    ],
    "id": "Reed2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "interactive_fiction",
    "language": "en-US",
    "publisher": "Course Technology",
    "publisher-place": "Boston, MA, USA",
    "title": "Creating interactive fiction with Inform 7",
    "type": "book"
  },
  {
    "author": [
      {
        "dropping-particle": "van",
        "family": "Reenen",
        "given": "Pieter"
      },
      {
        "family": "Mulder",
        "given": "Maaike"
      }
    ],
    "container-title": "Corpus, méthodologie et applications linguistiques",
    "editor": [
      {
        "family": "Bilger",
        "given": "Mireille"
      }
    ],
    "id": "Reenen2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, dutch",
    "language": "fr-FR",
    "page": "209-217",
    "publisher": "Champion/Presses Universitaires de Perpignan",
    "publisher-place": "Paris",
    "title": "Un corpus linguistique de 3000 chartes en Moyen Néerlandais du 14<sup>e</sup> siècle",
    "type": "chapter"
  },
  {
    "DOI": "10.1016/j.envsoft.2007.02.004",
    "ISSN": "13648152",
    "URL": "http://dx.doi.org/10.1016/j.envsoft.2007.02.004",
    "abstract": "A terminology and typology of uncertainty is presented together with a framework for the modelling process, its interaction with the broader water management process and the role of uncertainty at different stages in the modelling processes. Brief reviews have been made of 14 different (partly complementary) methods commonly used in uncertainty assessment and characterisation: data uncertainty engine (DUE), error propagation equations, expert elicitation, extended peer review, inverse modelling (parameter estimation), inverse modelling (predictive uncertainty), Monte Carlo analysis, multiple model simulation, NUSAP, quality assurance, scenario analysis, sensitivity analysis, stakeholder involvement and uncertainty matrix. The applicability of these methods has been mapped according to purpose of application, stage of the modelling process and source and type of uncertainty addressed. It is concluded that uncertainty assessment is not just something to be added after the completion of the modelling work. Instead uncertainty should be seen as a red thread throughout the modelling study starting from the very beginning, where the identification and characterisation of all uncertainty sources should be performed jointly by the modeller, the water manager and the stakeholders.",
    "author": [
      {
        "family": "Refsgaard",
        "given": "Jens C."
      },
      {
        "dropping-particle": "van der",
        "family": "Sluijs",
        "given": "Jeroen P."
      },
      {
        "family": "Højberg",
        "given": "Anker L."
      },
      {
        "family": "Vanrolleghem",
        "given": "Peter A."
      }
    ],
    "container-title": "Environmental Modelling & Software",
    "id": "Refsgaard2007",
    "issue": "11",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "formal_models, uncertainty",
    "page": "1543-1556",
    "title": "Uncertainty in the environmental modelling process – a framework and guidance",
    "type": "article-journal",
    "volume": "22"
  },
  {
    "URL": "http://kups.ub.uni-koeln.de/2939/",
    "abstract": "New information technologies help in realizing old dreams of codicological and palaeographical scholarship but also lead to new hopes and wishes. How long will it be before we are able to carry out automatic text recognition of medieval manuscripts? Will there be a universal virtual catalogue in the near future which gives access to a completely digitised manuscript heritage on the Internet? Could collaborative platforms evolve in which historical records are comprehensively described, transcribed and edited?21 selected papers provide an up-to-date overview of scholarly projects, education and research in codicology (study of books) and palaeography (study of handwriting). They outline current developments and future perspectives of the fields in a digital world. With Contributions by Francesco Bernardi, Paolo Eleuteri, Barbara Vanin; Antonio Cartelli,Andrea Daltri, Paola Errani, Marco Palma, Paolo Zanfi ni; Christian Speer; Timothy Stinson; Pamela Kalning, Karin Zimmermann; Zdeněk Uhlíř, Adolf Knoll; Daniel Deckers, Lutz Koch, Cristina Vertan; Christina Wolf; Silke Kamp; Antonio Cartelli, Marco Palma; Bernard J. Muir; Hugh A. Cayless; Patrick Shiel, Malte Rehbein, John Keating; Daniele Fusi; Gilbert Tomasi, Roland Tomasi; Arianna Ciula; Mark Stansbury; Maria Gurrado; Wernfried Hofmeister, Andrea Hofmeister-Winter,Georg Thallinger; Mark Aussems, Axel Brink; Peter A. Stokes. Introduction by Georg Vogeler. In Collaboration with: Bernhard Assmann, Franz Fischer, Christiane Fritze.",
    "collection-number": "2",
    "collection-title": "Schriften des instituts für dokumentologie und editorik",
    "editor": [
      {
        "family": "Rehbein",
        "given": "Malte"
      },
      {
        "family": "Schaßan",
        "given": "Torsten"
      },
      {
        "family": "Sahle",
        "given": "Patrick"
      }
    ],
    "id": "Rehbein2008",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "publisher": "BoD",
    "publisher-place": "Norderstedt, Germany",
    "title": "Kodikologie und paläographie im digitalen zeitalter – codicology and palaeography in the digital age",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-642-00382-0\\_29",
    "ISBN": "978-3-642-00381-3",
    "ISSN": "0302-9743",
    "URL": "http://dx.doi.org/10.1007/978-3-642-00382-0\\_29",
    "abstract": "Automated language identification of written text is a well-established research domain that has received considerable attention in the past. By now, efficient and effective algorithms based on character n-grams are in use, mainly with identification based on Markov models or on character n-gram profiles. In this paper we investigate the limitations of these approaches when applied to real-world web pages. The challenges to be overcome include language identification on very short texts, correctly handling texts of unknown language and texts comprised of multiple languages. We propose and evaluate a new method, which constructs language models based on word relevance and addresses these limitations. We also extend our method to allow us to efficiently and automatically segment the input text into blocks of individual languages, in case of multiple-language documents.",
    "author": [
      {
        "family": "Řehůřek",
        "given": "Radim"
      },
      {
        "family": "Kolkus",
        "given": "Milan"
      }
    ],
    "chapter-number": "29",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Computational linguistics and intelligent text processing. 10<sup>th</sup> international conference, CICLing 2009, mexico city, mexico, march 1-7, 2009. proceedings",
    "editor": [
      {
        "family": "Gelbukh",
        "given": "Alexander"
      }
    ],
    "id": "Rehurek2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "page": "357-368",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Language identification on the Web: Extending the dictionary method",
    "title-short": "Language identification on the Web",
    "type": "paper-conference",
    "volume": "5449"
  },
  {
    "URL": "http://archive.xmlprague.cz/2017/files/xmlprague-2017-proceedings.pdf",
    "author": [
      {
        "family": "Reichardt",
        "given": "Marcus"
      }
    ],
    "container-title": "Proceedings of XML Prague 2017",
    "editor": [
      {
        "family": "Kosek",
        "given": "Jiř’i"
      }
    ],
    "id": "Reichardt2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "page": "101-118",
    "publisher": "University of Economics",
    "publisher-place": "Prague",
    "title": "The HTML 5.1 DTD",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/567446.567449",
    "author": [
      {
        "family": "Reid",
        "given": "Brian K."
      }
    ],
    "container-title": "POPL ’80: Proceedings of the 7<sup>th</sup> ACM SIGPLAN-SIGACT symposium on principles of programming languages",
    "id": "Reid1980",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "page": "24-31",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A high-level approach to computer document formatting",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Reid",
        "given": "Brian K."
      }
    ],
    "genre": "PhD thesis",
    "id": "Reid1981",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "publisher": "Carnegie-Mellon University",
    "publisher-place": "Pittsburgh, PA",
    "title": "Scribe: A document specification language and its compiler",
    "title-short": "Scribe",
    "type": "thesis"
  },
  {
    "author": [
      {
        "family": "Reid",
        "given": "Brian K."
      }
    ],
    "collection-title": "Cambridge series on electronic publishing",
    "container-title": "Structured documents",
    "editor": [
      {
        "family": "André",
        "given": "Jacques"
      },
      {
        "family": "Furuta",
        "given": "Richard"
      },
      {
        "family": "Quint",
        "given": "Vincent"
      }
    ],
    "id": "Reid1989",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "distributed_systems, functional, lisp, markup",
    "page": "107-118",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge",
    "title": "Electronic mail of structured documents: Representation, transmission, and archiving",
    "title-short": "Electronic mail of structured documents",
    "type": "chapter"
  },
  {
    "URL": "http://www.imb-uni-augsburg.de/files/Arbeitsbericht16.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Reinmann",
        "given": "Gabi"
      }
    ],
    "genre": "Arbeitsbericht",
    "id": "Reinmann2007",
    "issued": {
      "date-parts": [
        [
          2007,
          9
        ]
      ]
    },
    "number": "16",
    "publisher": "Universität Augsburg",
    "publisher-place": "Augsburg",
    "title": "Bologna in Zeiten des Web 2.0: Assessment als Gestaltungsfaktor",
    "type": "report"
  },
  {
    "DOI": "10.1145/234313.234423",
    "ISSN": "0360-0300",
    "URL": "http://dx.doi.org/10.1145/234313.234423",
    "abstract": "Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.",
    "author": [
      {
        "family": "Reiss",
        "given": "Steven P."
      }
    ],
    "container-title": "ACM Comput. Surv.",
    "id": "Reiss1996",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "interactive_editing",
    "page": "281-284",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Software tools and environments",
    "type": "article-journal",
    "volume": "28"
  },
  {
    "URL": "http://learning.ericsson.net/socrates/doc/norway.doc",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Rekkedal",
        "given": "Torstein"
      },
      {
        "family": "Qvist-Eriksen",
        "given": "Svein"
      }
    ],
    "genre": "Project report \\q{Student Support Services in\n                  e-Learning}",
    "id": "Rekkedal2003",
    "issued": {
      "date-parts": [
        [
          2003,
          3
        ]
      ]
    },
    "publisher": "NKI Distance Education",
    "title": "Internet based e-learning, pedagogy and support systems",
    "type": "report"
  },
  {
    "DOI": "10.1145/1863523.1863525",
    "abstract": "Parsers and pretty-printers for a language are often quite similar, yet both are typically implemented separately, leading to redundancy and potential inconsistency. We propose a new interface of syntactic descriptions, with which both parser and pretty-printer can be described as a single program. Whether a syntactic description is used as a parser or as a pretty-printer is determined by the implementation of the interface. Syntactic descriptions enable programmers to describe the connection between concrete and abstract syntax once and for all, and use these descriptions for parsing or pretty-printing as needed. We also discuss the generalization of our programming technique towards an algebra of partial isomorphisms.",
    "author": [
      {
        "family": "Rendel",
        "given": "Tillmann"
      },
      {
        "family": "Ostermann",
        "given": "Klaus"
      }
    ],
    "collection-title": "Haskell ’10",
    "container-title": "Proceedings of the third ACM haskell symposium on haskell",
    "id": "Rendel2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "parsing, pretty-printing",
    "page": "1-12",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Invertible syntax descriptions: Unifying parsing and pretty printing",
    "title-short": "Invertible syntax descriptions",
    "type": "paper-conference"
  },
  {
    "URL": "http://cds.library.brown.edu/resources/stg/monographs/ohco.html",
    "abstract": "We examine the claim that ’text is an ordered hierarchy of content objects’; this thesis was affirmed by the authors, and others, in the late 1980s and has been associated with certain approaches to text processing and the encoding of literary texts. First we discuss the nature of this claim and its connection with the history of text processing and text encoding standardization projects such as SGML and the Text Encoding Initiative. We then describe how the experience of the text encoding community, as represented and codified in the TEI Guidelines, has raised difficulties for this thesis. Next we consider two progressively weaker versions of this thesis formulated in response to these difficulties. Ultimately we find that no version appears to be free from counterexample. Although none of these formulations proves to be theoretically sound, they are nonetheless methodologically illuminating as each generalizes actual encoding practices, making explicit certain assumptions that, even though they have been fundamental to the working methodologies of most text encoding projects, have never been explicitly articulated, let alone explained or defended. The counterexamples to the different versions of the OHCO thesis also arise in actual encoding projects – so although our focus is theoretical it is grounded in the methodology and problems of contemporary encoding practices. The problems discussed here have implications not only for text encoding and our understanding of the nature of textual communication, but raise very fundamental issues in the logic and methodology of the humanities.",
    "author": [
      {
        "family": "Renear",
        "given": "Allen H."
      },
      {
        "family": "Mylonas",
        "given": "Elli"
      },
      {
        "family": "Durand",
        "given": "David"
      }
    ],
    "container-title": "Research in humanities computing 4: Selected papers from the 1992 ACH/ALLC conference",
    "editor": [
      {
        "family": "Hockey",
        "given": "Susan"
      },
      {
        "family": "Ide",
        "given": "Nancy"
      }
    ],
    "id": "Renear1992",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "document_research, formal_models, markup, markup_overlap",
    "language": "en-US",
    "page": "263-280",
    "publisher": "Clarendon Press",
    "publisher-place": "Oxford",
    "title": "Refining our notion of what text really is: The problem of overlapping hierarchies",
    "title-short": "Refining our notion of what text really is",
    "type": "paper-conference"
  },
  {
    "ISSN": "0301-102X",
    "URL": "https://www.escholar.manchester.ac.uk/uk-ac-man-scw:1m2308",
    "abstract": "Practitioners and researchers in the area of computer text processing have reached the conclusion that documents, at least qua intellectual objects, are best represented as syntactically complex structures of ’content objects’. Some familiar examples of content objects in this sense are: chapters, sections and subsections, titles and headings, prose paragraphs, sentences, prose extracts, verse extracts, lists, list items,tables, figures, equations, bibliographical citations and footnotes. In software and ’textbase’ development the superiority of the content object approach over other possible models for representing text can be shown easily- it is by far the simplest and most functional way to create, modify and format texts, and it is required to support effectively document management, text retrieval, browsing, various kinds of analysis and many other sorts of special processing. Finally, texts represented according to this model are much more easily shared among different software applications and computing systems. Not surprisingly this view of text, which turns out to be theoretically illuminating as well as practical, is in part a generalization of portions of the methodologies,theories and practices of the many humanistic disciplines in which texts,books and documents figure importantly. One may perhaps say,somewhat more ambitiously, that the reason this model of text is so functional and effective is that it reflects what text really is.",
    "author": [
      {
        "family": "Renear",
        "given": "Allen H."
      }
    ],
    "container-title": "Bulletin of the John Rylands Library",
    "id": "Renear1992a",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "document_research, markup, sgml",
    "language": "en-US",
    "page": "221-248",
    "title": "Representing text on the computer: Lessons for and from philosophy",
    "title-short": "Representing text on the computer",
    "type": "article-journal",
    "volume": "74"
  },
  {
    "DOI": "10.1093/acprof:oso/9780198236634.003.0005",
    "ISBN": "9780198236634",
    "abstract": "This chapter focuses on computer text processing, and, more specifically, computer text encoding. It shows two things. First, that the particular community which has been designing and configuring computer text-processing and encoding systems has evolved a rich body of illuminating theory about the nature of text — theory that is useful to anyone who would create, manage, or use electronic text. Second, the chapter suggests that the significance of this body of theory and analysis extends well beyond the specific concerns of text processing and text encoding and contributes directly to the understanding of the deepest issues of textuality and textual communication in general. In addition, the chapter describes the domain of text processing and text encoding and discusses several representational strategies involved in text processing, and content-based encoding.",
    "author": [
      {
        "family": "Renear",
        "given": "Allen H."
      }
    ],
    "container-title": "Electronic text: Investigations in method and theory",
    "editor": [
      {
        "family": "Sutherland",
        "given": "Kathryn"
      }
    ],
    "id": "Renear1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "document_research, markup, sgml",
    "language": "en-US",
    "page": "107-124",
    "publisher": "Oxford University Press",
    "title": "Out of praxis: Three (Meta)Theories of textuality",
    "title-short": "Out of praxis",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/585058.585081",
    "ISBN": "1-58113-594-7",
    "abstract": "Although XML Document Type Definitions provide a mechanism for specifying, in machine-readable form, the syntax of an XML markup language, there is no comparable mechanism for specifying the semantics of an XML vocabulary. That is, there is no way to characterize the meaning of XML markup so that the facts and relationships represented by the occurrence of XML constructs can be explicitly, comprehensively, and mechanically identified. This has serious practical and theoretical consequences. On the positive side, XML constructs can be assigned arbitrary semantics and used in application areas not foreseen by the original designers. On the less positive side, both content developers and application engineers must rely upon prose documentation, or, worse, conjectures about the intention of the markup language designer—a process that is time-consuming, error-prone, incomplete, and unverifiable, even when the language designer properly documents the language. In addition, the lack of a substantial body of research in markup semantics means that digital document processing is undertheorized as an engineering application area. Although there are some related projects underway (XML Schema, RDF, the Semantic Web) which provide relevant results, none of these projects directly and comprehensively address the core problems of XML markup semantics. This paper (i) summarizes the history of the concept of markup meaning, (ii) characterizes the specific problems that motivate the need for a formal semantics for XML and (iii) describes an ongoing research project—the BECHAMEL Markup Semantics Project—that is attempting to develop such a semantics.",
    "author": [
      {
        "family": "Renear",
        "given": "Allen H."
      },
      {
        "family": "Dubin",
        "given": "David"
      },
      {
        "family": "Sperberg-McQueen",
        "given": "C. M."
      }
    ],
    "container-title": "Proceedings of the 2002 ACM symposium on document engineering (DocEng ’02)",
    "id": "Renear2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "knowledge_representation, markup, semantics, sgml, xml",
    "page": "119-126",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Towards a semantics for XML markup",
    "type": "paper-conference"
  },
  {
    "ISBN": "9023213947",
    "author": [
      {
        "family": "Rescher",
        "given": "Nicholas"
      }
    ],
    "id": "Rescher1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Van Gorcum",
    "publisher-place": "Assen",
    "title": "Plausible reasoning: An introduction to the theory and practice of plausibilistic inference",
    "title-short": "Plausible reasoning",
    "type": "book"
  },
  {
    "URL": "http://www.respondus.com/downloads/Respondus35UserGuidePN.doc",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "id": "Respondus2008",
    "issued": {
      "date-parts": [
        [
          2008,
          6
        ]
      ]
    },
    "publisher": "Respondus, Inc.",
    "publisher-place": "Redmond, WA, USA",
    "title": "Respondus user guide",
    "type": "book"
  },
  {
    "URL": "http://www.tab.fzk.de/de/projekt/zusammenfassung/ab107.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Revermann",
        "given": "Christoph"
      }
    ],
    "genre": "TAB-Arbeitsbericht",
    "id": "Revermann2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "language": "de-DE",
    "number": "107",
    "publisher": "Büro für Technikfolgen-Abschätzung beim Deutschen Bundestag (TAB)",
    "publisher-place": "Berlin",
    "title": "eLearning in Forschung, Lehre und Weiterbildung in Deutschland: Sachstandsbericht zum Monitoring eLearning",
    "type": "report"
  },
  {
    "URL": "http://ilk.uvt.nl/~mre/TISC.PhD.MartinReynaert.pdf.gz",
    "abstract": "The main contribution of this dissertation is a novel approximate string matching algorithm for indexed text search. The algorithm is based on a hashing function which uniquely identifies strings composed of the same subsets of characters, i.e. anagrams, by means of a numeric value. The numeric value allows for searching for character strings differing from a particular string by a predefined number of characters. As such it forms an ideal basis for a novel spelling error detection and correction algorithm, which we call Text-Induced Spelling Correction. Our system uses nothing but lexical and word cooccurrence information derived from a corpus, a very large collection of texts in a particular language, to perform context-sensitive spelling error correction of non-words. Non-words are word strings produced unintentionally by a typist that deviate from a convention about how words are to be spelled in order to be considered real-words within the language.",
    "author": [
      {
        "family": "Reynaert",
        "given": "Martin"
      }
    ],
    "genre": "PhD thesis",
    "id": "Reynaert2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "spelling_correction",
    "publisher": "Tilburg University",
    "publisher-place": "Tilburg, The Netherlands",
    "title": "Text-Induced spelling correction",
    "type": "thesis"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/pdf/477_paper.pdf",
    "abstract": "Some time in the future, some spelling error correction system will correct all the errors, and only the errors. We need evaluation metrics that will tell us when this has been achieved and that can help guide us there. We survey the current practice in the form of the evaluation scheme of the latest major publication on spelling correction in a leading journal. We are forced to conclude that while the metric used there can tell us exactly when the ultimate goal of spelling correction research has been achieved, it offers little in the way of directions to be followed to eventually get there. We propose to consistently use the well-known metrics Recall and Precision, as combined in the F score, on 5 possible levels of measurement that should guide us more informedly along that path. We describe briefly what is then measured or measurable at these levels and propose a framework that should allow for concisely stating what it is one performs in one’s evaluations. We finally contrast our preferred metrics to Accuracy, which is widely used in this field to this day and to the Area-Under-the-Curve, which is increasingly finding acceptance in other fields.",
    "author": [
      {
        "family": "Reynaert",
        "given": "Martin"
      }
    ],
    "container-title": "Proceedings of the 6<sup>th</sup> international conference on language resources and evaluation (LREC 2008)",
    "id": "Reynaert2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "spelling_correction",
    "page": "1867-1872",
    "publisher": "European Language Resources Association (ELRA)",
    "title": "All, and only, the errors: More complete and consistent spelling and OCR-error correction evaluation",
    "title-short": "All, and only, the errors",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-540-78135-6_53",
    "ISBN": "978-3-540-78134-9",
    "abstract": "This paper proposes a non-interactive system for reducing the level of OCR-induced typographical variation in large text collections, contemporary and historical. Text-Induced Corpus Clean-up or ticcl (pronounce ’tickle’) focuses on high-frequency words derived from the corpus to be cleaned and gathers all typographical variants for any particular focus word that lie within the predefined Levenshtein distance (henceforth: ld ). Simple text-induced filtering techniques help to retain as many as possible of the true positives and to discard as many as possible of the false positives. ticcl has been evaluated on a contemporary OCR-ed Dutch text corpus and on a corpus of historical newspaper articles, whose OCR-quality is far lower and which is in an older Dutch spelling. Representative samples of typographical variants from both corpora have allowed us not only to properly evaluate our system, but also to draw effective conclusions towards the adaptation of the adopted correction mechanism to OCR-error resolution. The performance scores obtained up to ld 2 mean that the bulk of undesirable OCR-induced typographical variation present can fully automatically be removed.",
    "author": [
      {
        "family": "Reynaert",
        "given": "Martin"
      }
    ],
    "chapter-number": "53",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Computational linguistics and intelligent text processing",
    "editor": [
      {
        "family": "Gelbukh",
        "given": "Alexander"
      }
    ],
    "id": "Reynaert2008a",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ocr, spelling_correction",
    "page": "617-630",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Non-interactive OCR post-correction for giga-scale digitization projects",
    "type": "chapter",
    "volume": "4919"
  },
  {
    "DOI": "10.1145/1568296.1568310",
    "ISBN": "978-1-60558-496-6",
    "abstract": "We present a new approach based on anagram hashing to globally handle the typographical variation in large and possibly noisy text collections. Typographical variation is typically handled in a local fashion: given one particular text string some system of retrieving near-neighbours is applied, where near-neighbours are other text strings that differ from the particular string by a given number of characters. The difference in characters between the original string and one of its retrieved near-neighbours we call a particular character confusion. We present a global way of performing this action: given a possible particular character confusion, we identify - in parallel, i.e. in one single operation on anagram-hash derived bit vectors - all the pairs of text strings in the text collection to which the particular confusion applies. The algorithm proposed here is evaluated on about 23,000 English attested typos from the Reuters rcv1 text collection. We further explore its usefulness for unsupervised linking of a historical Dutch word list to its contemporary counterpart.",
    "author": [
      {
        "family": "Reynaert",
        "given": "Martin"
      }
    ],
    "container-title": "Proceedings of the third workshop on analytics for noisy unstructured text data (AND ’09)",
    "id": "Reynaert2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "spelling_correction",
    "page": "77-84",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Parallel identification of the spelling variants in corpora",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s10032-010-0133-5",
    "ISSN": "1433-2833",
    "abstract": "We present a new approach based on anagram hashing to handle globally the lexical variation in large and noisy text collections. Lexical variation addressed by spelling correction systems is primarily typographical variation. This is typically handled in a local fashion: given one particular text string some system of retrieving near-neighbors is applied, where near-neighbors are other text strings that differ from the particular string by a given number of characters. The difference in characters between the original string and one of its retrieved near-neighbors constitutes a particular character confusion. We present a global way of performing this action: for all possible particular character confusions given a particular edit distance, we sequentially identify all the pairs of text strings in the text collection that display a particular confusion. We work on large digitized corpora, which contain lexical variation due to both the OCR process and typographical or typesetting error and show that all these types of variation can be handled equally well in the framework we present. The character confusion-based prototype of Text-Induced Corpus Clean-up ( ticcl ) is compared to its focus word-based counterpart and evaluated on 6 years’ worth of digitized Dutch Parliamentary documents. The character confusion approach is shown to gain an order of magnitude in speed on its word-based counterpart on large corpora. Insights gained about the useful contribution of global corpus variation statistics are shown to also benefit the more traditional word-based approach to spelling correction. Final tests on a held-out set comprising the 1918 edition of the Dutch daily newspaper ’Het Volk’ show that the system is not sensitive to domain variation.",
    "author": [
      {
        "family": "Reynaert",
        "given": "Martin"
      }
    ],
    "container-title": "International Journal on Document Analysis and Recognition",
    "id": "Reynaert2011",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "ocr, spelling_correction",
    "page": "173-187",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Character confusion versus focus word-based correction of spelling and OCR variants in corpora",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "URL": "http://www.microsoft.com/technet/archive/itsolutions/ecommerce/maintain/operate/contmgt.mspx",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Reynolds",
        "given": "Jim"
      },
      {
        "family": "Kaur",
        "given": "Arminder"
      }
    ],
    "genre": "Microsoft Enterprise Services White Paper",
    "id": "Reynolds2000",
    "issued": {
      "date-parts": [
        [
          2000,
          4
        ]
      ]
    },
    "publisher": "Microsoft Corporation",
    "title": "Content management",
    "type": "report"
  },
  {
    "abstract": "Error annotation is a key feature of modern learner corpora. Error identification is always based on some kind of reconstructed learner utterance (target hypothesis). Since a single target hypothesis can only cover a certain amount of linguistic information while ignoring other aspects, the need for multiple target hypotheses becomes apparent. Using the German learner corpus Falko as an example we therefore argue for a flexible multi-layer standoff corpus architecture where competing target hypotheses can be coded simultaneously. Surface differences between the learner text and the target hypotheses can then be exploited for automatic error annotation.",
    "author": [
      {
        "family": "Reznicek",
        "given": "Marc"
      },
      {
        "family": "Lüdeling",
        "given": "Anke"
      },
      {
        "family": "Hirschmann",
        "given": "Hagen"
      }
    ],
    "container-title": "Automatic treatment and analysis of learner corpus data",
    "editor": [
      {
        "family": "Díaz-Negrillo",
        "given": "Ana"
      }
    ],
    "id": "Reznicek2013",
    "issued": {
      "date-parts": []
    },
    "keyword": "corpus_linguistics, learner_corpora, writing_research, xml",
    "publisher": "John Benjamins",
    "publisher-place": "Amsterdam",
    "status": "forthcoming",
    "title": "Competing target hypotheses in the Falko Corpus. A flexible multi-layer corpus architecture",
    "type": "chapter"
  },
  {
    "URL": "https://www.aaai.org/Papers/Symposia/Spring/1996/SS-96-02/SS96-02-022.pdf",
    "abstract": "The Remembrance Agent (RA) is a program which augments human memory by displaying a list of documents which might be relevant to the user’s current context. Unlike most information retrieval systems, the RA runs continuously without user intervention. Its unobtrusive interface allows a user to pursue or ignore the RA’s suggestions as desired.",
    "author": [
      {
        "family": "Rhodes",
        "given": "Bradley J."
      },
      {
        "family": "Starner",
        "given": "Thad"
      }
    ],
    "container-title": "Proceedings of the first international conference on the practical application of intelligent agents and multi agent technology (PAAM ’96)",
    "id": "Rhodes1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "emacs, interactive_editing, ir",
    "page": "487-495",
    "title": "Remembrance agent: A continuously running automated information retrieval system",
    "title-short": "Remembrance agent",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/INFOVIS.2000.10001",
    "author": [
      {
        "family": "Ribler",
        "given": "Randy L."
      },
      {
        "family": "Abrams",
        "given": "Marc"
      }
    ],
    "container-title": "INFOVIS ’00: Proceedings of the IEEE symposium on information vizualization 2000",
    "id": "Ribler2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "page": "173-177",
    "publisher": "IEEE Computer Society",
    "publisher-place": "Washington, DC, USA",
    "title": "Using visualization to detect plagiarism in computer science classes",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Rice",
        "given": "Stanley"
      }
    ],
    "id": "Rice1978a",
    "issued": {
      "date-parts": [
        [
          1978
        ]
      ]
    },
    "publisher": "Bowker",
    "publisher-place": "New York, NY, USA",
    "title": "Book design: Systematic aspects",
    "title-short": "Book design",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Rice",
        "given": "Stanley"
      }
    ],
    "id": "Rice1978b",
    "issued": {
      "date-parts": [
        [
          1978
        ]
      ]
    },
    "publisher": "Bowker",
    "publisher-place": "New York, NY, USA",
    "title": "Book design: Text format models",
    "title-short": "Book design",
    "type": "book"
  },
  {
    "abstract": "Given a bitmapped image of a page from any document, a page-reading system identifies the characters on the page and stores them in a text file. This “OCR-generated” text is represented by a string and compared with the correct string to determine the accuracy of this process. The string editing problem is applied to find an optimal correspondence of these strings using an appropriate cost function. The ISRI annual test of page-reading systems utilizes the following performance measures, which are defined in terms of this correspondence and the string edit distance: character accuracy, throughput, accuracy by character class, marked character efficiency, word accuracy, non-stopword accuracy, and phrase accuracy. It is shown that the universe of cost functions is divided into equivalence classes, and the cost functions related to the longest common subsequence (LCS) are identified. The computation of a LCS can be made faster by a linear-time preprocessing step.",
    "author": [
      {
        "family": "Rice",
        "given": "Stephen V."
      }
    ],
    "genre": "PhD thesis",
    "id": "Rice1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "ocr",
    "publisher": "University of Nevada",
    "publisher-place": "Las Vegas",
    "title": "Measuring the accuracy of Page-Reading systems",
    "type": "thesis"
  },
  {
    "ISBN": "9782020236799",
    "author": [
      {
        "family": "Ricœur",
        "given": "Paul"
      }
    ],
    "id": "Ricoeur1965",
    "issued": {
      "date-parts": [
        [
          1965
        ]
      ]
    },
    "keyword": "classic, philosophy_of_science, uncertainty",
    "language": "fr-FR",
    "publisher": "Seuil",
    "publisher-place": "Paris",
    "title": "De l’interprétation: Essai sur Freud",
    "title-short": "De l’interprétation",
    "type": "book"
  },
  {
    "ISBN": "9782757868348",
    "abstract": "Ces « essais d’herméneutique » portent la marque du bouillonnement intellectuel des années 1960 : les sciences humaines font éclater les cadres reçus de l’interprétation. Le premier mérite de Ricœur est de reprendre ce que disent les sciences de l’homme pour mesurer comment et pourquoi naît ce « conflit des interprétations ». On est loin ici d’une philosophie se contentant de rappeler à hauts cris le « sens » face à la « mort du sujet ». S’il s’agit « d’explorer les voies ouvertes à la philosophie contemporaine » par cette nouvelle donne, il faut passer par le détour de l’analyse longue, par l’exégèse de la Bible et les disciplines religieuses, qui furent à l’origine du problème herméneutique. Au terme de cette analyse seulement, il sera possible à nouveau de « donner un sens acceptable à la notion d’existence ».",
    "author": [
      {
        "family": "Ricœur",
        "given": "Paul"
      }
    ],
    "id": "Ricoeur1969-2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "classic, philosophy_of_science, uncertainty, hermeneutics",
    "language": "fr-FR",
    "publisher": "Seuil",
    "publisher-place": "Paris",
    "title": "Le conflit des interprétations: Essais d’herméneutique",
    "title-short": "Le conflit des interprétations",
    "type": "book"
  },
  {
    "URL": "http://dro.dur.ac.uk/1929/",
    "abstract": "The purpose of this report is: ? to assert the centrality of assessment in education systems ? to identify ?drivers? of assessment, and their likely impact on assessment, and thence on education systems ? to describe current, radical plans for increased use of high-stakes e-assessment in the UK ? to describe and exemplify current uses of ICT in assessment ? to explore the potential of new technologies for enhancing current assessment (and pedagogic) practices ? to identify opportunities and to suggest ways forward ? to ?drip feed? criteria for good assessment throughout (set out explicitly in an appendix). This report has been designed to: present key findings on research in assessment; describe current UK government plans, and likely future developments; provide links to interesting examples of e-assessment; offer speculations on possible future developments; and to stimulate a debate on the role of e-assessment in assessment, teaching, and learning.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Ridgway",
        "given": "Jim"
      },
      {
        "family": "McCusker",
        "given": "Sean"
      },
      {
        "family": "Pead",
        "given": "Daniel"
      }
    ],
    "genre": "Report",
    "id": "Ridgway2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "assessment, e-learning",
    "number": "10",
    "publisher": "FutureLab",
    "publisher-place": "Bristol, U.K.",
    "title": "Literature review of e-assessment",
    "type": "report"
  },
  {
    "DOI": "10.1145/1289600.1289602",
    "ISSN": "1550-4875",
    "URL": "http://dx.doi.org/10.1145/1289600.1289602",
    "abstract": "For the success of lexical text correction, high coverage of the underlying background dictionary is crucial. Still, most correction tools are built on top of static dictionaries that represent fixed collections of expressions of a given language. When treating texts from specific domains and areas, often a significant part of the vocabulary is missed. In this situation, both automated and interactive correction systems produce suboptimal results. In this article, we describe strategies for crawling Web pages that fit the thematic domain of the given input text. Special filtering techniques are introduced to avoid pages with many orthographic errors. Collecting the vocabulary of filtered pages that meet the vocabulary of the input text, dynamic dictionaries of modest size are obtained that reach excellent coverage values. A tool has been developed that automatically crawls dictionaries in the indicated way. Our correction experiments with crawled dictionaries, which address English and German document collections from a variety of thematic fields, show that with these dictionaries even the error rate of highly accurate texts can be reduced, using completely automated correction methods. For interactive text correction, more sensible candidate sets for correcting erroneous words are obtained and the manual effort is reduced in a significant way. To complete this picture, we study the effect when using word trigram models for correction. Again, trigram models from crawled corpora outperform those obtained from static corpora.",
    "author": [
      {
        "family": "Ringlstetter",
        "given": "Christoph"
      },
      {
        "family": "Schulz",
        "given": "Klaus U."
      },
      {
        "family": "Mihov",
        "given": "Stoyan"
      }
    ],
    "container-title": "ACM Transactions on Speech and Language Processing",
    "id": "Ringlstetter2007",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "spelling_correction",
    "page": "Article 9+",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Adaptive text correction with Web-crawled domain-dependent dictionaries",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "DOI": "10.1515/9783110211429.1.53",
    "URL": "http://dx.doi.org/10.1515/9783110211429.1.53",
    "author": [
      {
        "family": "Rissanen",
        "given": "Matti"
      }
    ],
    "chapter-number": "4",
    "collection-title": "Handbooks of linguistics and communication science",
    "container-title": "Corpus linguistics. An international handbook",
    "editor": [
      {
        "family": "Lüdeling",
        "given": "Anke"
      },
      {
        "family": "Kytö",
        "given": "Merja"
      }
    ],
    "id": "Rissanen2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage",
    "page": "53-68+",
    "publisher": "Mouton de Gruyter",
    "title": "Corpus linguistics and historical linguistics",
    "type": "chapter",
    "volume": "Part 1"
  },
  {
    "DOI": "10.1002/j.1538-7305.1978.tb02136.x",
    "ISSN": "00058580",
    "abstract": "UNIX is a general-purpose, multi-user, interactive operating system for the larger Digital Equipment Corporation PDP-11 and the Interdata 8/32 computers. It offers a number of features seldom found even in larger operating systems, including (i) A hierarchical file system incorporating demountable volumes, (ii) Compatible file, device, and inter-process I/O, (iii) The ability to initiate asynchronous processes, (iv) System command language selectable on a per-user basis, (v) Over 100 subsystems including a dozen languages, (vi) High degree of portability. This paper discusses the nature and implementation of the file system and of the user command interface.",
    "author": [
      {
        "family": "Ritchie",
        "given": "D. M."
      },
      {
        "family": "Thompson",
        "given": "K."
      }
    ],
    "container-title": "Bell System Technical Journal",
    "id": "Ritchie1978",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          1978
        ]
      ]
    },
    "keyword": "classic, unix",
    "language": "en-US",
    "page": "1905-1929",
    "title": "The UNIX time-sharing system",
    "type": "article-journal",
    "volume": "57"
  },
  {
    "DOI": "10.1145/1083356.1083396",
    "ISBN": "1-59593-168-6",
    "URL": "http://dx.doi.org/10.1145/1083356.1083396",
    "abstract": "This paper explains some of the theoretical underpinnings informing the framework of the Virtual Humanities Lab [9] at Brown University. We argue that humanists can and should perform research collaboratively online, provided the availability of tools that suit their individual and collective needs. VHL’s implementation of some such tools is extendable to treating a variety of primary sources. In its first year of development, VHL enables scholars to participate in the editing and annotation of a diverse typology of semantically encoded texts from Early Modern Italy.",
    "author": [
      {
        "family": "Riva",
        "given": "Massimo"
      },
      {
        "family": "Zafrin",
        "given": "Vika"
      }
    ],
    "container-title": "HYPERTEXT ’05: Proceedings of the sixteenth ACM conference on hypertext and hypermedia",
    "id": "Riva2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "page": "205-207",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Extending the text: Digital editions and the hypertextual paradigm",
    "title-short": "Extending the text",
    "type": "paper-conference"
  },
  {
    "URL": "http://people.csail.mit.edu/rivest/Sexp.txt",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Rivest",
        "given": "Ronald L."
      }
    ],
    "genre": "Internet Draft",
    "id": "Rivest1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "publisher": "Internet Engineering Task Force",
    "title": "S-expressions",
    "type": "report"
  },
  {
    "DOI": "10.1007/s00778-007-0058-x",
    "ISSN": "1066-8888",
    "URL": "http://dx.doi.org/10.1007/s00778-007-0058-x",
    "abstract": "In this paper we address the problem of modeling and implementing temporal data in XML. We propose a data model for tracking historical information in an XML document and for recovering the state of the document as of any given time. We study the temporal constraints imposed by the data model, and present algorithms for validating a temporal XML document against these constraints, along with methods for fixing inconsistent documents. In addition, we discuss different ways of mapping the abstract representation into a temporal XML document, and introduce TXPath, a temporal XML query language that extends XPath 2.0. In the second part of the paper, we present our approach for summarizing and indexing temporal XML documents. In particular we show that by indexing continuous paths, i.e., paths that are valid continuously during a certain interval in a temporal XML graph, we can dramatically increase query performance. To achieve this, we introduce a new class of summaries, denoted TSummary, that adds the time dimension to the well-known path summarization schemes. Within this framework, we present two new summaries: LCP and Interval summaries. The indexing scheme, denoted TempIndex, integrates these summaries with additional data structures. We give a query processing strategy based on TempIndex and a type of ancestor-descendant encoding, denoted temporal interval encoding. We present a persistent implementation of TempIndex, and a comparison against a system based on a non-temporal path index, and one based on DOM. Finally, we sketch a language for updates, and show that the cost of updating the index is compatible with real-world requirements.",
    "author": [
      {
        "family": "Rizzolo",
        "given": "Flavio"
      },
      {
        "family": "Vaisman",
        "given": "Alejandro A."
      }
    ],
    "container-title": "VLDB Journal",
    "id": "Rizzolo2008",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2008,
          8
        ]
      ]
    },
    "keyword": "temporal_data, xml, xquery",
    "page": "1179-1212",
    "publisher": "Springer-Verlag New York, Inc.",
    "publisher-place": "Secaucus, NJ, USA",
    "title": "Temporal XML: Modeling, indexing, and query processing",
    "title-short": "Temporal XML",
    "type": "article-journal",
    "volume": "17"
  },
  {
    "DOI": "10.1162/089120101750300526",
    "ISSN": "0891-2017",
    "URL": "http://dx.doi.org/10.1162/089120101750300526",
    "abstract": "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling. A lexicalized probabilistic top-down parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers. A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity. Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model. A small recognition experiment also demonstrates the utility of the model.",
    "author": [
      {
        "family": "Roark",
        "given": "Brian"
      }
    ],
    "container-title": "Computational Linguistics",
    "id": "Roark2001",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "interactive_parsing",
    "page": "249-276",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Probabilistic top-down parsing and language modeling",
    "type": "article-journal",
    "volume": "27"
  },
  {
    "DOI": "10.1145/800045.801614",
    "ISBN": "0-89791-121-0",
    "abstract": "The organization of text editing behavior can be characterized by graph structures containing goals, subgoals, goal outcomes, and actions. Here we propose a model to represent the goals and plans of text editor users based on goal-fate analysis (Schank &amp; Abelson, 1977). The representation captures relationships between a user’s multiple goals and shows how errors can result from badly formed plans. We discuss some data from a psychological experiment which supports the hypothesis that text editing behavior is chunked into distinct plan units. The cognitive components of pause times between keystrokes were revealed by statistically removing the physical time required between keystrokes. Finally, we suggest how a system which builds goal-fate graphs from keystroke input might be useful in providing specific help information that is keyed to a user’s intentions.",
    "author": [
      {
        "family": "Robertson",
        "given": "Scott P."
      },
      {
        "family": "Black",
        "given": "John B."
      }
    ],
    "container-title": "CHI ’83: Proceedings of the SIGCHI conference on human factors in computing systems",
    "id": "Robertson1983",
    "issued": {
      "date-parts": [
        [
          1983
        ]
      ]
    },
    "keyword": "interactive_editing",
    "page": "217-221",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Planning units in text editing behavior",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/133160.133208",
    "abstract": "This paper discusses the application of algorithmic spelling-correction techniques to the identification of those words in a database of 17th-century English text that are most similar to a query word in modern English. The experiments have used n-gram matching, non-phonetic coding and dynamic programming methods for spelling correction, and have demonstrated that high-recall searches can be carried out, although some of the searches are very demanding of computational resources. The methods are, in principle, applicable to historical texts in many languages and from many diffeent periods.",
    "author": [
      {
        "family": "Robertson",
        "given": "Alexander M."
      },
      {
        "family": "Willett",
        "given": "Peter"
      }
    ],
    "container-title": "SIGIR ’92: Proceedings of the 15<sup>th</sup> annual international ACM SIGIR conference on research and development in information retrieval",
    "id": "Robertson1992",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "cultural_heritage, ir",
    "page": "256-265",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Searching for historical word-forms in a database of 17<sup>th</sup>-century English text using spelling-correction methods",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/8.3.143",
    "abstract": "This paper discusses the application of algorithmic spelling-correction techniques to the identification of those words in databases of sixteenth-, seventeenth-, and eighteenth-century English texts that are most similar to a query word in modern English. The experiments involved the n-gram matching, phonetic and non-phonetic coding, and dynamic-programming methods for spelling correction, and demonstrate the general effectiveness of this approach to the identification of historical word forms. The best results are given by the editcost-distance and longest-common-subsequence methods, which use a dynamic-programming algorithm; however, these are only slightly more effective than the digram-matching method, which is far faster in operation.",
    "author": [
      {
        "family": "Robertson",
        "given": "Alexander M."
      },
      {
        "family": "Willett",
        "given": "Peter"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Robertson1993",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1993,
          1
        ]
      ]
    },
    "keyword": "cultural_heritage, ir, spelling_correction",
    "page": "143-152",
    "title": "A comparison of spelling-correction methods for the identification of word forms in historical text databases",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "ISSN": "1938-4122",
    "URL": "http://www.digitalhumanities.org/dhq/vol/003/1/000026/000026.html",
    "abstract": "The Web, though full of historical information, lacks a means of organizing that information, searching on it or visualizing it. The Historical Event Markup and Linking Project (Heml) was begun six years ago to explore how disparate historical materials on the Internet can be navigated and visualized, and for the past four years has used an XML data format defined in W3C Schemas. This format aims for conforming data that can be quickly parsed but provide a variety of facets on which to search for historical materials. While the project’s graphical visualizations are in some respects successful, they have revealed some deficiencies in the underlying data format: it ought to provide for nested events, it ought to represent relations of causality between events and it ought to express the varieties of scholarly opinion about the attributes of events. By encoding the Heml data in the Resource Description Framework (RDF) it is possible to undertake these improvements. Moreover, an RDF-encoded Heml process provides easier access to CIDOC-CRM data into Heml events. Finally, a historical RDF language would simplify the discovery of references to historical events in digitized texts, thereby automating a growing network of historical information on the Web.",
    "author": [
      {
        "family": "Robertson",
        "given": "Bruce"
      }
    ],
    "container-title": "Digital Humanities Quarterly",
    "id": "Robertson2009",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "digital_humanities, rdf, semantic_web",
    "title": "Exploring historical RDF with heml",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "URL": "http://dhdebates.gc.cuny.edu/debates/text/76",
    "abstract": "My chapter, “The Differences between Digital Humanities and Digital History,” has appeared in Debates in Digital Humanities 2016, expertly edited by Lauren Klein and Matt Gold (available in print and an open access online edition). The chapter is a revised and expanded version of the post that first appeared on this blog in May 2014. In this form, the piece is part of what Klein and Gold describe in their introduction as a shift to “more forceful grappling with DH at the disciplinary level.” A section of the collection is devoted to “Digital Humanities and the Disciplines,” and includes contributions of particular relevance to historians by Cameron Blevins and my colleague Sheila Brennan. Unfortunately, the chapter also remains relevant as a corrective to the persistence elision of digital history in discussions of digital humanities, most recently in the pages of the LA Review of Books, and their ongoing series “The Digital in the Humanities” (notwithstanding the recent interview with my colleague Sharon Leon)",
    "author": [
      {
        "family": "Robertson",
        "given": "Stephen"
      }
    ],
    "container-title": "Debates in the digital humanities 2016",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      },
      {
        "family": "Klein",
        "given": "Lauren F."
      }
    ],
    "id": "Robertson2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "289-307",
    "publisher": "University of Minnesota Press",
    "publisher-place": "Minneapolis, MN, USA",
    "title": "The differences between digital humanities and digital history",
    "type": "chapter"
  },
  {
    "DOI": "10.3233/AO-2010-0077",
    "ISSN": "1570-5838",
    "abstract": "The purpose of this paper is to argue states are not organizations, but rather the objective legal persons of international law. The ontological similarities and differences between states and organizations are examined, but, by drawing upon literature in political geography and international law, ultimately shows that states cannot be organizations based in large part on the fact that states can survive the destruction of their organizational structures. Bottazzi and Ferrario’s DOLCE-based ontology of organizations is of specific interest because it provides “The State of Italy” as an example of an organization that fits their ontological structure. This claim is evaluated and challenged. It is argued that while a state’s government may be an organization, the state must be an entity independent from its government or broader socio-political and economic structure. It is argued that when a certain set of conditions is satisfied, a new non-physical legal person is brought into being that is independent of any existing entity. Finally, placement of the state as a legal person within the DOLCE ontology is considered, especially with the inclusion of Bottazzi and Ferrario’s ontology of organizations.",
    "author": [
      {
        "family": "Robinson",
        "given": "Edward H."
      }
    ],
    "container-title": "Applied Ontology",
    "id": "Robinson2010",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "ontologies",
    "page": "109-125",
    "publisher": "IOS Press",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "An ontological analysis of states: Organizations vs. Legal persons",
    "title-short": "An ontological analysis of states",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.1080/14650045.2014.913027",
    "ISSN": "1557-3028",
    "abstract": "This article is concerned with the existence of states as a matter of fact, and it approaches that subject within the context of the ontology of social reality as a whole. It argues, first, that states do not have a place in the traditional Platonist duality of the concrete and the abstract. Second, that states belong to a third category – the quasi-abstract – that has received philosophical attention with a recently emerging theory of documentality. Documentality, derived from Austin’s theory of performative utterances, claims that documents acts can bring quasi-abstract objects, such as states into being. Third and finally, it argues that the existence of quasi-abstract states should not be rejected on the basis of the Principle of Parsimony, because geopolitical theories that recognise the existence of quasi-abstract states will have greater explanatory power than theories that deny their existence.",
    "author": [
      {
        "family": "Robinson",
        "given": "Edward H."
      }
    ],
    "container-title": "Geopolitics",
    "id": "Robinson2014",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "ontologies",
    "page": "461-489",
    "publisher": "Routledge",
    "title": "A documentary theory of states and their existence as Quasi-Abstract entities",
    "type": "article-journal",
    "volume": "19"
  },
  {
    "URL": "http://go.editlib.org/p/8835",
    "abstract": "This article gives an overview of the first generation of course-support systems designed to support World Wide Web (WWW)-based learning. It starts with a perspective in terms of pedagogic and technological time scales, traces their origin, discusses their features, reports on their numbers, elucidates their architecture, and talks about the potential for the next generation.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Robson",
        "given": "Robby"
      }
    ],
    "container-title": "International Journal of Educational Telecommunications",
    "id": "Robson1999",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "e-learning",
    "note": "Journal version of an ED-Media paper: http://www.eduworks.com/Documents/Workshops/Webnet2000/supptsys.html",
    "page": "271-282",
    "publisher": "AACE",
    "publisher-place": "Charlottesville, VA",
    "title": "WWW-based course-support systems: The first generation",
    "title-short": "WWW-based course-support systems",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.1145/1594173.1594176",
    "ISSN": "1559-1131",
    "abstract": "This article uses data from the social bookmarking site del.icio.us to empirically examine the dynamics of collaborative tagging systems and to study how coherent categorization schemes emerge from unsupervised tagging by individual users. First, we study the formation of stable distributions in tagging systems, seen as an implicit form of “consensus” reached by the users of the system around the tags that best describe a resource. We show that final tag frequencies for most resources converge to power law distributions and we propose an empirical method to examine the dynamics of the convergence process, based on the Kullback-Leibler divergence measure. The convergence analysis is performed for both the most utilized tags at the top of tag distributions and the so-called long tail. Second, we study the information structures that emerge from collaborative tagging, namely tag correlation (or folksonomy) graphs. We show how community-based network techniques can be used to extract simple tag vocabularies from the tag correlation graphs by partitioning them into subsets of related tags. Furthermore, we also show, for a specialized domain, that shared vocabularies produced by collaborative tagging are richer than the vocabularies which can be extracted from large-scale query logs provided by a major search engine. Although the empirical analysis presented in this article is based on a set of tagging data obtained from del.icio.us, the methods developed are general, and the conclusions should be applicable across other websites that employ tagging.",
    "author": [
      {
        "family": "Robu",
        "given": "Valentin"
      },
      {
        "family": "Halpin",
        "given": "Harry"
      },
      {
        "family": "Shepherd",
        "given": "Hana"
      }
    ],
    "container-title": "ACM Trans. Web",
    "id": "Robu2009",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "semantic_web, vocabularies",
    "page": "1-34",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Emergence of consensus and shared vocabularies in collaborative tagging systems",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "ISBN": "9780262034357",
    "author": [
      {
        "family": "Rockwell",
        "given": "Geoffrey"
      },
      {
        "family": "Sinclair",
        "given": "Stéfan"
      }
    ],
    "id": "Rockwell2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "digital_humanities, hermeneutics, literature",
    "language": "en-US",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA",
    "title": "Hermeneutica: Computer-assisted interpretation in the humanities",
    "title-short": "Hermeneutica",
    "type": "book"
  },
  {
    "URL": "https://www.journals.uio.no/index.php/osla/article/download/104/202",
    "abstract": "We present an overview of an ongoing project which has the aim of developing methods for building a treebank of Icelandic. The treebank will contain texts from various different periods. Since Icelandic is an example of what has been called a less-resourced language when it comes to computational linguistics and language technology, it is essential to utilize the limited resources available as economically and efficiently as possible. We emphasize the importance of open source software and the interplay between linguistic knowledge and technological skills. We describe the workflow in the construction of the treebank and show how the different software tools work together towards the final representation. Finally, we show how the treebank can be used in studying some well known phenomena in Icelandic syntax.",
    "author": [
      {
        "family": "Rögnvaldsson",
        "given": "Eiríkur"
      },
      {
        "family": "Ingason",
        "given": "Anton K."
      },
      {
        "family": "Sigurðsson",
        "given": "Einar F."
      }
    ],
    "container-title": "Oslo Studies in Language",
    "id": "Roegnvaldsson2011",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, icelandic",
    "page": "97-112",
    "title": "Coping with variation in the Icelandic Parsed Historical Corpus (IcePaHC)",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "URL": "http://www.aclweb.org/anthology-new/W/W10/W10-0401.bib",
    "abstract": "In Technical Documentation, Authoring Tools are used to maintain a consistent text quality—especially with regard to the often followed translation of the original documents into several languages using a Translation Memory System. Hitherto these tools have often been used separately one after the other. Additionally Authoring tools often have no linguistic intelligence and thus the quality level of the automated checks is very poor. In this paper I will describe the integration of a linguistically intelligent Authoring Tool into a Translation Memory System, thereby combining linguistic intelligence with the advantages of both systems in a single environment. The system allows you not only the use of common authoring aids (spell, grammar and style checker) in source and target language—by using a single environment the terminology database of the Translation Memory System can be used by the authoring aid to control terminology both in the source and target document. Moreover, the linguistically intelligent Authoring Tool enables automatic extraction of term candidates from existing documents directly to the terminology database of the Translation Memory System.",
    "author": [
      {
        "family": "Rösener",
        "given": "Christoph"
      }
    ],
    "container-title": "Proceedings of the NAACL HLT 2010 workshop on computational linguistics and writing: Writing processes and authoring aids",
    "editor": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Dale",
        "given": "Robert"
      }
    ],
    "id": "Roesener2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "authoring, nlp",
    "page": "1-6",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Computational linguistics in the translator’s Workflow—Combining authoring tools and translation memory systems",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Rösner",
        "given": "Dietmar"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      }
    ],
    "container-title": "Proceedings of the 4<sup>th</sup> IASTED international conference on Web-based education (WBE 2005), february 21–23, 2005, grindelwald, switzerland",
    "id": "Roesner2005a",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "page": "655-660",
    "publisher": "IASTED; ACTA Press",
    "title": "A Web-based environment to support teaching of programming paradigms",
    "type": "paper-conference"
  },
  {
    "abstract": "Wir berichten über Entwurf, Implementierung und Einsatz des Systems LlsChecker. LlsChecker ist eine in ein Content-Management-System (CMS) für Lehr- und Lernmaterialien integrierte Komponente zur automatischen Überprüfung studentischer Lösungen für Programmieraufgaben in unterschiedlichen funktionalen Programmiersprachen. Das System ist so generisch organisiert, dass die Ausweitung der Dienste auf weitere Sprachen – zumindest für funktionale Programmiersprachen – allein durch eine XML-basierte Deklaration möglich ist.",
    "author": [
      {
        "family": "Rösner",
        "given": "Dietmar"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "collection-title": "Lecture notes in informatics",
    "container-title": "DeLFI2005: 3. Deutsche e-learning fachtagung informatik der gesellschaft für informatik e.v.",
    "editor": [
      {
        "family": "Jörg M. Haake",
        "given": "Djamshid Tavangarian",
        "suffix": "Ulrike Lucke"
      }
    ],
    "id": "Roesner2005b",
    "issued": {
      "date-parts": [
        [
          2005,
          9
        ]
      ]
    },
    "page": "307-318",
    "publisher": "GI-Verlag",
    "publisher-place": "Rostock, Germany",
    "title": "LlsChecker – ein CAA-System für die Lehre im Bereich Programmiersprachen",
    "type": "paper-conference",
    "volume": "P-66"
  },
  {
    "abstract": "Übungen sind ein zentrales Element in der Informatiklehre. Ausgehend von didaktischen Überlegungen, wie der Übungsbetrieb durch Komponenten des ELearning, insbesondere durch Formen des Computer-Aided Assessment, intensiviert und effizienter gestaltet werden kann, haben wir die EduComponents entwickelt. Dabei handelt es sich um eine Sammlung von Erweiterungsmodulen, die ein allgemeines CMS (Plone) um E-Learning-Funktionalität ergänzen. Seit mehreren Semestern werden diese frei verfügbaren Module sowohl in allen Lehrveranstaltungen unserer Arbeitsgruppe als auch an anderen Institutionen erfolgreich eingesetzt.",
    "author": [
      {
        "family": "Rösner",
        "given": "Dietmar"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "collection-title": "Lecture notes in informatics (LNI) – proceedings",
    "container-title": "2. GI-fachtagung hochschuldidaktik der informatik",
    "editor": [
      {
        "family": "Forbrig",
        "given": "Peter"
      },
      {
        "family": "Siegel",
        "given": "Günter"
      },
      {
        "family": "Schneider",
        "given": "Markus"
      }
    ],
    "id": "Roesner2006",
    "issued": {
      "date-parts": [
        [
          2006,
          12
        ]
      ]
    },
    "page": "89-102",
    "publisher": "GI-Verlag",
    "publisher-place": "Munich, Germany",
    "title": "E-Learning-Komponenten zur Intensivierung der Übungen in der Informatik-Lehre – ein Erfahrungsbericht",
    "type": "paper-conference",
    "volume": "P-100"
  },
  {
    "URL": "http://edoc.mpg.de/315523",
    "abstract": "This paper presents our approach for supporting face-to-face courses with software components for e-learning based on a general-purpose content management system (CMS). These components—collectively named eduComponents—can be combined with other modules to create tailor-made, sustainable learning environments, which help to make teaching and learning more efficient and effective. We give a short overview of these components, and we report on our practical experiences with the software in our courses.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Rösner",
        "given": "Dietmar"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      }
    ],
    "container-title": "Proceedings of the german e-science conference (GES 2007)",
    "editor": [
      {
        "family": "Bühler",
        "given": "Wilhelm"
      }
    ],
    "id": "Roesner2007",
    "issued": {
      "date-parts": [
        [
          2007,
          5
        ]
      ]
    },
    "publisher": "Max-Planck-Gesellschaft",
    "publisher-place": "Baden-Baden, Germany",
    "title": "A sustainable learning environment based on an open source content management system",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1108/eb026883",
    "ISSN": "0022-0418",
    "abstract": "An increasing volume of historical text is being converted into machine-readable form so as to allow database searches to be carried out. The age of the material in these databases means that they contain many spellings that are different from those used today. This characteristic means that, once the databases become available for general online access, users will need to be familiar with all of the possible historical spellings for their topic of interest if a search is to be carried out successfully. This paper investigates the use of computational techniques that have been developed for the correction of spelling errors to identify historical spellings of a user’s search terms. Two classes of spelling correction method are tested, these being the reverse error and phonetic coding methods. Experiments with words from the Hartlib Papers Collection show that these methods can correctly identify a large number of historical forms of modern-day word spellings.",
    "author": [
      {
        "family": "Rogers",
        "given": "Heather J."
      },
      {
        "family": "Willett",
        "given": "Peter"
      }
    ],
    "container-title": "Journal of Documentation",
    "id": "Rogers1991",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "keyword": "cultural_heritage, english, ir, spelling_correction, spelling_normalization",
    "page": "333-353",
    "title": "Searching for historical word forms in text databases using spelling-correction methods: Reverse error and phonetic coding methods",
    "title-short": "Searching for historical word forms in text databases using spelling-correction methods",
    "type": "article-journal",
    "volume": "47"
  },
  {
    "abstract": "We describe experiments with morphosyntactic tagging of Old Norse narrative texts using different tagging models for the TnT tagger(Brants, 2000) and a tagset of almost 700 tags. It is shown that by using a model that has been trained on both Modern Icelandic texts and Old Norse texts, we can get 92.7% tagging accuracy which is considerably better than the 90.4% that have been reported for Modern Icelandic. In the second half of the paper, we show that the richness of our tagset enables us to use the morphosyntactic tags insearching for certain syntactic constructions and features in a large corpus of Old Norse narrative texts. We demonstrate this by searching for ­ and finding ­ previously undiscovered examples of two syntactic constructions in the corpus. We conclude that in an inflectional language like Old Norse, a morphologically tagged corpus like this can be an important tool in studying syntactic variation andchange.",
    "author": [
      {
        "family": "Rögnvaldsson",
        "given": "Eiríkur"
      },
      {
        "family": "Helgadóttir",
        "given": "Sigrún"
      }
    ],
    "container-title": "Proceedings of the LREC 2008 workshop on language technology for cultural heritage data (LaTeCH 2008)",
    "editor": [
      {
        "family": "Ribarov",
        "given": "Kiril"
      },
      {
        "family": "Sporleder",
        "given": "Caroline"
      }
    ],
    "id": "Rognvaldsson2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, morphology, swedish",
    "page": "40-46",
    "title": "Morphological tagging of Old Norse texts and its use in studying syntactic variation and change",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-20227-8_4",
    "ISBN": "978-3-642-20226-1",
    "abstract": "We describe experiments with morphosyntactic tagging of Old Icelandic (Old Norse) narrative texts using different tagging models for the TnT tagger [3] and a tagset of almost 700 tags, originally developed for Modern Icelandic. It is shown that by using a model that has been trained on both Old and Modern Icelandic texts, we can get 92.7% tagging accuracy which is considerably better than the 90.4% that have been reported for Modern Icelandic. Although our tagging is morphological in nature, the tags carry a substantial amount of syntactic information and the tagging is detailed enough for the syntactic function of words to be more or less deduced from their morphology and the adjacent words. We show that the morphosyntactic tags can be very useful in locating certain syntactic constructions and features in a large corpus of Old Icelandic narrative texts. We demonstrate this by searching for—and finding—previously undiscovered examples of a number of syntactic constructions in the corpus.We conclude that in a highly inflectional language, a morphologically tagged corpus can be an important tool in studying syntactic variation and change, in the absence of a fully parsed corpus which of course gives more possibilities.",
    "author": [
      {
        "family": "Rögnvaldsson",
        "given": "Eiríkur"
      },
      {
        "family": "Helgadóttir",
        "given": "Sigrún"
      }
    ],
    "chapter-number": "4",
    "collection-title": "Theory and applications of natural language processing",
    "container-title": "Language technology for cultural heritage",
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "family": "Bosch",
        "given": "Antal"
      },
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      }
    ],
    "id": "Rognvaldsson2011b",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, icelandic, pos_tagging",
    "page": "63-76",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Morphosyntactic tagging of Old Icelandic texts and its use in studying syntactic variation and change",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/1557914.1557987",
    "ISBN": "978-1-60558-486-7",
    "abstract": "Modern critical editions of ancient works generally include manually created indices of other sources quoted in the text. Since indices can be considered as a form of domain specific language, the paper presents a parsing-based approach to the problem of extracting information from them to support the creation of a collection of fragmentary texts. This paper first considers the characteristics and structure of quotation indices and their importance when dealing with fragmentary texts. It then presents the results of applying a fuzzy parser to the OCR transcription of an index of quotations to extract information from potentially noisy input.",
    "author": [
      {
        "family": "Romanello",
        "given": "Matteo"
      },
      {
        "family": "Berti",
        "given": "Monica"
      },
      {
        "family": "Babeu",
        "given": "Alison"
      },
      {
        "family": "Crane",
        "given": "Gregory"
      }
    ],
    "container-title": "Proceedings of the 20<sup>th</sup> ACM conference on hypertext and hypermedia (HT ’09)",
    "id": "Romanello2009a",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_edition, indices",
    "page": "357-358",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "When printed hypertexts go digital: Information extraction from the parsing of indices",
    "title-short": "When printed hypertexts go digital",
    "type": "paper-conference"
  },
  {
    "ISBN": "978-1-932432-58-9",
    "URL": "http://portal.acm.org/citation.cfm?id=1699750.1699763",
    "abstract": "Scholars of Classics cite ancient texts by using abridged citations called canonical references. In the scholarly digital library, canonical references create a complex textile of links between ancient and modern sources reflecting the deep hypertextual nature of texts in this field. This paper aims to demonstrate the suitability of Conditional Random Fields (CRF) for extracting this particular kind of reference from unstructured texts in order to enhance the capabilities of navigating and aggregating scholarly electronic resources. In particular, we developed a parser which recognizes word level n-grams of a text as being canonical references by using a CRF model trained with both positive and negative examples.",
    "author": [
      {
        "family": "Romanello",
        "given": "Matteo"
      },
      {
        "family": "Boschetti",
        "given": "Federico"
      },
      {
        "family": "Crane",
        "given": "Gregory"
      }
    ],
    "collection-title": "NLPIR4DL ’09",
    "container-title": "Proceedings of the 2009 workshop on text and citation analysis for scholarly digital libraries",
    "id": "Romanello2009b",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "todo",
    "page": "80-87",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Citations in the digital library of classics: Extracting canonical references by using conditional random fields",
    "title-short": "Citations in the digital library of classics",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/2517978.2517981",
    "ISBN": "978-1-4503-2199-0",
    "URL": "http://dx.doi.org/10.1145/2517978.2517981",
    "abstract": "Annotations played a major role in Classics since the very beginning of the discipline. Some of the first attested examples of philological work, the so-called scholia, were in fact marginalia, namely comments written at the margins of a text. Over the centuries this kind of scholarship evolved until it became a genre on its own, the classical commentary, thus moving away from the text with the result that philologists had to devise a solution to linking together the commented and the commenting text. The solution to this problem is the system of canonical citations, a special kind of bibliographic references that are at the same time very precise and highly interoperable. In this paper we present HuCit, an ontology that models in depth the semantics of canonical citations. We discuss how it can be used to a) support the automatic extraction of canonical citations from texts and b) to publish them in machine-readable format on the Semantic Web. Finally, we describe how HuCit’s machine-generated citation data can also be expressed as annotations by using the Open Annotation Collaboration (OAC) ontology, to the aim of increasing reuse and semantic interoperability.",
    "author": [
      {
        "family": "Romanello",
        "given": "Matteo"
      },
      {
        "family": "Pasin",
        "given": "Michele"
      }
    ],
    "collection-title": "DH-CASE ’13",
    "container-title": "Proceedings of the 1st international workshop on collaborative annotations in shared environment: Metadata, vocabularies and techniques in the digital humanities",
    "id": "Romanello2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "todo",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Citations and annotations in classics: Old problems and new perspectives",
    "title-short": "Citations and annotations in classics",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.center.kva.se/svenska/forskning/NS\\_147\\_Program.html",
    "author": [
      {
        "family": "Romary",
        "given": "Laurent"
      }
    ],
    "collection-title": "Nobel symposium",
    "container-title": "Going digital. Evolutionary and revolutionary aspects of digitization",
    "editor": [
      {
        "family": "Grandin",
        "given": "Karl"
      }
    ],
    "id": "Romary2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "tei, xml",
    "page": "188-218",
    "publisher": "Science History Publications",
    "publisher-place": "New York, New York, USA",
    "title": "Stabilising knowledge through standards: A perspective for the humanities",
    "title-short": "Stabilising knowledge through standards",
    "type": "chapter",
    "volume": "147"
  },
  {
    "DOI": "10.1007/978-3-319-75759-9",
    "editor": [
      {
        "family": "Romele",
        "given": "Alberto"
      },
      {
        "family": "Terrone",
        "given": "Enrico"
      }
    ],
    "id": "Romele2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Palgrave Macmillan",
    "title": "Towards a philosophy of digital media",
    "type": "book"
  },
  {
    "URL": "https://aclanthology.org/W11-4114",
    "author": [
      {
        "family": "Romero",
        "given": "Veronica"
      },
      {
        "family": "Serrano",
        "given": "Nicolas"
      },
      {
        "family": "Toselli",
        "given": "Alejandro H."
      },
      {
        "family": "Sanchez",
        "given": "Joan A."
      },
      {
        "family": "Vidal",
        "given": "Enrique"
      }
    ],
    "container-title": "Proceedings of the RANLP 2011 workshop on language technologies for digital humanities and cultural heritage",
    "id": "Romero2011",
    "issued": {
      "date-parts": [
        [
          2011,
          9
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "page": "90-96",
    "publisher-place": "Hissar, Bulgaria",
    "title": "Handwritten text recognition for historical documents",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1016/j.specom.2007.04.006",
    "ISSN": "01676393",
    "URL": "http://dx.doi.org/10.1016/j.specom.2007.04.006",
    "abstract": "In multilingual countries, text-to-speech synthesis systems often have to deal with texts containing inclusions of multiple other languages in form of phrases, words, or even parts of words. In such multilingual cultural settings, listeners expect a high-quality text-to-speech synthesis system to read such texts in a way that the origin of the inclusions is heard, i.e., with correct language-specific pronunciation and prosody. The challenge for a text analysis component of a text-to-speech synthesis system is to derive from mixed-lingual sentences the correct polyglot phone sequence and all information necessary to generate natural sounding polyglot prosody. This article presents a new approach to analyze mixed-lingual sentences. This approach centers around a modular, mixed-lingual morphological and syntactic analyzer, which additionally provides accurate language identification on morpheme level and word and sentence boundary identification in mixed-lingual texts. This approach can also be applied to word identification in languages without a designated word boundary symbol like Chinese or Japanese. To date, this mixed-lingual text analysis supports any mixture of English, French, German, Italian, and Spanish. Because of its modular design it is easily extensible to additional languages.",
    "author": [
      {
        "family": "Romsdorfer",
        "given": "Harald"
      },
      {
        "family": "Pfister",
        "given": "Beat"
      }
    ],
    "container-title": "Speech Communication",
    "id": "Romsdorfer2007",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          2007,
          9
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "page": "697-724",
    "title": "Text analysis and language identification for polyglot text-to-speech synthesis",
    "type": "article-journal",
    "volume": "49"
  },
  {
    "abstract": "Internet and intranet technologies offer tremendous opportunities to bring learning into the mainstream of business. E-Learning outlines how to develop an organization-wide learning strategy based on cutting-edge technologies and explains the dramatic strategic, organizational, and technology issues involved.<p>Written for professionals responsible for leading the revolution in workplace learning, E-Learning takes a broad, strategic perspective on corporate learning. This wake-up call for executives everywhere discusses:<br>? Requirements for building a viable e-learning strategy<br>? How online learning will change the nature of training organizations <br>? Knowledge management and other new forms of e-learning <p>Marc J. Rosenberg, Ph.D. (Hillsborough, NJ) is an independent consultant specializing in knowledge management, e-learning strategy and the reinvention of training. Prior to this, he was a senior direction and kowledge management field leader for consulting firm DiamondCluster International.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Rosenberg",
        "given": "Marc J."
      }
    ],
    "id": "Rosenberg2000",
    "issued": {
      "date-parts": [
        [
          2000,
          10
        ]
      ]
    },
    "keyword": "e-learning",
    "publisher": "Hardcover; McGraw-Hill",
    "title": "E-learning: Strategies for delivering knowledge in the digital age",
    "title-short": "E-learning",
    "type": "book"
  },
  {
    "DOI": "10.1145/800045.801604",
    "ISBN": "0-89791-121-0",
    "URL": "http://dx.doi.org/10.1145/800045.801604",
    "abstract": "What are the effects of experience on text editing behavior? Do users inevitably develop optimal strategies for getting their work done? The answer to such questions are becoming increasingly important, as more and more individuals begin to use word processing equipment routinely. In the best of all possible worlds, experienced users do become experts, able to quickly and accurately choose and execute optimal procedures to accomplish any given goal. Such a state of affairs would make designers of editing systems very happy indeed. But another alternative exists, that at least some proportion of experienced and frequent users stabilize at some nonoptimal level of skill. An initial survey of relevant research is encouraging. So, for example, Card, Moran, and Newell (1980) were able to deduce selection rules from experienced users’ editing behavior that predicted 80% of their editing decisions, suggesting that experienced users have fairly well-defined heuristics for carrying out editing tasks. Tyler and Roth (1982) followed up on this work, demonstrating that novices were less likely to demonstrate selection rules than experienced users, preferring instead to rely on a single, sometimes inefficient, strategy. Finally, Folley and Williges (1982) demonstrated that when confronted with the description of a novel editor, users experienced on other systems make use of a greater number of commands in carrying out a paper-and-pencil application of the editor than do complete novices.",
    "author": [
      {
        "family": "Rosson",
        "given": "Mary B."
      }
    ],
    "container-title": "CHI ’83: Proceedings of the SIGCHI conference on human factors in computing systems",
    "id": "Rosson1983",
    "issued": {
      "date-parts": [
        [
          1983
        ]
      ]
    },
    "keyword": "interactive_editing",
    "page": "171-175",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Patterns of experience in text editing",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-13084-7_5",
    "ISBN": "978-3-642-13083-0",
    "abstract": "As a low-cost ressource that is up-to-date, Wikipedia recently gains attention as a means to provide cross-language brigding for information retrieval. Contradictory to a previous study, we show that standard Latent Dirichlet Allocation (LDA) can extract cross-language information that is valuable for IR by simply normalizing the training data. Furthermore, we show that LDA and Explicit Semantic Analysis (ESA) complement each other, yielding significant improvements when combined. Such a combination can significantly contribute to retrieval based on machine translation, especially when query translations contain errors. The experiments were perfomed on the Multext JOC corpus und a CLEF dataset.",
    "author": [
      {
        "family": "Roth",
        "given": "Benjamin"
      },
      {
        "family": "Klakow",
        "given": "Dietrich"
      }
    ],
    "chapter-number": "5",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Advances in multidisciplinary retrieval",
    "editor": [
      {
        "family": "Cunningham",
        "given": "Hamish"
      },
      {
        "family": "Hanbury",
        "given": "Allan"
      },
      {
        "family": "Rüger",
        "given": "Stefan"
      }
    ],
    "id": "Roth2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "ir, wikipedia",
    "page": "47-59",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Combining Wikipedia-based concept models for cross-language retrieval",
    "type": "chapter",
    "volume": "6107"
  },
  {
    "DOI": "10.1093/llc/fqy057",
    "ISSN": "2055-7671",
    "abstract": "The term ’digital humanities’ may be understood in three different ways: as ’digitized humanities’, by dealing essentially with the constitution, management, and processing of digitized archives; as ’numerical humanities’, by putting the emphasis on mathematical abstraction and the development of numerical and formal models; and as ’humanities of the digital’, by focusing on the study of computer-mediated interactions and online communities. Discussing their methods and actors, we show how these three potential acceptations cover markedly distinct epistemological endeavors and, eventually, non-overlapping scientific communities.",
    "author": [
      {
        "family": "Roth",
        "given": "Camille"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Roth2018",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "616-632",
    "title": "Digital, digitized, and numerical humanities",
    "type": "article-journal",
    "volume": "34"
  },
  {
    "DOI": "10.1016/0169-7552(94)90134-1",
    "author": [
      {
        "family": "Rousseau",
        "given": "Bertrand"
      },
      {
        "family": "Ruggier",
        "given": "Mario"
      }
    ],
    "container-title": "Selected papers of the first conference on world-wide web",
    "editor": [
      {
        "family": "Cailliau",
        "given": "Robert"
      }
    ],
    "id": "Rousseau1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "framemaker",
    "page": "205-214",
    "publisher": "Elsevier",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "Writing documents for paper and WWW: A strategy based on FrameMaker and WebMaker",
    "title-short": "Writing documents for paper and WWW",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/biomet/63.3.581",
    "ISSN": "0006-3444",
    "abstract": "When making sampling distribution inferences about the parameter of the data, θ, it is appropriate to ignore the process that causes missing data if the missing data are ’missing at random’ and the observed data are ’observed at random’, but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about θ, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is ’distinct’ from θ. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.",
    "author": [
      {
        "family": "Rubin",
        "given": "Donald B."
      }
    ],
    "container-title": "Biometrika",
    "id": "Rubin1976",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "581-592",
    "title": "Inference and missing data",
    "type": "article-journal",
    "volume": "63"
  },
  {
    "ISBN": "9783412211103",
    "author": [
      {
        "family": "Rüsen",
        "given": "Jörn"
      }
    ],
    "id": "Ruesen2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "history",
    "language": "de-DE",
    "publisher": "Böhlau",
    "publisher-place": "Wien",
    "title": "Historik: Theorie der Geschichtswissenschaft",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Rütter",
        "given": "Theodor"
      }
    ],
    "id": "Ruetter1973",
    "issued": {
      "date-parts": [
        [
          1973
        ]
      ]
    },
    "publisher": "Beck",
    "publisher-place": "München",
    "title": "Formen der testaufgabe: Eine einführung für didaktische zwecke",
    "title-short": "Formen der testaufgabe",
    "type": "book"
  },
  {
    "ISSN": "1615-3014",
    "URL": "http://www.linguistik-online.de/25_05/ruge.pdf",
    "abstract": "Morphematic reorientation of German orthography takes place in conformity with a general law in the history of writing. Alphabetic writing systems, being necessarily phonographic, tend to develop towards the encoding of non-phonetic units. The emergence of morphematic elements in German is preceded by the evolution of word-separation by regular spaces which had been adopted throughout Europe by the end of the 13th century. Based on a corpus containing 157 High German texts (late 15th to late 18th centuries), this paper will demonstrate that morphematic reorientation of German orthography can be explained neither as an invisible-hand-process nor as the outcome of prescriptive grammar, but as the result of interaction between orthograpic norm and usage. Three cases will be considered in detail: (1) Graphic assimilation of allomorphic plosive variation emerges as early as the 12th century, reflecting regional final devoicing. By the early 17th century, the rule governing orthographic representation of final devoicing in present-day German is fully adopted in usage. Its morphematic reinterpretation does not follow before the end of the 17th century. (2) Morphematic graphic representation of [a]-Umlaut emerges during the 14th century in Upper Germany as a phonetic reflex of open [e]. It is recommended by Middle German grammarians since the 1560s, with explicit mention of morphological factors. Around 1700 the writing rule imposes itself in usage. (3) The use of double consonant letters occuring in final positions of ’graphic’ syllables (<soll> according to <sollen>) rests inhibited until the 18th century, in particular to prevent tri- or tessaragraphs (<sollt>, <sollst>). It is the influence of Adelung’s grammar which leads to the final adaption of the present-day rule.",
    "author": [
      {
        "family": "Ruge",
        "given": "Nikolaus"
      }
    ],
    "container-title": "Linguistik online",
    "id": "Ruge2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "corpus_linguistics, german, orthography",
    "language": "de-DE",
    "page": "65-83",
    "title": "Zur morphembezogenen Überformung der deutschen Orthographie",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "DOI": "10.1007/978-3-642-25093-4_27",
    "ISBN": "978-3-642-25092-7",
    "abstract": "Since the Linked Data is continuously growing on the Web, the quality of overall data can rapidly degrade over time. The research proposed here deals with the quality assessment in the Linked Data and the temporal linking techniques. First, we conduct an in-depth study of appropriate dimensions and their respectively metrics by defining a data quality framework that evaluates, along these dimensions, linked published data on the Web. Second, since the assessment and improvement of the Linked Data quality such as accuracy or the resolution of heterogeneities is performed through record linkage techniques, we propose an extended technique that apply time in similarity computation which can improve over traditional linkage techniques. This paper describes the core problem, presents the proposed approach, reports on initial results, and lists planned future tasks.",
    "author": [
      {
        "family": "Rula",
        "given": "Anisa"
      }
    ],
    "chapter-number": "27",
    "collection-title": "Lecture notes in computer science",
    "container-title": "The semantic web – ISWC 2011",
    "editor": [
      {
        "family": "Aroyo",
        "given": "Lora"
      },
      {
        "family": "Welty",
        "given": "Chris"
      },
      {
        "family": "Alani",
        "given": "Harith"
      },
      {
        "family": "Taylor",
        "given": "Jamie"
      },
      {
        "family": "Bernstein",
        "given": "Abraham"
      },
      {
        "family": "Kagal",
        "given": "Lalana"
      },
      {
        "family": "Noy",
        "given": "Natasha"
      },
      {
        "family": "Blomqvist",
        "given": "Eva"
      }
    ],
    "id": "Rula2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "rdf, temporal_data",
    "language": "en-US",
    "page": "341-348",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "DC proposal: Towards linked data assessment and linking temporal facts",
    "title-short": "DC proposal",
    "type": "chapter",
    "volume": "7032"
  },
  {
    "ISBN": "1935928155",
    "abstract": "The debate over whether the Net is good or bad for us fills the airwaves and the blogosphere. But for all the heat of claim and counter-claim, the argument is essentially beside the point: it’s here; it’s everywhere. The real question is, do we direct technology, or do we let ourselves be directed by it and those who have mastered it? \"Choose the former,\" writes Rushkoff, \"and you gain access to the control panel of civilization. Choose the latter, and it could be the last real choice you get to make.\" In ten chapters, composed of ten \"commands\" accompanied by original illustrations from comic artist Leland Purvis, Rushkoff provides cyberenthusiasts and technophobes alike with the guidelines to navigate this new universe. In this spirited, accessible poetics of new media, Rushkoff picks up where Marshall McLuhan left off, helping readers come to recognize programming as the new literacy of the digital age—and as a template through which to see beyond social conventions and power structures that have vexed us for centuries. This is a friendly little book with a big and actionable message. World-renowned media theorist and counterculture figure Douglas Rushkoff is the originator of ideas such as \"viral media,\" \"social currency\" and \"screenagers.\" He has been at the forefront of digital society from its beginning, correctly predicting the rise of the net, the dotcom boom and bust, as well as the current financial crisis. He is a familiar voice on NPR, face on PBS, and writer in publications from Discover Magazine to the New York Times.",
    "author": [
      {
        "family": "Rushkoff",
        "given": "Douglas"
      }
    ],
    "id": "Rushkoff2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "digital_humanities, philosophy, programming",
    "language": "en-US",
    "publisher": "OR Books",
    "publisher-place": "New York, NY, USA",
    "title": "Program or be programmed: Ten commands for a digital age",
    "title-short": "Program or be programmed",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Russell",
        "given": "Robert C."
      }
    ],
    "id": "Russell1918",
    "issued": {
      "date-parts": [
        [
          1918
        ]
      ]
    },
    "note": "Filing Date: October 25, 1917",
    "publisher": "United States Patent 1 261 167",
    "title": "Index: Specification of letters",
    "title-short": "Index",
    "type": ""
  },
  {
    "author": [
      {
        "family": "Russell",
        "given": "Robert C."
      }
    ],
    "id": "Russell1922",
    "issued": {
      "date-parts": [
        [
          1922
        ]
      ]
    },
    "note": "Filing Date: November 28, 1921",
    "publisher": "United States Patent 1 435 663",
    "title": "Index",
    "type": ""
  },
  {
    "ISBN": "0-7695-1939-3",
    "URL": "http://portal.acm.org/citation.cfm?id=827207",
    "abstract": "Early modern books written in Latin contain many abbreviations of common words that are derived from earlier manuscript practice. While these abbreviations are usually easily deciphered by a reader well-versed in Latin, they pose technical problems for full text digitization: they are difficult to OCR or have typed and—if they are not expanded correctly—they limit the effectiveness of information retrieval and reading support tools in the digital library. In this paper, I will describe a method for the automatic expansion and disambiguation of these abbreviations.",
    "author": [
      {
        "family": "Rydberg Cox",
        "given": "Jeffrey A."
      }
    ],
    "container-title": "Proceedings of the 3rd ACM/IEEE-CS joint conference on digital libraries",
    "id": "Rydberg-Cox2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "page": "372-373",
    "publisher": "IEEE Computer Society",
    "publisher-place": "Washington, DC, USA",
    "title": "Automatic disambiguation of Latin abbreviations in early modern texts for humanities digital libraries",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-04131-0",
    "ISBN": "978-3-642-04130-3",
    "abstract": "This book constitutes the refereed proceedings of the Workshop on Systems and Frameworks for Computational Morphology, SFCM 2009, held in Zürich, Switzerland, in September 2009. The workshop had three main goals: to stimulate discussion among reseachers and developers and to offer an up-to-date overview of available systems for German morphology which provide deep analyses and are suitable for generating specific word forms; to stimulate discussion among developers of general frameworks that can be used to implement morphological components for several languages; to discuss aspects of evaluation of morphology systems and possible future competitions or tasks, such as a new edition of the Morpholympics.",
    "collection-title": "Communications in computer and information science",
    "editor": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "SFCM2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "morphology, mxp",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "State of the art in computational morphology: Workshop on systems and frameworks for computational morphology, SFCM 2009, zurich, switzerland, september 2009, proceedings (communications in computer and information science)",
    "title-short": "State of the art in computational morphology",
    "type": "book",
    "volume": "41"
  },
  {
    "author": [
      {
        "literal": "ISO (International Organization for Standardization)"
      }
    ],
    "id": "SGML",
    "issued": {
      "date-parts": [
        [
          1986
        ]
      ]
    },
    "title": "ISO 8879:1986(E). Information processing — Text and office systems — Standard Generalized Markup Language (SGML)",
    "type": ""
  },
  {
    "URL": "http://www.lt-world.org/HLT_Survey/master.pdf",
    "abstract": "Languages, in all their forms, are the more efficient and natural means for people to communicate. Enormous quantities of information are produced, distributed and consumed using languages. Human language technology’s main purpose is to allow the use of automatic systems and tools to assist humans in producing and accessing information, to improve communication between humans, and to assist humans in communicating with machines. This book, sponsored by the Directorate General XIII of the European Union and the Information Science and Engineering Directorate of the National Science Foundation, USA, offers the first comprehensive overview of the human language technology field.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "editor": [
      {
        "family": "Cole",
        "given": "Ronald"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Uszkoreit",
        "given": "Hans"
      },
      {
        "family": "Zaenen",
        "given": "Annie"
      },
      {
        "family": "Zue",
        "given": "Viktor"
      }
    ],
    "id": "SSAHLT1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "publisher": "Cambridge University Press",
    "title": "Survey of the state of the art in human language technology (studies in natural language processing)",
    "type": "book"
  },
  {
    "collection-title": "Sammlung Schweizerischer Rechtsquellen",
    "editor": [
      {
        "literal": "Rechtsquellenstiftung des Schweizerischen Juristenverbandes"
      }
    ],
    "id": "SSRQ-ARAI-1",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "de-DE",
    "note": "Prepared by Nathalie Büsser, indexing by Margrit Meyer Kälin",
    "publisher": "Schwabe",
    "publisher-place": "Basel, Switzerland",
    "title": "Appenzeller Landbücher",
    "type": "book",
    "volume": "SSRQ AR/AI 1 (XIII. Abteilung: Die Rechtsquellen der Kantone Appenzell, Band 1)"
  },
  {
    "collection-title": "Sammlung Schweizerischer Rechtsquellen",
    "editor": [
      {
        "literal": "Rechtsquellenstiftung des Schweizerischen Juristenverbandes"
      }
    ],
    "id": "SSRQ-FR-I-1-1",
    "issued": {
      "date-parts": [
        [
          1925
        ]
      ]
    },
    "language": "de-DE",
    "note": "Prepared by Emil Welti",
    "publisher": "Sauerländer",
    "publisher-place": "Aarau, Switzerland",
    "title": "Das Stadtrecht von Murten",
    "type": "book",
    "volume": "SSRQ FR I/1/1 (IX. Abteilung: Die Rechtsquellen des Kantons Freiburg, Erster Teil: Stadtrechte, Erste Reihe: Landstädte, Band 1)"
  },
  {
    "collection-title": "Sammlung Schweizerischer Rechtsquellen",
    "editor": [
      {
        "literal": "Rechtsquellenstiftung des Schweizerischen Juristenverbandes"
      }
    ],
    "id": "SSRQ-GR-B-II-2",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "language": "de-DE",
    "note": "Prepared by Elisabeth Meyer-Marthaler and Martin Salzmann, indices by Evelyn Ingold",
    "publisher": "Schwabe",
    "publisher-place": "Basel, Switzerland",
    "title": "Landesherrschaft und Bundesrecht",
    "type": "book",
    "volume": "SSRQ GR B II/2 (XV. Abteilung: Die Rechtsquellen des Kantons Graubünden, B. Die Statuten der Gerichtsgemeinden, Zweiter Teil: Der Zehngerichtenbund)"
  },
  {
    "collection-title": "Sammlung Schweizerischer Rechtsquellen",
    "editor": [
      {
        "literal": "Rechtsquellenstiftung des Schweizerischen Juristenverbandes"
      }
    ],
    "id": "SSRQ-SG-II-2-1",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "language": "de-DE",
    "note": "Prepared by Pascale Sutter",
    "publisher": "Schwabe",
    "publisher-place": "Basel, Switzerland",
    "title": "Rechtsquellen der Stadt und Herrschaft Rapperswil",
    "type": "book",
    "volume": "SSRQ SG II/2/1 (XIV. Abteilung: Die Rechtsquellen des Kantons St. Gallen, Zweiter Teil: Die Stadtrechte von St. Gallen und Rapperswil, 2. Reihe: Die Rechtsquellen der Stadt und Herrschaft Rapperswil, Band 1)"
  },
  {
    "collection-title": "Sammlung Schweizerischer Rechtsquellen",
    "editor": [
      {
        "literal": "Rechtsquellenstiftung"
      }
    ],
    "id": "SSRQ-SG-II-2-1-short",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "language": "de-DE",
    "note": "Prepared by Pascale Sutter",
    "publisher": "Schwabe",
    "publisher-place": "Basel, Switzerland",
    "title": "Rechtsquellen der Stadt und Herrschaft Rapperswil",
    "type": "book",
    "volume": "SSRQ SG II/2/1: Die Rechtsquellen der Stadt und Herrschaft Rapperswil)"
  },
  {
    "collection-title": "Sammlung Schweizerischer Rechtsquellen",
    "editor": [
      {
        "literal": "Rechtsquellenstiftung des Schweizerischen Juristenverbandes"
      }
    ],
    "id": "SSRQ-ZH-NF-II-1",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "language": "de-DE",
    "note": "Prepared by Thomas Weibel",
    "publisher": "Sauerländer",
    "publisher-place": "Aarau, Switzerland",
    "title": "Das Neuamt",
    "type": "book",
    "volume": "SSRQ ZH NF II/1 (I. Abteilung: Die Rechtsquellen des Kantons Zürich, Neue Folge: Zweiter Teil: Rechte der Landschaft, Band 1)"
  },
  {
    "DOI": "10.1093/llc/fqr024",
    "author": [
      {
        "family": "Sabin",
        "given": "Philip"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Sabin2011",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "formal_models, digital_humanities",
    "language": "en-US",
    "page": "323-328",
    "title": "The benefits and limits of computerization in conflict simulation",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "abstract": "This paper describes ongoing work on the integration of a machine- translation software, Multra, into the multilingual document processing environment of Scania CV AB. Multra is a prototype of a modular, transfer-based MT-system with Swedish as its source language. It handles translation into English and German, based on a common analysis structure. In order to guarantee consistency in the original as well as in the translated versions of the documents, a controlled language, ScaniaSwedish, is defined. Also, a language checker for this language is developed. The core of the checker is the analysis component of Multra. The checker will provide two kinds of results, i.e. a controlled version of the text and the text as a sequence of grammatical structures. These structures can then be forwarded to transfer and further generation. In other words, checking the text means taking the first step in the translation process. The checker is developed in parallel with the definition of ScaniaSwedish.",
    "author": [
      {
        "family": "Sågvall Hein",
        "given": "Anna"
      }
    ],
    "container-title": "Proceedings of the 7th international conference on theoretical and methodological issues in machine translation",
    "id": "SagvallHein1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "grammar_checking, machine_translation, swedish",
    "language": "en-US",
    "title": "Language control and machine translation",
    "type": "paper-conference"
  },
  {
    "URL": "http://nbn-resolving.de/urn:nbn:de:hbz:38-53534",
    "abstract": "Die wissenschaftliche Edition zielt auf die zuverlässige Wiedergabe des Textes. Aber was ist dieser Text eigentlich? Bei genauerer Betrachtung erlaubt nur ein erweiterter Textbegriff und ein neues pluralistisches Textmodell eine Beschreibung aller textuellen Phänomene, die in einer wissenschaftlichen Edition zu berücksichtigen sind. Auch unsere Technologien und Methodologien der Textcodierung, hier vor allem die Auszeichnungssprachen im Allgemeinen und die Beschreibungsempfehlungen der Text Encoding Initiative (TEI) im Besonderen können unter dieser Schablone genauer beschrieben und hinsichtlich ihrer Grenzen charakterisiert werden. Schließlich erlaubt das pluralistische Textmodell auch die präzisere theoretische Fundierung jener Prozesse, die als \"Transkription\" Grundlage und Herzstück einer jeden wissenschaftlichen Edition sind.",
    "author": [
      {
        "family": "Sahle",
        "given": "Patrick"
      }
    ],
    "collection-title": "Schriften des Instituts für Dokumentologie und Editorik",
    "id": "Sahle2013-bd3",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "document_research, markup, tei, xml",
    "language": "de-DE",
    "publisher": "BoD",
    "publisher-place": "Norderstedt",
    "title": "Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. Teil 3: Textbegriffe und Recodierung",
    "title-short": "Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. Teil 3",
    "type": "book",
    "volume": "9"
  },
  {
    "DOI": "10.17175/sb001_004",
    "abstract": "Die Digital Humanities sind entweder ein Forschungsfeld oder eine Disziplin, möglicherweise auch beides. Sie verfügen jedenfalls über eine gut ausgebaute Infrastruktur der Organisation, Information und Kommunikation und blicken in Bezug auf ihre vielfältigen Forschungsansätze teilweise auf lange Traditionen zurück. Als Bindeglied zwischen den Geisteswissenschaften und der Informatik scheint das Feld heute nicht nur für diese beiden Bereiche, sondern auch für die Organe der Forschungsförderung von besonders hoher Attraktivität zu sein. Neben der Wissenschaft haben selbst die Massenmedien in den letzten Jahren die Digital Humanities entdeckt. Die hohe Anziehungskraft des Feldes hat erfreulich integrative Tendenzen. Allerdings birgt dieser DH-›Hype‹ auch Gefahren. Diese reichen von der bloßen Aneignung des Etiketts über explizite Abwehrhaltungen bis hin zu Ignoranz und Verleugnung: »DH? Das gibt es doch gar nicht!«",
    "author": [
      {
        "family": "Sahle",
        "given": "Patrick"
      }
    ],
    "container-title": "Grenzen und Möglichkeiten der Digital Humanities (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 1)",
    "editor": [
      {
        "family": "Baum",
        "given": "Constanze"
      },
      {
        "family": "Stäcker",
        "given": "Thomas"
      }
    ],
    "id": "Sahle2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "de-DE",
    "publisher": "Forschungsverbund Marbach Weimar Wolfenbüttel",
    "publisher-place": "Wolfenbüttel",
    "title": "Digital Humanities? Gibt’s doch gar nicht!",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/377435.377666",
    "author": [
      {
        "family": "Saikkonen",
        "given": "Riku"
      },
      {
        "family": "Malmi",
        "given": "Lauri"
      },
      {
        "family": "Korhonen",
        "given": "Ari"
      }
    ],
    "container-title": "ITiCSE ’01: Proceedings of the 6<sup>th</sup> annual conference on innovation and technology in computer science education",
    "id": "Saikkonen2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "page": "133-136",
    "publisher": "ACM",
    "title": "Fully automatic assessment of programming exercises",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/ITRE.2006.381582",
    "abstract": "Local cultural heritage document collections are characterized by contents strongly attached to a territory and its history. Our contribution aims at enhancing such a content retrieval process efficiently each time a query includes geographic criteria. We propose a core model for a formal representation of geographic information. It takes into account the characteristics of different expression modes: written language and captures of drawings, maps, photographs, etc. We have developed a prototype fully implementing geographic information extraction (IE) and geographic information retrieval (IR) processes. We approach geographic IE from semantic processings additionally to classic IE approaches. This paper focuses on IR and information visualization (IV) proposals relying on the geospatial characteristics of documents.",
    "author": [
      {
        "family": "Sallaberry",
        "given": "Christian"
      },
      {
        "family": "Etcheverry",
        "given": "Patrick"
      },
      {
        "family": "Marquesuzaà",
        "given": "Christophe"
      }
    ],
    "container-title": "Information technology: Research and education, 2006. ITRE ’06. International conference on",
    "id": "Sallaberry2006",
    "issued": {
      "date-parts": [
        [
          2006,
          10,
          19
        ]
      ]
    },
    "keyword": "cultural_heritage, geo_ir",
    "language": "en-US",
    "page": "277-282",
    "title": "Information retrieval and visualization based on documents’ geospatial semantics",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1460007.1460008",
    "ISBN": "978-1-60558-253-5",
    "abstract": "Natural Language ’schematizes’ space; textual geographic information is usually a selection of certain aspects of a referent scene while neglecting others. Thus, an indexing process relying on such information obviously contains some degree of imprecision and uncertainty. The PIV prototype is a GIR system dedicated to geographic evocations tagging, geo-computing, indexing, querying and visualizing in wide corpora of travel books. The aim of this paper is to focus on the PIV spatial relationships management of vagueness for distance, direction and topology relationships. The proposed approach extends GIS operators with fuzzy spatial relationship functions like proximity and cardinal direction.",
    "author": [
      {
        "family": "Sallaberry",
        "given": "Christian"
      },
      {
        "family": "Gaio",
        "given": "Mauro"
      },
      {
        "family": "Palacio",
        "given": "Damien"
      },
      {
        "family": "Lesbegueries",
        "given": "Julien"
      }
    ],
    "container-title": "GIR ’08: Proceeding of the 2nd international workshop on geographic information retrieval",
    "id": "Sallaberry2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "geo_ir, geocoding, nlp",
    "language": "en-US",
    "page": "1-8",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Fuzzying GIS topological functions for GIR needs",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/iit.2007.4430506",
    "abstract": "In many software development systems semantic checking is performed after the editing process and within an \"edit-compile-execute\" cycle. When such systems are used in a teaching role, students may construct programs, and larger-scale software systems, which contain type errors. All type errors must be corrected if the software is to execute properly and this involves a significant overhead. In the IOPL (Initial Object-Oriented Programming Language) system, incremental type checking is provided via an editor that manipulates the persistent abstract syntax representation of an application, ensuring that no syntax errors and no type errors occur during the editing process.",
    "author": [
      {
        "family": "Sallabi",
        "given": "Omar M."
      },
      {
        "family": "Harrison",
        "given": "Chris"
      }
    ],
    "container-title": "4th international conference on innovations in information technology, 2007 (IIT ’07)",
    "id": "Sallabi2008",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "307-311",
    "title": "The implementation of a persistent Type-Safe Object-Oriented programming language",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/502187.502201",
    "ISBN": "1-58113-432-0",
    "abstract": "The shift from SGML to XML has created new demands for managing structured documents. Many XML documents will be transient representations for the purpose of data exchange between different types of applications, but there will also be a need for effective means to manage persistent XML data as a database. In this paper we explore requirements for an XML database management system. The purpose of the paper is not to suggest a single type of system covering all necessary features. Instead the purpose is to initiate discussion of the requirements arising from document collections, to offer a context in which to evaluate current and future solutions, and to encourage the development of proper models and systems for XML database management. Our discussion addresses issues arising from data modelling, data definition, and data manipulation.",
    "author": [
      {
        "family": "Salminen",
        "given": "Airi"
      },
      {
        "family": "Tompa",
        "given": "Frank"
      }
    ],
    "collection-title": "DocEng ’01",
    "container-title": "Proceedings of the 2001 ACM symposium on document engineering",
    "id": "Salminen2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "database, xml",
    "language": "en-US",
    "page": "85-94",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Requirements for XML document database systems",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/321439.321441",
    "ISSN": "0004-5411",
    "abstract": "Automatic indexing methods are evaluated and design criteria for modern information systems are derived.",
    "author": [
      {
        "family": "Salton",
        "given": "Gerard"
      },
      {
        "family": "Lesk",
        "given": "Michael E."
      }
    ],
    "container-title": "Journal of the ACM",
    "id": "Salton1968",
    "issued": {
      "date-parts": [
        [
          1968
        ]
      ]
    },
    "keyword": "classic, ir",
    "language": "en-US",
    "page": "8-36",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Computer evaluation of indexing and text processing",
    "type": "article-journal",
    "volume": "15"
  },
  {
    "DOI": "10.1145/361219.361220",
    "ISSN": "0001-0782",
    "abstract": "In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.",
    "author": [
      {
        "family": "Salton",
        "given": "G."
      },
      {
        "family": "Wong",
        "given": "A."
      },
      {
        "family": "Yang",
        "given": "C. S."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Salton1975",
    "issue": "11",
    "issued": {
      "date-parts": [
        [
          1975,
          11
        ]
      ]
    },
    "keyword": "classic, ir",
    "language": "en-US",
    "page": "613-620",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A vector space model for automatic indexing",
    "type": "article-journal",
    "volume": "18"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2010/pdf/535_Paper.pdf",
    "abstract": "In this article we describe two different strategies for the automatic tagging of a Spanish diachronic corpus involving the adaptation of existing NLP tools developed for modern Spanish. In the initial approach we follow a state-of-the-art strategy, which consists on standardizing the spelling and the lexicon. This approach boosts POS-tagging accuracy to 90%, which represents a raw improvement of over 20% with respect to the results obtained without any pre-processing. In order to enable non-expert users in NLP to use this new resource, the corpus has been integrated into IAC (Corpora Interface Access). We discuss the shortcomings of the initial approach and propose a new one, which does not consist in adapting the source texts to the tagger, but rather in modifying the tagger for the direct treatment of the old variants .This second strategy addresses some important shortcomings in the previous approach and is likely to be useful not only in the creation of diachronic linguistic resources but also for the treatment of dialectal or non-standard variants of synchronic languages as well.",
    "author": [
      {
        "family": "Sánchez-Marco",
        "given": "Cristina"
      },
      {
        "family": "Boleda",
        "given": "Gemma"
      },
      {
        "family": "Fontana",
        "given": "Josep M."
      },
      {
        "family": "Domingo",
        "given": "Judith"
      }
    ],
    "container-title": "Proceedings of the seventh international conference on language resources and evaluation (LREC’10)",
    "editor": [
      {
        "family": "Calzolari",
        "given": "Nicoletta"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Maegaard",
        "given": "Bente"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Odijk",
        "given": "Jan"
      },
      {
        "family": "Piperidis",
        "given": "Stelios"
      },
      {
        "family": "Rosner",
        "given": "Mike"
      },
      {
        "family": "Tapias",
        "given": "Daniel"
      }
    ],
    "id": "Sanchez-Marco2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, spanish",
    "language": "en-US",
    "publisher": "European Language Resources Association (ELRA)",
    "title": "Annotation and representation of a diachronic corpus of Spanish",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/W11-1501",
    "abstract": "We present a general and simple method to adapt an existing NLP tool in order to enable it to deal with historical varieties of languages. This approach consists basically in expanding the dictionary with the old word variants and in retraining the tagger with a small training corpus. We implement this approach for Old Spanish. The results of a thorough evaluation over the extended tool show that using this method an almost state-of-the-art performance is obtained, adequate to carry out quantitative studies in the humanities: 94.5% accuracy for the main part of speech and 92.6% for lemma. To our knowledge, this is the first time that such a strategy is adopted to annotate historical language varieties and we believe that it could be used as well to deal with other non-standard varieties of languages.",
    "author": [
      {
        "family": "Sánchez-Marco",
        "given": "Cristina"
      },
      {
        "family": "Boleda",
        "given": "Gemma"
      },
      {
        "family": "Padró",
        "given": "Lluís"
      }
    ],
    "container-title": "Proceedings of the 5th ACL-HLT workshop on language technology for cultural heritage, social sciences, and humanities",
    "id": "Sanchez-Marco2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, pos_tagging, spanish",
    "page": "1-9",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Portland, OR, USA",
    "title": "Extending the tool, or how to annotate historical language varieties",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s10844-009-0103-x",
    "ISSN": "0925-9902",
    "URL": "http://dx.doi.org/10.1007/s10844-009-0103-x",
    "abstract": "Estimation of the degree of semantic similarity/distance between concepts is a very common problem in research areas such as natural language processing, knowledge acquisition, information retrieval or data mining. In the past, many similarity measures have been proposed, exploiting explicit knowledge—such as the structure of a taxonomy—or implicit knowledge—such as information distribution. In the former case, taxonomies and/or ontologies are used to introduce additional semantics; in the latter case, frequencies of term appearances in a corpus are considered. Classical measures based on those premises suffer from some problems: in the first case, their excessive dependency of the taxonomical/ontological structure; in the second case, the lack of semantics of a pure statistical analysis of occurrences and/or the ambiguity of estimating concept statistical distribution from term appearances. Measures based on Information Content (IC) of taxonomical concepts combine both approaches. However, they heavily depend on a properly pre-tagged and disambiguated corpus according to the ontological entities in order to compute accurate concept appearance probabilities. This limits the applicability of those measures to other ontologies –like specific domain ontologies- and massive corpus –like the Web-. In this paper, several of the presented issues are analyzed. Modifications of classical similarity measures are also proposed. They are based on a contextualized and scalable version of IC computation in the Web by exploiting taxonomical knowledge. The goal is to avoid the measures’ dependency on the corpus pre-processing to achieve reliable results and minimize language ambiguity. Our proposals are able to outperform classical approaches when using the Web for estimating concept probabilities.",
    "author": [
      {
        "family": "Sánchez",
        "given": "David"
      },
      {
        "family": "Batet",
        "given": "Montserrat"
      },
      {
        "family": "Valls",
        "given": "Aida"
      },
      {
        "family": "Gibert",
        "given": "Karina"
      }
    ],
    "container-title": "Journal of Intelligent Information Systems",
    "id": "Sanchez2010",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2010,
          12
        ]
      ]
    },
    "keyword": "ir",
    "page": "383-413",
    "publisher": "Springer Netherlands",
    "title": "Ontology-driven web-based semantic similarity",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "URL": "https://www.journals.uio.no/index.php/osla/article/view/100/203",
    "abstract": "In this paper I present briefly Linguateca, an infrastructure project for Portuguese which is over ten years old, showing how it provides several possibilities to study grammatical and semantical differences between varieties of the language. After a short history of Portuguese corpus linguistics, presenting the main projects in the area, I discuss in some detail the AC/DC project and what is called the AC/DC cluster (encompassing other related corpus projects sharing the same core). Emphasizing its potential for language variation studies, the paper also (i) describes CONDIVport’s integration as an impetus for new capabilities, and (ii) provides a sketch of newly added functionalities to AC/DC.",
    "author": [
      {
        "family": "Santos",
        "given": "Diana"
      }
    ],
    "container-title": "Oslo Studies in Language",
    "id": "Santos2011",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, portuguese",
    "page": "113-128",
    "title": "Linguateca’s infrastructure for Portuguese and how it allows the detailed study of language varieties",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "URL": "http://wikimania2006.wikimedia.org/wiki/Proceedings:CS1",
    "abstract": "Almost 20 years ago, Leslie Lamport wrote about the advantages of logical document production over visual document production [Lamport 87]. Until recently it seemed that his insights in the disadvantages of visual text production with so called WYSIWYG editors seemed almost forgotten, because of the ubiquity of programs like Word. Only a small group of people, mainly in research, still used the logical approach to document production by using his LaTex program, appreciating the ability to concentrate on the content and leave the formatting to the program–until recently. With the success of editable web pages by the advent of wikis and its most popular application, Wikipedia, logical text production as Lesley Lamport described it, has found its way into the mainstream. There is a long standing discussion in the wiki world on whether to move back to visual text production in wikis again, since end users who only know WYSIWYG editors are suddenly confronted with wiki markup which they find disturbing at first.",
    "author": [
      {
        "family": "Sauer",
        "given": "Christoph"
      }
    ],
    "container-title": "Proceedings of wikimania 2006",
    "id": "Sauer2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "markup",
    "title": "What you see is wiki – questioning WYSIWYG in the Internet age",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1149453.1149478",
    "abstract": "In becoming wiki editors, non-technical users face many obstacles, one of the largest being that every wiki has its own unique syntax. A common wiki markup would facilitate not only learning and teaching wikis, but also developing advanced wiki editors and tools. The wiki markup standard should be intuitive and unlikely to interfere with existing text. Critics say users will not change, but if a wiki standard were developed, all wiki engines could take advantage of it. A wiki markup standard is critical to advancing wikis across the board.",
    "author": [
      {
        "family": "Sauer",
        "given": "Christoph"
      },
      {
        "family": "Smith",
        "given": "Chuck"
      },
      {
        "family": "Benz",
        "given": "Tomas"
      }
    ],
    "container-title": "WikiSym ’06: Proceedings of the 2006 international symposium on wikis",
    "id": "Sauer2006a",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "markup,standardization,standards,wiki",
    "page": "129-130",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Wiki markup standard workshop",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1296951.1296966",
    "author": [
      {
        "family": "Sauer",
        "given": "Christoph"
      },
      {
        "family": "Smith",
        "given": "Chuck"
      },
      {
        "family": "Benz",
        "given": "Tomas"
      }
    ],
    "container-title": "WikiSym ’07: Proceedings of the 2007 international symposium on wikis",
    "id": "Sauer2007b",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "implementation,markup,standardization,wiki",
    "page": "131-142",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "WikiCreole: A common wiki markup",
    "title-short": "WikiCreole",
    "type": "paper-conference"
  },
  {
    "URL": "http://sens-public.org/article1287.html",
    "author": [
      {
        "family": "Sauret",
        "given": "Nicolas"
      }
    ],
    "container-title": "Sens public",
    "id": "Sauret2017",
    "issued": {
      "date-parts": [
        [
          2017,
          12,
          15
        ]
      ]
    },
    "language": "fr-FR",
    "title": "Epistémologie du modèle: Des Humanités syntaxiques?",
    "title-short": "Epistémologie du modèle",
    "type": "article-journal"
  },
  {
    "DOI": "10.1016/j.ipm.2004.01.004",
    "URL": "http://dx.doi.org/10.1016/j.ipm.2004.01.004",
    "abstract": "This paper evaluates and compares the retrieval effectiveness of various search models, based on either automatic text-word indexing or on manually assigned controlled descriptors. Retrieval is from a relatively large collection of bibliographic material written in French. Moreover, for this French collection we evaluate improvements that result from combining automatic and manual indexing. First, when considering various contexts, this study reveals that the combined indexing strategy always obtains the best retrieval performance. Second, when users wish to conduct exhaustive searches with minimal effort, we demonstrate that manually assigned terms are essential. Third, the evaluations presented in this paper study reveal the comparative retrieval performances that result from manual and automatic indexing in a variety of circumstances.",
    "author": [
      {
        "family": "Savoy",
        "given": "Jacques"
      }
    ],
    "container-title": "Information Processing & Management",
    "id": "Savoy2005",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "ir, terminology",
    "page": "873-890",
    "title": "Bibliographic database access using free-text and controlled vocabulary: An evaluation",
    "title-short": "Bibliographic database access using free-text and controlled vocabulary",
    "type": "article-journal",
    "volume": "41"
  },
  {
    "DOI": "10.1093/llc/fqx007",
    "author": [
      {
        "family": "Savoy",
        "given": "Jacques"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Savoy2018",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "page": "143-159",
    "title": "Analysis of the style and the rhetoric of the 2016 US presidential primaries",
    "type": "article-journal",
    "volume": "33"
  },
  {
    "ISBN": "9789462981362",
    "URL": "http://www.oapen.org/record/624771",
    "abstract": "As more and more aspects of everyday life are turned into machine-readable data, researchers are provided with rich resources for researching society. The novel methods and innovative tools to work with this data not only require new knowledge and skills, but also raise issues concerning the practices of investigation and publication. This book critically reflects on the role of data in academia and society and challenges overly optimistic expectations considering data practices as means for understanding social reality. It introduces its readers to the practices and methods for data analysis and visualization and raises questions not only about the politics of data tools, but also about the ethics in collecting, sifting through data, and presenting data research.",
    "editor": [
      {
        "family": "Schäfer",
        "given": "Mirko T."
      },
      {
        "dropping-particle": "van",
        "family": "Es",
        "given": "Karin"
      }
    ],
    "id": "Schaefer2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities",
    "publisher": "Amsterdam University Press",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "The datafied society. Studying culture through data",
    "type": "book"
  },
  {
    "URL": "http://search.epnet.com/login.aspx?direct=true&db=pbh&an=11280884",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Schaeffer",
        "given": "Gary A."
      },
      {
        "family": "Henderson-Montero",
        "given": "Dianne"
      },
      {
        "family": "Julian",
        "given": "Marc"
      },
      {
        "family": "Bené",
        "given": "Nancy H."
      }
    ],
    "container-title": "Educational Assessment",
    "id": "Schaeffer2002",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2002,
          12
        ]
      ]
    },
    "page": "317-340",
    "title": "A comparison of three scoring methods for tests with selected-response and constructed-response items",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "URL": "http://cl2011.org.uk/archives.html",
    "author": [
      {
        "family": "Scheible",
        "given": "Silke"
      },
      {
        "family": "Bennett",
        "given": "Paul"
      },
      {
        "family": "Durrell",
        "given": "Martin"
      },
      {
        "family": "Whitt",
        "given": "Richard J."
      }
    ],
    "container-title": "Proceedings of the corpus linguistics conference CL 2011",
    "editor": [
      {
        "family": "Thompson",
        "given": "Paul"
      },
      {
        "family": "Hunston",
        "given": "Susan"
      },
      {
        "family": "Groom",
        "given": "Nicholas"
      },
      {
        "family": "Mason",
        "given": "Oliver"
      },
      {
        "family": "Millar",
        "given": "Neil"
      },
      {
        "family": "Moon",
        "given": "Rosamund"
      },
      {
        "family": "Walker",
        "given": "Crayton"
      },
      {
        "family": "Waddell",
        "given": "Nathan"
      }
    ],
    "id": "Scheible2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, german",
    "title": "The GerManC project: Creating an annotated historical corpus of German",
    "title-short": "The GerManC project",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/W11-1503",
    "abstract": "The goal of this study is to evaluate an “off-the-shelf” POS-tagger for modern German on historical data from the Early Modern period (1650-1800). With no specialised tagger available for this particular stage of the language, our findings will be of particular interest to smaller, humanities-based projects wishing to add POS annotations to their historical data but which lack the means or resources to train a POS tagger themselves. Our study assesses the effects of spelling variation on the performance of the tagger, and investigates to what extent tagger performance can be improved by using “normalised” input, where spelling variants in the corpus are standardised to a modern form. Our findings show that adding such a normalisation layer improves tagger performance considerably.",
    "author": [
      {
        "family": "Scheible",
        "given": "Silke"
      },
      {
        "family": "Whitt",
        "given": "Richard J."
      },
      {
        "family": "Durrell",
        "given": "Martin"
      },
      {
        "family": "Bennett",
        "given": "Paul"
      }
    ],
    "container-title": "Proceedings of the 5th ACL-HLT workshop on language technology for cultural heritage, social sciences, and humanities",
    "id": "Scheible2011b",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, german, pos_tagging",
    "language": "en-US",
    "page": "19-23",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Portland, OR, USA",
    "title": "Evaluating an “off-the-shelf” POS-tagger on Early Modern German text",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/W11-0415",
    "abstract": "This paper describes an annotated gold standard sample corpus of Early Modern German containing over 50,000 tokens of text manually annotated with POS tags, lemmas, and normalised spelling variants. The corpus is the first resource of its kind for this variant of German, and represents an ideal test bed for evaluating and adapting existing NLP tools on historical data. We describe the corpus format, annotation levels, and challenges, providing an example of the requirements and needs of smaller humanities-based corpus projects.",
    "author": [
      {
        "family": "Scheible",
        "given": "Silke"
      },
      {
        "family": "Whitt",
        "given": "Richard J."
      },
      {
        "family": "Durrell",
        "given": "Martin"
      },
      {
        "family": "Bennett",
        "given": "Paul"
      }
    ],
    "container-title": "Proceedings of the 5<sup>th</sup> linguistic annotation workshop (LAW v)",
    "id": "Scheible2011c",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, german",
    "language": "en-US",
    "page": "124-128",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "A gold standard corpus of Early Modern German",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqu058",
    "ISSN": "2055-7671",
    "author": [
      {
        "family": "Scheirer",
        "given": "Walter"
      },
      {
        "family": "Forstall",
        "given": "Christopher"
      },
      {
        "family": "Coffee",
        "given": "Neil"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Scheirer2016",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2016,
          4
        ]
      ]
    },
    "keyword": "digital_humanities, intertextuality",
    "language": "en-US",
    "page": "204-217",
    "title": "The sense of a connection: Automatic tracing of intertextuality by meaning",
    "title-short": "The sense of a connection",
    "type": "article-journal",
    "volume": "31"
  },
  {
    "URL": "http://muse.jhu.edu/journals/eservice_journal/v001/1.2schell.html",
    "abstract": "University faculties are under increasing pressure to do more with less. They are expected to continuously improve educational experiences to more students at less expense. Without assistance, the pressures of increasing demands and decreasing resources may prove too great to maintain a high quality learning environment. Merlot is an online repository of e-Learning objects designed to enable a broad base of faculty to provide high quality, online material to enhance college level learning. The materials in Merlot may be peer reviewed by both experts in the discipline and users of the materials. As teaching takes a more important role in promotion and tenure decisions at universities, Merlot provides a mechanism for evaluating online learning and teaching materials in a fashion similar to other scholarly research. Quality of content, potential effectiveness as a teaching tool, and ease of use are the key criteria in the Merlot review process. This paper provides a description of Merlot, the peer review process, examples of Merlot objects, and the role Merlot plays in the promotion, evaluation, and dissemination of e-Learning objects.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Schell",
        "given": "George P."
      },
      {
        "family": "Burns",
        "given": "Max"
      }
    ],
    "container-title": "e-Service Journal",
    "id": "Schell2002",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "language": "en-US",
    "page": "53-64",
    "title": "Merlot: A repository of e-Learning objects for higher education",
    "title-short": "Merlot",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "URL": "http://universaar.uni-saarland.de/monographien/volltexte/2010/12/",
    "abstract": "This paper discusses work on data collection for Swiss German dialects taking into account the continuous nature of the dialect landscape, and proposes to integrate these data into natural language processing models. We present knowledge-based models for machine translation into any Swiss German dialect, for dialect identification, and for multidialectal parsing. In a dialect continuum, rules cannot be applied uniformly, but have restricted validity in well-defined geographic areas. Therefore, the rules are parametrized with probability maps extracted from dialectological atlases.",
    "author": [
      {
        "family": "Scherrer",
        "given": "Yves"
      },
      {
        "family": "Rambow",
        "given": "Owen"
      }
    ],
    "container-title": "Semantic approaches in natural language processing: Proceedings of the conference on natural language processing 2010 (KONVENS)",
    "editor": [
      {
        "family": "Pinkal",
        "given": "Manfred"
      },
      {
        "family": "Rehbein",
        "given": "Ines"
      },
      {
        "dropping-particle": "Schulte im",
        "family": "Walde",
        "given": "Sabine"
      },
      {
        "family": "Storrer",
        "given": "Angelika"
      }
    ],
    "id": "Scherrer2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, german, morphology",
    "language": "en-US",
    "page": "93-102",
    "publisher": "Universaar",
    "publisher-place": "Saarbrücken, Germany",
    "title": "Natural language processing for the Swiss German dialect area",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s10639-005-6749-2",
    "abstract": "Starting from a general framework for web-based e-learning systems that is based on an abstraction layer model, this paper presents a conceptual modelling approach, which captures the modelling of learners, the modelling of courses, the personalisation of courses, and the management of data in e-learning systems. Courses are modelled by outline graphs, which are further refined by some form of process algebra. The linguistic analysis of word fields referring to an application domain helps to set up these course outlines. Learners are modelled by classifying value combinations for their characteristic properties. Each learner type gives rise to intentions as well as rights and obligations in using a learning system. Intentions can be formalised as postconditions, while rights and obligations lead to deontic constraints. The intentions can be used for the personalisation of the learning system to a learner type. Finally, the management of data in an e-learning system is approached on two different levels dealing with the content of individual learning units and the integrated content of the whole system, respectively. This leads to supporting databases and views defined on them.",
    "author": [
      {
        "family": "Schewe",
        "given": "Klaus-Dieter"
      },
      {
        "family": "Thalheim",
        "given": "Bernhard"
      },
      {
        "family": "Binemann-Zdanowicz",
        "given": "Aleksander"
      },
      {
        "family": "Kaschek",
        "given": "Roland"
      },
      {
        "family": "Kuss",
        "given": "Thomas"
      },
      {
        "family": "Tschiedel",
        "given": "Bernd"
      }
    ],
    "container-title": "Education and Information Technologies",
    "id": "Schewe2005",
    "issue": "1-2",
    "issued": {
      "date-parts": [
        [
          2005,
          1
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "83-110",
    "publisher": "Kluwer Academic Publishers",
    "title": "A conceptual view of Web-based e-learning systems",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "DOI": "10.1045/march2006-schibel",
    "abstract": "One could argue that there are no true libraries with millions of volumes.1 Buildings exist that do contain and provide access to collections of this size, but no single human being can make productive use of more than a fraction of such collections. Digital libraries make available even richer collections and have already begun to increase the set of documents we can examine. Books locked away in special collections and rare book libraries, where a handful of visitors used them in previous years, now attract thousands of readers from around the world. But even large-scale digital libraries serve users with particular needs at particular times. To understand the impact of digital libraries we need to think not only about global access via an OPAC or a search engine but also about access to thematically coherent clusters of information. This article considers the needs of an area of study that stands to benefit particularly from emerging comprehensive digital libraries, early modern printed books. These books, from Gutenberg to the middle of the eighteenth century, are printed with fixed letter forms and are the earliest documents that lend themselves to optical character recognition. Immense challenges lie before us as we struggle to make these new materials accessible in digital libraries; actual OCR technology, for example, is optimized for contemporary print and performs poorly with most early printed books. In no area of historical study do the limitations of print publication and distribution present so many obstacles and so constrain our ability to understand this period.",
    "author": [
      {
        "family": "Schibel",
        "given": "Wolfgang"
      },
      {
        "family": "Rydberg-Cox",
        "given": "Jeffrey A."
      }
    ],
    "container-title": "D-Lib Magazine",
    "id": "Schibel2006",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "title": "Early modern culture in a comprehensive digital library",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "DOI": "10.1007/978-3-540-78246-9_47",
    "ISBN": "978-3-540-78239-1",
    "abstract": "Spelling correction is the task of correcting words in texts. Most of the available spelling correction tools only work on isolated words and compute a list of spelling suggestions ranked by edit-distance, letter-n-gram similarity or comparable measures. Although the probability of the best ranked suggestion being correct in the current context is high, user intervention is usually necessary to choose the most appropriate suggestion (Kukich, 1992). Based on preliminary work by Sabsch (2006), we developed an efficient context sensitive spelling correction system dcClean by combining two approaches: the edit distance based ranking of an open source spelling corrector and neighbour co-occurrence statistics computed from a domain specific corpus. In combination with domain specific replacement and abbreviation lists we are able to significantly improve the correction precision compared to edit distance or context based spelling correctors applied on their own.",
    "author": [
      {
        "family": "Schierle",
        "given": "Martin"
      },
      {
        "family": "Schulz",
        "given": "Sascha"
      },
      {
        "family": "Ackermann",
        "given": "Markus"
      }
    ],
    "chapter-number": "47",
    "container-title": "Data analysis, machine learning and applications",
    "editor": [
      {
        "family": "Preisach",
        "given": "Christine"
      },
      {
        "family": "Burkhardt",
        "given": "Hans"
      },
      {
        "family": "Schmidt-Thieme",
        "given": "Lars"
      },
      {
        "family": "Decker",
        "given": "Reinhold"
      }
    ],
    "id": "Schierle2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "spelling_correction",
    "language": "en-US",
    "page": "397-404",
    "publisher": "Springer Berlin Heidelberg",
    "publisher-place": "Berlin, Heidelberg",
    "title": "From spelling correction to text cleaning – using context information",
    "type": "chapter"
  },
  {
    "URL": "http://www.ims.uni-stuttgart.de/projekte/corplex/TagSets/stts-1999.pdf",
    "author": [
      {
        "family": "Schiller",
        "given": "Anne"
      },
      {
        "family": "Teufel",
        "given": "Simone"
      },
      {
        "family": "Stöckert",
        "given": "Christine"
      },
      {
        "family": "Thielen",
        "given": "Christine"
      }
    ],
    "id": "Schiller1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "german, pos_tagging",
    "language": "en-US",
    "publisher": "Universität Stuttgart, Universität Tübingen",
    "publisher-place": "Stuttgart, Germany",
    "title": "Guidelines für das Tagging deutscher Textcorpora mit STTS (kleines und großes Tagset)",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Schiltz",
        "given": "Guillaume"
      },
      {
        "family": "Langlotz",
        "given": "Andreas"
      }
    ],
    "container-title": "International conference on computers in education 2004.",
    "editor": [
      {
        "family": "McKay",
        "given": "Elspeth"
      }
    ],
    "id": "Schiltz2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "note": "Book of Abstracts, full paper on CD-ROM",
    "page": "149",
    "publisher": "Asia-Pacific Society for Computers in Education (APSCE); Common Ground",
    "publisher-place": "Altona, Victoria, Australia",
    "title": "eHistling – integrating communication and cooperation in the humanities",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Schiltz",
        "given": "Guillaume"
      },
      {
        "family": "Langlotz",
        "given": "Andreas"
      }
    ],
    "container-title": "Learning by effective utilization of technologies: Facilitating intercultural understanding",
    "editor": [
      {
        "family": "Mizoguchi",
        "given": "Riichiro"
      },
      {
        "family": "Dillenbourg",
        "given": "Pierre"
      },
      {
        "family": "Zhu",
        "given": "Zhiting"
      }
    ],
    "id": "Schiltz2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "page": "119-122",
    "publisher": "IOS Press",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "The COLAC model: Collaborative paper-writing in the humanities",
    "title-short": "The COLAC model",
    "type": "chapter"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=1621034.1621040",
    "abstract": "This paper presents the compilation of the CroCo Corpus, an English-German translation corpus. Corpus design, annotation and alignment are described in detail. In order to guarantee the searchability and exchangeability of the corpus, XML stand-off mark-up is used as representation format for the multi-layer annotation. On this basis it is shown how the corpus can be queried using XQuery. Furthermore, the generalisation of results in terms of linguistic and translational research questions is briefly discussed.",
    "author": [
      {
        "family": "Hansen-Schirra",
        "given": "Silvia"
      },
      {
        "family": "Neumann",
        "given": "Stella"
      },
      {
        "family": "Vela",
        "given": "Mihaela"
      }
    ],
    "collection-title": "NLPXML ’06",
    "container-title": "Proceedings of the 5th workshop on NLP and XML: Multi-dimensional markup in natural language processing",
    "id": "Schirra2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "computational_linguistics, english, german, xml, xquery",
    "language": "en-US",
    "page": "35-42",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Multi-dimensional annotation and alignment in an English-German translation corpus",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqp022",
    "abstract": "This article addresses the need for TEI display tools. In order to illustrate the need for display tools, we begin with a brief review of the tools that are currently available, summarizing in particular those listed on the TEI Wiki Tools page. We then turn to a discussion of our work on the development of the TEIViewer (http://teiviewer.org), a simple, JavaScript-driven, portable display tool designed to facilitate the online representation of and interaction with elements and attributes described within select modules of the TEI P5 Guidelines and encoded as layers of data and metadata in TEI-XML documents. We explain how the TEIViewer works by describing the interactions between the XML source layer, the display layer generated via XSL, and the interactive layer powered by jQuery and CSS; and we explain why we chose the jQuery JavaScript library to manage the Viewer’s functionality as well as the advantages of this decision. Finally we describe current implementations and plans for release.",
    "author": [
      {
        "family": "Schlitz",
        "given": "Stephanie A."
      },
      {
        "family": "Bodine",
        "given": "Garrick S."
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Schlitz2009",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, tei, xml",
    "language": "en-US",
    "page": "339-346",
    "title": "The TEIViewer: Facilitating the transition from XML to web display",
    "title-short": "The TEIViewer",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "URL": "http://e-collection.ethbib.ethz.ch/view/eth:28088",
    "author": [
      {
        "family": "Schluep",
        "given": "Samuel"
      }
    ],
    "genre": "PhD thesis",
    "id": "Schluep2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "language": "en-US",
    "publisher": "ETH Zürich",
    "title": "Modularization and structured markup for Web-based learning content in an academic environment",
    "type": "thesis"
  },
  {
    "ISSN": "1537-2456",
    "URL": "http://editlib.org/p/21740",
    "abstract": "This article aims to present a flexible component model for modular, web-based learning content, and a simple structured markup schema for the separation of content and presentation. The article will also contain an overview of the dynamic Learning Content Management System (dLCMS) project, which implements these concepts. Content authors are a key factor for the successful application of these concepts. To support the authors creating modular contents the learning unit development guidelines were developed as part of the dLCMS project. An evaluation of the dLCMS and the guidelines from the point of view of learning content authors in an academic environment and a student evaluation of learning units which are composed of small, self-contained learning components is presented.",
    "author": [
      {
        "family": "Schluep",
        "given": "Samuel"
      },
      {
        "family": "Bettoni",
        "given": "Marco"
      },
      {
        "family": "Schär",
        "given": "Sissel G."
      }
    ],
    "container-title": "International Journal on E-Learning",
    "id": "Schluep2006",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "language": "en-US",
    "page": "35-44",
    "publisher": "AACE",
    "publisher-place": "Chesapeake, VA",
    "title": "Modularization and structured markup for learning content in an academic environment",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "ISBN": "3205785533",
    "abstract": "Digitale Geschichtswissenschaft ist mehr als nur der Einsatz digitaler Techniken und Medien, sie ist mehr als EDV, Web oder Internet. Sie eröffnet neue Forschungs- und Darstellungsmöglichkeiten, sie antwortet auf die liquiden Eigenschaften unserer gegenwärtigen Netzwerkzivilisation. Digitale Geschichtswissenschaft ersetzt dabei nicht die traditionelle monographische Geschichtswissenschaft, sondern verhilft dieser zu einer Renaissance im Feld der großen historischen Erzählung, die nicht obsolet ist. Digitale Geschichtswissenschaft gewinnt ihr Profil aus den vielfältigen Möglichkeiten der semantischen Techniken und der Forschungsvernetzung, sie stellt auch dem Nichthistoriker Techniken des Geschichtelernens von hoher Qualität zur Verfügung. Geschichte passiert auch im Netz - Geschichtswissenschaft wird wieder spannend.",
    "author": [
      {
        "family": "Schmale",
        "given": "Wolfgang"
      }
    ],
    "id": "Schmale2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "de-DE",
    "publisher": "Böhlau",
    "publisher-place": "Wien",
    "title": "Digitale Geschichtswissenschaft",
    "type": "book"
  },
  {
    "URL": "http://wolfgangschmale.eu/digital-humanities-zwischen-app-und-big-data/",
    "author": [
      {
        "family": "Schmale",
        "given": "Wolfgang"
      }
    ],
    "container-title": "Blog “Mein Europa”",
    "id": "Schmale2015a",
    "issued": {
      "date-parts": [
        [
          2015,
          8
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "de-DE",
    "title": "Digital Humanities zwischen App und Big Data",
    "type": "chapter"
  },
  {
    "URL": "http://wolfgangschmale.eu/digital-humanities-zwischen-app-und-big-data/",
    "author": [
      {
        "family": "Schmale",
        "given": "Wolfgang"
      }
    ],
    "container-title": "Blog “Mein Europa”",
    "id": "Schmale2015b",
    "issued": {
      "date-parts": [
        [
          2015,
          12
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "de-DE",
    "title": "Digital Musicology im Kontext der Digital Humanities",
    "type": "chapter"
  },
  {
    "URL": "http://wolfgangschmale.eu/digitalisieren-wir-eigentlich-das-richtige/",
    "author": [
      {
        "family": "Schmale",
        "given": "Wolfgang"
      }
    ],
    "container-title": "Blog “Mein Europa”",
    "id": "Schmale2015c",
    "issued": {
      "date-parts": [
        [
          2015,
          11
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "de-DE",
    "title": "Digitalisieren wir eigentlich das Richtige?",
    "type": "chapter"
  },
  {
    "abstract": "We present a morphological analyser for German inflection and word formation implemented in finite state technology. Unlike purely lexicon-based approaches, it can account for productive word formation like derivation and composition. The implementation is based on the Stuttgart Finite State Transducer Tools (SFST-Tools), a non-commercial FST platform. It is fast and achieves a high coverage.",
    "author": [
      {
        "family": "Schmid",
        "given": "Helmut"
      },
      {
        "family": "Fitschen",
        "given": "Arne"
      },
      {
        "family": "Heid",
        "given": "Ulrich"
      }
    ],
    "container-title": "Proceedings of the IVth international conference on language resources and evaluation (LREC 2004)",
    "id": "Schmid2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "german, morphology, nlp",
    "page": "1263-1266",
    "title": "A german computational morphology covering derivation, composition, and inflection",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1515/9783110211429.4.527",
    "author": [
      {
        "family": "Schmid",
        "given": "Helmut"
      }
    ],
    "chapter-number": "24",
    "collection-title": "Handbooks of linguistics and communication science",
    "container-title": "Corpus linguistics. An international handbook",
    "editor": [
      {
        "family": "Lüdeling",
        "given": "Anke"
      },
      {
        "family": "Kytö",
        "given": "Merja"
      }
    ],
    "id": "Schmid2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "computational_linguistics",
    "language": "en-US",
    "page": "527-551",
    "publisher": "Mouton de Gruyter",
    "title": "Tokenizing and part-of-speech tagging",
    "type": "chapter",
    "volume": "1"
  },
  {
    "URL": "http://www.alain-pave.fr/app/download/8529920/48-CSL-AP-Modélisation-Confluences-2008.pdf",
    "abstract": "The word “model” appeared in the scientific language in the 1960’s and “modelling” in the 70’s. Within 30 years, the purpose and the approach have gained ground in almost all the science fields, most of the time for the best, but sometimes for the least good, in particular when people have striven to build up reality from the model, which is necessarily simplifying. Conversely, integrating the model to the experimental method assures a close link with reality.We’ll try herein to collate the role of the model in the major scientific fields, from physics to the sciences of man and societies. Eventually, a common methodology, freed from the traditional stranglehold of the disciplines, will be drawn.",
    "author": [
      {
        "family": "Schmidt-Lainé",
        "given": "Claudine"
      },
      {
        "family": "Pavé",
        "given": "Alain"
      }
    ],
    "container-title": "Les Cahiers du musée des Confluences",
    "id": "Schmidt-Laine2008",
    "issued": {
      "date-parts": [
        [
          2008,
          12
        ]
      ]
    },
    "keyword": "formal_models",
    "language": "fr-FR",
    "page": "21-34",
    "title": "La modélisation au cœur de la démarche scientifique et à la confluence des disciplines",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.1093/llc/fqq007",
    "abstract": "Embedded generalized markup, as applied by digital humanists to the recording and studying of our textual cultural heritage, suffers from a number of serious technical drawbacks. As a result of its evolution from early printer control languages, generalized markup can only express a document’s “logical” structure via a repertoire of permissible printed format structures. In addition to the well-researched overlap problem, the embedding of markup codes into texts that never had them when written leads to a number of further difficulties: the inclusion of potentially obsolescent technical and subjective information into texts that are supposed to be archivable for the long term, the manual encoding of information that could be better computed automatically, and the obscuring of the text by highly complex technical data. Many of these problems can be alleviated by asserting a separation between the versions of which many cultural heritage texts are composed, and their content. In this way the complex interconnections between versions can be handled automatically, leaving only simple markup for individual versions to be handled by the user.",
    "author": [
      {
        "family": "Schmidt",
        "given": "Desmond"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Schmidt2010",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2010,
          9
        ]
      ]
    },
    "keyword": "cultural_heritage, markup, xml",
    "language": "en-US",
    "page": "337-356",
    "title": "The inadequacy of embedded markup for cultural heritage texts",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "DOI": "10.1145/2517978.2517980",
    "ISBN": "978-1-4503-2199-0",
    "abstract": "In this paper, we describe Berkeley Prosopography Services (BPS), a new set of tools for prosopography - the identification of individuals and study of their interactions - in support of humanities research. The BPS tools include 1) functionality to import TEI documents and convert to our data model, 2) a disambiguation engine to associate names to persons based upon configurable heuristic rules, 3) an assertion model that supports flexible researcher curation and tracks provenance, 4) social network analysis and 5) graph visualization tools to analyze and understand social relations, and 6) a workspace model supporting exploratory research and collaboration. We contrast the BPS model that uses configurable heuristic rules to other approaches for automated text analysis, and explain how our model facilitates interpretation by humanist researchers. We describe the significance of our curation model that improves upon traditional curation and annotation as a fact-based model by adding a more flexible model in which researchers assert conclusions or possibilities, allowing them to override automated inference, to explore ideas in what-if scenarios, and to formally publish and subscribe-to asserted annotations among colleagues, and/or with students. We detail the architecture and our implementation of the tools as a set of reusable web services and web application UI. We present an initial evaluation of researchers’ experience using the tools to study corpora of cuneiform tablets, and describe plans to expand the application of the tools to a broader range of corpora.",
    "author": [
      {
        "family": "Schmitz",
        "given": "Patrick"
      },
      {
        "family": "Pearce",
        "given": "Laurie"
      }
    ],
    "container-title": "Proceedings of the 1<sup>st</sup> international workshop on collaborative annotations in shared environment: Metadata, vocabularies and techniques in the digital humanities (DH-CASE ’13)",
    "id": "Schmitz2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities, tei",
    "language": "en-US",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Berkeley Prosopography Services: Ancient families, modern tools",
    "title-short": "Berkeley Prosopography Services",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/2644866.2644870",
    "ISBN": "978-1-4503-2949-1",
    "abstract": "In this paper, we describe Berkeley Prosopography Services (BPS), a new set of tools for prosopography - the identification of individuals and study of their interactions - in support of humanities research. Prosopography is an example of \"big data\" in the humanities, characterized not by the size of the datasets, but by the way that computational and data-driven methods can transform scholarly workflows. BPS is based upon re-usable infrastructure, supporting generalized web services for corpus management, social network analysis, and visualization. The BPS disambiguation model is a formal implementation of the traditional heuristics used by humanists, and supports plug-in rules for adaptation to a wide range of domain corpora. A workspace model supports exploratory research and collaboration. We contrast the BPS model of configurable heuristic rules to other approaches for automated text analysis, and explain how our model facilitates interpretation by humanist researchers. We describe the significance of the BPS assertion model in which researchers assert conclusions or possibilities, allowing them to override automated inference, to explore ideas in what-if scenarios, and to formally publish and subscribe-to asserted annotations among colleagues, and/or with students. We present an initial evaluation of researchers’ experience using the tools to study corpora of cuneiform tablets, and describe plans to expand the application of the tools to a broader range of corpora.",
    "author": [
      {
        "family": "Schmitz",
        "given": "Patrick"
      },
      {
        "family": "Pearce",
        "given": "Laurie"
      }
    ],
    "container-title": "DocEng 2014: Proceedings of the 2014 ACM symposium on document engineering",
    "id": "Schmitz2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "179-188",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Humanist-centric tools for big data: Berkeley Prosopography Services",
    "title-short": "Humanist-centric tools for big data",
    "type": "paper-conference"
  },
  {
    "URL": "http://ieeexplore.ieee.org/iel4/5584/14955/00680693.pdf?tp=&isnumber=14955&arnumber=680693&punumber=5584",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Schneck",
        "given": "Marjorie A."
      }
    ],
    "container-title": "Proceedings of EdCompCon ’84",
    "id": "Schneck1984",
    "issued": {
      "date-parts": [
        [
          1984
        ]
      ]
    },
    "page": "91-96",
    "title": "PLATO implementations and evaluations: New behaviors and objectives for education",
    "title-short": "PLATO implementations and evaluations",
    "type": "paper-conference"
  },
  {
    "abstract": "This thesis proposes a robust, hybrid, deep-syntatic dependency-based pars- ing architecture and presents its implementation and evaluation. The architecture and the implementation are carefully designed to keep search-spaces small with- out compromising much on the linguistic performance or adequacy. The resulting parser is deep-syntactic like a formal grammar-based parser but at the same time mostly context-free and fast enough for large-scale application to unrestricted texts. It combines a number of successful current approaches into a hybrid, comparatively simple, modular and open model.",
    "author": [
      {
        "family": "Schneider",
        "given": "Gerold"
      }
    ],
    "genre": "PhD thesis",
    "id": "Schneider2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "computational_linguistics, parsing",
    "language": "en-US",
    "publisher": "Universität Zürich",
    "publisher-place": "Zurich, Switzerland",
    "title": "Hybrid Long-Distance functional dependency parsing",
    "type": "thesis"
  },
  {
    "URL": "http://www.cl.uzh.ch/research/techreport/TR_2011_02.pdf",
    "abstract": "Pro3Gres is a robust, hybrid, deep-syntatic dependency-based parser. The architecture and the implementation are carefully designed to keep search-spaces small without compromising much on the linguistic performance or adequacy. The resulting parser is deep-syntactic like a formal grammar-based parser but at the same time mostly context-free and fast enough for large-scale application to unrestricted texts. It combines a number of successful current approaches into a hybrid, comparatively simple, modular and open model. This technical documentation gives an overview of the code. Due to the complexity of the program, many simplifications in the description were unavoidable.",
    "accessed": {
      "date-parts": [
        [
          2012,
          6,
          30
        ]
      ]
    },
    "author": [
      {
        "family": "Schneider",
        "given": "Gerold"
      }
    ],
    "id": "Schneider2011",
    "issued": {
      "date-parts": [
        [
          2011,
          2
        ]
      ]
    },
    "number": "CL-2011.01",
    "publisher": "University of Zurich, Institute of Computational Linguistics",
    "publisher-place": "Zurich, Switzerland",
    "title": "Pro3Gres technical documentation",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Schneider",
        "given": "Gerold"
      }
    ],
    "container-title": "Proceedings of the helsinki corpus festival",
    "editor": [
      {
        "family": "Jukka Tyrkkö",
        "given": "Matti Rissanen",
        "suffix": "Terttu Nevalainen"
      },
      {
        "family": "Kilpiö",
        "given": "Matti"
      }
    ],
    "id": "Schneider2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "note": "To appear",
    "title": "Adapting a parser to historical English",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqu001",
    "ISSN": "2055-768X",
    "abstract": "We describe, evaluate, and improve the automatic annotation of diachronic corpora at the levels of word-class, lemma, chunks, and dependency syntax. As corpora we use the ARCHER corpus (texts from 1600 to 2000) and the ZEN corpus (texts from 1660 to 1800). Performance on Modern English is considerably lower than on Present Day English (PDE). We present several methods that improve performance. First we use the spelling normalization tool VARD to map spelling variants to their PDE equivalent, which improves tagging. We investigate the tagging changes that are due to the normalization and observe improvements, deterioration, and missing mappings. We then implement an optimized version, using VARD rules and preprocessing steps to improve normalization. We evaluate the improvement on parsing performance, comparing original text, standard VARD, and our optimized version. Over 90% of the normalization changes lead to improved parsing, and 17.3% of all 422 manually annotated sentences get a net improved parse. As a next step, we adapt the parser’s grammar, add a semantic expectation model and a model for prepositional phrases (PP)-attachment interaction to the parser. These extensions improve parser performance, marginally on PDE, more considerably on earlier texts—2–5% on PP-attachment relations (e.g. from 63.6 to 68.4% and from 70 to 72.9% on 17th century texts). Finally, we briefly outline linguistic applications and give two examples: gerundials and auxiliary verbs in the ZEN corpus, showing that despite high noise levels linguistic signals clearly emerge, opening new possibilities for large-scale research of gradient phenomena in language change.",
    "author": [
      {
        "family": "Schneider",
        "given": "Gerold"
      },
      {
        "family": "Lehmann",
        "given": "Hans M."
      },
      {
        "family": "Schneider",
        "given": "Peter"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Schneider2014",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_humanities, english, parsing",
    "language": "en-US",
    "page": "423-439",
    "title": "Parsing early and late modern english corpora",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "DOI": "10.1002/9780470999875",
    "URL": "http://digitalhumanities.org/companion",
    "abstract": "This Companion offers a thorough, concise overview of the emerging field of humanities computing.",
    "editor": [
      {
        "family": "Schreibman",
        "given": "Susan"
      },
      {
        "family": "Siemens",
        "given": "Ray"
      },
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "id": "Schreibman2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "digital_humanities, cultural_heritage",
    "language": "en-US",
    "publisher": "Blackwell",
    "publisher-place": "Oxford",
    "title": "A companion to digital humanities",
    "type": "book"
  },
  {
    "DOI": "10.1002/9780470999875.fmatter",
    "author": [
      {
        "family": "Schreibman",
        "given": "Susan"
      },
      {
        "family": "Siemens",
        "given": "Ray"
      },
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "container-title": "A companion to digital humanities",
    "editor": [
      {
        "family": "Schreibman",
        "given": "Susan"
      },
      {
        "family": "Siemens",
        "given": "Ray"
      },
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "id": "Schreibman2004-intro",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "language": "en-US",
    "page": "xxii-xxvii",
    "publisher": "Blackwell",
    "title": "The digital humanities and humanities computing: An introduction",
    "title-short": "The digital humanities and humanities computing",
    "type": "chapter"
  },
  {
    "DOI": "10.1002/9780470999875.ch1",
    "URL": "http://www.digitalhumanities.org/companion/",
    "author": [
      {
        "family": "Hockey",
        "given": "Susan"
      }
    ],
    "chapter-number": "1",
    "container-title": "A companion to digital humanities",
    "editor": [
      {
        "family": "Schreibman",
        "given": "Susan"
      },
      {
        "family": "Siemens",
        "given": "Ray"
      },
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "id": "Hockey2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "language": "en-US",
    "page": "1-19",
    "publisher": "Blackwell",
    "title": "The history of humanities computing",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Schriver",
        "given": "Karen A."
      }
    ],
    "id": "Schriver1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "document_research, typesetting, typography",
    "language": "en-US",
    "publisher": "Wiley",
    "publisher-place": "New York, NY, USA",
    "title": "Dynamics in document design: Creating text for readers",
    "title-short": "Dynamics in document design",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Schriver",
        "given": "Karen A."
      }
    ],
    "container-title": "Dynamics in document design: Creating text for readers",
    "id": "Schriver1996-rhetorical",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "document_research, layout, typography, visual_languages",
    "language": "en-US",
    "page": "283-286",
    "publisher": "Wiley",
    "title": "The rhetorical appropriateness of typography: Its role in seeing the text",
    "title-short": "The rhetorical appropriateness of typography",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Schulmeister",
        "given": "Rolf"
      }
    ],
    "edition": "2<sup>nd</sup>",
    "id": "Schulmeister1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "publisher": "Oldenbourg",
    "publisher-place": "München",
    "title": "Grundlagen hypermedialer lernsysteme",
    "type": "book"
  },
  {
    "URL": "http://www.izhd.uni-hamburg.de/paginae/Book/Frames/Start_FRAME.html",
    "accessed": {
      "date-parts": [
        [
          2008,
          5,
          13
        ]
      ]
    },
    "author": [
      {
        "family": "Schulmeister",
        "given": "Rolf"
      }
    ],
    "id": "Schulmeister1997_en",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "e-learning",
    "note": "English online translation of: Rolf Schulmeister. <i>Grundlagen hypermedialer Lernsysteme</i>. Oldenbourg, München, 2<sup>nd</sup> edition, 1997.",
    "title": "Hypermedia learning systems: Theory – didactics – design",
    "title-short": "Hypermedia learning systems",
    "type": ""
  },
  {
    "author": [
      {
        "family": "Schulmeister",
        "given": "Rolf"
      }
    ],
    "id": "Schulmeister2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "e-learning",
    "publisher": "Oldenbourg",
    "publisher-place": "München",
    "title": "Lernplattformen für das virtuelle lernen. Evaluation und didaktik",
    "type": "book"
  },
  {
    "DOI": "10.18637/jss.v046.i03",
    "ISSN": "1548-7660",
    "author": [
      {
        "family": "Schulte",
        "given": "Eric"
      },
      {
        "family": "Davison",
        "given": "Dan"
      },
      {
        "family": "Dye",
        "given": "Thomas"
      },
      {
        "family": "Dominik",
        "given": "Carsten"
      }
    ],
    "container-title": "Journal of Statistical Software",
    "id": "Schulte2011",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2012,
          1,
          25
        ]
      ]
    },
    "page": "1-24",
    "title": "A multi-language computing environment for literate programming and reproducible research",
    "type": "article-journal",
    "volume": "46"
  },
  {
    "URL": "http://www.opus.ub.uni-erlangen.de/opus/volltexte/2004/77/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Schulze",
        "given": "Markus"
      }
    ],
    "genre": "PhD thesis",
    "id": "Schulze2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
    "publisher-place": "Erlangen",
    "title": "Ein sprachunabhängiger Ansatz zur Entwicklung deklarativer, robuster LA-Grammatiken mit einer exemplarischen Anwendung auf das Deutsche und das Englische",
    "type": "thesis"
  },
  {
    "URL": "http://www.alta.asn.au/events/altss_w2003_proc/altw/papers/schwitter-final.pdf",
    "accessed": {
      "date-parts": [
        [
          2009,
          1,
          25
        ]
      ]
    },
    "author": [
      {
        "family": "Schwitter",
        "given": "Rolf"
      }
    ],
    "container-title": "Proceedings of the australasian language technology workshop 2003",
    "id": "Schwitter2003",
    "issued": {
      "date-parts": [
        [
          2003,
          12
        ]
      ]
    },
    "page": "1-8",
    "publisher-place": "Melbourne, Australia",
    "title": "Incremental chart parsing with predictive hints",
    "type": "paper-conference"
  },
  {
    "URL": "http://hdl.handle.net/2134/1890",
    "abstract": "IMS has been promising question and test interoperability (QTI) for a number of years. Reported advantages of interoperability include the avoidance of \"lock in\" to one proprietary system, the ability to integrate systems from different vendors, and the facilitation of an exchange of questions and tests between institutions. The QTI specification, while not yet an international standard for the exchange of questions, tests and results, now appears to be stable enough for vendors to have developed systems which implement such an exchange in a fairly sophisticated way. The costs to software companies of implementing QTI \"compliance\" in their existing CAA systems, however, are high. Allowing users to move their data to other systems may not seem to make commercial sense either. As awareness of the advantages of interoperability increases within education, software companies are realising that adding QTI import and export facilities to their products can be a selling point. A handful of vendors have signed up to the concept of interoperability and have taken part in the IMS QTI Working Group. Others state that their virtual learning environments or CAA systems are \"conformant\" with IMS QTI but do these assertions stand up when the packages are tested together? The CETIS Assessment Special Interest Group has been monitoring developments in this area for over a year and has carried out an analysis of tools which exploit the QTI specifications. This paper describes to what extent the tools genuinely interoperate and examines the likely benefits for users and future prospects for CAA interoperability.",
    "author": [
      {
        "family": "Sclater",
        "given": "Niall"
      },
      {
        "family": "Low",
        "given": "Boon"
      },
      {
        "family": "Barr",
        "given": "Niall"
      }
    ],
    "container-title": "Proceedings of the 6<sup>th</sup> international conference on computer-assisted assessment",
    "editor": [
      {
        "family": "Danson",
        "given": "Myles"
      }
    ],
    "id": "Sclater2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "assessment, e-learning",
    "language": "en-US",
    "page": "317-326",
    "publisher": "Loughborough University",
    "title": "Interoperability with CAA: Does it work in practice?",
    "title-short": "Interoperability with CAA",
    "type": "paper-conference"
  },
  {
    "URL": "http://ceur-ws.org/Vol-317/paper07.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Sclater",
        "given": "Niall"
      }
    ],
    "collection-title": "CEUR workshop proceedings",
    "container-title": "Proceedings of the workshop on exchanging experiences in technology enhanced learning – what went wrong? What went right? (WWWrong’07)",
    "editor": [
      {
        "family": "Davis",
        "given": "Hugh"
      },
      {
        "family": "Duval",
        "given": "Erik"
      },
      {
        "family": "Muramatsu",
        "given": "Brandon"
      },
      {
        "family": "White",
        "given": "Su"
      },
      {
        "family": "Van Assche",
        "given": "Frans"
      }
    ],
    "id": "Sclater2007",
    "issued": {
      "date-parts": [
        [
          2007,
          9
        ]
      ]
    },
    "language": "en-US",
    "page": "70-74",
    "publisher": "CEUR-WS.org",
    "publisher-place": "Sissi, Greece",
    "title": "The demise of eAssessment interoperability?",
    "type": "paper-conference",
    "volume": "317"
  },
  {
    "author": [
      {
        "family": "Scott",
        "given": "Peter"
      },
      {
        "family": "Vanoirbeek",
        "given": "Christine"
      }
    ],
    "container-title": "ERCIM News",
    "id": "Scott2007",
    "issue": "71",
    "issued": {
      "date-parts": [
        [
          2007,
          10
        ]
      ]
    },
    "page": "12-13",
    "title": "Technology-enhanced learning",
    "type": "article-journal",
    "volume": "2007"
  },
  {
    "URL": "http://www.ppgia.pucpr.br/ismir2013/wp-content/uploads/2013/09/83_Paper.pdf",
    "abstract": "Web applications and mobile tablets are changing the way musicians practice their instrument. Now, they can access instantaneously thousands of musical scores online and play them while watching their tablet, put on their music stand. However musicians may have difficulties in getting appropriate tips and advice to play the chosen piece correctly. This is why we conceived a collaborative platform to annotate digital scores on tablets in previous work. However, we noticed that the current Music Ontology (MO) do not allow to tag these annotations appropriately. Thus, we present in this paper a proposition for a Musical Forms and Structures Ontology (MFSO) and a Musical Performance Ontology (MPO) based on music practice. A construction methodology and a model are first detailed. Then, a practical use case is presented. Lastly, inherent theoretical and practical difficulties encountered during the ontology framework’s conception are discussed.",
    "author": [
      {
        "family": "Sébastien",
        "given": "Véronique"
      },
      {
        "family": "Sébastien",
        "given": "Didier"
      },
      {
        "family": "Conruyt",
        "given": "Noël"
      }
    ],
    "container-title": "Proceedings of the 14<sup>th</sup> international society for music information retrieval conference",
    "editor": [
      {
        "dropping-particle": "de",
        "family": "Souza Britto Jr.",
        "given": "Alceu"
      },
      {
        "family": "Gouyon",
        "given": "Fabien"
      },
      {
        "family": "Dixon",
        "given": "Simon"
      }
    ],
    "id": "Sebastien2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "music, ontologies, semantic_web",
    "language": "en-US",
    "page": "451-456",
    "title": "Annotating works for music education: Propositions for a musical forms and structures ontology and a musical performance ontology",
    "title-short": "Annotating works for music education",
    "type": "paper-conference"
  },
  {
    "ISBN": "9783525013885",
    "abstract": "Wie führt man am besten in die Geschichtswissenschaft ein? Es gibt natürlich verschiedene Wege. Der Leser dieser Einführung lernt, geschichtswissenschaftlich zu denken und zu arbeiten, indem er von Anfang an in geschichtswissenschaftliches Argumentieren hineingezogen wird. Nicht allgemeine Informationen bietet das Buch, sondern es ist eine Einführung, die sich auf methodische Grundfragen konzentriert und in die Praxis hineinführt. Volker Sellin vermittelt Denkweisen und Argumentationsformen, zugleich einen Begriff von Arbeitsweise und Verfahren des Historikers und macht so mit der geschichtswissenschaftlichen Praxis vertraut. Hierbei nehmen Beispiele einen breiten Raum ein. An Beispielen und den Schwierigkeiten ihrer Interpretation wird immer wieder die Notwendigkeit methodischer Reflexion dargetan, die Beispiele ermöglichen aber auch, eine Reihe von Methoden und Teildisziplinen der Geschichtswissenschaft vorzustellen. Einzelne geschichtswissenschaftliche Probleme werden aus Alltagserfahrungen entwickelt. Historische Vorkenntnisse, ein spezielles Wissen muß man nicht mitbringen: »Das Buch setzt nichts voraus als die Bereitschaft, die vorgetragenen Gedankengänge mitzudenken.« Jedem Kapitel folgen ganz knappe Literaturangaben, ein bibliographischer Essay nennt weiterführende Literatur.",
    "author": [
      {
        "family": "Sellin",
        "given": "Volker"
      }
    ],
    "edition": "2",
    "id": "Sellin2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "history",
    "language": "de-DE",
    "publisher": "Vandenhoeck & Ruprecht",
    "publisher-place": "Göttingen",
    "title": "Einführung in die Geschichtswissenschaft",
    "type": "book"
  },
  {
    "DOI": "10.1007/s10844-011-0174-3",
    "ISSN": "0925-9902",
    "abstract": "The availability of an abundance of knowledge sources has spurred a large amount of effort in the development and enhancement of Information Retrieval techniques. Users’ information needs are expressed in natural language and successful retrieval is very much dependent on the effective communication of the intended purpose. Natural language queries consist of multiple linguistic features which serve to represent the intended search goal. Linguistic characteristics that cause semantic ambiguity and misinterpretation of queries as well as additional factors such as the lack of familiarity with the search environment affect the users’ ability to accurately represent their information needs, coined by the concept ” intention gap”. The latter directly affects the relevance of the returned search results which may not be to the users’ satisfaction and therefore is a major issue impacting the effectiveness of information retrieval systems. Central to our discussion is the identification of the significant constituents that characterize the query intent and their enrichment through the addition of meaningful terms, phrases or even latent representations, either manually or automatically to capture their intended meaning. Specifically, we discuss techniques to achieve the enrichment and in particular those utilizing the information gathered from statistical processing of term dependencies within a document corpus or from external knowledge sources such as ontologies. We lay down the anatomy of a generic linguistic based query expansion framework and propose its module-based decomposition, covering topical issues from query processing, information retrieval, computational linguistics and ontology engineering. For each of the modules we review state-of-the-art solutions in the literature categorized and analyzed under the light of the techniques used.",
    "author": [
      {
        "family": "Selvaretnam",
        "given": "Bhawani"
      },
      {
        "family": "Belkhatir",
        "given": "Mohammed"
      }
    ],
    "container-title": "Journal of Intelligent Information Systems",
    "id": "Selvaretnam2011",
    "issued": {
      "date-parts": [
        [
          2011,
          8
        ]
      ]
    },
    "keyword": "ir, nlp",
    "language": "en-US",
    "page": "1-32",
    "publisher": "Springer Netherlands",
    "title": "Natural language technology and query expansion: Issues, state-of-the-art and perspectives",
    "title-short": "Natural language technology and query expansion",
    "type": "article-journal"
  },
  {
    "URL": "http://www.openbel.org/",
    "abstract": "The Biological Expression Language (BEL)is a language for representing scientific findings in the life sciences in a computable form. BEL is designed to represent scientific findings by capturing causal and correlative relationships in context, where context can include information about the biological and experimental system in which the relationships were observed, the supporting publications cited and the process of curation.",
    "author": [
      {
        "literal": "Selventa"
      }
    ],
    "id": "Selventa2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "ontologies, semantic_web",
    "language": "en-US",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Biological expression language V1.0 overview",
    "type": "book"
  },
  {
    "URL": "http://www.zfhd.de/index.php?document_id=1000138&view=set",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Sengstag",
        "given": "Christian"
      },
      {
        "family": "Miller",
        "given": "Damian"
      }
    ],
    "container-title": "Zeitschrift für Hochschuldidaktik",
    "id": "Sengstag2005",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2005,
          6
        ]
      ]
    },
    "page": "63-74",
    "title": "Von der klassischen Vorlesung zur Bologna-kompatiblen Lehrveranstaltung: Redesign einer Lehrveranstaltung",
    "type": "article-journal",
    "volume": "2005"
  },
  {
    "DOI": "10.1145/1390334.1390432",
    "ISBN": "978-1-60558-164-4",
    "abstract": "Text reuse occurs in many different types of documents and for many different reasons. One form of reuse, duplicate or near-duplicate documents, has been a focus of researchers because of its importance in Web search. Local text reuse occurs when sentences, facts or passages, rather than whole documents, are reused and modified. Detecting this type of reuse can be the basis of new tools for text analysis. In this paper, we introduce a new approach to detecting local text reuse and compare it to other approaches. This comparison involves a study of the amount and type of reuse that occurs in real documents, including TREC newswire and blog collections.",
    "author": [
      {
        "family": "Seo",
        "given": "Jangwon"
      },
      {
        "family": "Croft",
        "given": "W. Bruce"
      }
    ],
    "container-title": "Proceedings of the 31<sup>st</sup> annual international ACM SIGIR conference on research and development in information retrieval",
    "id": "Seo2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ir, plagiarism",
    "language": "en-US",
    "page": "571-578",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Local text reuse detection",
    "type": "paper-conference"
  },
  {
    "URL": "https://drtc.isibang.ac.in/handle/1849/229",
    "abstract": "A number of international efforts have been initiated during the past few years leading to the evolvement of various educational metadata specifications for the commonly agreed description of educational resources. Educational metadata can significantly enhance the effective description, search and retrieval of learning objects resulting in efficient organization of educational resources for technology supported instruction. As more and more applications are implemented using educational metadata, it becomes obvious that it would be difficult for a single metadata model to accommodate the functional requirements of all applications. This paper focuses on different existing educational metadata standards with the relative merits of each one, it will also examine the fundamental elements or basic structure of each one of the existing standards, and discuss the interoperability issues. Because of the various E-learning metadata standards that exist, interoperability is a major issue. A major barrier limiting system’s interoperability is the use of different specifications that define the structure and content of learning objects.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Seth",
        "given": "Renu"
      }
    ],
    "container-title": "Conference on ICT for facilitating digital learning environments",
    "id": "Seth2006",
    "issued": {
      "date-parts": [
        [
          2006,
          1
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "language": "en-US",
    "publisher": "DRTC",
    "publisher-place": "Bangalore, India",
    "title": "Learning Object Metadata and its application",
    "type": "paper-conference"
  },
  {
    "URL": "http://www-personal.umich.edu/~csev/papers/2006/2006_07_Roadmap_Interop.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          24
        ]
      ]
    },
    "author": [
      {
        "family": "Severance",
        "given": "Charles"
      },
      {
        "family": "Hardin",
        "given": "Joseph"
      }
    ],
    "id": "Severance2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "publisher": "Sakai Foundation",
    "title": "Strategic directions for sakai and data interoperability",
    "type": "report"
  },
  {
    "URL": "http://www.humanities.mcmaster.ca/~sevigny/dig-korea.pdf",
    "accessed": {
      "date-parts": [
        [
          2009,
          1,
          27
        ]
      ]
    },
    "author": [
      {
        "family": "Sévigny",
        "given": "Alexandre"
      }
    ],
    "container-title": "Linguistics Association of Korea Journal",
    "id": "Sevigny2002",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "page": "65-91",
    "title": "Discourse information grammar",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "DOI": "10.1109/INMIC.2009.5383078",
    "ISBN": "978-1-4244-4872-2",
    "abstract": "Document image analysis is the field of converting paper documents into an editable electronic representation by performing optical character recognition (OCR). In recent years, there has been a tremendous amount of progress in the development of open source OCR systems. OCRopus is one of the leading open source document analysis system with a modular and pluggable architecture. This paper presents an overview of different steps involved in a document image analysis system and illustrates them with examples from OCRopus.",
    "author": [
      {
        "family": "Shafait",
        "given": "Faisal"
      }
    ],
    "container-title": "IEEE 13<sup>th</sup> international multitopic conference (INMIC 2009)",
    "id": "Shafait2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "ocr",
    "language": "en-US",
    "page": "1-6",
    "publisher": "IEEE",
    "publisher-place": "New York, NY, USA",
    "title": "Document image analysis with OCRopus",
    "type": "paper-conference"
  },
  {
    "abstract": "Both in science and in practical affairs we reason by combining facts only inconclusively supported by evidence. Building on an abstract understanding of this process of combination, this book constructs a new theory of epistemic probability. The theory draws on the work of A. P. Dempster but diverges from Depster’s viewpoint by identifying his \"lower probabilities\" as epistemic probabilities and taking his rule for combining \"upper and lower probabilities\" as fundamental. The book opens with a critique of the well-known Bayesian theory of epistemic probability. It then proceeds to develop an alternative to the additive set functions and the rule of conditioning of the Bayesian theory: set functions that need only be what Choquet called \"monotone of order of infinity.\" and Dempster’s rule for combining such set functions. This rule, together with the idea of \"weights of evidence,\" leads to both an extensive new theory and a better understanding of the Bayesian theory. The book concludes with a brief treatment of statistical inference and a discussion of the limitations of epistemic probability. Appendices contain mathematical proofs, which are relatively elementary and seldom depend on mathematics more advanced that the binomial theorem.",
    "author": [
      {
        "family": "Shafer",
        "given": "Glenn"
      }
    ],
    "id": "Shafer1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "keyword": "classic, uncertainty",
    "language": "en-US",
    "publisher": "Princeton University Press",
    "publisher-place": "Princeton, NJ, USA",
    "title": "A mathematical theory of evidence",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Shannon",
        "given": "Claude E."
      }
    ],
    "container-title": "The Bell System Technical Journal",
    "id": "Shannon1948",
    "issued": {
      "date-parts": [
        [
          1948
        ]
      ]
    },
    "keyword": "classic, information_theory, uncertainty",
    "language": "en-US",
    "page": "379-432",
    "title": "A mathematical theory of communications",
    "type": "article-journal",
    "volume": "27"
  },
  {
    "author": [
      {
        "family": "Sharoff",
        "given": "Serge"
      }
    ],
    "container-title": "Wacky! Working papers on the web as corpus",
    "editor": [
      {
        "family": "Baroni",
        "given": "Marco"
      },
      {
        "family": "Bernardini",
        "given": "Silvia"
      }
    ],
    "id": "Sharoff2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "corpus",
    "publisher": "Online Publication; GEDIT",
    "publisher-place": "Bologna",
    "title": "Creating General-Purpose Corpora Using Automated Search Engine Queries",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Sharples",
        "given": "Mike"
      },
      {
        "family": "Goodlet",
        "given": "James"
      },
      {
        "family": "Pemberton",
        "given": "Lyn"
      }
    ],
    "chapter-number": "3",
    "container-title": "Computers and writing: Models and tools",
    "editor": [
      {
        "family": "Williams",
        "given": "Noel"
      },
      {
        "family": "Holt",
        "given": "Patrick O’Brian"
      }
    ],
    "id": "Sharples1989",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "interactive_editing, style_checking, authoring",
    "page": "22-37",
    "publisher": "Intellect",
    "publisher-place": "Bristol, UK",
    "title": "Developing a writer’s assistant",
    "type": "chapter"
  },
  {
    "DOI": "10.1300/j104v37n03_04",
    "abstract": "A nine-stage procedure to build a thesaurus systematically is presented. Each stage offers exercises to put the theory into practice, using agriculture as the sample topic area. Model solutions are given and discussed.",
    "author": [
      {
        "family": "Shearer",
        "given": "James R."
      }
    ],
    "container-title": "Cataloging & Classification Quarterly",
    "id": "Shearer2004",
    "issue": "3-4",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "terminology",
    "language": "en-US",
    "page": "35-56",
    "publisher": "Routledge",
    "title": "A practical exercise in building a thesaurus",
    "type": "article-journal",
    "volume": "37"
  },
  {
    "editor": [
      {
        "family": "Shermis",
        "given": "Mark D."
      },
      {
        "family": "Burstein",
        "given": "Jill"
      }
    ],
    "id": "Shermis2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "publisher": "Lawrence Erlbaum",
    "publisher-place": "Hillsdale, NJ",
    "title": "Automated essay scoring: A cross disciplinary perspective",
    "title-short": "Automated essay scoring",
    "type": "book"
  },
  {
    "URL": "https://eric.ed.gov/?id=ED218943",
    "abstract": "The experimental addition of speech output to computer-based Esperanto lessons using speech synthesized from text is described. Because of Esperanto’s phonetic spelling and simple rhythm, it is particularly easy to describe the mechanisms of Esperanto synthesis. Attention is directed to how the text-to-speech conversion is performed and the ways in which this kind of speech output has been exploited to provide novel kinds of learning situations. The apparatus outputs various sounds according to numerical commands. It not only produces the sounds but also produces the necessary transitions from one sound to the next. Esperanto is written phonetically, with each letter corresponding to just one sound, independent of environment (allophonic variation is relatively slight in the spoken language). Word stress is always on the next-to-last vowel, and monosyllables are almost always unstressed function words. These factors greatly simplify the task of converting text to speech. The conversion of Esperanto text to speech is performed with the aid of a table which contains for each input letter not only the corresponding synthesizer phoneme code(s) but also numbers which specify special subroutines to be invoked for certain letters. The PLATO system is one of the few computer systems which makes explicit provision for diacritics. A critical advantage of speech synthesis in computer-based education is the ability to generate a message as needed by the program. In the case of dictation drills, beginning students have difficulty, mainly due to failings of the synthesizer in producing good consonants. The quality of the synthetic speech is likely to be improved in the next few years.",
    "author": [
      {
        "family": "Sherwood",
        "given": "Bruce"
      }
    ],
    "container-title": "Studies in Language Learning",
    "id": "Sherwood1981",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "language": "en-US",
    "page": "171-181",
    "title": "Speech synthesis applied to language teaching",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.1016/s0306-4573(01)00044-9",
    "ISSN": "0306-4573",
    "abstract": "Most work in NLP requires that texts have been previously segmented into sentences and words. Segmenting a text into sentences and words, however, is a complex task, due to the ambiguity of many punctuation marks and spaces. Furthermore, Web texts such as HTML documents are more difficult to make into well refined and segmented texts because they are described in a more free style, with many sentence boundary and spacing errors. The objective of this paper introduces a multi-strategic integrated text preprocessing method for difficult problems of sentence boundary disambiguation and word boundary disambiguation of Web texts. We have applied a hybrid method (the regular expression rule, the heuristic rule, and the inductive learning of statistical decision trees, using a C4.5 learner) synergically to the task of raw corpus preprocessing. This work contributes to a more correct morphological analysis and guarantees a more stable working of application systems. We tackle easily definable problems with automatically acquired constraints and we use inductively learned decision trees to solve ill-defined ambiguity problems by incorporating multiple features ( n -grams, relative frequency, entropy, tri-dictionary index). The multistrategy approach was thoroughly tested: it achieved approximately 99.12% (with punctuation marks) and 98.04% (without any punctuation marks) accuracy in sentence boundary disambiguation, 95.39% accuracy of word spacing correction, and 94.61% accuracy for whole intermixed text preprocessing problems, from Korean news script Web documents.",
    "author": [
      {
        "family": "Shim",
        "given": "Junhyeok"
      },
      {
        "family": "Kim",
        "given": "Dongseok"
      },
      {
        "family": "Cha",
        "given": "Jeongwon"
      },
      {
        "family": "Lee",
        "given": "Gary G."
      },
      {
        "family": "Seo",
        "given": "Jungyun"
      }
    ],
    "container-title": "Inf. Process. Manage.",
    "id": "Shim2002",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "nlp",
    "language": "en-US",
    "page": "509-527",
    "publisher": "Pergamon Press, Inc.",
    "publisher-place": "Tarrytown, NY, USA",
    "title": "Integrated multi-strategic web document pre-processing for sentence and word boundary detection",
    "type": "article-journal",
    "volume": "38"
  },
  {
    "DOI": "10.1087/2009202",
    "ISSN": "0953-1513",
    "abstract": "Recent developments in Web technology can be used for semantic enhancement of scholarly journal articles, by aiding publication of data and metadata and providing ’lively’ interactive access to content. Such semantic enhancements are already being undertaken by leading STM publishers, and automated text processing will help these enhancements become affordable and routine. Publisher, editor, and author all have primary roles in that process; an incremental approach is needed. Publication of data and metadata to the Web make possible added-value ’ecosystem services’; semantic publishing will bring substantial benefits to scholarly communication.",
    "author": [
      {
        "family": "Shotton",
        "given": "David"
      }
    ],
    "container-title": "Learned Publishing",
    "id": "Shotton2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "nanopublications, scientific_publishing, semantic_web",
    "language": "en-US",
    "page": "85-94",
    "publisher": "Association of Learned and Professional Society Publishers",
    "title": "Semantic publishing: The coming revolution in scientific journal publishing",
    "title-short": "Semantic publishing",
    "type": "article-journal",
    "volume": "22"
  },
  {
    "DOI": "10.3115/974358.974363",
    "URL": "http://www.docrec.com/anlp.ps",
    "abstract": "Many documents are available to a computer only as images from paper. However, most natural language processing systems expect their input as character-coded text, which may be difficult or expensive to extract accurately from the page. We describe a method for converting a document image into character shape codes and word shape tokens. We believe that this representation, which is both cheap and robust, is sufficient for many NLP tasks. In this paper, we show that the representation is sufficient for determining which of 23 languages the document is written in, using only a small number of features, with greater than 90% accuracy overall.",
    "author": [
      {
        "family": "Sibun",
        "given": "Penelope"
      },
      {
        "family": "Spitz",
        "given": "A. Lawrence"
      }
    ],
    "container-title": "Proceedings of the 4<sup>th</sup> conference on applied natural language processing",
    "id": "Sibun1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "language_identification, nlp, ocr",
    "language": "en-US",
    "page": "15-21",
    "title": "Language determination: Natural language processing from scanned document images",
    "title-short": "Language determination",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/AICT.2005.13",
    "abstract": "Learning object repositories play a key role in the vision of reusable learning contents and learning designs, serving as providers for learning-oriented artefacts. Nevertheless, current metadata creation practices result in artefact collections that lack machine-understandable metadata, which seriously hampers opportunities for reuse. Semantic Web ontologies can be used to improve the quality of learning object metadata records, but they are not enough by themselves. In order to respond to requests by returning adequate resources, the repository is required to be aware of the amount, type and quality of the metadata records it stores. The design of a learning object repository approach to \"semantic lifecycle\" is described and illustrated through the concrete architecture of the prototype of the SLOR (semantic learning object repository).",
    "author": [
      {
        "family": "Sicilia",
        "given": "Miguel-Angel"
      },
      {
        "family": "García-Barriocanal",
        "given": "Elena"
      },
      {
        "family": "Sánchez-Alonso",
        "given": "Salvador"
      },
      {
        "family": "Soto",
        "given": "Jesús"
      }
    ],
    "container-title": "Telecommunications, 2005. Advanced industrial conference on telecommunications/service assurance with partial and intermittent resources conference/e-learning on telecommunications workshop. AICT/SAPIR/ELETE 2005. proceedings",
    "editor": [
      {
        "family": "Dini",
        "given": "Petre"
      },
      {
        "family": "Lorenz",
        "given": "Pascal"
      },
      {
        "family": "Freire",
        "given": "Mario"
      },
      {
        "family": "Rolin",
        "given": "Pierre"
      },
      {
        "family": "Szulakiewicz",
        "given": "Pawel"
      },
      {
        "family": "Atmaca",
        "given": "Tulin"
      }
    ],
    "id": "Sicilia2005",
    "issued": {
      "date-parts": [
        [
          2005,
          7
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "language": "en-US",
    "page": "466-471",
    "publisher": "IEEE Computer Society",
    "publisher-place": "Lisbon",
    "title": "A semantic lifecycle approach to learning object repositories",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqp021",
    "abstract": "This article investigates the issues to be faced while producing a digital edition of a representative medieval text: Statuta Comunis Vicentie (1264). Statuta comunis are collections of civic rules very common in Northern Italy since the twelfth century. We are dealing specifically with the ones of Vicenza, a town near Venice. Statuta were usually organized in a single or multiple codices, including the collection of the civic rules, matched and allotted in big chapters (libri) according to the subject. Another fundamental characteristic is the constant review of the original text in different periods, generally due to changes in the government or in the organization of the city. Therefore, the most relevant matters to deal with are: first, description of metadata; second, structural analysis of the text; third, the markup of the additions and amendments; and fourth; identification of specific semantic values, in particular personal names, organizational names and names of places. This article outlines the reasons for choosing XML/TEI for the project, how to address the four matters listed above and how the chosen standard can be customized to treat the peculiar aspects of this text according to the traditional editing practice.",
    "author": [
      {
        "family": "Siciliano",
        "given": "Luigi"
      },
      {
        "family": "Salardi",
        "given": "Viviana"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Siciliano2009",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, tei, xml",
    "language": "en-US",
    "page": "329-338",
    "title": "The digital edition of the statuta comunis vicentie of 1264",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "ISSN": "00104817",
    "URL": "http://jstor.org/stable/30200526",
    "abstract": "If there is such a thing as a new computer-assisted literary criticism, its expression lies in a model that is as broad-based as that presented in John Smith’s seminal article, \"Computer Criticism,\" and is as encompassing of the discipline of literary studies as it is tied to the evolving nature of the electronic literary text that lies at the heart of its intersection with computing. It is the desire to establish the parameters of such a model for the interaction between literary studies and humanities computing - for a model of the new computer-assisted literary criticism - that gave rise to the papers in this collection and to the several conference panel-presentations and discussions that, in their print form, these papers represent.",
    "author": [
      {
        "family": "Siemens",
        "given": "Ray"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Siemens2002",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "259-267",
    "publisher": "Springer",
    "title": "A new Computer-Assisted literary criticism?",
    "type": "article-journal",
    "volume": "36"
  },
  {
    "author": [
      {
        "family": "Siemens",
        "given": "Ray"
      },
      {
        "family": "Sayers",
        "given": "Jentery"
      }
    ],
    "chapter-number": "11",
    "container-title": "Between the humanities and the digital",
    "editor": [
      {
        "family": "Svensson",
        "given": "Patrik"
      },
      {
        "family": "Goldberg",
        "given": "David T."
      }
    ],
    "id": "Siemens2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "en-US",
    "page": "145-161",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Toward Problem-Based modeling in the digital humanities",
    "type": "chapter"
  },
  {
    "URL": "http://icame.uib.no/ij21/lampcorp.pdf",
    "abstract": "The Lampeter Corpus of Early Modern English Tracts, compiled at the Chemnitz University of Technology, has recently been completed and will shortly be available through ICAME and the Oxford Text Archive. The following article gives a brief outline of the corpus set-up and its coding conventions, and places it within its historical and corpus-linguistic background. For a more detailed account of the corpus, see the manual of information, “Life is ruled and governed by opinion”: The Lampeter Corpus of Early Modern English Tracts (Schmied, Claridge and Siemund forthcoming), which will be published by ICAME this autumn.",
    "author": [
      {
        "family": "Siemund",
        "given": "Rainer"
      },
      {
        "family": "Claridge",
        "given": "Claudia"
      }
    ],
    "container-title": "ICAME Journal",
    "id": "Siemund1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, english",
    "language": "en-US",
    "page": "61-70",
    "title": "The Lampeter corpus of early modern english tracts",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "DOI": "10.2307/1978637",
    "author": [
      {
        "family": "Silverman",
        "given": "Robert E."
      }
    ],
    "container-title": "The Journal of Higher Education",
    "id": "Silverman1960",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          1960
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "481-486",
    "publisher": "Ohio State University Press",
    "title": "Auto-instructional devices: Some theoretical and practical considerations",
    "title-short": "Auto-instructional devices",
    "type": "article-journal",
    "volume": "31"
  },
  {
    "URL": "http://cognet.mit.edu/sites/default/files/books/9780262281805/pdfs/9780262281805_chap1.pdf",
    "abstract": "This chapter treats rather concisely a range of topics that help define the scope of cognitive science and the numerous dimensions along which the field can be explored. It focuses more on identifying issues than on providing definite answers, a task more appropriately left for the contributors on specific topics.",
    "author": [
      {
        "family": "Simon",
        "given": "Herbert A."
      },
      {
        "family": "Kaplan",
        "given": "Craig A."
      }
    ],
    "chapter-number": "1",
    "container-title": "The foundations of cognitive science",
    "editor": [
      {
        "family": "Posner",
        "given": "Michael I."
      }
    ],
    "id": "Simon1989",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "cogsci, formal_models",
    "language": "en-US",
    "page": "1-47",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Foundations of cognitive science",
    "type": "chapter"
  },
  {
    "DOI": "10.1287/opre.38.1.7",
    "ISSN": "0030-364X",
    "abstract": "Modeling is a principal tool for studying complex systems. Since models may be used for predictions, for analysis, or for prescription, we must ask what our goals are before we build our models. Historically, predictive numerical models have dominated our practice. Since the world we are modeling is orders of magnitude more complex than even the largest models our computers can handle, we must conserve computational power, first, by asking how much temporal detail we need and how much can be supported by available data and theories, second, by asking whether knowledge of steady states may not be more important than knowledge of temporal paths, third, by using the hierarchical properties of systems to aggregate and thereby simplify them, and, fourth, by substituting symbolic modeling, where appropriate, for numerical modeling.",
    "author": [
      {
        "family": "Simon",
        "given": "Herbert A."
      }
    ],
    "container-title": "Operations Research",
    "id": "Simon1990",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "formal_models",
    "language": "en-US",
    "page": "7-14",
    "title": "Prediction and prescription in systems modeling",
    "type": "article-journal",
    "volume": "38"
  },
  {
    "DOI": "10.1080/15420353.2017.1307303",
    "ISSN": "1542-0353",
    "abstract": "Recogito 2 is an open source annotation tool currently under development by Pelagios, an international initiative aimed at facilitating better linkages between online resources documenting the past. With Recogito 2, we aim to provide an environment for efficient semantic annotation?i.e., the task of enriching content with references to controlled vocabularies?in order to facilitate links between online data. At the same time, we address a perceived gap in the performance of existing tools, by emphasizing the development of mechanisms for manual intervention and editorial control that support the curation of quality data. While Recogito 2 provides an online workspace for general-purpose document annotation, it is particularly well-suited for geo-annotation, in other words annotating documents with references to gazetteers, and supports the annotation of both texts and images (i.e., digitized maps). Already available for testing at http://recogito.pelagios.org, its formal release to the public occurred in December 2016.",
    "author": [
      {
        "family": "Simon",
        "given": "Rainer"
      },
      {
        "family": "Barker",
        "given": "Elton"
      },
      {
        "family": "Isaksen",
        "given": "Leif"
      },
      {
        "family": "De Soto Cañamares",
        "given": "Pau"
      }
    ],
    "container-title": "Journal of Map & Geography Libraries",
    "id": "Simon2017",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "annotation, geocoding, spatio-temporal_annotation",
    "language": "en-US",
    "page": "111-132",
    "title": "Linked data annotation without the pointy brackets: Introducing Recogito 2",
    "title-short": "Linked data annotation without the pointy brackets",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "DOI": "10.1177/1474022202001002004",
    "ISSN": "1474-0222",
    "author": [
      {
        "family": "Sinclair",
        "given": "Stéfan"
      },
      {
        "family": "Gouglas",
        "given": "Sean W."
      }
    ],
    "container-title": "Arts and Humanities in Higher Education",
    "id": "Sinclair2002",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2002,
          10
        ]
      ]
    },
    "language": "en-US",
    "page": "167-183",
    "title": "Theory into practice: A case study of the humanities computing master of arts programme at the University of Alberta",
    "title-short": "Theory into practice",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "URL": "http://aclweb.org/anthology/W06-1109",
    "abstract": "To determine how close two language models (e.g., n-grams models) are, we can use several distance measures. If we can represent the models as distributions,then the similarity is basically the similarity of distributions. And a number of measures are based on information theoretic approach. In this paper we present some experiments on using such similarity measures for an old Natural Language Processing (NLP) problem. One of the measures considered is perhaps a novel one, which we have called mutual cross entropy. Other measures are either well known or based on well known measures,but the results obtained with them vis-a-vis one-another might help in gaining an insight into how similarity measures work in practice. The first step in processing a text is to identify the language and encoding of its contents. This is a practical problem since for many languages, there are no universally followed text encoding standards. The method we have used in this paper for language and encoding identification uses pruned character n-grams, alone as well augmented with word n-grams. This method seems to give results comparable to other methods.",
    "author": [
      {
        "family": "Kumar Singh",
        "given": "Anil"
      }
    ],
    "container-title": "Proceedings of the workshop on linguistic distances",
    "id": "Singh2006",
    "issued": {
      "date-parts": [
        [
          2006,
          7
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "language": "en-US",
    "page": "63-72",
    "title": "Study of some distance measures for language and encoding identification",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/icde.2008.4497514",
    "ISBN": "978-1-4244-1836-7",
    "abstract": "The inherent uncertainty of data present in numerous applications such as sensor databases, text annotations, and information retrieval motivate the need to handle imprecise data at the database level. Uncertainty can be at the attribute or tuple level and is present in both continuous and discrete data domains. This paper presents a model for handling arbitrary probabilistic uncertain data (both discrete and continuous) natively at the database level. Our approach leads to a natural and efficient representation for probabilistic data. We develop a model that is consistent with possible worlds semantics and closed under basic relational operators. This is the first model that accurately and efficiently handles both continuous and discrete uncertainty. The model is implemented in a real database system (PostgreSQL) and the effectiveness and efficiency of our approach is validated experimentally.",
    "author": [
      {
        "family": "Singh",
        "given": "Sarvjeet"
      },
      {
        "family": "Mayfield",
        "given": "Chris"
      },
      {
        "family": "Shah",
        "given": "Rahul"
      },
      {
        "family": "Prabhakar",
        "given": "Sunil"
      },
      {
        "family": "Hambrusch",
        "given": "Susanne"
      },
      {
        "family": "Neville",
        "given": "Jennifer"
      },
      {
        "family": "Cheng",
        "given": "Reynold"
      }
    ],
    "container-title": "2008 IEEE 24<sup>th</sup> International Conference on Data Engineering",
    "id": "Singh2008a",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "database, uncertainty",
    "language": "en-US",
    "page": "1053-1061",
    "publisher": "IEEE",
    "title": "Database support for probabilistic attributes and tuples",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1376616.1376744",
    "ISBN": "9781605581026",
    "abstract": "Orion is a state-of-the-art uncertain database management system with built-in support for probabilistic data as first class data types. In contrast to other uncertain databases, Orion supports both attribute and tuple uncertainty with arbitrary correlations. This enables the database engine to handle both discrete and continuous pdfs in a natural and accurate manner. The underlying model is closed under the basic relational operators and is consistent with Possible Worlds Semantics. We demonstrate how Orion simplifies the design and enhances the capabilities of two example applications: managing sensor data (continuous uncertainty) and inferring missing values (discrete uncertainty).",
    "author": [
      {
        "family": "Singh",
        "given": "Sarvjeet"
      },
      {
        "family": "Mayfield",
        "given": "Chris"
      },
      {
        "family": "Mittal",
        "given": "Sagar"
      },
      {
        "family": "Prabhakar",
        "given": "Sunil"
      },
      {
        "family": "Hambrusch",
        "given": "Susanne"
      },
      {
        "family": "Shah",
        "given": "Rahul"
      }
    ],
    "container-title": "Proceedings of the 2008 ACM SIGMOD international conference on management of data (SIGMOD ’08)",
    "id": "Singh2008b",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "database, uncertainty",
    "language": "en-US",
    "page": "1239-1241",
    "title": "Orion 2.0: Native support for uncertain data",
    "title-short": "Orion 2.0",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1007996.1008030",
    "author": [
      {
        "family": "Sitthiworachart",
        "given": "Jirarat"
      },
      {
        "family": "Joy",
        "given": "Mike"
      }
    ],
    "container-title": "ITiCSE ’04: Proceedings of the 9<sup>th</sup> annual SIGCSE conference on innovation and technology in computer science education",
    "id": "Sitthiworachart2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "122-126",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Effective peer assessment for learning computer programming",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.jlcl.org/2011_Heft2/7.pdf",
    "abstract": "We present our experiments with annotating a Latin corpus using an assisted annotation procedure where the corpus to be annotated is pre-annotated by a statistical tagger. This assisted procedure gives a notable reduction in an notator error compared to the unassisted annotation of previous annotation efforts, even with a huge tagset (1 000 tags) and modest tagger accuracy due to limited training data and domain effects.",
    "author": [
      {
        "family": "Skjærholt",
        "given": "Arne"
      }
    ],
    "container-title": "Journal for Language Technology and Computational Linguistics",
    "id": "Skjaerholt2011",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, latin, pos_tagging",
    "language": "en-US",
    "page": "151-163",
    "title": "More, faster: Accelerated corpus annotation with statistical taggers",
    "title-short": "More, faster",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "URL": "http://www.atarimagazines.com/v3n3/platorising.html",
    "abstract": "Atari’s PLATO cartridge (officially known as the Atari Access Software for the PLATO Services Network) is the Atari user’s link to Control Data Corporation’s PLATO network, the \"ultimate\" electronic information and communication utility. PLATO was developed in 1962 at the University of Illinois. After 14 years of development, Control Data (CDC) bought the marketing and trademark rights to the PLATO software. In the early 1970’s, a number of large corporations began to develop specialized vocational software for use on CDC’s mainframe computers. This software was used to tutor employees, who accessed the material through expensive remote graphics terminals. There was a $50/hour surcharge for use of CDC’s computers, so only enormous organizations such as Boeing, lockheed and major universities could afford to use them. Then, during the mid-1970’s, CDC began to sell Cyber supercomputers running the PLATO operating system to corporations and universities all over the world. Electronic mail, interactive games, bulletin boards, and many other services associated with electronic utilities evolved on PLATO during this period. By 1978, the PLATO terminal evolved into a high-resolution, touch-sensitive screen with a dedicated keyboard, graphics printer and 1200-baud modem. As personal computers became more capable, CDC developed terminal \"emulators\" that gave some PC’s, such as the zenith Z-100 and the IBM PC, the ability to connect to the PLATO system. This eventually led to the Homelink \"after-hours\" service, which was affordably priced at $5 per hour; as a result, a whole new generation of users discovered PLATO. According to officials at Atari Inc., Atari’s access software for the PLATO Homelink service should be released by the third quarter of this year. For stylistic reasons, we’ll refer to this product as the Atari PLATO cartridge, although this is not its officle name.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Small",
        "given": "David"
      },
      {
        "family": "Small",
        "given": "Sandy"
      }
    ],
    "container-title": "ANTIC",
    "id": "Small1984",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1984,
          7
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "language": "en-US",
    "page": "36-44",
    "title": "Plato rising",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.1007/978-1-4615-6245-0_8",
    "ISBN": "978-1-4613-7865-5",
    "abstract": "Imperfection, be it imprecision or uncertainty, pervades real-world scenarios and must therefore be incorporated into every information system that attempts to provide a complete and accurate model of the real world. Yet, this is hardly achieved in today’s information systems. A major reason might be the inherent difficulty of understanding the various aspects of imprecision and uncertainty.",
    "author": [
      {
        "family": "Smets",
        "given": "Philippe"
      }
    ],
    "chapter-number": "8",
    "container-title": "Uncertainty management in information systems",
    "editor": [
      {
        "family": "Motro",
        "given": "Amihai"
      },
      {
        "family": "Smets",
        "given": "Philippe"
      }
    ],
    "id": "Smets1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "225-254",
    "publisher": "Springer",
    "publisher-place": "Boston, MA, USA",
    "title": "Imperfect information: Imprecision and uncertainty",
    "title-short": "Imperfect information",
    "type": "chapter"
  },
  {
    "DOI": "10.1109/34.895972",
    "ISSN": "01628828",
    "abstract": "Presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap.",
    "author": [
      {
        "family": "Smeulders",
        "given": "Arnold W. M."
      },
      {
        "family": "Worring",
        "given": "Marcel"
      },
      {
        "family": "Santini",
        "given": "Simone"
      },
      {
        "family": "Gupta",
        "given": "Amarnath"
      },
      {
        "family": "Jain",
        "given": "Ramesh"
      }
    ],
    "container-title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "id": "Smeulders2000",
    "issue": "12",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "ir",
    "language": "en-US",
    "page": "1349-1380",
    "title": "Content-based image retrieval at the end of the early years",
    "type": "article-journal",
    "volume": "22"
  },
  {
    "DOI": "10.1016/0022-2836(81)90087-5",
    "ISSN": "0022-2836",
    "PMID": "7265238",
    "abstract": "The identification of maximally homologous subsequences among sets of long sequences is an important problem in molecular sequence analysis. The problem is straightforward only if one restricts consideration to contiguous subsequences (segments) containing no internal deletions or insertions. The more general problem has its solution in an extension of sequence metrics (Sellers 1974; Waterman et al., 1976) developed to measure the minimum number of \"events\" required to convert one sequence into another.",
    "author": [
      {
        "family": "Smith",
        "given": "Temple F."
      },
      {
        "family": "Waterman",
        "given": "Michael S."
      }
    ],
    "container-title": "Journal of molecular biology",
    "id": "Smith1981",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "keyword": "approximate_matching",
    "language": "en-US",
    "page": "195-197",
    "title": "Identification of common molecular subsequences.",
    "type": "article-journal",
    "volume": "147"
  },
  {
    "DOI": "10.1007/3-540-44796-2_12",
    "ISBN": "978-3-540-42537-3",
    "abstract": "Geographic interfaces provide natural, scalable visualizations for many digital library collections, but the wide range of data in digital libraries presents some particular problems for identifying and disambiguating place names. We describe the toponym-disambiguation system in the Perseus digital library and evaluate its performance. Name categorization varies significantly among different types of documents, but toponym disambiguation performs at a high level of precision and recall with a gazetteer an order of magnitude larger than most other applications.",
    "author": [
      {
        "family": "Smith",
        "given": "David"
      },
      {
        "family": "Crane",
        "given": "Gregory"
      }
    ],
    "chapter-number": "12",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Research and advanced technology for digital libraries",
    "editor": [
      {
        "family": "Constantopoulos",
        "given": "Panos"
      },
      {
        "family": "Sølvberg",
        "given": "Ingeborg T."
      }
    ],
    "id": "Smith2001",
    "issued": {
      "date-parts": [
        [
          2001,
          8,
          30
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_library, geocoding, toponym_resolution",
    "language": "en-US",
    "page": "127-136",
    "publisher": "Springer Berlin Heidelberg",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Disambiguating geographic names in a historical digital library",
    "type": "chapter",
    "volume": "2163"
  },
  {
    "DOI": "10.1145/544220.544260",
    "ISBN": "1-58113-513-0",
    "abstract": "Digital libraries of historical documents provide a wealth of information about past events, often in unstructured form. Once dates and place names are identified and disambiguated, using methods that can differ by genre, we examine collocations to detect events. Collocations can be ranked by several measures, which vary in effectiveness according to type of events, but the log-likelihood measure (-2 log λ) offers a reasonable balance between frequently and infrequently mentioned events and between larger and smaller spatial and temporal ranges. Significant date-place collocations can be displayed on timelines and maps as an interface to digital libraries. More detailed displays can highlight key names and phrases associated with a given event.",
    "author": [
      {
        "family": "Smith",
        "given": "David A."
      }
    ],
    "container-title": "JCDL ’02: Proceedings of the 2nd ACM/IEEE-CS joint conference on digital libraries",
    "id": "Smith2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "cultural_heritage, geocoding, ir",
    "language": "en-US",
    "page": "191-196",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Detecting events with date and place information in unstructured text",
    "type": "paper-conference"
  },
  {
    "DOI": "10.3115/1119394.1119401",
    "abstract": "We present minimally supervised methods for training and testing geographic name disambiguation (GND) systems. We train data-driven place name classifiers using toponyms already disambiguated in the training text — by such existing cues as \"Nashville, Tenn.\" or \"Springfield, MA\" — and test the system on texts where these cues have been stripped out and on hand-tagged historical texts. We experiment on three English-language corpora of varying provenance and complexity: newsfeed from the 1990s, personal narratives from the 19th century American west, and memoirs and records of the U.S. Civil War. Disambiguation accuracy ranges from 87% for news to 69% for some historical collections.",
    "author": [
      {
        "family": "Smith",
        "given": "David A."
      },
      {
        "family": "Mann",
        "given": "Gideon S."
      }
    ],
    "container-title": "Proceedings of the HLT-NAACL 2003 workshop on analysis of geographic references",
    "id": "Smith2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "cultural_heritage, geocoding, nlp, toponym_resolution",
    "language": "en-US",
    "page": "45-49",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Morristown, NJ, USA",
    "title": "Bootstrapping toponym classifiers",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/bigdata.2013.6691675",
    "abstract": "Texts propagate through many social networks and provide evidence for their structure. We present efficient algorithms for detecting clusters of reused passages embedded within longer documents in large collections. We apply these techniques to analyzing the culture of reprinting in the United States before the Civil War. Without substantial copyright enforcement, stories, poems, news, and anecdotes circulated freely among newspapers, magazines, and books. From a collection of OCR’d newspapers, we extract a new corpus of reprinted texts, explore the geographic spread and network connections of different publications, and analyze the time dynamics of different genres.",
    "author": [
      {
        "family": "Smith",
        "given": "David A."
      },
      {
        "family": "Cordell",
        "given": "Ryan"
      },
      {
        "family": "Dillon",
        "given": "Elizabeth M."
      }
    ],
    "container-title": "IEEE international conference on big data, 2013",
    "id": "Smith2013",
    "issued": {
      "date-parts": [
        [
          2013,
          10
        ]
      ]
    },
    "keyword": "digital_humanities, plagiarism",
    "language": "en-US",
    "page": "86-94",
    "publisher": "IEEE",
    "publisher-place": "New York, NY, USA",
    "title": "Infectious texts: Modeling text reuse in nineteenth-century newspapers",
    "title-short": "Infectious texts",
    "type": "paper-conference"
  },
  {
    "URL": "http://ceur-ws.org/Vol-1304/STIDS2014_T01_SmithEtAl.pdf",
    "abstract": "Disputes over territory are a major contributing factor to the disruption of international relations. We believe that a cumulative, integrated, and continuously updated resource providing information about such disputes in an easily accessible form would be of benefit to intelligence analysts, military strategists, political scientists, and also to historians and others concerned with international disputes. We propose an ontology-based strategy for creating such a resource. The resource will contain information about territorial disputes, arguments for and against claims pertaining to sovereignty, proffered evidence for such claims, political and military motives (overt or hidden), and associated conflicts. Our approach is designed to address several issues surrounding the representation of geopolitical conflict, including the tracking and individuation of disputes and the validation of disseminated information.",
    "author": [
      {
        "family": "Smith",
        "given": "Barry"
      },
      {
        "family": "Otte",
        "given": "Neil"
      },
      {
        "family": "Donohue",
        "given": "Brian"
      }
    ],
    "collection-title": "CEUR workshop proceedings",
    "container-title": "Proceedings of the ninth conference on semantic technologies for intelligence, defense, and security (STIDS 2014)",
    "editor": [
      {
        "family": "Laskey",
        "given": "Kathryn B."
      },
      {
        "family": "Emmons",
        "given": "Ian"
      },
      {
        "family": "Costa",
        "given": "Paulo C. G."
      }
    ],
    "id": "Smith2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "linked_data, ontologies, rdf, semantic_web",
    "language": "en-US",
    "page": "2-9",
    "publisher": "George Mason University",
    "publisher-place": "Fairfax, VA, USA",
    "title": "An ontological approach to territorial disputes",
    "type": "paper-conference",
    "volume": "1304"
  },
  {
    "DOI": "10.1057/978-1-137-49944-8",
    "ISBN": "978-1-137-49944-8",
    "author": [
      {
        "family": "Smithies",
        "given": "James"
      }
    ],
    "id": "Smithies2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Palgrave Macmillan",
    "publisher-place": "London",
    "title": "The digital humanities and the digital modern",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-1-4612-3628-3",
    "ISBN": "978-0-387-96945-9",
    "abstract": "Ignorance and Uncertainty overviews a variety of approaches to the problem of indeterminacies in human thought and behavior. This book examines, in depth, trends in the psychology of judgment and decision-making under uncertainty or ignorance. Research from the fields of cognitive psychology, social psychology, organizational studies, sociology, and social anthroplogy are reviewed here in anticipation of what Dr. Smithson characterizes as the beginning of a \"creative dialogue between these researchers\". Ignorance and Uncertainty offers the conceptual framework for understanding the paradigms associated with current research. It discusses the ways in which attitudes toward ignorance and uncertainty are changing, and addresses issues previously ignored.",
    "author": [
      {
        "family": "Smithson",
        "given": "Michael"
      }
    ],
    "id": "Smithson1989",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "publisher": "Springer New York",
    "publisher-place": "New York, NY, USA",
    "title": "Ignorance and uncertainty",
    "type": "book"
  },
  {
    "URL": "http://hdl.handle.net/2134/1784",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Smythe",
        "given": "Colin"
      },
      {
        "family": "Roberts",
        "given": "P."
      }
    ],
    "container-title": "Proceedings of the 4<sup>th</sup> CAA conference",
    "id": "Smythe2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "publisher": "Loughborough University",
    "publisher-place": "Loughborough, England",
    "title": "An overview of the IMS Question & Test Interoperability specification",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1561/1500000014",
    "ISSN": "1554-0669",
    "abstract": "In this paper, we review 300 references on video retrieval, indicating when text-only solutions are unsatisfactory and showing the promising alternatives which are in majority concept-based. Therefore, central to our discussion is the notion of a semantic concept: an objective linguistic description of an observable entity. Specifically, we present our view on how its automated detection, selection under uncertainty, and interactive usage might solve the major scientific problem for video retrieval: the semantic gap. To bridge the gap, we lay down the anatomy of a concept-based video search engine. We present a component-wise decomposition of such an interdisciplinary multimedia system, covering influences from information retrieval, computer vision, machine learning, and human-computer interaction. For each of the components we review state-of-the-art solutions in the literature, each having different characteristics and merits. Because of these differences, we cannot understand the progress in video retrieval without serious evaluation efforts such as carried out in the NIST TRECVID benchmark. We discuss its data, tasks, results, and the many derived community initiatives in creating annotations and baselines for repeatable experiments. We conclude with our perspective on future challenges and opportunities.",
    "author": [
      {
        "family": "Snoek",
        "given": "Cees G. M."
      },
      {
        "family": "Worring",
        "given": "Marcel"
      }
    ],
    "container-title": "Foundations and Trends in Information Retrieval",
    "id": "Snoek2009",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "ir",
    "language": "en-US",
    "page": "215-322",
    "publisher": "Now",
    "publisher-place": "Hanover, MA, USA",
    "title": "Concept-Based video retrieval",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.2307/1578601",
    "author": [
      {
        "family": "Snow",
        "given": "C. P."
      }
    ],
    "container-title": "The two cultures: And a second look",
    "id": "Snow1964",
    "issued": {
      "date-parts": [
        [
          1964
        ]
      ]
    },
    "keyword": "classic, digital_humanities, philosophy_of_science",
    "language": "en-US",
    "page": "1-21",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge, UK",
    "title": "The Rede lecture, 1959",
    "type": "chapter"
  },
  {
    "abstract": "This paper is a progress report of the retrodigitization project of the Czech Digital Mathematics Library, DML CZ. We are aiming to digitize valuable mathematical journals and books (250,000 pages) published in the Czech and Slovak Republics, and make them publicly available in digital form. We describe here the project workflow: the key concept is a gradual enhancement of the digital material by ’knowledge enhancing’ filters applied to the markup-rich XML data.",
    "author": [
      {
        "family": "Sojka",
        "given": "Petr"
      }
    ],
    "container-title": "Znalosti 2006, sborník příspěvků 5. Ročníku konference",
    "editor": [
      {
        "family": "Paralič",
        "given": "Jan"
      },
      {
        "family": "Dvorský",
        "given": "Jiří"
      },
      {
        "family": "Kratký",
        "given": "Michal"
      }
    ],
    "id": "Sojka2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_library, language_identification",
    "language": "en-US",
    "page": "243-247",
    "title": "Workflow in the digital mathematics library project",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.knihovna.zcu.cz/Caslin/Caslin09.pdf",
    "abstract": "A succesfully built institutional or community repository (e.g. set of workflows) needs a coordinated effort of librarians, IT specialists and representatives of users ­ content specialists. We will explain and discuss design, technical a political decisions behind building the Czech Digital Mathematics Library DML-CZ (http://dml.cz) in the context of other succesfull thematical community projects (PubMed Central, ADS, SCOAP3 and planned EuDML). A framework developed for handling different types of mathematical publications is presented. It integrates workflow for the articles scanned from a paper, for documents from retro-born digital period (data were available in some type of electronic form) and for born-digital papers (newly published data from publishers). Experience gained, lessons learned and tools prepared during development of the Czech Digital Mathematics Library DML-CZ are described. We describe problems of migration of existing workflows (born-digital, retro-digital) into the repository, negotiations with Google Scholar towards better visibility, indexing and search, and problems of copyright and sustainability issues we have faced.",
    "accessed": {
      "date-parts": [
        [
          2010,
          9,
          15
        ]
      ]
    },
    "author": [
      {
        "family": "Sojka",
        "given": "Petr"
      }
    ],
    "container-title": "Proceedings of CASLIN 2009, institutional online repositories and open access, 16th international seminar",
    "id": "Sojka2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "page": "74-78",
    "publisher": "University of West Bohemia",
    "publisher-place": "Pilsen",
    "title": "An experience with building digital open access repository DML-CZ (classification of multilingual mathematical papers in DML-CZ)",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1860559.1860563",
    "ISBN": "978-1-4503-0231-9",
    "abstract": "This paper describes several innovative document transformations and tools that have been developed in the process of building the Digital Mathematical Library DML-CZ http://dml.cz. The main result presented in this paper is our PDF re-compression tools developed using a jbig2enc library. Together with other programs, especially pdfsizeopt.py by Péter Szabó, we have managed to decrease PDF storage size and transmission needs be 62%: using both programs we reduced the size of the original PDFs to 38%. This paper briefly describes other approaches and tools developed while creating the digital library. The batch digital signature stamper, the document similarity metrics which uses four different methods, a [meta]data validation process and some math OCR tools represent some of the main byproducts of this project. These ways of document engineering, together with Google Scholar indexing optimizations have led to the success of serving digitized and born-digital scientific math documents to the public in DML=CZ, and will be employed also in the project of The European Digital Mathematics Library, EuDML.",
    "author": [
      {
        "family": "Sojka",
        "given": "Petr"
      },
      {
        "family": "Hatlapatka",
        "given": "Radim"
      }
    ],
    "collection-title": "DocEng ’10",
    "container-title": "Proceedings of the 10th ACM symposium on document engineering",
    "id": "Sojka2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "language": "en-US",
    "page": "3-12",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Document engineering for a digital library: PDF recompression using JBIG2 and other optimizations of PDF documents",
    "title-short": "Document engineering for a digital library",
    "type": "paper-conference"
  },
  {
    "URL": "http://jstor.org/stable/j.ctt12877vs.20",
    "author": [
      {
        "family": "Solleveld",
        "given": "Floris"
      }
    ],
    "container-title": "The making of the humanities, vol. III: The making of the modern humanities",
    "id": "Solleveld2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "intertextuality",
    "language": "en-US",
    "page": "265-280",
    "publisher": "Amsterdam University Press",
    "title": "What books are made of: Scholarship and intertextuality in the history of the humanities",
    "title-short": "What books are made of",
    "type": "chapter"
  },
  {
    "URL": "http://www.clef-campaign.org/2008/working_notes/sorg_paperCLEF2008.pdf",
    "author": [
      {
        "family": "Sorg",
        "given": "Philipp"
      },
      {
        "family": "Cimiano",
        "given": "Philipp"
      }
    ],
    "container-title": "Working notes for the CLEF 2008 workshop",
    "id": "Sorg2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ir, wikipedia",
    "language": "en-US",
    "title": "Cross-lingual information retrieval with explicit semantic analysis",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-12550-8_4",
    "abstract": "Explicit Semantic Analysis (ESA) has been recently proposed as an approach to computing semantic relatedness between words (and indirectly also between texts) and has thus a natural application in information retrieval, showing the potential to alleviate the vocabulary mismatch problem inherent in standard Bag-of-Word models. The ESA model has been also recently extended to cross-lingual retrieval settings, which can be considered as an extreme case of the vocabulary mismatch problem. The ESA approach actually represents a class of approaches and allows for various instantiations. As our first contribution, we generalize ESA in order to clearly show the degrees of freedom it provides. Second, we propose some variants of ESA along different dimensions, testing their impact on performance on a cross-lingual mate retrieval task on two datasets (JRC-ACQUIS and Multext). Our results are interesting as a systematic investigation has been missing so far and the variations between different basic design choices are significant. We also show that the settings adopted in the original ESA implementation are reasonably good, which to our knowledge has not been demonstrated so far, but can still be significantly improved by tuning the right parameters (yielding a relative improvement on a cross-lingual mate retrieval task of between 62% (Multext) and 237% (JRC-ACQUIS) with respect to the original ESA model).",
    "author": [
      {
        "family": "Sorg",
        "given": "Philipp"
      },
      {
        "family": "Cimiano",
        "given": "Philipp"
      }
    ],
    "container-title": "Natural language processing and information systems: 14<sup>th</sup> international conference on applications of natural language to information systems (NLDB 2009)",
    "editor": [
      {
        "family": "Horacek",
        "given": "Helmut"
      },
      {
        "family": "Métais",
        "given": "Elisabeth"
      },
      {
        "family": "Muñoz",
        "given": "Rafael"
      },
      {
        "family": "Wolska",
        "given": "Magdalena"
      }
    ],
    "id": "Sorg2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "ir, wikipedia",
    "language": "en-US",
    "page": "36-48",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "An experimental comparison of explicit semantic analysis implementations for Cross-Language retrieval",
    "type": "paper-conference",
    "volume": "5723"
  },
  {
    "URL": "http://halshs.archives-ouvertes.fr/halshs-00396452/",
    "abstract": "Unlike most modern languages, Middle French is a language whose spelling is not yet stabilized. There is a great deal of variation in the spelling of a word and accordingly the traditional methods for lemmatization cannot be used. LGeRM (Lemmes, Graphies et Règles Morphologiques) proposes a solution based on a databank containing known lemmatized spellings and a set of graphical and morphological rules specific to the medieval language. LGeRM can provide help in consulting a dictionary, browsing or lemmatizing medieval texts, and it can be useful in the electronic edition of manuscripts and the automatic construction of glossaries. This multipurpose tool is accessible on the Internet at www.atilf.fr/dmf.",
    "author": [
      {
        "family": "Souvay",
        "given": "Gilles"
      },
      {
        "family": "Pierrel",
        "given": "Jean-Marie"
      }
    ],
    "container-title": "Traitement Automatique des Langues",
    "id": "Souvay2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, french, morphology, nlp",
    "language": "fr-FR",
    "page": "149-172",
    "title": "LGeRM: Lemmatisation des mots en Moyen Français",
    "type": "article-journal",
    "volume": "50"
  },
  {
    "author": [
      {
        "family": "Sowa",
        "given": "John F."
      }
    ],
    "id": "Sowa1991",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Morgan Kaufmann",
    "publisher-place": "San Mateo, CA, USA",
    "title": "Principles of semantic networks: Explorations in the representation of knowledge",
    "title-short": "Principles of semantic networks",
    "type": "book"
  },
  {
    "DOI": "10.1145/1998076.1998080",
    "ISBN": "978-1-4503-0744-4",
    "abstract": "Topic models are emerging tools for improved browsing and searching within digital libraries. These techniques collapse words within documents into unordered \"bags of words,\" ignoring word order. In this paper, we present a method that examines syntactic dependency parse trees from Wikipedia article titles to learn expected patterns between relative lexical arguments. This process is highly dependent on the global word ordering of a sentence, modeling how each word interacts with other words to gain an aggregate perspective on how words interact over all 3.2 million titles. Using this information, we analyze how coherent a given topic is by comparing the relative usage vectors between the top 5 words in a topic. Results suggest that this technique can identify poor topics based on how well the relative usages align with each other within a topic, potentially aiding digital library indexing.",
    "author": [
      {
        "family": "Spagnola",
        "given": "Steve"
      },
      {
        "family": "Lagoze",
        "given": "Carl"
      }
    ],
    "container-title": "Proceeding of the 11th annual international ACM/IEEE joint conference on digital libraries (JCDL ’11)",
    "id": "Spagnola2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "ir, nlp, topic_modeling, wikipedia",
    "language": "en-US",
    "page": "21-24",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Word order matters: Measuring topic coherence with lexical argument structure",
    "title-short": "Word order matters",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1080/1355800750120205",
    "abstract": "Six spatial and three typographic coding systems were used to give 18 different styles of presentation for bibliographies. Two experiments are reported: in Experiment 1 a list of authors surnames was attached to each bibliography and subjects were asked to find as many as possible of the corresponding entries in 45 seconds; in Experiment 2 lists of titles were used with a time limit of 1 minute 45 seconds. Analysis of variance indicated that spatial coding was more effective than typographic coding in Experiment 1, whereas the reverse was true for Experiment 2. The advantages of the different coding systems are discussed in the light of the results obtained.",
    "author": [
      {
        "family": "Spencer",
        "given": "Herbert"
      },
      {
        "family": "Reynolds",
        "given": "Linda"
      },
      {
        "family": "Coe",
        "given": "Brian"
      }
    ],
    "container-title": "Innovations in Education and Teaching International",
    "id": "Spencer1975",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1975
        ]
      ]
    },
    "keyword": "document_research, typography",
    "language": "en-US",
    "page": "95-101",
    "publisher": "Routledge",
    "title": "Spatial and typographic coding with bibliographical entries",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "DOI": "10.1007/s10579-004-8682-1",
    "ISSN": "0010-4817",
    "abstract": "To reconstruct a stemma or do any other kind of statistical analysis of a text tradition, one needs accurate data on the variants occurring at each location in each witness. These data are usually obtained from computer collation programs. Existing programs either collate every witness against a base text or divide all texts up into segments as long as the longest variant phrase at each point. These methods do not give ideal data for stemma reconstruction. We describe a better collation algorithm (progressive multiple alignment) that collates all witnesses word by word without a base text, adding groups of witnesses one at a time, starting with the most closely related pair.",
    "author": [
      {
        "family": "Spencer",
        "given": "Matthew"
      },
      {
        "family": "Howe",
        "given": "Christopher J."
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Spencer2004",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "cultural_heritage, greek, latin, ocr, spelling_correction",
    "language": "en-US",
    "page": "253-270",
    "title": "Collating texts using progressive multiple alignment",
    "type": "article-journal",
    "volume": "38"
  },
  {
    "DOI": "10.1093/llc/6.1.34",
    "ISSN": "0268-1145",
    "abstract": "This paper discusses characteristic problems in designing methods of encoding texts in machine-readable form for textual study. Any electronic representation of a text embodies specific ideas of what is important in that text. A well-developed encoding scheme is thus in some sense a theory of the texts it is intended scheme is thus in some sense a theory of the texts it is intended to mark up. The paper describes, with examples, the theory implicit in the work of the Text Encoding Initiative (TEI), a project to develop guidelines for the encoding of machine-readable texts. Any machine-readable representation of texts must use markup, but no finite vocabulary of markup items can be complete, since neither the set of textual features worth marking nor the set of texts to be studied is finite. Any useful markup scheme must therefore be extensible. Additionally, a markup scheme must allow several discrete views of texts. Texts are both linguistic and physical objects. They have simultaneously a linear, a hierarchical, and a directed-graph structure. They refer to objects in real or fictive universes Texts, finally, are cultural and thus historical objects a useful encoding scheme must be able to represent textual variation, parallel texts, and the gradual accretion of interpretation and commentary with which human culture adorns venerated texts.",
    "author": [
      {
        "family": "Sperberg-McQueen",
        "given": "C. M."
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Sperberg-McQueen1991",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "keyword": "document_research, markup, sgml, tei",
    "language": "en-US",
    "page": "34-46",
    "title": "Text in the electronic age: Textual study and text encoding, with examples from medieval texts",
    "title-short": "Text in the electronic age",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "URL": "http://cmsmcq.com/2000/mim.html",
    "author": [
      {
        "family": "Sperberg-McQueen",
        "given": "C. M."
      },
      {
        "family": "Huitfeldt",
        "given": "Claus"
      },
      {
        "family": "Renear",
        "given": "Allen H."
      }
    ],
    "container-title": "Markup Languages: Theory & Practice",
    "id": "Sperberg-McQueen2000",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "page": "215-234",
    "title": "Meaning and interpretation of markup",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "URL": "http://conferences.idealliance.org/extreme/html/2002/CMSMcQ01/EML2002CMSMcQ01.html",
    "author": [
      {
        "family": "Sperberg-McQueen",
        "given": "C. M."
      },
      {
        "family": "Dubin",
        "given": "David"
      },
      {
        "family": "Huitfeldt",
        "given": "Claus"
      },
      {
        "family": "Renear",
        "given": "Allen H."
      }
    ],
    "container-title": "Proceedings of extreme markup languages 2002",
    "editor": [
      {
        "family": "Usdin",
        "given": "B. Tommie"
      },
      {
        "family": "Newcomb",
        "given": "Steven R."
      }
    ],
    "id": "Sperberg-McQueen2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "language": "en-US",
    "page": "20",
    "title": "Drawing inferences on the basis of markup",
    "type": "paper-conference"
  },
  {
    "ISBN": "059651798X",
    "abstract": "What are the ingredients of robust, elegant, flexible, and maintainablesoftware architecture? _Beautiful Architecture_ answers this question througha collection of intriguing essays from more than a dozen of today’s leadingsoftware designers and architects. In each essay, contributors present anotable software architecture, and analyze what makes it innovative and idealfor its purpose.Some of the engineers in this book reveal how they developed a specificproject, including decisions they faced and tradeoffs they made. Others take astep back to investigate how certain architectural aspects have influencedcomputing as a whole. With this book, you’ll discover: * How Facebook’s architecture is the basis for a data-centric applicationecosystem * The effect of Xen’s well-designed architecture on the way operatingsystems evolve * How community processes within the KDE project help software architecturesevolve from rough sketches to beautiful systems * How creeping featurism has helped GNU Emacs gain unanticipatedfunctionality * The magic behind the Jikes RVM self-optimizable, self-hosting runtime * Design choices and building blocks that made Tandem the choice platform inhigh-availability environments for over two decades * Differences and similarities between object-oriented and functionalarchitectural views * How architectures can affect the software’s evolution and the developers’engagementGo behind the scenes to learn what it takes to design elegant softwarearchitecture, and how it can shape the way you approach your own projects,with _Beautiful Architecture_.",
    "author": [
      {
        "family": "Spinellis",
        "given": "Diomidis"
      },
      {
        "family": "Gousios",
        "given": "Georgios"
      }
    ],
    "id": "Spinellis2009",
    "issued": {
      "date-parts": [
        [
          2009,
          1,
          29
        ]
      ]
    },
    "keyword": "software_components",
    "language": "en-US",
    "publisher": "Paperback; O’Reilly",
    "publisher-place": "Sebastopol, CA, USA",
    "title": "Beautiful architecture: Leading thinkers reveal the hidden beauty in software design",
    "title-short": "Beautiful architecture",
    "type": "book"
  },
  {
    "URL": "http://dhdebates.gc.cuny.edu/debates/text/38",
    "abstract": "Even as the digital humanities (DH) is being hailed as the “next big thing,” members of the DH community have been debating what counts as digital humanities and what does not, who is in and who is out, and whether DH is about making or theorizing, computation or communication, practice or politics. Soon after William Pannapacker declared the arrival of digital humanities at the Modern Languages Association (MLA) conference in 2009 (Pannapacker, “The MLA and the Digital Humanities”), David Parry wrote a much-debated blog post insisting that DH should aim to “challenge and change scholarship” rather than “us[e] computers to ’tag up Milton’” (Parry). MLA 2011 unleashed another round of debates, as Pannapacker pointed to a DH in-crowd, an ironic label for a group of people who have long felt like misfits (Pannapacker, “Digital Humanities Triumphant?”).",
    "author": [
      {
        "family": "Spiro",
        "given": "Lisa"
      }
    ],
    "container-title": "Debates in the digital humanities",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      }
    ],
    "id": "Spiro2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "16-35",
    "publisher": "University of Minnesota Press",
    "publisher-place": "Minneapolis, MN, USA",
    "title": "“This is why we fight”: Defining the values of the digital humanities",
    "title-short": "“This is why we fight”",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/978-3-642-20227-8",
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "dropping-particle": "van den",
        "family": "Bosch",
        "given": "Antal"
      },
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      }
    ],
    "id": "Sporleder2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Language technology for cultural heritage: Selected papers from the LaTeCH workshop series",
    "title-short": "Language technology for cultural heritage",
    "type": "book"
  },
  {
    "DOI": "10.1007/s00287-016-1004-3",
    "abstract": "Der folgende Artikel gibt Auskunft über den in den vergangenen Jahren erzielten Durchbruch auf dem Gebiet der OCR alter Drucke auf der Basis von rekurrenten neuronalen Netzen.",
    "author": [
      {
        "family": "Springmann",
        "given": "Uwe"
      }
    ],
    "id": "Springmann2016",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "ocr",
    "language": "en-US",
    "page": "459-462",
    "publisher": "Springer Berlin Heidelberg",
    "title": "OCR für alte drucke",
    "type": "article-journal",
    "volume": "39"
  },
  {
    "ISBN": "0521663407",
    "URL": "http://www.cslu.ogi.edu/~sproatr/newindex/wsbook.pdf",
    "abstract": "Most general books on writing systems are written or edited by scholars who are specialists in a small subset of the writing systems that they cover, and who have developed their views on writing in general based on their own experience in their particular specialized area. This book is different: I cannot claim to be an expert on any particular writing system. My interest in writing systems stems in part from my interest in text-to-speech synthesis systems, and in particular the problem of converting from written text into a linguistic representation that represents how that text would be read. Given that problem, it is natural to inquire about the formal nature of the relationship between the written form, and the linguistic representation that the written form encodes: What linguistic elements do written symbols encode? Do writing systems differ in the abstractness of the linguistic representation encoded by orthography, and if so how? What are the formal constraints on the mapping between linguistic representation and writing? Some of these issues have, of course, been addressed elsewhere, though usually in an informal fashion. This book is an attempt to answer these questions in the context of a formal, computational theory of writing systems.",
    "author": [
      {
        "family": "Sproat",
        "given": "Richard"
      }
    ],
    "collection-title": "Studies in natural language processing",
    "id": "Sproat2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "computational_linguistics, orthography",
    "language": "en-US",
    "publisher": "Cambridge University Press",
    "title": "A computational theory of writing systems",
    "type": "book"
  },
  {
    "ISBN": "3-211-81106-0",
    "author": [
      {
        "family": "Stachowiak",
        "given": "Herbert"
      }
    ],
    "id": "Stachowiak1973",
    "issued": {
      "date-parts": [
        [
          1973
        ]
      ]
    },
    "keyword": "formal_models, models_in_general, classic, theory",
    "language": "de-DE",
    "publisher": "Springer",
    "publisher-place": "Wien, New York",
    "title": "Allgemeine Modelltheorie",
    "type": "book"
  },
  {
    "editor": [
      {
        "family": "Stachowiak",
        "given": "Herbert"
      }
    ],
    "id": "Stachowiak1983",
    "issued": {
      "date-parts": [
        [
          1983
        ]
      ]
    },
    "keyword": "models_in_general, theory, philosophy_of_science, formal_models",
    "language": "de-DE",
    "publisher": "Wilhelm Fink",
    "publisher-place": "München",
    "title": "Modelle – Konstruktion der Wirklichkeit",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Müller",
        "given": "Roland"
      }
    ],
    "container-title": "Modelle – Konstruktion der Wirklichkeit",
    "editor": [
      {
        "family": "Stachowiak",
        "given": "Herbert"
      }
    ],
    "id": "Mueller1983",
    "issued": {
      "date-parts": [
        [
          1983
        ]
      ]
    },
    "keyword": "models_in_general, theory, philosophy_of_science, formal_models",
    "language": "de-DE",
    "page": "17-86",
    "publisher": "Wilhelm Fink",
    "publisher-place": "München",
    "title": "Zur Geschichte des Modelldenkens und des Modellbegriffs",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/800209.806466",
    "author": [
      {
        "family": "Stallman",
        "given": "Richard M."
      }
    ],
    "container-title": "Proceedings of the ACM SIGPLAN SIGOA symposium on text manipulation",
    "id": "Stallman1981",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "147-156",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "EMACS: The extensible, customizable self-documenting display editor",
    "title-short": "EMACS",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/2063576.2063754",
    "ISBN": "978-1-4503-0717-8",
    "abstract": "In this paper a novel method for detecting plagiarized passages in document collections is presented. In contrast to previous work in this field that uses mainly content terms to represent documents, the proposed method is based on structural information provided by occurrences of a small list of stopwords (i.e., very frequent words). We show that stopword n-grams are able to capture local syntactic similarities between suspicious and original documents. Moreover, an algorithm for detecting the exact boundaries of plagiarized and source passages is proposed. Experimental results on a publicly-available corpus demonstrate that the performance of the proposed approach is competitive when compared with the best reported results. More importantly, it achieves significantly better results when dealing with difficult plagiarism cases where the plagiarized passages are highly modified by replacing most of the words or phrases with synonyms to hide the similarity with the source documents.",
    "author": [
      {
        "family": "Stamatatos",
        "given": "Efstathios"
      }
    ],
    "container-title": "CIKM ’11: Proceedings of the 20<sup>th</sup> ACM international conference on information and knowledge management",
    "id": "Stamatatos2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "document_engineering, e-learning, intertextuality, plagiarism",
    "language": "en-US",
    "page": "1221-1230",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Plagiarism detection based on structural information",
    "type": "paper-conference"
  },
  {
    "URL": "http://sagehill.net/docbookxsl/",
    "author": [
      {
        "family": "Stayton",
        "given": "Bob"
      }
    ],
    "edition": "4.",
    "id": "Stayton2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "publisher": "Sagehill Enterprises",
    "publisher-place": "Santa Cruz, CA",
    "title": "DocBook XSL: The complete guide",
    "title-short": "DocBook XSL",
    "type": "book"
  },
  {
    "URL": "http://acl.ldc.upenn.edu/W/W06/W06-3510.pdf",
    "accessed": {
      "date-parts": [
        [
          2009,
          1,
          27
        ]
      ]
    },
    "author": [
      {
        "family": "Steels",
        "given": "Luc"
      },
      {
        "family": "De Beule",
        "given": "Joachim"
      }
    ],
    "container-title": "Proceedings of the 3<sup>rd</sup> international workshop on scalable natural language understanding (ScaNaLU 2006)",
    "id": "Steels2006",
    "issued": {
      "date-parts": [
        [
          2006,
          6
        ]
      ]
    },
    "page": "73-80",
    "publisher-place": "New York, NY, USA",
    "title": "A (very) brief introduction to Fluid Construction Grammar",
    "type": "paper-conference"
  },
  {
    "ISBN": "978-1-4244-6432-6",
    "ISSN": "2157-5525",
    "URL": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5679928",
    "abstract": "Finding regularities in large data sets requires implementations of systems that are efficient in both time and space requirements. Here, we describe a newly developed system that exploits the internal structure of the enhanced suffixarray to find significant patterns in a large collection of sequences. The system searches exhaustively for all significantly compressing patterns where patterns may consist of symbols and skips or wildcards. We demonstrate a possible application of the system by detecting interesting patterns in a Dutch and an English corpus.",
    "author": [
      {
        "family": "Stehouwer",
        "given": "Herman"
      },
      {
        "dropping-particle": "van",
        "family": "Zaanen",
        "given": "Menno"
      }
    ],
    "container-title": "Proceedings of the 2010 international multiconference on computer science and information technology (IMCSIT 2010)",
    "id": "Stehouwer2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "approximate_matching",
    "language": "en-US",
    "page": "505-511",
    "publisher": "IEEE",
    "publisher-place": "New York, NY, USA",
    "title": "Finding patterns in strings using suffixarrays",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1515/9783110891751.253",
    "ISBN": "9783110891751",
    "author": [
      {
        "family": "Steiger",
        "given": "Johann A."
      }
    ],
    "collection-title": "Beihefte zu editio",
    "container-title": "Jeremias Gotthelf – Wege zu einer neuen Ausgabe",
    "editor": [
      {
        "family": "Mahlmann-Bauer",
        "given": "Barbara"
      },
      {
        "dropping-particle": "von",
        "family": "Zimmermann",
        "given": "Christian"
      }
    ],
    "id": "Steiger2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "todo",
    "language": "de-DE",
    "page": "253-280",
    "publisher": "De Gruyter",
    "publisher-place": "Berlin",
    "title": "Sprache, Heimat und Endzeit. Jeremias Gotthelfs Predigten und Erzählwerk",
    "type": "chapter",
    "volume": "24"
  },
  {
    "URL": "http://www.uni-stuttgart.de/lingrom/stein/forschung/altfranz/afrlemma.pdf",
    "abstract": "Cette contribution décrit les premières étapes vers une annotation automatique de textes d’ancien français avec information morphologique et lemmes. À cet effet, plusieurs ressources établies par d’autres chercheurs ont été réunies et fondues en un lexique de formes fléchies. Un étiqueteur probabiliste de parties du discours, ayant fait un apprentissage sur un corpus annoté, a été utilisé ensuite pour analyser un texte sans annotation.",
    "author": [
      {
        "family": "Stein",
        "given": "Achim"
      }
    ],
    "container-title": "Ancien et moyen français sur le Web: enjeux méthodologiques et analyse de discours (Actes du colloque organisé à Ottawa en octobre 2002)",
    "editor": [
      {
        "family": "Kunstmann",
        "given": "Pierre"
      },
      {
        "family": "Martineau",
        "given": "France"
      },
      {
        "family": "Forget",
        "given": "Danielle"
      }
    ],
    "id": "Stein2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "cultural_heritage, french, morphology, pos_tagging",
    "language": "fr-FR",
    "page": "273-284",
    "publisher": "David",
    "publisher-place": "Ottawa",
    "title": "Étiquetage morphologique et lemmatisation de textes d’ancien français",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Stein",
        "given": "Achim"
      }
    ],
    "collection-title": "Usage based linguistic informatics",
    "container-title": "Corpus-based perspectives in linguistics",
    "editor": [
      {
        "family": "Takagaki",
        "given": "Toshihiro"
      },
      {
        "family": "Tomimori",
        "given": "Nobuo"
      },
      {
        "family": "Tsuruga",
        "given": "Yoichiro"
      }
    ],
    "id": "Stein2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage, french, pos_tagging",
    "language": "en-US",
    "page": "217-229",
    "publisher": "John Benjamins",
    "publisher-place": "Amsterdam/Philadelphia",
    "title": "Resources and tools for Old French text corpora",
    "type": "chapter",
    "volume": "6"
  },
  {
    "author": [
      {
        "family": "Kunstmann",
        "given": "Pierre"
      },
      {
        "family": "Stein",
        "given": "Achim"
      }
    ],
    "collection-title": "Zeitschrift für französische Sprache und Literatur – Beihefte. Neue Folge (ZFSL-B)",
    "container-title": "Le Nouveau Corpus d’Amsterdam. Actes de l’atelier de Lauterbad, 23–26 février 2006",
    "id": "Stein2007a",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, french, morphology",
    "language": "fr-FR",
    "page": "9-27",
    "publisher": "Steiner",
    "publisher-place": "Stuttgart, Germany",
    "title": "Le Nouveau Corpus d’Amsterdam",
    "type": "chapter"
  },
  {
    "URL": "http://corpus.revues.org/index1510.html",
    "abstract": "In the first section, this contribution provides a survey on medieval corpora, mostly for English and French, and related annotation projects, focussing on syntactic annotation. The second section deals with specific aspects of syntactic annotation for Old French texts, and focuses on the interaction between automatic pre-annotation and tools for manual annotation (TMA).",
    "author": [
      {
        "family": "Stein",
        "given": "Achim"
      }
    ],
    "container-title": "Corpus",
    "id": "Stein2008",
    "issue": "7",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage, french, pos_tagging",
    "language": "en-US",
    "page": "157-171",
    "title": "Syntactic annotation of Old French text corpora",
    "type": "article-journal"
  },
  {
    "URL": "https://eric.ed.gov/?id=ED148298",
    "abstract": "This report is intended to serve as a resource for the development of management and instructional guidelines for computer-based education (CBE). Although the data in it were gathered from PLATO projects only, they represent projects which varied widely in target populations (elementary through professional students), subject matter content, type of implementation, and size and scope. Critical incidents are defined in terms of four criteria. Then, more than 125 case histories of critical incidents are documented. They are organized by topics which will serve as a taxonomy of matters or issues critical during project development. The report includes summaries and analyses of the processes and procedures and their subsequent effects. CBE was implemented most smoothly when there was a conscious effort to develop good relations with instructors and administrators. Successful projects were those which had initial plans for procedures, organization, objectives, and evaluation. Evaluation was essential, throughout lesson development as well as after lesson completion. Successful staff members were those whose expertise was not limited to a single area.",
    "editor": [
      {
        "family": "Steinberg",
        "given": "Esther R."
      }
    ],
    "id": "Steinberg1977",
    "issued": {
      "date-parts": [
        [
          1977,
          6
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "language": "en-US",
    "publisher": "University of Illinois",
    "publisher-place": "Urbana, IL",
    "title": "Critical incidents in the evolution of PLATO projects",
    "type": "report"
  },
  {
    "ISBN": "3436014435",
    "author": [
      {
        "family": "Steinbuch",
        "given": "Karl"
      }
    ],
    "container-title": "Informationen über Information: Probleme der Kybernetik",
    "editor": [
      {
        "dropping-particle": "von",
        "family": "Ditfurth",
        "given": "Hoimar"
      }
    ],
    "id": "Steinbuch1971b",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "keyword": "cybernetics",
    "language": "de-DE",
    "page": "55-79",
    "publisher": "Fischer Taschenbuch",
    "publisher-place": "Frankfurt am Main",
    "title": "Technische Modelle biologischer Vorgänge",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/2095536.2095562",
    "ISBN": "978-1-4503-0784-0",
    "abstract": "The MonArch Digital Archiving System is a metadata repository that can be used for the spatial capture, semantical tagging, management and storage of digital documents. It has been successfully applied to the digital documentation of built heritage and archaeological sites as well as to archiving and securing digitized archivals such as pictures and maps. In contrast to document management systems, the MonArch Digital Archiving System uses semantical technologies to improve content descriptions. Furthermore, GIS functionalities and navigation maps allow for geo-referencing documents and thus visually locating and selecting the archived content. Together, the semantical and spatial markups used throughout the archive allow for powerful queries that are fully adaptable to the needs of a wide variety of scenarios. This paper gives an overview of the ideas underneath the MonArch Digital Archiving System as well as its functions and features.",
    "author": [
      {
        "family": "Stenzer",
        "given": "Alexander"
      },
      {
        "family": "Woller",
        "given": "Claudia"
      },
      {
        "family": "Freitag",
        "given": "Burkhard"
      }
    ],
    "container-title": "Proceedings of the 13<sup>th</sup> international conference on information integration and web-based applications and services (iiWAS ’11)",
    "id": "Stenzer2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, digital_library, document_management, spatio-temporal_annotation",
    "language": "en-US",
    "page": "144-151",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "MonArch: Digital archives for cultural heritage",
    "title-short": "MonArch",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Sterne",
        "given": "Jonathan"
      }
    ],
    "container-title": "Between the humanities and the digital",
    "editor": [
      {
        "family": "Svensson",
        "given": "Patrik"
      },
      {
        "family": "Goldberg",
        "given": "David T."
      }
    ],
    "id": "Sterne2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "17-33",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "The example: Some historical considerations",
    "title-short": "The example",
    "type": "chapter"
  },
  {
    "DOI": "10.2307/3586655",
    "author": [
      {
        "family": "Stevens",
        "given": "Vance"
      }
    ],
    "container-title": "TESOL Quarterly",
    "id": "Stevens1983",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1983
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "language": "en-US",
    "page": "293-300",
    "publisher": "Teachers of English to Speakers of Other Languages, Inc. (TESOL)",
    "title": "Review: [English lessons on PLATO: Remedial grammar review]",
    "title-short": "Review",
    "type": "article-journal",
    "volume": "17"
  },
  {
    "URL": "https://eric.ed.gov/?id=ED200244",
    "abstract": "This report provides a detailed description of the architecture and programming of the PLATO V terminal, which contains an 8080 microprocessor and is capable of being operated by programs located in a host computer. The terminal contains 8k of memory for storing local programs, a 4k ROM resident program which supervises terminal operation, a 2k ROM character set and 2k of spare ROM memory.",
    "author": [
      {
        "family": "Stifle",
        "given": "Jack E."
      }
    ],
    "genre": "CERL Report",
    "id": "Stifle1978",
    "issued": {
      "date-parts": [
        [
          1978,
          4
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "language": "en-US",
    "number": "X-50",
    "publisher": "University of Illinois",
    "publisher-place": "Urbana, IL",
    "title": "The PLATO V terminal",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Штофф",
        "given": "Виктор Александрович"
      }
    ],
    "id": "Stoff1966",
    "issued": {
      "date-parts": [
        [
          1966
        ]
      ]
    },
    "language": "ru-RU",
    "publisher": "Наука",
    "title": "Моделирвание и Философия",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Štoff",
        "given": "Viktor Aleksandrovič"
      }
    ],
    "id": "Stoff1969",
    "issued": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Akademie-Verlag",
    "title": "Modellierung und Philosophie",
    "translator": [
      {
        "family": "Wollgast",
        "given": "Siegfried"
      }
    ],
    "type": "book"
  },
  {
    "URL": "http://www.dh2012.uni-hamburg.de/conference/programme/abstracts/modeling-medieval-handwriting-a-new-approach-to-digital-palaeography/",
    "abstract": "As many hundreds of thousands of medieval manuscripts are now being digitised, with many millions of pages becoming available, the question of how to find specialised content in this material is becoming increasingly urgent. In this paper I present a new conceptual model for the description and therefore retrieval of features of handwriting in Western medieval script. Digital Humanities requires first a theoretical model which outlines all of the features of a given domain and the relationships between them (McCarty 2004), and this is the focus of the present paper. However, the implications of this work are very much wider: just as the TEI has lead to ’a new data description language that substantially improves our ability to describe textual features’ (Renear 2004: 235), so the formal model of handwriting presented here sharpens and could even resolve long-standing problems in palaeographers’ own terminology and practice.",
    "author": [
      {
        "family": "Stokes",
        "given": "Peter"
      }
    ],
    "container-title": "Digital humanities 2012 conference abstracts",
    "editor": [
      {
        "family": "Meister",
        "given": "Jan C."
      }
    ],
    "id": "Stokes2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities, tei",
    "language": "en-US",
    "page": "382-385",
    "publisher": "University of Hamburg; Hamburg University Press",
    "publisher-place": "Hamburg",
    "title": "Modeling medieval handwriting: A new approach to digital palaeography",
    "title-short": "Modeling medieval handwriting",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1860559.1860606",
    "ISBN": "978-1-4503-0231-9",
    "abstract": "This paper is about off-line handwritten text comparison of historic documents. The long-term motivation is the support of palaeographic research, in particular to back up decisions as to whether two handwritings can be ascribed to the same author. In this paper, a first fundamental step is presented for extracting relevant structures from handwritten texts. Such structures are represented by skeletons, due to their resemblance to original writing movements. Core result is an approach to the simplification of skeleton structures. While skeletons represent constitutive structures for a wide variety of subsequent algorithms, simplification algorithms usually focus on pruning branches off the skeleton instead of simplifying the skeleton as a whole. By contrast, our approach reduces the amount of elements in a skeleton based on a global error level, reducing the skeleton’s complexity while keeping its structure as close to the original exemplar as possible. The results are much easier to analyse while relevant information is maintained.",
    "author": [
      {
        "family": "Stoppe",
        "given": "Jannis"
      },
      {
        "family": "Gottfried",
        "given": "Björn"
      }
    ],
    "collection-title": "DocEng ’10",
    "container-title": "Proceedings of the 10th ACM symposium on document engineering",
    "id": "Stoppe2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "language": "en-US",
    "page": "215-218",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Down to the bone: Simplifying skeletons",
    "title-short": "Down to the bone",
    "type": "paper-conference"
  },
  {
    "collection-title": "Text, translation, computational processing",
    "editor": [
      {
        "family": "Storrer",
        "given": "Angelika"
      },
      {
        "family": "Geyken",
        "given": "Alexander"
      },
      {
        "family": "Siebert",
        "given": "Alexander"
      },
      {
        "family": "Würzner",
        "given": "Kay-Michael"
      }
    ],
    "id": "Storrer2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "publisher": "Mouton de Gruyter",
    "publisher-place": "Berlin/New York",
    "title": "Text resources and lexical knowledge",
    "type": "book",
    "volume": "8"
  },
  {
    "URL": "http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1590",
    "abstract": "We describe a generic framework for representing and reasoning with annotated Semantic Web data, formalise the annotated language, the corresponding deductive system, and address the query answering problem. We extend previous contributions on RDF annotations by providing a unified reasoning formalism and allowing the seamless combination of different annotation domains. We demonstrate the feasibility of our method by instantiating it on (i) temporal RDF; (ii) fuzzy RDF; (iii) and their combination. A prototype shows that implementing and combining new domains is easy and that RDF stores can easily be extended to our framework.",
    "author": [
      {
        "family": "Straccia",
        "given": "Umberto"
      },
      {
        "family": "Lopes",
        "given": "Nuno"
      },
      {
        "family": "Lukacsy",
        "given": "Gergely"
      },
      {
        "family": "Polleres",
        "given": "Axel"
      }
    ],
    "container-title": "Proceedings of the twenty-fourth AAAI conference on artificial intelligence",
    "id": "Straccia2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "rdf, semantic_web, temporal_data",
    "language": "en-US",
    "publisher": "AAAI",
    "publisher-place": "Palo Alto, CA, USA",
    "title": "A general framework for representing and reasoning with annotated Semantic Web data",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Stremlow",
        "given": "Matthias"
      }
    ],
    "id": "Stremlow1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Haupt",
    "publisher-place": "Berne",
    "title": "Die Alpen aus der Untersicht – von der Verheissung der nahen Fremde zur Sportarena",
    "type": "book"
  },
  {
    "DOI": "10.1007/11788713_81",
    "abstract": "The IMS Global Learning Consortium developed the QTI (Question and Test Interoperability) specification to allow the exchange of question and test data, and their corresponding result reports, between learning systems. QTI 1.2 had some accessibility issues, as the VISUAL project discovered when transforming QTI tests into accessible HTML and voice user interfaces. Some problems were due to an insufficient mechanism to specify alternative text, other problems were due to the ambiguity of the intent of certain interaction types. QTI 2.0 solved the issue of alternative text, but the ambiguity with regard to the intent of interaction types was not sufficiently addressed.",
    "author": [
      {
        "family": "Strobbe",
        "given": "Christophe"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Computers helping people with special needs. 10<sup>th</sup> international conference, ICCHP 2006, linz, austria, july 11–13, 2006. proceedings",
    "id": "Strobbe2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "page": "544-551",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Accessibility issues in two specifications for e-learning tests: IMS QTI 1.2 and IMS QTI 2.0.",
    "title-short": "Accessibility issues in two specifications for e-learning tests",
    "type": "paper-conference",
    "volume": "4061"
  },
  {
    "DOI": "10.1145/2232817.2232859",
    "ISBN": "978-1-4503-1154-0",
    "abstract": "Textual data ranging from corpora of digitized historic documents to large collections of news feeds provide a rich source for temporal and geographic information. Such types of information have recently gained a lot of interest in support of different search and exploration tasks, e.g., by organizing news along a timeline or placing the origin of documents on a map. However, for this, temporal and geographic information embedded in documents is often considered in isolation. We claim that through combining such information into (chronologically ordered) event-like features interesting and meaningful search and exploration tasks are possible. In this paper, we present a framework for the extraction, exploration, and visualization of event information in document collections. For this, one has to identify and combine temporal and geographic expressions from documents, thus enriching a document collection by a set of normalized events. Traditional search queries then can be enriched by conditions on the events relevant to the search subject. Most important for our event-centric approach is that a search result consists of a sequence of events relevant to the search terms and not just a document hit-list. Such events can originate from different documents and can be further explored, in particular events relevant to a search query can be ordered chronologically. We demonstrate the utility of our framework by different (multilingual) search and exploration scenarios using a Wikipedia corpus.",
    "author": [
      {
        "family": "Strötgen",
        "given": "Jannik"
      },
      {
        "family": "Gertz",
        "given": "Michael"
      }
    ],
    "container-title": "Proceedings of the 12th ACM/IEEE-CS joint conference on digital libraries (JCDL ’12)",
    "id": "Stroetgen2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "cultural_heritage, spatio-temporal_annotation, wikipedia",
    "language": "en-US",
    "page": "223-232",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Event-centric search and exploration in document collections",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.linguistics.ruhr-uni-bochum.de/\\~{}strunk/LSreport.pdf",
    "abstract": "In this paper, I describe and compare different solutions to the problem of information retrieval for languages that lack a fixed orthography. For these languages, even simple Boolean queries are a problem, because the person writing the query might use a different orthographic system from the people whose documents have been indexed. Moreover, if the aim is to find all documents containing a certain word, this is very difficult or nearly impossible for such languages because of the sheer variety of different but equivalent orthographic forms. The goal of the project described in this paper was to develop and evaluate different approximate matching algorithms in order to achieve a better retrieval performance for such languages. I compare different variants of edit distance algorithms and briefly discuss other possible solutions.",
    "author": [
      {
        "family": "Strunk",
        "given": "Jan"
      }
    ],
    "genre": "Seminar paper",
    "id": "Strunk2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "approximate_matching, cultural_heritage, ir, spelling_normalization",
    "language": "en-US",
    "publisher": "Stanford University",
    "publisher-place": "Stanford, CA, USA",
    "title": "Information retrieval for languages that lack a fixed orthography",
    "type": "thesis"
  },
  {
    "URL": "http://www.lsureveille.com/news/1.762154",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Stuart",
        "given": "Emily"
      }
    ],
    "container-title": "Daily Reveille",
    "id": "Stuart2008",
    "issue": "127",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "title": "University to implement Moodle by end of year",
    "type": "article-journal",
    "volume": "112"
  },
  {
    "DOI": "10.1007/s10032-007-0060-2",
    "ISSN": "1433-2833",
    "abstract": "Given a specific information need, documents of the wrong genre can be considered as noise. From this perspective, genre classification helps to separate relevant documents from noise. Orthographic errors represent a second, finer notion of noise. Since specific genres often include documents with many errors, an interesting question is whether this ” micro-noise” can help to classify genre. In this paper we consider both problems. After introducing a comprehensive hierarchy of genres, we present an intuitive method to build specialized and distinctive classifiers that also work for very small training corpora. Special emphasis is given to the selection of intelligent high-level features. We then investigate the correlation between genre and micro noise. Using special error dictionaries, we estimate the typical error rates for each genre. Finally, we test if the error rate of a document represents a useful feature for genre classification.",
    "author": [
      {
        "family": "Stubbe",
        "given": "Andrea"
      },
      {
        "family": "Ringlstetter",
        "given": "Christoph"
      },
      {
        "family": "Schulz",
        "given": "Klaus U."
      }
    ],
    "container-title": "International Journal on Document Analysis and Recognition",
    "id": "Stubbe2007",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "noisy_text",
    "language": "en-US",
    "page": "199-209",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Genre as noise: Noise in genre",
    "title-short": "Genre as noise",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "DOI": "10.12775/llp.2011.019",
    "ISSN": "1425-3305",
    "abstract": "Internalization is a key property of justification logics. It states that justification logics internalize their own notion of proof which is essential for the proof of the realization theorem. The aim of this note is to show how to make use of internalization to track where an agent’s knowledge comes from and how to apply this to the problem of data privacy.",
    "author": [
      {
        "family": "Studer",
        "given": "Thomas"
      }
    ],
    "container-title": "Logic and Logical Philosophy",
    "id": "Studer2011",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "logic, uncertainty",
    "language": "en-US",
    "page": "297-306",
    "title": "Justification logic, inference tracking, and data privacy",
    "type": "article-journal",
    "volume": "20"
  },
  {
    "DOI": "10.1007/978-3-642-29709-0_30",
    "abstract": "Justification logics are epistemic logics that include explicit justifications for an agent’s knowledge. In the present paper, we introduce a justification logic \\mathcal{JALC} over the description logic \\mathcal{ALC} . We provide a deductive system and a semantics for our logic and we establish soundness and completeness results. Moreover, we show that our logic satisfies the so-called internalization property stating that it internalizes its own notion of proof. We then sketch two applications of \\mathcal{JALC} : (i) the justification terms can be used to generate natural language explanations why an \\mathcal{ALC} statement holds and (ii) the terms can be used to study data privacy issues for description logic knowledge bases.",
    "author": [
      {
        "family": "Studer",
        "given": "Thomas"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Perspectives of systems informatics",
    "editor": [
      {
        "family": "Clarke",
        "given": "Edmund"
      },
      {
        "family": "Virbitskaite",
        "given": "Irina"
      },
      {
        "family": "Voronkov",
        "given": "Andrei"
      }
    ],
    "id": "Studer2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "knowledge_representation, uncertainty",
    "language": "en-US",
    "page": "349-361",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Justified terminological reasoning",
    "type": "paper-conference",
    "volume": "7162"
  },
  {
    "DOI": "10.1145/1568296.1568315",
    "ISBN": "978-1-60558-496-6",
    "abstract": "Often, in the real world noise is ubiquitous in text communications. Text produced by processing signals intended for human use are often noisy for automated computer processing. Automatic speech recognition, optical character recognition and machine translation all introduce processing noise. Also digital text produced in informal settings such as online chat, SMS, emails, message boards, newsgroups, blogs, wikis and web pages contain considerable noise. In this paper, we present a survey of the existing measures for noise in text. We also cover application areas that ingest this noisy text for various tasks like Information Retrieval and Information Extraction.",
    "author": [
      {
        "family": "Subramaniam",
        "given": "L. Venkata"
      },
      {
        "family": "Roy",
        "given": "Shourya"
      },
      {
        "family": "Faruquie",
        "given": "Tanveer A."
      },
      {
        "family": "Negi",
        "given": "Sumit"
      }
    ],
    "container-title": "Proceedings of the third workshop on analytics for noisy unstructured text data (AND ’09)",
    "id": "Subramaniam2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, ir, spelling_correction, spelling_normalization",
    "language": "en-US",
    "page": "115-122",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A survey of types of text noise and techniques to handle noisy text",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Succi",
        "given": "Chiara"
      },
      {
        "family": "Cantoni",
        "given": "Lorenzo"
      }
    ],
    "container-title": "Proceedings of world conference on educational multimedia, hypermedia and telecommunications 2006",
    "id": "Succi2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "page": "912-919",
    "publisher": "AACE",
    "publisher-place": "Chesapeake, VA",
    "title": "Looking for a comprehensive eLearning acceptance framework: A literature review and a tentative map",
    "title-short": "Looking for a comprehensive eLearning acceptance framework",
    "type": "paper-conference"
  },
  {
    "DOI": "10.2200/s00362ed1v01y201105dtm016",
    "ISSN": "2153-5418",
    "abstract": "Probabilistic databases are databases where the value of some attributes or the presence of some records are uncertain and known only with some probability. Applications in many areas such as information extraction, RFID and scientific data management, data cleaning, data integration, and financial risk assessment produce large volumes of uncertain data, which are best modeled and processed by a probabilistic database. This book presents the state of the art in representation formalisms and query processing techniques for probabilistic data. It starts by discussing the basic principles for representing large probabilistic databases, by decomposing them into tuple-independent tables, block-independent-disjoint tables, or U-databases. Then it discusses two classes of techniques for query evaluation on probabilistic databases. In extensional query evaluation, the entire probabilistic inference can be pushed into the database engine and, therefore, processed as effectively as the evaluation of standard SQL queries. The relational queries that can be evaluated this way are called safe queries. In intensional query evaluation, the probabilistic inference is performed over a propositional formula called lineage expression: every relational query can be evaluated this way, but the data complexity dramatically depends on the query being evaluated, and can be #P-hard. The book also discusses some advanced topics in probabilistic data management such as top-k query processing, sequential probabilistic databases, indexing and materialized views, and Monte Carlo databases.",
    "author": [
      {
        "family": "Suciu",
        "given": "Dan"
      },
      {
        "family": "Olteanu",
        "given": "Dan"
      },
      {
        "family": "Ré",
        "given": "Christopher"
      },
      {
        "family": "Koch",
        "given": "Christoph"
      }
    ],
    "collection-title": "Synthesis lectures on data management",
    "id": "Suciu2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "database, uncertainty",
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "publisher-place": "San Rafael, CA, USA",
    "title": "Probabilistic databases",
    "type": "book",
    "volume": "16"
  },
  {
    "DOI": "10.1007/s10032-007-0053-1",
    "ISSN": "1433-2833",
    "abstract": "Handwritten Chinese character recognition is difficult due to the unstructured and noisy nature of its training examples. There are often too few training examples for a statistical learner like SVM to overcome the noise and extract useful information reliably. Existing prior domain knowledge represents a valuable source of information for classifying handwritten characters. Explanation-based learning (EBL) provides a way to incorporating prior domain knowledge into the learner. The dynamic bias formed by the interaction of domain knowledge with training examples can yield solution knowledge of potential higher quality. Two EBL approaches, one that uses a special feature kernel function in SVM, the other uses a conventional kernel for the SVM but provides additional preference in choosing the classification hyperplane, are reported.",
    "author": [
      {
        "family": "Sun",
        "given": "Qiang"
      },
      {
        "family": "Wang",
        "given": "Li-Lun"
      },
      {
        "family": "Lim",
        "given": "Shiau H."
      },
      {
        "family": "DeJong",
        "given": "Gerald"
      }
    ],
    "container-title": "International Journal on Document Analysis and Recognition",
    "id": "Sun2007",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "handwriting_recognition, noisy_text",
    "language": "en-US",
    "page": "175-186",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Robustness through prior knowledge: Using explanation-based learning to distinguish handwritten Chinese characters",
    "title-short": "Robustness through prior knowledge",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "DOI": "10.1007/978-94-010-3667-2_16",
    "ISBN": "978-94-010-3669-6",
    "annote": "The contributions of this volume first appeared as special issue of Synthese (Volume 12, Issue 2-3, September 1960) https://link.springer.com/journal/11229/12/2/page/1",
    "author": [
      {
        "family": "Suppes",
        "given": "Patrick"
      }
    ],
    "chapter-number": "16",
    "container-title": "The concept and the role of the model in mathematics and natural and social sciences",
    "editor": [
      {
        "family": "Freudenthal",
        "given": "Hans"
      }
    ],
    "id": "Suppes1961",
    "issued": {
      "date-parts": [
        [
          1961
        ]
      ]
    },
    "keyword": "formal_models, models_in_general",
    "language": "en-US",
    "page": "163-177",
    "publisher": "D. Reidel",
    "publisher-place": "Dordrecht",
    "title": "A comparison of the meaning and uses of models in mathematics and the empirical sciences",
    "type": "paper-conference"
  },
  {
    "URL": "http://zope.org/Members/jwashin/Survey/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "id": "Survey",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "title": "ZClasses Survey/Quiz product for Zope",
    "type": ""
  },
  {
    "URL": "http://www.ischool.washington.edu/sasutton/IEEE1484.html",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Sutton",
        "given": "Stuart A."
      }
    ],
    "genre": "Learning Object Metadata: Draft Document",
    "id": "Sutton1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "number": "v3.6",
    "publisher": "IEEE Learning Technology Standards Committee (LTSC)",
    "title": "IEEE 1484 LOM mappings to Dublin Core",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Svensson",
        "given": "Patrik"
      }
    ],
    "chapter-number": "7",
    "container-title": "Defining digital humanities",
    "editor": [
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Vanhoutte",
        "given": "Edward"
      }
    ],
    "id": "Svensson2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "en-US",
    "page": "159-186",
    "publisher": "Ashgate",
    "publisher-place": "Farnham",
    "title": "Humanities computing as digital humanities",
    "type": "chapter"
  },
  {
    "abstract": "Between Humanities and the Digital offers an expansive vision of how the humanities engage with digital and information technology, providing a range of perspectives on a quickly evolving, contested, and exciting field. It documents the multiplicity of ways that humanities scholars have turned increasingly to digital and information technology as both a scholarly tool and a cultural object in need of analysis. The contributors explore the state of the art in digital humanities from varied disciplinary perspectives, offer a sample of digitally inflected work that ranges from an analysis of computational literature to the collaborative development of a “Global Middle Ages” humanities platform, and examine new models for knowledge production and infrastructure. Their contributions show not only that the digital has prompted the humanities to move beyond traditional scholarly horizons, but also that the humanities have pushed the digital to become more than a narrowly technical application.",
    "editor": [
      {
        "family": "Svensson",
        "given": "Patrik"
      },
      {
        "family": "Goldberg",
        "given": "David T."
      }
    ],
    "id": "Svensson2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Between humanities and the digital",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Szyperski",
        "given": "Clemens"
      },
      {
        "family": "Gruntz",
        "given": "Dominik"
      },
      {
        "family": "Murer",
        "given": "Stephan"
      }
    ],
    "id": "Szyperski2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "software_components",
    "language": "en-US",
    "publisher": "Addison-Wesley",
    "publisher-place": "Harlow, UK",
    "title": "Component software: Beyond object-oriented programming",
    "title-short": "Component software",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Dervaric",
        "given": "Christian"
      }
    ],
    "genre": "Diplom\\-arbeit",
    "id": "T:Dervaric2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "language": "de-DE",
    "note": "Advisor: Prof. Dr. Dietmar Rösner",
    "publisher": "Fakultät für Informatik, Otto-von-Guericke-Universität",
    "publisher-place": "Magdeburg",
    "title": "Erkennung und Behandlung von Plagiaten bei Lösungen zu Übungsaufgaben",
    "type": "thesis"
  },
  {
    "author": [
      {
        "family": "Feustel",
        "given": "Thomas"
      }
    ],
    "genre": "Master's Thesis",
    "id": "T:Feustel2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "language": "de-DE",
    "note": "Advisor: Prof. Dr. Dietmar Rösner",
    "publisher": "Fakultät für Informatik, Otto-von-Guericke-Universität",
    "publisher-place": "Magdeburg",
    "title": "Analyse von Texteingaben in einem CAA-Werkzeug zur elektronischen Einreichung und Auswertung von Aufgaben",
    "type": "thesis"
  },
  {
    "URL": "http://www.tei-c.org/Guidelines/P5/",
    "editor": [
      {
        "literal": "TEI Consortium"
      }
    ],
    "id": "TEIP5",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "markup, tei, xml",
    "language": "en-US",
    "publisher": "TEI Consortium",
    "publisher-place": "Charlottesville, VA, USA",
    "title": "TEI P5: Guidelines for electronic text encoding and interchange",
    "title-short": "TEI P5",
    "type": "book"
  },
  {
    "URL": "https://www.getty.edu/research/tools/vocabularies/guidelines/\\#tgn",
    "accessed": {
      "date-parts": [
        [
          2014,
          6,
          4
        ]
      ]
    },
    "id": "TGNGuidelines",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "publisher": "J. Paul Getty Trust",
    "publisher-place": "Los Angeles, CA, USA",
    "title": "Getty thesaurus of geographic names: Editorial guidelines",
    "title-short": "Getty thesaurus of geographic names",
    "type": "book"
  },
  {
    "URL": "http://www.tustep.uni-tuebingen.de/pdf/handbuch.pdf",
    "abstract": "Das »Tübinger System von Textverarbeitungs-Programmen« TUSTEP wurde am Zentrum für Datenverarbeitung der Universität Tübingen in der Abteilung »Literarische und Dokumentarische Datenverarbeitung« mit dem Ziel entwickelt, ein leistungsfähiges Werkzeug zum wissenschaftlichen Umgang mit Textdaten zur Verfügung zu stellen.",
    "id": "TUSTEP2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "cultural_heritage, typesetting",
    "language": "de-DE",
    "publisher": "Universität Tübingen, Zentrum für Datenverarbeitung",
    "publisher-place": "Tübingen, Germany",
    "title": "TUSTEP: Handbuch und Referenz",
    "title-short": "TUSTEP",
    "type": "book"
  },
  {
    "ISBN": "902723230X",
    "abstract": "Middle English Medical Texts (MEMT) is an electronic corpus including 86 texts and 495,322 words from three traditions of medical writing (surgical treatises, specialized texts, and remedy books) from 1375 to 1500, and an appendix of recipes from c. 1330. MEMT provides a new research resource for several kinds of study. Medical writing shows a great deal of variation even at this early phase, and it is possible to trace linguistic and conceptual developments. MEMT is intended for historical linguists, philologists, and historians of medicine. Most text samples come from edited treatises, but new material (unpublished theses, new transcripts) is also offered. The text samples of the corpus cover 79 codices/manuscripts.",
    "author": [
      {
        "family": "Taavitsainen",
        "given": "Irma"
      },
      {
        "family": "Pahta",
        "given": "Päivi"
      },
      {
        "family": "Mäkinen",
        "given": "Martti"
      }
    ],
    "id": "Taavitsainen2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, english",
    "language": "en-US",
    "publisher": "John Benjamins",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "Middle english medical texts",
    "type": "book"
  },
  {
    "ISBN": "9027211779",
    "abstract": "The corpus Early Modern English Medical Texts (EMEMT) is the second component of the Corpus of Early English Medical Writing (CEEM), a three-part series of historical corpora of medical writing from 1375-1800. EMEMT contains a two-million word representative sample of the entire field of English medical writings that appeared in print between 1500 and 1700, and provides continuity to Middle English Medical Texts (MEMT), published on CD-ROM by John Benjamins in 2005. The EMEMT corpus includes c. 230 texts, ranging from theoretical treatises rooted in academic traditions of medicine to popularized and utilitarian texts verging on household literature. The texts are grouped into six text categories that facilitate systematic research into the history of medical writing in its disciplinary context: general treatises and textbooks; treatises on specific topics; recipe collections and materia medica ; regimen and health guides; surgical treatises; and samples of the first scientific journal, the Philosophical Transactions. EMEMT is released on CD-Rom with EMEMT Presenter, purpose-designed software by Raymond Hickey.",
    "editor": [
      {
        "family": "Taavitsainen",
        "given": "Irma"
      },
      {
        "family": "Pahta",
        "given": "Päivi"
      }
    ],
    "id": "Taavitsainen2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, english",
    "language": "en-US",
    "publisher": "John Benjamins",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "Early modern english medical texts: Corpus description and studies",
    "title-short": "Early modern english medical texts",
    "type": "book"
  },
  {
    "DOI": "10.1016/j.csi.2011.07.001",
    "ISSN": "09205489",
    "abstract": "This study presents an analysis of users’ queries directed at different search engines to investigate trends and suggest better search engine capabilities. The query distribution among search engines that includes spawning of queries, number of terms per query and query lengths is discussed to highlight the principal factors affecting a user’s choice of search engines and evaluate the reasons of varying the length of queries. The results could be used to develop long to short term business plans for search engine service providers to determine whether or not to opt for more focused topic specific search offerings to gain better market share.",
    "author": [
      {
        "family": "Taghavi",
        "given": "Mona"
      },
      {
        "family": "Patel",
        "given": "Ahmed"
      },
      {
        "family": "Schmidt",
        "given": "Nikita"
      },
      {
        "family": "Wills",
        "given": "Christopher"
      },
      {
        "family": "Tew",
        "given": "Yiqi"
      }
    ],
    "container-title": "Computer Standards & Interfaces",
    "id": "Taghavi2012",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "ir, web_search",
    "language": "en-US",
    "page": "162-170",
    "title": "An analysis of Web proxy logs with query distribution pattern approach for search engines",
    "type": "article-journal",
    "volume": "34"
  },
  {
    "URL": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.81.8901",
    "abstract": "In many of our OCR/IR experiments, we found that “garbage strings” generated by OCR devices caused complications for information retrieval systems. In particular, index sizes more than doubled, formulas used to determine statistical evidence were distorted, and document ranking was affected. To help correct these problems, we implemented a set of rules to automatically “clean up” the OCR text prior to indexing. We have applied this system, called rmgarbage, in several of our experiments and it is currently being applied by the Department of Energy (DOE) to their large scale Licensing Support Network (LSN) which consists primarily of OCR text.",
    "author": [
      {
        "family": "Taghva",
        "given": "Kazem"
      },
      {
        "family": "Nartker",
        "given": "Tom"
      },
      {
        "family": "Condit",
        "given": "Allen"
      },
      {
        "family": "Borsack",
        "given": "Julie"
      }
    ],
    "container-title": "Proceedings of the 5<sup>th</sup> world multi-conference on systemics, cybernetics and informatics",
    "id": "Taghva2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "ocr, spelling_correction",
    "language": "en-US",
    "title": "Automatic removal of “garbage strings” in OCR text: An implementation",
    "title-short": "Automatic removal of “garbage strings” in OCR text",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/s10032-007-0056-y",
    "ISSN": "1433-2833",
    "abstract": "This paper presents a technique for adding sentence boundaries to text obtained by Automatic Speech Recognition (ASR) of conversational speech audio. We show that starting with imprecise boundary information, added using only silence information from an ASR system, we can improve boundary detection using Head and Tail phrases. We develop our technique and show its effectiveness on two manually transcribed and one automatically transcribed corpus. The main purpose of adding sentence boundaries to ASR transcripts is to improve linguistic analysis, namely information extraction, for text mining systems that handle huge volumes of textual data and analyze trends and features of the concepts. Hence, we also show how the addition of boundaries improves two basic natural language processing tasks—PoS label assignment and adjective-noun extraction.",
    "author": [
      {
        "family": "Takeuchi",
        "given": "Hironori"
      },
      {
        "family": "Subramaniam",
        "given": "L. Venkata"
      },
      {
        "family": "Roy",
        "given": "Shourya"
      },
      {
        "family": "Punjani",
        "given": "Diwakar"
      },
      {
        "family": "Nasukawa",
        "given": "Tetsuya"
      }
    ],
    "container-title": "International Journal on Document Analysis and Recognition",
    "id": "Takeuchi2007",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "noisy_text",
    "language": "en-US",
    "page": "147-155",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Sentence boundary detection in conversational speech transcripts using noisily labeled examples",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "DOI": "10.1007/978-3-642-02121-3_25",
    "ISBN": "978-3-642-02120-6",
    "abstract": "Many applications operate on time- sensitive data. Some of these data are only valid for certain intervals (e.g., job-assignments, versions of software code), others describe temporal events that happened at certain points in time (e.g., a person’s birthday). Until recently, the only way to incorporate time into Semantic Web models was as a data type property. Temporal RDF, however, considers time as an additional dimension in data preserving the semantics of time. In this paper we present a syntax and storage format based on named graphs to express temporal RDF. Given the restriction to preexisting RDF-syntax, our approach can perform any temporal query using standard SPARQL syntax only. For convenience, we introduce a shorthand format called τ-SPARQL for temporal queries and show how τ-SPARQL queries can be translated to standard SPARQL. Additionally, we show that, depending on the underlying data’s nature, the temporal RDF approach vastly reduces the number of triples by eliminating redundancies resulting in an increased performance for processing and querying. Last but not least, we introduce a new indexing approach method that can significantly reduce the time needed to execute time point queries (e.g., what happened on January 1st).",
    "author": [
      {
        "family": "Tappolet",
        "given": "Jonas"
      },
      {
        "family": "Bernstein",
        "given": "Abraham"
      }
    ],
    "chapter-number": "25",
    "collection-title": "Lecture notes in computer science",
    "container-title": "The semantic web: Research and applications, 6th european semantic web conference, ESWC 2009, heraklion, crete, greece, may 31-june 4, 2009, proceedings",
    "editor": [
      {
        "family": "Aroyo",
        "given": "Lora"
      },
      {
        "family": "Traverso",
        "given": "Paolo"
      },
      {
        "family": "Ciravegna",
        "given": "Fabio"
      },
      {
        "family": "Cimiano",
        "given": "Philipp"
      },
      {
        "family": "Heath",
        "given": "Tom"
      },
      {
        "family": "Hyvönen",
        "given": "Eero"
      },
      {
        "family": "Mizoguchi",
        "given": "Riichiro"
      },
      {
        "family": "Oren",
        "given": "Eyal"
      },
      {
        "family": "Sabou",
        "given": "Marta"
      },
      {
        "family": "Simperl",
        "given": "Elena Paslaru Bontas"
      },
      {
        "family": "Aroyo",
        "given": "Lora"
      },
      {
        "family": "Traverso",
        "given": "Paolo"
      },
      {
        "family": "Ciravegna",
        "given": "Fabio"
      },
      {
        "family": "Cimiano",
        "given": "Philipp"
      },
      {
        "family": "Heath",
        "given": "Tom"
      },
      {
        "family": "Hyvönen",
        "given": "Eero"
      },
      {
        "family": "Mizoguchi",
        "given": "Riichiro"
      },
      {
        "family": "Oren",
        "given": "Eyal"
      },
      {
        "family": "Sabou",
        "given": "Marta"
      },
      {
        "family": "Simperl",
        "given": "Elena Paslaru Bontas"
      }
    ],
    "id": "Tappolet2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, rdf, semantic_web",
    "language": "en-US",
    "page": "308-322",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Applied temporal RDF: Efficient temporal querying of RDF data with SPARQL",
    "title-short": "Applied temporal RDF",
    "type": "paper-conference",
    "volume": "5554"
  },
  {
    "DOI": "10.1093/llc/fqr015",
    "author": [
      {
        "family": "Tarte",
        "given": "Ségolène M."
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Tarte2011",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "digital_humanities, uncertainty",
    "language": "en-US",
    "page": "349-358",
    "title": "Digitizing the act of papyrological interpretation: Negotiating spurious exactitude and genuine uncertainty",
    "title-short": "Digitizing the act of papyrological interpretation",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "DOI": "10.1145/358746.358755",
    "abstract": "Programs are not text; they are hierarchical compositions of computational structures and should be edited, executed, and debugged in an environment that consistently acknowledges and reinforces this viewpoint. The Cornell Program Synthesizer demands a structural perspective at all stages of program development. Its separate features are unified by a common foundation: a grammar for the programming language. Its full-screen derivation-tree editor and syntax-directed diagnostic interpreter combine to make the Synthesizer a powerful and responsive interactive programming tool.",
    "author": [
      {
        "family": "Teitelbaum",
        "given": "Tim"
      },
      {
        "family": "Reps",
        "given": "Thomas"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Teitelbaum1981",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          1981,
          9
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "563-573",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The Cornell Program Synthesizer: A syntax-directed programming environment",
    "title-short": "The Cornell Program Synthesizer",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "DOI": "10.1145/800209.806448",
    "ISBN": "0-89791-050-8",
    "abstract": "The Cornell Program Synthesizer is a syntax-directed programming environment that has been used in introductory programming courses since June, 1979. We present our experience with the Synthesizer by introducing its main features, by presenting our basic principles of design, and by discussing important design decisions.",
    "author": [
      {
        "family": "Teitelbaum",
        "given": "Tim"
      },
      {
        "family": "Reps",
        "given": "Thomas"
      },
      {
        "family": "Horwitz",
        "given": "Susan"
      }
    ],
    "container-title": "Proceedings of the ACM SIGPLAN SIGOA symposium on text manipulation",
    "id": "Teitelbaum1981b",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "8-16",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The why and wherefore of the Cornell Program Synthesizer",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1463434.1463458",
    "abstract": "News articles contain a wealth of implicit geographic content that if exposed to readers improves understanding of today’s news. However, most articles are not explicitly geotagged with their geographic content, and few news aggregation systems expose this content to users. A new system named NewsStand is presented that collects, analyzes, and displays news stories in a map interface, thus leveraging on their implicit geographic content. NewsStand monitors RSS feeds from thousands of online news sources and retrieves articles within minutes of publication. It then extracts geographic content from articles using a custom-built geotagger, and groups articles into story clusters using a fast online clustering algorithm. By panning and zooming in NewsStand’s map interface, users can retrieve stories based on both topical significance and geographic region, and see substantially different stories depending on position and zoom level.",
    "author": [
      {
        "family": "Teitler",
        "given": "Benjamin E."
      },
      {
        "family": "Lieberman",
        "given": "Michael D."
      },
      {
        "family": "Panozzo",
        "given": "Daniele"
      },
      {
        "family": "Sankaranarayanan",
        "given": "Jagan"
      },
      {
        "family": "Samet",
        "given": "Hanan"
      },
      {
        "family": "Sperling",
        "given": "Jon"
      }
    ],
    "container-title": "GIS ’08: Proceedings of the 16th ACM SIGSPATIAL international conference on advances in geographic information systems",
    "id": "Teitler2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "page": "1-10",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "NewsStand: A new view on news",
    "title-short": "NewsStand",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.sfs.uni-tuebingen.de/resources/tuebadz-sty-2009.pdf",
    "abstract": "This stylebook is an updated version of Telljohann et al. (2006). It describes the design principles and the annotation scheme for the German treebank TüBa-D/Z developed by the Division of Computational Linguistics (Lehrstuhl Prof. Hinrichs) at the Department of Linguistics (Seminar für Sprachwissenschaft – SfS) of the Eberhard Karls Universität Tübingen, Germany. The guidelines focus on the syntactic annotation of written language data taken from the German newspaper ’die tageszeitung’ (taz). The unannotated taz newspaper material was taken from the Science CD (Wissenschafts-CD) of ’die tageszeitung’ (taz) that can be licensed from contrapress media GmbH (http://shop.taz.de/index.php?cat=c18_taz-Archiv.html).At present, the treebank comprises 45,200 sentences. The newspaper material is taken from the taz editions from 1992 July 10, 11, 13, 14 1995 October 14, 16, 171999 April 30, May 3 – 7 The average sentence length is 17.6 words and the total number of tokens currently amounts to 794,079. The TüBa-D/Z treebank is still under development. Thus, the number of annotated sentences will increase over time. Periodic data updates and accompanying updates of this stylebook will be made available at:http://www.sfs.uni-tuebingen.de/en/de_tuebadz.shtmlPlease consult this website in order to ensure that you are using the most recent and most complete version of the treebank.The annotation scheme for the TüBa-D/Z treebank is derived from the verbmobil treebank for spoken German, developed earlier (1997–2000) by the Division of Computational Linguistics of the SfS (Hinrichs et al. 2000). The TüBa-D/Z annotation scheme has been extended along various dimensions to accommodate the characteristics of written texts. In order to ensure the reusability of the data, a surface-oriented annotation scheme has been adopted that is inspired by the notion of topological fields and is enriched by a level of predicate-argument structure. The linguistic inventory used in the treebank annotation is based on a minimal set of assumptions that are uncontroversial among major syntactic theories. In this sense it is an attempt at theory-neutrality.",
    "accessed": {
      "date-parts": [
        [
          2011,
          7,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Telljohann",
        "given": "Heike"
      },
      {
        "family": "Hinrichs",
        "given": "Erhard W."
      },
      {
        "family": "Kübler",
        "given": "Sandra"
      },
      {
        "family": "Zinsmeister",
        "given": "Heike"
      },
      {
        "family": "Beck",
        "given": "Kathrin"
      }
    ],
    "id": "Telljohann2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "corpus, german, nlp, tueba",
    "publisher": "Universität Tübingen, Seminar für Sprachwissenschaft",
    "title": "Stylebook for the Tübingen treebank of written German (TüBa-D/Z)",
    "type": "report"
  },
  {
    "URL": "http://nbn-resolving.de/urn:nbn:de:hbz:38-43429",
    "abstract": "It is assumed, in palaeography, papyrology and epigraphy, that a certain amount of uncertainty is inherent in the reading of damaged and abraded texts. Yet we have not really grappled with the fact that, nowadays, as many scholars tend to deal with digital images of texts, rather than handling the texts themselves, the procedures for creating digital images of texts can insert further uncertainty into the representation of the text created. Technical distortions can lead to the unintentional introduction of ’artefacts’ into images, which can have an effect on the resulting representation. If we cannot trust our digital surrogates of texts, can we trust the readings from them? How do scholars acknowledge the quality of digitised images of texts? Furthermore, this leads us to the type of discussions of representation that have been present in Classical texts since Plato: digitisation can be considered as an alternative form of representation, bringing to the modern debate of the use of digital technology in Classics the familiar theories of mimesis (imitation) and ekphrasis (description): the conversion of visual evidence into explicit descriptions of that information, stored in computer files in distinct linguistic terms, with all the difficulties of conversion understood in the ekphratic process. The community has not yet considered what becoming dependent on digital texts means for the field, both in practical and theoretical terms. Issues of quality, copying, representation, and substance should be part of our dialogue when we consult digital surrogates of documentary material, yet we are just constructing understandings of what it means to rely on virtual representations of artefacts. It is necessary to relate our understandings of uncertainty in palaeography and epigraphy to our understanding of the mechanics of visualization employed by digital imaging techniques, if we are to fully understand the impact that these will have.",
    "author": [
      {
        "family": "Terras",
        "given": "Melissa"
      }
    ],
    "container-title": "Kodikologie und Paläographie im digitalen Zeitalter 2/Codicology and palaeography in the digital age 2",
    "editor": [
      {
        "family": "Fischer",
        "given": "Franz"
      },
      {
        "family": "Fritze",
        "given": "Christiane"
      },
      {
        "family": "Vogeler",
        "given": "Georg"
      }
    ],
    "id": "Terras2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "digital_humanities, uncertainty",
    "language": "en-US",
    "page": "43-61",
    "publisher": "BoD",
    "publisher-place": "Norderstedt",
    "title": "Artefacts and errors: Acknowledging issues of representation in the digital imaging of ancient texts",
    "title-short": "Artefacts and errors",
    "type": "chapter",
    "volume": "2"
  },
  {
    "DOI": "10.4324/9781315576251",
    "abstract": "Digital Humanities is becoming an increasingly popular focus of academic endeavour. There are now hundreds of Digital Humanities centres worldwide and the subject is taught at both postgraduate and undergraduate level. Yet the term ’Digital Humanities’ is much debated. This reader brings together, for the first time, in one core volume the essential readings that have emerged in Digital Humanities. We provide a historical overview of how the term ’Humanities Computing’ developed into the term ’Digital Humanities’, and highlight core readings which explore the meaning, scope, and implementation of the field. To contextualize and frame each included reading, the editors and authors provide a commentary on the original piece. There is also an annotated bibliography of other material not included in the text to provide an essential list of reading in the discipline. This text will be required reading for scholars and students who want to discover the history of Digital Humanities through its core writings, and for those who wish to understand the many possibilities that exist when trying to define Digital Humanities.",
    "editor": [
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Vanhoutte",
        "given": "Edward"
      }
    ],
    "id": "Terras2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "publisher": "Ashgate",
    "publisher-place": "Farnham",
    "title": "Defining digital humanities",
    "type": "book"
  },
  {
    "DOI": "10.4324/9781315576251",
    "abstract": "Digital Humanities is becoming an increasingly popular focus of academic endeavour. There are now hundreds of Digital Humanities centres worldwide and the subject is taught at both postgraduate and undergraduate level. Yet the term ’Digital Humanities’ is much debated. This reader brings together, for the first time, in one core volume the essential readings that have emerged in Digital Humanities. We provide a historical overview of how the term ’Humanities Computing’ developed into the term ’Digital Humanities’, and highlight core readings which explore the meaning, scope, and implementation of the field. To contextualize and frame each included reading, the editors and authors provide a commentary on the original piece. There is also an annotated bibliography of other material not included in the text to provide an essential list of reading in the discipline. This text will be required reading for scholars and students who want to discover the history of Digital Humanities through its core writings, and for those who wish to understand the many possibilities that exist when trying to define Digital Humanities.",
    "author": [
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "chapter-number": "2",
    "container-title": "Defining digital humanities",
    "editor": [
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Vanhoutte",
        "given": "Edward"
      }
    ],
    "id": "Unsworth2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "page": "35-48",
    "publisher": "Ashgate",
    "publisher-place": "Farnham",
    "title": "What is humanities computing and what is not?",
    "type": "chapter"
  },
  {
    "DOI": "10.4324/9781315576251",
    "abstract": "Digital Humanities is becoming an increasingly popular focus of academic endeavour. There are now hundreds of Digital Humanities centres worldwide and the subject is taught at both postgraduate and undergraduate level. Yet the term ’Digital Humanities’ is much debated. This reader brings together, for the first time, in one core volume the essential readings that have emerged in Digital Humanities. We provide a historical overview of how the term ’Humanities Computing’ developed into the term ’Digital Humanities’, and highlight core readings which explore the meaning, scope, and implementation of the field. To contextualize and frame each included reading, the editors and authors provide a commentary on the original piece. There is also an annotated bibliography of other material not included in the text to provide an essential list of reading in the discipline. This text will be required reading for scholars and students who want to discover the history of Digital Humanities through its core writings, and for those who wish to understand the many possibilities that exist when trying to define Digital Humanities.",
    "author": [
      {
        "family": "Terras",
        "given": "Melissa"
      }
    ],
    "chapter-number": "18",
    "container-title": "Defining digital humanities",
    "editor": [
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Vanhoutte",
        "given": "Edward"
      }
    ],
    "id": "Terras2011-2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "page": "263-270",
    "publisher": "Ashgate",
    "publisher-place": "Farnham",
    "title": "Peering inside the big tent: Digital humanities and the crisis of inclusion",
    "title-short": "Peering inside the big tent",
    "type": "chapter"
  },
  {
    "DOI": "10.4000/books.oep.226",
    "author": [
      {
        "family": "Terras",
        "given": "Melissa"
      }
    ],
    "container-title": "Read/Write Book 2: Une introduction aux humanités numériques",
    "editor": [
      {
        "family": "Mounier",
        "given": "Pierre"
      }
    ],
    "id": "Terras2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "fr-FR",
    "original-date": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "page": "89-97",
    "publisher": "Open Edition Press",
    "title": "Un regard jeté sous le chapiteau: les humanités numériques et la crise de l’inclusion",
    "title-short": "Un regard jeté sous le chapiteau",
    "translator": [
      {
        "family": "Mounier",
        "given": "Pierre"
      }
    ],
    "type": "chapter"
  },
  {
    "DOI": "10.4000/books.oep.226",
    "editor": [
      {
        "family": "Mounier",
        "given": "Pierre"
      }
    ],
    "id": "Mounier2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Open Edition Press",
    "title": "Read/Write Book 2: Une introduction aux humanités numériques",
    "title-short": "Read/Write Book 2",
    "type": "book"
  },
  {
    "URL": "http://www94.web.cern.ch/WWW94/PrelimProcs.html",
    "author": [
      {
        "dropping-particle": "Stephenson von",
        "family": "Tetzchner",
        "given": "Jon"
      }
    ],
    "container-title": "First international world wide web conference",
    "id": "Tetzchner1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "framemaker",
    "language": "en-US",
    "title": "Converting formatted documents to HTML",
    "type": "paper-conference"
  },
  {
    "ISBN": "1-932432-73-6",
    "URL": "http://portal.acm.org/citation.cfm?id=1610075.1610091",
    "abstract": "Citation function is defined as the author’s reason for citing a given paper (e.g. acknowledgement of the use of the cited method). The automatic recognition of the rhetorical function of citations in scientific text has many applications, from improvement of impact factor calculations to text summarisation and more informative citation indexers. We show that our annotation scheme for citation function is reliable, and present a supervised machine learning framework to automatically classify citation function, using both shallow and linguistically-inspired features. We find, amongst other things, a strong relationship between citation function and sentiment classification.",
    "author": [
      {
        "family": "Teufel",
        "given": "Simone"
      },
      {
        "family": "Siddharthan",
        "given": "Advaith"
      },
      {
        "family": "Tidhar",
        "given": "Dan"
      }
    ],
    "collection-title": "EMNLP ’06",
    "container-title": "Proceedings of the 2006 conference on empirical methods in natural language processing",
    "id": "Teufel2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "todo",
    "language": "en-US",
    "page": "103-110",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Automatic classification of citation function",
    "type": "paper-conference"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=2391171.2391176",
    "abstract": "Sentiment analysis of citations in scientific papers is a new and interesting problem which can open up many exciting new applications in bibliometrics. Current research assumes that using just the citation sentence is enough for detecting sentiment. In this paper, we show that this approach misses much of the existing sentiment. We present a new corpus in which all mentions of a cited paper have been annotated. We explore methods to automatically identify these mentions and show that the inclusion of implicit citations in citation sentiment analysis improves the quality of the overall sentiment assignment.",
    "author": [
      {
        "family": "Athar",
        "given": "Awais"
      },
      {
        "family": "Teufel",
        "given": "Simone"
      }
    ],
    "collection-title": "ACL ’12",
    "container-title": "Proceedings of the workshop on detecting structure in scholarly discourse",
    "id": "Teufel2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "todo",
    "language": "en-US",
    "page": "18-26",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Detection of implicit citations for sentiment detection",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/bf00124151",
    "abstract": "Since 1978 research in the development of software dedicated to the specific problems of historical research has been undertaken at the Max-Planck-Institute für Geschichte in Göttingen. From a background of practical experiences during these years, a concept of what an appropriate ’workstation” for an historian would be has been derived. It stresses the necessity of three components: (a) software, derived from a detailed analysis of what differentiates information contained in historical sources from such present in current material, (b) databases which are as easily available as printed books and (c) knowledge bases which allow software and data bases to draw upon the information contained in historical reference works. A loose network of European research projects, dedicated to the realization of such a setup, is described.",
    "author": [
      {
        "family": "Thaller",
        "given": "Manfred"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Thaller1991",
    "issue": "2–3",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "keyword": "digital_humanities, history",
    "language": "en-US",
    "page": "149-162",
    "title": "The historical workstation project",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "abstract": "Analog zu Band B1 bietet dieses Buch die vollständige Beschreibung des Systems mit einer Definition sämtlicher existierender Möglichkeiten in englischer Sprache. Der Band beschreibt das System in der Version 5.1./6.1., also u.a. mit der englischsprachigen Kommandosyntax. Als Anhang ist dem Buch eine komplette Konkordanz zur englischen und lateinischen Kommandosyntax beigegeben.",
    "author": [
      {
        "family": "Thaller",
        "given": "Manfred"
      }
    ],
    "collection-title": "Half-gray series on historical information technology",
    "id": "Thaller1993",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "cultural_heritage, kleio_system",
    "language": "en-US",
    "publisher": "Max Planck Institute for History",
    "publisher-place": "St. Katharinen, Germany",
    "title": "Kleio. A database system",
    "type": "book",
    "volume": "B11"
  },
  {
    "URL": "http://nbn-resolving.de/urn:nbn:de:0168-ssoar-378617",
    "abstract": "Observations on the current stage of the Digital Humanities and their environment identify four dangers: (1) The focus on infrastructures for the Digital Humanities may obscure that research ultimately is driven by analytical methods and tools, not just by the provision of data or publishing tools. (2) Information technology can support the Humanities in many forms and national traditions. That textual analysis is much discussed right now, should not hide the view of a broader disciplinary field. (3) The mobile revolution looming may once again lead to a repetition of highly destructive processes observed at the PC and the internet revolutions. (4) The Digital Humanities may have to take a much stronger part in the development, not only the reception, of technology. - A series of concrete and controversial questions, which allow the discussion of some of these trends, is derived.",
    "author": [
      {
        "family": "Thaller",
        "given": "Manfred"
      }
    ],
    "container-title": "Historical Social Research",
    "id": "Thaller2012",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "7-23",
    "title": "Controversies around the digital humanities: An agenda",
    "title-short": "Controversies around the digital humanities",
    "type": "article-journal",
    "volume": "37"
  },
  {
    "author": [
      {
        "family": "Thaller",
        "given": "Manfred"
      }
    ],
    "chapter-number": "1",
    "container-title": "Digital Humanities: Eine Einführung",
    "editor": [
      {
        "family": "Jannidis",
        "given": "Fotis"
      },
      {
        "family": "Kohle",
        "given": "Hubertus"
      },
      {
        "family": "Rehbein",
        "given": "Malte"
      }
    ],
    "id": "Thaller2017a",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "de-DE",
    "page": "3-12",
    "publisher": "J. B. Metzler",
    "publisher-place": "Stuttgart",
    "title": "Geschichte der Digital Humanities",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/978-3-476-05446-3_2",
    "author": [
      {
        "family": "Thaller",
        "given": "Manfred"
      }
    ],
    "chapter-number": "2",
    "container-title": "Digital Humanities: Eine Einführung",
    "editor": [
      {
        "family": "Jannidis",
        "given": "Fotis"
      },
      {
        "family": "Kohle",
        "given": "Hubertus"
      },
      {
        "family": "Rehbein",
        "given": "Malte"
      }
    ],
    "id": "Thaller2017b",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "de-DE",
    "page": "13-18",
    "publisher": "J. B. Metzler",
    "publisher-place": "Stuttgart",
    "title": "Digital Humanities als Wissenschaft",
    "type": "chapter"
  },
  {
    "DOI": "10.12759/hsr.suppl.29.2017.7-109",
    "abstract": "The author was one of the earliest representatives of computer applications within historical research in Germany, later being appointed to the first professorship for computer applications in the Humanities in Germany outside of linguistics. The following text describes his experiences as part of that development, which lead from the beginnings in the seventies to the current state of ” Digital Humanties”. His view on this development of an interdisciplinary area left him with rather mixed memories: behind a sparkling front story of an enfolding field, he frequently had the feeling, that there was a tendency to ignore the huge epistemic potential of a serious attempt to apply computer science to the field of history in favor of glamorous but shallow short term goals.",
    "author": [
      {
        "family": "Thaller",
        "given": "Manfred"
      }
    ],
    "container-title": "Historical Social Research Supplement",
    "id": "Thaller2017c",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "en-US",
    "page": "7-109",
    "title": "Between the chairs: An interdisciplinary career",
    "title-short": "Between the chairs",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "URL": "http://www.faz.net/-gsn-71khe",
    "author": [
      {
        "family": "Thiel",
        "given": "Thomas"
      }
    ],
    "id": "Thiel2012",
    "issued": {
      "date-parts": [
        [
          2012,
          7,
          24
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "FAZ.NET",
    "title": "Digital Humanities. Eine empirische Wende für die Geisteswissenschaften?",
    "type": "webpage"
  },
  {
    "author": [
      {
        "family": "Thomas",
        "given": "Ludger"
      }
    ],
    "collection-number": "P-111",
    "collection-title": "Lecture notes in informatics",
    "container-title": "DeLFI 2007: 5. E-learning fachtagung informatik",
    "editor": [
      {
        "family": "Eibl",
        "given": "Christian"
      },
      {
        "family": "Magenheim",
        "given": "Johannes"
      },
      {
        "family": "Schubert",
        "given": "Sigrid"
      },
      {
        "family": "Wessner",
        "given": "Martin"
      }
    ],
    "id": "Thomas2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "note": "Auch als technischer bericht http://publica.fraunhofer.de/eprints/N-66737.pdf",
    "page": "127-138",
    "publisher": "GI-Verlag",
    "publisher-place": "Bonn",
    "title": "Entwicklung rekonfigurierbarer Lerninhalte mit (edu) DocBook",
    "type": "paper-conference"
  },
  {
    "URL": "http://telearn.noe-kaleidoscope.org/warehouse/271-280_(001326v1).pdf",
    "annote": "Zitat: eduDocBook, as well as related tools provide solutions that enable the realization of numerous approaches. In contrast to the many proprietary solutions (e.g. LMML [12], eEML [13], <Ml>3 [14]), DocBook is an established standard, worldwide in use and with a very active user group, as well as numerous related technologies. Due to the distribution and use of standard technologies eduDocBook appears to be medium-term future-proof.",
    "author": [
      {
        "family": "Thomas",
        "given": "Ludger"
      },
      {
        "family": "Trapp",
        "given": "Sonja"
      }
    ],
    "container-title": "Proceedings of the 3<sup>rd</sup> balkan conference in informatics, BCI 2007",
    "editor": [
      {
        "family": "Boyanov",
        "given": "Kiril"
      },
      {
        "family": "Nikolov",
        "given": "R."
      },
      {
        "family": "Nikolova",
        "given": "I."
      },
      {
        "family": "Nisheva",
        "given": "M."
      }
    ],
    "id": "Thomas2007b",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "page": "271-280",
    "publisher-place": "Sofia",
    "title": "Building re-configurable blended-learning arrangements",
    "type": "paper-conference",
    "volume": "2"
  },
  {
    "URL": "http://corescholar.libraries.wright.edu/knoesis/559",
    "abstract": "We present a method for growing the amount of knowledge available on the Web using a hermeneutic method that involves background knowledge, Information Extraction techniques and validation through discourse and use of the extracted information. We exemplify this using Linked Data as background knowledge, automatic Model/Ontology creation for the IE part and a Semantic Browser for evaluation. The hermeneutic approach, however, is open to be used with other IE techniques and other evaluation methods. We will present results from the model creation and anecdotal evidence for the feasibility of ”Validation through Use”.",
    "author": [
      {
        "family": "Thomas",
        "given": "Christopher"
      },
      {
        "family": "Wang",
        "given": "Wenbo"
      },
      {
        "family": "Mehra",
        "given": "Pankaj"
      },
      {
        "family": "Cameron",
        "given": "Delroy H."
      },
      {
        "family": "Mendes",
        "given": "Pablo N."
      },
      {
        "family": "Sheth",
        "given": "Amit P."
      }
    ],
    "container-title": "Proceedings of WebSci10",
    "id": "Thomas2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "linked_data, semantic_web",
    "language": "en-US",
    "title": "What goes around comes around: Improving linked open data through On-Demand model creation",
    "title-short": "What goes around comes around",
    "type": "paper-conference"
  },
  {
    "DOI": "10.2307/327690",
    "author": [
      {
        "family": "Thrush",
        "given": "Jo Ann"
      },
      {
        "family": "Thrush",
        "given": "Randolph S."
      }
    ],
    "container-title": "The Modern Language Journal",
    "id": "Thrush1984",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1984
        ]
      ]
    },
    "keyword": "e-learning, plato_system, ticcit",
    "language": "en-US",
    "page": "21-27",
    "publisher": "Blackwell Publishing on behalf of the National Federation of Modern Language Teachers Associations",
    "title": "Microcomputers in foreign language instruction",
    "type": "article-journal",
    "volume": "68"
  },
  {
    "DOI": "10.2200/S00367ED1V01Y201106HLT014",
    "abstract": "This book provides an overview of various techniques for the alignment of bitexts. It describes general concepts and strategies that can be applied to map corresponding parts in parallel documents on various levels of granularity. Bitexts are valuable linguistic resources for many different research fields and practical applications. The most predominant application is machine translation, in particular, statistical machine translation. However, there are various other threads that can be followed which may be supported by the rich linguistic knowledge implicitly stored in parallel resources. Bitexts have been explored in lexicography, word sense disambiguation, terminology extraction, computer-aided language learning and translation studies to name just a few. The book covers the essential tasks that have to be carried out when building parallel corpora starting from the collection of translated documents up to sub-sentential alignments. In particular, it describes various approaches to document alignment, sentence alignment, word alignment and tree structure alignment. It also includes a list of resources and a comprehensive review of the literature on alignment techniques. Table of Contents: Introduction / Basic Concepts and Terminology / Building Parallel Corpora / Sentence Alignment / Word Alignment / Phrase and Tree Alignment / Concluding Remarks",
    "author": [
      {
        "family": "Tiedemann",
        "given": "Jörg"
      }
    ],
    "collection-title": "Synthesis lectures on human language technologies",
    "id": "Tiedemann2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "machine_translation, nlp",
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "publisher-place": "San Rafael, CA, USA",
    "title": "Bitext alignment",
    "type": "book",
    "volume": "14"
  },
  {
    "DOI": "10.1353/aq.2018.0046",
    "ISSN": "1080-6490",
    "author": [
      {
        "family": "Tilton",
        "given": "Lauren"
      }
    ],
    "container-title": "American Quarterly",
    "id": "Tilton2018",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "page": "633--639",
    "title": "American studies + computational humanities",
    "type": "article-journal",
    "volume": "70"
  },
  {
    "author": [
      {
        "family": "Tomasin",
        "given": "Lorenzo"
      }
    ],
    "id": "Tomasin2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Antipodes",
    "publisher-place": "Lausanne",
    "title": "L’Empreinte digitale: Culture humaniste et technologie",
    "title-short": "L’Empreinte digitale",
    "translator": [
      {
        "family": "Rosselli",
        "given": "Walter"
      }
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Tomasin",
        "given": "Lorenzo"
      }
    ],
    "container-title": "L’Empreinte digitale: Culture humaniste et technologie",
    "id": "Tomasin2018-nager",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "fr-FR",
    "page": "23-35",
    "publisher": "Antipodes",
    "publisher-place": "Lausanne",
    "title": "Nager et naviguer",
    "translator": [
      {
        "family": "Rosselli",
        "given": "Walter"
      }
    ],
    "type": "chapter"
  },
  {
    "abstract": "Reciprocal paired qualitative formative peer assessment of academic writing was undertaken by 12 postgraduate students of educational psychology. Overall, staff and peer assessments showed a very similar balance between positive and negative statements, but this varied according to assessment criterion. However, only half of all detailed formative assessment statements made showed some degree of correspondence between staff and peers. Nevertheless, there was very little evidence of conflict between the views of staff and peers-rather, they focused on different details. Subjective feedback from students indicated that most found the process time consuming, intellectually challenging and socially uncomfortable, but effective in improving the quality of their own subsequent written work and developing other transferable skills. The reliability and validity of this type of peer assessment thus appeared adequate, and the partiality of overlap in detail between staff and peer assessments suggested that the triangulation peer assessment offers is likely to add value. However, caution is indicated regarding the generalisation of this finding. Implications for action are outlined.",
    "author": [
      {
        "family": "Topping",
        "given": "Keith J."
      },
      {
        "family": "Smith",
        "given": "Elaine F."
      },
      {
        "family": "Swanson",
        "given": "Ian"
      },
      {
        "family": "Elliot",
        "given": "Audrey"
      }
    ],
    "container-title": "Assessment & Evaluation in Higher Education",
    "id": "Topping2000",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2000,
          6
        ]
      ]
    },
    "keyword": "assessment",
    "language": "en-US",
    "page": "149-169",
    "publisher": "Routledge",
    "title": "Formative peer assessment of academic writing between postgraduate students",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "DOI": "10.1145/2555595",
    "ISSN": "1559-1131",
    "abstract": "The Internet is increasingly used by young children for all kinds of purposes. Nonetheless, there are not many resources especially designed for children on the Internet and most of the content online is designed for grown-up users. This situation is problematic if we consider the large differences between young users and adults since their topic interests, computer skills, and language capabilities evolve rapidly during childhood. There is little research aimed at exploring and measuring the difficulties that children encounter on the Internet when searching for information and browsing for content. In the first part of this work, we employed query logs from a commercial search engine to quantify the difficulties children of different ages encounter on the Internet and to characterize the topics that they search for. We employed query metrics (e.g., the fraction of queries posed in natural language), session metrics (e.g., the fraction of abandoned sessions), and click activity (e.g., the fraction of ad clicks). The search logs were also used to retrace stages of child development. Concretely, we looked for changes in interests (e.g., the distribution of topics searched) and language development (e.g., the readability of the content accessed and the vocabulary size). In the second part of this work, we employed toolbar logs from a commercial search engine to characterize the browsing behavior of young users, particularly to understand the activities on the Internet that trigger search. We quantified the proportion of browsing and search activity in the toolbar sessions and we estimated the likelihood of a user to carry out search on the Web vertical and multimedia verticals (i.e., videos and images) given that the previous event is another search event or a browsing event. We observed that these metrics clearly demonstrate an increased level of confusion and unsuccessful search sessions among children. We also found a clear relation between the reading level of the clicked pages and characteristics of the users such as age and educational attainment. In terms of browsing behavior, children were found to start their activities on the Internet with a search engine (instead of directly browsing content) more often than adults. We also observed a significantly larger amount of browsing activity for the case of teenager users. Interestingly we also found that if children visit knowledge-related Web sites (i.e., information-dense pages such as Wikipedia articles), they subsequently do more Web searches than adults. Additionally, children and especially teenagers were found to have a greater tendency to engage in multimedia search, which calls to improve the aggregation of multimedia results into the current search result pages.",
    "author": [
      {
        "family": "Torres",
        "given": "Sergio D."
      },
      {
        "family": "Weber",
        "given": "Ingmar"
      },
      {
        "family": "Hiemstra",
        "given": "Djoerd"
      }
    ],
    "container-title": "ACM Transactions on the Web",
    "id": "Torres2014",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "keyword": "ir, web_search",
    "language": "en-US",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Analysis of search and browsing behavior of young users on the Web",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "DOI": "10.1007/978-3-642-20227-8_2",
    "ISBN": "978-3-642-20226-1",
    "abstract": "An alignment method based on the Viterbi algorithm is proposed to find mappings between word images of a given handwritten document and their respective (ASCII) words on its transcription. The approach takes advantage of the underlying segmentation made by Viterbi decoding in handwritten text recognition based on Hidden Markov Models (HMMs). Two levels of alignments are considered: the traditional one at word level and the one at text-line level where pages are transcribed without line break synchronization. According to various metrics used to measure the quality of the alignments, satisfactory results are obtained. Furthermore, the presented alignment approach is tested on two HMMs modelling schemes: one using 78 HMMs (one HMM per character class) and other using two HMMs (for blank space and no-blank characters respectively).",
    "author": [
      {
        "family": "Toselli",
        "given": "Alejandro H."
      },
      {
        "family": "Romero",
        "given": "Verónica"
      },
      {
        "family": "Vidal",
        "given": "Enrique"
      }
    ],
    "chapter-number": "2",
    "collection-title": "Theory and applications of natural language processing",
    "container-title": "Language technology for cultural heritage",
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "family": "Bosch",
        "given": "Antal"
      },
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      }
    ],
    "id": "Toselli2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr",
    "language": "en-US",
    "page": "23-37",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Alignment between text images and their transcripts for handwritten documents language technology for cultural heritage",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/986213.986218",
    "URL": "http://portal.acm.org/citation.cfm?id=986213.986218\\&coll=ACM\\&dl=ACM\\&CFID=61388429\\&CFTOKEN=16823301",
    "abstract": "Markup languages provide efficient ways of storing information on the Web and make the retrieval process easier. Indeed, its simplicity and added intelligence to the document makes it useful for messaging between clients and server.",
    "author": [
      {
        "family": "Toshniwal",
        "given": "Rishi"
      },
      {
        "family": "Agrawal",
        "given": "Dharma P."
      }
    ],
    "container-title": "Commun. ACM",
    "id": "Toshniwal2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "page": "95-98",
    "title": "Tracing the roots of markup languages",
    "type": "article-journal",
    "volume": "47"
  },
  {
    "DOI": "10.1093/llc/fqs055",
    "abstract": "In 1457, the Florentine merchant, Giovanni Rucellai, started to compose a family records and commonplace book, the Zibaldone Quaresimale. His goal was to transmit knowledge to his sons. This article is a report on the computer-assisted analysis of the Zibaldone and the knowledge Giovanni intended to transmit. The 2nd and 3rd sections treat the theoretical framework applied for knowledge structure analysis and the technical requirements this imposed. In the 4th section, I explain how I transformed the manuscript into a semantically and linguistically annotated corpus with the help of semi-automatic procedures. I also discuss the integrated research environment that I developed to store and link information from secondary and primary sources. In the 6th section, through a concrete research problem, the analysis of social perception, I demonstrate the practical application and the power of the digital environment. This section also presents how the knowledge system described in the codex is reconstructed with the help of a Web Ontology Language ontology. Finally, I argue that a project in Digital Humanities can bring new insights only if a comprehensive theoretical model directs the design and the use of the digital environment; one that functions as a framework to interpret the data that the digital environment produces.",
    "author": [
      {
        "family": "Tóth",
        "given": "Gábor M."
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Toth2013",
    "issued": {
      "date-parts": [
        [
          2013,
          1,
          3
        ]
      ]
    },
    "keyword": "cultural_heritage, semantic_web",
    "language": "en-US",
    "publisher": "Published online as Advance Access",
    "title": "The computer-assisted analysis of a medieval commonplace book and diary (MS Zibaldone Quaresimale by Giovanni Rucellai)",
    "type": "article-journal"
  },
  {
    "ISBN": "0521534836",
    "abstract": "This reissue of the modern classic on the study of argumentation features a new Introduction by the author. A central theme throughout the impressive series of philosophical books and articles Stephen Toulmin has published since 1948 is the way in which assertions and opinions concerning all sorts of topics, brought up in everyday life or in academic research, can be rationally justified. Is there one universal system of norms, by which all sorts of arguments in all sorts of fields must be judged, or must each sort of argument be judged according to its own norms? In The Uses of Argument (1958) Toulmin sets out his views on these questions for the first time. In spite of initial criticisms from logicians and fellow philosophers, The Uses of Argument has been an enduring source of inspiration and discussion to students of argumentation from all kinds of disciplinary background for more than forty years.",
    "author": [
      {
        "family": "Toulmin",
        "given": "Stephen E."
      }
    ],
    "edition": "Updated",
    "id": "Toulmin1958-2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge",
    "title": "The uses of argument",
    "type": "book"
  },
  {
    "URL": "http://atala.org/IMG/pdf/TAL-2006-47-3-06-Tran.pdf",
    "abstract": "This paper presents the modelling of Proper Name domain defined by the Prolex project. This modelling is based on two main concepts: the Conceptual Proper Name and the Prolexeme. The Conceptual Proper Name do not represents the referent, but a point of view on this referent. It has a specific concept in each language, the Prolexeme, that is a structured family of lexemes. Around them, we have defined other concepts and relations (synonymy, meronymy, accessibility, eponymy...). Each Conceptual Proper Name is an hyponym of a type and an existence within an ontology.",
    "author": [
      {
        "family": "Tran",
        "given": "Mickaël"
      },
      {
        "family": "Maurel",
        "given": "Denis"
      }
    ],
    "container-title": "Traitement Automatique des Langues",
    "id": "Tran2006",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "computational_linguistics, lexicography, proper_names",
    "language": "fr-FR",
    "page": "115-139",
    "title": "Prolexbase: Un dictionnaire relationnel multilingue de noms propres",
    "title-short": "Prolexbase",
    "type": "article-journal",
    "volume": "47"
  },
  {
    "DOI": "10.1145/1871437.1871463",
    "ISBN": "978-1-4503-0099-5",
    "abstract": "An important challenge for biomedical information retrieval (IR) is dealing with the complex, inconsistent and ambiguous biomedical terminology. Frequently, a concept-based representation defined in terms of a domain-specific terminological resource is employed to deal with this challenge. In this paper, we approach the incorporation of a concept-based representation in monolingual biomedical IR from a cross-lingual perspective. In the proposed framework, this is realized by translating and matching between text and concept-based representations. The approach allows for deployment of a rich set of techniques proposed and evaluated in traditional cross-lingual IR. We compare six translation models and measure their effectiveness in the biomedical domain. We demonstrate that the approach can result in significant improvements in retrieval effectiveness over word-based retrieval. Moreover, we demonstrate increased effectiveness of a CLIR framework for monolingual biomedical IR if basic translations models are combined.",
    "author": [
      {
        "family": "Trieschnigg",
        "given": "Dolf"
      },
      {
        "family": "Hiemstra",
        "given": "Djoerd"
      },
      {
        "dropping-particle": "de",
        "family": "Jong",
        "given": "Franciska"
      },
      {
        "family": "Kraaij",
        "given": "Wessel"
      }
    ],
    "container-title": "Proceedings of the 19<sup>th</sup> ACM international conference on information and knowledge management (CIKM ’10)",
    "id": "Trieschnigg2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "ir",
    "language": "en-US",
    "page": "169-178",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A cross-lingual framework for monolingual biomedical information retrieval",
    "type": "paper-conference"
  },
  {
    "DOI": "10.3990/1.9789036530644",
    "URL": "http://purl.utwente.nl/publications/72481",
    "abstract": "In this thesis we investigate the possibility to integrate domain-specific knowledge into biomedical information retrieval (IR). Recent decades have shown a fast growing interest in biomedical research, reflected by an exponential growth in scientific literature. An important problem for biomedical IR is dealing with the complex and inconsistent terminology encountered in biomedical publications. Dealing with the terminology problem requires domain knowledge stored in terminological resources: controlled indexing vocabularies and thesauri. The integration of this knowledge in modern word-based information retrieval is, however, far from trivial. The first research theme investigates heuristics for obtaining word-based representations from biomedical text for robust word-based retrieval. We investigated the effect of choices in document preprocessing heuristics on retrieval effectiveness. Document preprocessing heuristics such as stop word removal, stemming, and breakpoint identification and normalization were shown to strongly affect retrieval performance. An effective combination of heurisitics was identified to obtain a word-based representation from text for the remainder of this thesis. The second research theme deals with concept-based retrieval. We compared a word-based to a concept-based representation and determined to what extent a manual concept-based representation can be automatically obatined from text. Retrieval based on only concepts was demonstrated to be significantly less effective than word-based retrieval. This deteriorated performance could be explained by errors in the classification process, limitations of the concept vocabularies and limited exhaustiveness of the concept-based document representations. Retrieval based on a combination of word-based and automatically obtained concept-based query representations did significantly improve word-only retrieval. In the third and last research theme we propose a cross-lingual framework for monolingual biomedical IR. In this framework, the integration of a concept-based representation is viewed as a cross-lingual matching problem involving a word-based and concept-based representation language. This framework gives us the opportunity to adopt a large set of established cross-lingual information retrieval methods and techniques for this domain. Experiments with basic term-to-term translation models demonstrate that this approach can significantly improve word-based retrieval. Directions for future work are using these concepts for communication between user and retrieval system, extending upon the translation models and extending CLIR-enhanced concept-based retrieval outside the biomedical domain.",
    "author": [
      {
        "family": "Trieschnigg",
        "given": "Dolf"
      }
    ],
    "collection-title": "SIKS dissertation series",
    "genre": "PhD thesis",
    "id": "Trieschnigg2010diss",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "ir",
    "language": "en-US",
    "publisher": "University of Twente",
    "publisher-place": "Enschede, The Netherlands",
    "title": "Proof of concept: Concept-based biomedical information retrieval",
    "title-short": "Proof of concept",
    "type": "thesis",
    "volume": "2010-35"
  },
  {
    "DOI": "10.1093/llc/fqq003",
    "ISSN": "1477-4615",
    "abstract": "Categorization and taxonomy are topical issues in intertextuality studies. Instead of increasing the number of overlapping or contradictory definitions (often established with reference to limited databases) which exist even for key concepts such as “allusion ” or “quotation”, we propose an electronically implemented data-driven approach based on the isolation, analysis and description of a number of relevant parameters such as general text relation, marking for quotation, modification etc. If a systematic parameter analysis precedes discussions of possible correlations and the naming of features bundles as composite categories, a dynamic approach to categorization emerges which does justice to the varied and complex phenomena in this field. The database is the HyperHamlet corpus, a chronologically and generically wide-ranging collection of Hamlet references that confront linguistic and literary researchers with a comprehensive range of formal and stylistic issues. Its multi-dimensional encodings and search facilities provide the indispensable ’freedom from the analytic limits of hardcopy’, as Jerome McGann put it. The methodological and heuristic gains include a more complete description of possible parameter settings, a clearer recognition of multiple parameter settings (as implicit in existing genre definitions), a better understanding of how parameters interact, descriptions of disregarded literary phenomena that feature unusual parameter combinations and, finally, descriptive labels for the most polysemous areas that may clarify matters without increasing taxonomical excess.",
    "author": [
      {
        "family": "Trillini",
        "given": "Regula H."
      },
      {
        "family": "Quassdorf",
        "given": "Sixta"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Trillini2010",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "intertextuality",
    "language": "en-US",
    "page": "269-286",
    "publisher": "Oxford University Press",
    "title": "A “key to all quotations”? A corpus-based parameter model of intertextuality",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "DOI": "10.1007/978-3-642-23577-1_22",
    "ISBN": "978-3-642-23576-4",
    "author": [
      {
        "family": "Trotman",
        "given": "Andrew"
      },
      {
        "family": "Alexander",
        "given": "David"
      },
      {
        "family": "Geva",
        "given": "Shlomo"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Comparative evaluation of focused retrieval",
    "editor": [
      {
        "family": "Geva",
        "given": "Shlomo"
      },
      {
        "family": "Kamps",
        "given": "Jaap"
      },
      {
        "family": "Schenkel",
        "given": "Ralf"
      },
      {
        "family": "Trotman",
        "given": "Andrew"
      }
    ],
    "id": "Trotman2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "language": "en-US",
    "page": "241-249",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Overview of the INEX 2010 Link the Wiki Track",
    "type": "chapter",
    "volume": "6932"
  },
  {
    "URL": "http://edweek.org/dd/articles/2008/06/09/01moodle.h02.html",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Trotter",
        "given": "Andrew"
      }
    ],
    "container-title": "Digital Directions",
    "id": "Trotter2008",
    "issue": "Spring/Summer",
    "issued": {
      "date-parts": [
        [
          2008,
          6
        ]
      ]
    },
    "page": "21",
    "title": "Blackboard vs. Moodle: Competition in course-management market grows",
    "title-short": "Blackboard vs. Moodle",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/pdf/423_paper.pdf",
    "abstract": "This paper proposes a method of increasing the size of a bilingual lexicon obtained from two other bilingual lexicons via a pivot language. When we apply this approach, there are two main challenges, “ambiguity” and “mismatch” of terms; we target the latter problem by improving the utilization ratio of the bilingual lexicons. Given two bilingual lexicons between language pairs Lf-Lp and Lp-Le, we compute lexical translation probabilities of word pairs by using a statistical word-alignment model, and term decomposition/composition techniques. We compare three approaches to generate the bilingual lexicon: “exact merging”, “word-based merging”, and our proposed “alignment-based merging”. In our method, we combine lexical translation probabilities and a simple language model for estimating the probabilities of translation pairs. The experimental results show that our method could drastically improve the number of translation terms compared to the two methods mentioned above. Additionally, we evaluated and discussed the quality of the translation outputs.",
    "author": [
      {
        "family": "Tsunakawa",
        "given": "Takashi"
      },
      {
        "family": "Okazaki",
        "given": "Naoaki"
      },
      {
        "family": "Tsujii",
        "given": "Jun’ichi"
      }
    ],
    "container-title": "Proceedings of the sixth international conference on language resources and evaluation (LREC’08)",
    "editor": [
      {
        "family": "Calzolari",
        "given": "Nicoletta"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Maegaard",
        "given": "Bente"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Odijk",
        "given": "Jan"
      },
      {
        "family": "Piperidis",
        "given": "Stelios"
      },
      {
        "family": "Tapias",
        "given": "Daniel"
      }
    ],
    "id": "Tsunakawa2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "less-resourced_languages",
    "language": "en-US",
    "publisher": "European Language Resources Association (ELRA)",
    "title": "Building bilingual lexicons using lexical translation probabilities via pivot languages",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.isrl.uiuc.edu/~amag/langev/paper/turchi06evoLangStatistics.html",
    "abstract": "We propose to address a series of questions related to the evolution of languages by statistical analysis of written text. We develop a \"statistical signature\" of a language, analogous to the genetic signature proposed by Karlin in biology, and we show its stability within languages and its discriminative power between languages. Using this representation, we address the question of its trajectory during language evolution. We first reconstruct a phylogenetic tree of IE languages using this property, in this way showing that it also contains enough information to act as a \"tracking\" tag for a language during its evolution. One advantage of this kind of phylogenetic trees is that they do not depend on any semantic assessment or on any choice of words. We use the \"statistical signature\" to analyze a time-series of documents from four romance languages, following their transition from latin. The languages are Italian, French, Spanish and Portuguese, and the time points correspond to all centuries from III bC to XX AD.",
    "author": [
      {
        "family": "Turchi",
        "given": "Marco"
      },
      {
        "family": "Cristianini",
        "given": "Nello"
      }
    ],
    "container-title": "Proceedings of the 6th international conference on the evolution of language",
    "id": "Turchi2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "language_identification, nlp",
    "language": "en-US",
    "page": "348-355",
    "title": "A statistical analysis of language evolution",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/2009916.2010042",
    "ISBN": "978-1-4503-0757-4",
    "abstract": "This work explores the problem of cross-lingual pairwise similarity, where the task is to extract similar pairs of documents across two different languages. Solutions to this problem are of general interest for text mining in the multi-lingual context and have specific applications in statistical machine translation. Our approach takes advantage of cross-language information retrieval (CLIR) techniques to project feature vectors from one language into another, and then uses locality-sensitive hashing (LSH) to extract similar pairs. We show that effective cross-lingual pairwise similarity requires working with similarity thresholds that are much lower than in typical monolingual applications, making the problem quite challenging. We present a parallel, scalable MapReduce implementation of the sort-based sliding window algorithm, which is compared to a brute-force approach on German and English Wikipedia collections. Our central finding can be summarized as \"no free lunch\": there is no single optimal solution. Instead, we characterize effectiveness-efficiency tradeoffs in the solution space, which can guide the developer to locate a desirable operating point based on application- and resource-specific constraints.",
    "author": [
      {
        "family": "Ture",
        "given": "Ferhan"
      },
      {
        "family": "Elsayed",
        "given": "Tamer"
      },
      {
        "family": "Lin",
        "given": "Jimmy"
      }
    ],
    "container-title": "Proceedings of the 34th international ACM SIGIR conference on research and development in information retrieval (SIGIR ’11)",
    "id": "Ture2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "approximate_matching",
    "language": "en-US",
    "page": "943-952",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "No free lunch: Brute force vs. Locality-sensitive hashing for cross-lingual pairwise similarity",
    "title-short": "No free lunch",
    "type": "paper-conference"
  },
  {
    "URL": "http://projects.ups.edu/rwthornton/LMS_Moodle_finaldraft.pdf",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "LMIS Committee"
      }
    ],
    "id": "UPS2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "note": "Final draft",
    "publisher": "University of Puget Sound",
    "publisher-place": "Tacoma, WA, USA",
    "title": "Learning management system recommendation",
    "type": "report"
  },
  {
    "DOI": "10.1007/11762256_36",
    "ISBN": "978-3-540-34544-2",
    "abstract": "There are numerous extensions of RDF that support temporal reasoning, reasoning about pedigree, reasoning about uncertainty, and so on. In this paper, we present Annotated RDF (or aRDF for short) in which RDF triples are annotated by members of a partially ordered set (with bottom element) that can be selected in any way desired by the user. We present a formal declarative semantics (model theory) for annotated RDF and develop algorithms to check consistency of aRDF theories and to answer queries to aRDF theories. We show that annotated RDF captures versions of all the forms of reasoning mentioned above within a single unified framework. We develop a prototype aRDF implementation and show that our algorithms work very fast indeed – in fact, in just a matter of seconds for theories with over 100,000 nodes.",
    "author": [
      {
        "family": "Udrea",
        "given": "Octavian"
      },
      {
        "family": "Recupero",
        "given": "Diego R."
      },
      {
        "family": "Subrahmanian",
        "given": "V. S."
      }
    ],
    "chapter-number": "36",
    "collection-title": "Lecture notes in computer science",
    "container-title": "The semantic web: Research and applications, 6th european semantic web conference, ESWC 2009, heraklion, crete, greece, may 31-june 4, 2009, proceedings",
    "editor": [
      {
        "family": "Sure",
        "given": "York"
      },
      {
        "family": "Domingue",
        "given": "John"
      }
    ],
    "id": "Udrea2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "rdf, temporal_data",
    "language": "en-US",
    "page": "487-501",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Annotated RDF",
    "type": "chapter",
    "volume": "4011"
  },
  {
    "URL": "http://tojet.net/articles/548.pdf",
    "abstract": "In this paper, the role of computers in writing process was investigated. Last 25 years of journals were searched to find related articles. Articles and books were classified under prewriting, composing, and revising and editing headings. The review results showed that computers can make writers’ job easy in the writing process. In addition, literature results revealed that teachers, peers, instructional strategies, and computer software all together have some important roles to develop students’ writing ability. Simplifying the revising process is the biggest expectation from computers.",
    "author": [
      {
        "family": "Ulusoy",
        "given": "Mustafa"
      }
    ],
    "container-title": "Turkish Online Journal of Educational Technology",
    "id": "Ulusoy2006",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2006,
          10
        ]
      ]
    },
    "keyword": "interactive_editing, writing_research",
    "language": "en-US",
    "page": "58-66",
    "title": "The role of computers in writing process",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "ISSN": "1746-8256",
    "URL": "http://www.ijdc.net/index.php/ijdc/article/view/152",
    "abstract": "Metadata extraction is a critical aspect of ingestion of collections into digital archives and libraries. A method for automatically recognizing document types and extracting metadata from digital records has been developed. The method is based on a method for automatically annotating semantic categories such as person’s names, job titles, dates, and postal addresses that may occur in a record. It extends this method by using the semantic annotations to identify the intellectual elements of a document’s form, parsing these elements using context-free grammars that define documentary forms, and interpreting the elements of the form of the document to identify metadata such as the chronological date, author(s), addressee(s), and topic. Context-free grammars were developed for fourteen of the documentary forms occurring in Presidential records. In an experiment, the document type recognizer successfully recognized the documentary form and extracted the metadata of two-thirds of the records in a series of Presidential e-records containing twenty-one document types.",
    "author": [
      {
        "family": "Underwood",
        "given": "William"
      }
    ],
    "container-title": "International Journal of Digital Curation",
    "id": "Underwood2010",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, document_analysis",
    "language": "en-US",
    "page": "148-159",
    "title": "Grammar-Based recognition of documentary forms and extraction of metadata",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "URL": "http://www.digitalhumanities.org/dhq/vol/11/2/000317/000317.html",
    "abstract": "It has recently become common to describe all empirical approaches to literature as subfields of digital humanities. This essay argues that distant reading has a largely distinct genealogy stretching back many decades before the advent of the internet – a genealogy that is not for the most part centrally concerned with computers. It would be better to understand this field as a conversation between literary studies and social science, inititated by scholars like Raymond Williams and Janice Radway, and moving slowly toward an explicitly experimental method. Candor about the social-scientific dimension of distant reading is needed now, in order to refocus a research agenda that can drift into diffuse exploration of digital tools. Clarity on this topic might also reduce miscommunication between distant readers and digital humanists.",
    "author": [
      {
        "family": "Underwood",
        "given": "Ted"
      }
    ],
    "container-title": "Digital Humanities Quarterly",
    "id": "Underwood2017",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities, literature",
    "language": "en-US",
    "title": "A genealogy of distant reading",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "URL": "http://www.uni-hamburg.de/newsletter/OLAT-ndash-eine-neue-eLearning-Plattform-an-der-Universitaet-Hamburg.html",
    "accessed": {
      "date-parts": [
        [
          2009,
          8,
          5
        ]
      ]
    },
    "author": [
      {
        "literal": "Universität Hamburg"
      }
    ],
    "container-title": "UHH Newsletter",
    "id": "Uni-Hamburg2009",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2009,
          6
        ]
      ]
    },
    "title": "OLAT – eine neue eLearning-Plattform an der Universität Hamburg",
    "type": "article-journal"
  },
  {
    "URL": "http://www.unicode.org/versions/Unicode6.1.0/",
    "author": [
      {
        "family": "The Unicode Consortium"
      }
    ],
    "id": "Unicode",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "publisher": "The Unicode Consortium",
    "publisher-place": "Mountain View, CA, USA",
    "title": "The unicode standard, version 6.1.0",
    "type": "book"
  },
  {
    "URL": "http://www.unicode.org/versions/Unicode6.0.0/",
    "author": [
      {
        "family": "The Unicode Consortium"
      }
    ],
    "id": "Unicode600",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "publisher": "The Unicode Consortium",
    "publisher-place": "Mountain View, CA, USA",
    "title": "The unicode standard, version 6.0.0",
    "type": "book"
  },
  {
    "URL": "http://joyds1.joensuu.fi/suomi-malaga/suomi.html",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Väisänen",
        "given": "Hannu"
      }
    ],
    "id": "Vaeisaenen2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "title": "Suomi-malaga",
    "type": "book"
  },
  {
    "abstract": "Abbreviated forms offer a special challenge in a historical corpus, since they show graphic variations, besides being frequent and ambiguous. The purpose of this paper is to present the process of building a large dictionary of historical Portuguese abbreviations, whose entries include the abbreviation and its expansion, as well as morphosyntactic and semantic information (a predefined set of named entities – NEs). This process has been carried out in a hybrid fashion that uses linguistic resources (such as a printed dictionary and lists of abbreviations) and abbreviations extracted from the Historical Dictionary of Brazilian Portuguese (HDPB) corpus via finite-state automata and regular expressions. Besides being useful to disambiguate the abbreviations found in the HDBP corpus, this dictionary can be used in other projects and tasks, mainly NE recognition.",
    "author": [
      {
        "family": "Vale",
        "given": "Oto"
      },
      {
        "family": "Candido Jr.",
        "given": "Arnaldo"
      },
      {
        "family": "Muniz",
        "given": "Marcelo"
      },
      {
        "family": "Bengtson",
        "given": "Clarissa"
      },
      {
        "family": "Cucatto",
        "given": "Lívia"
      },
      {
        "family": "Batista",
        "given": "Abner"
      },
      {
        "family": "Parreira",
        "given": "Maria C."
      },
      {
        "family": "Biderman",
        "given": "Maria T."
      },
      {
        "family": "Aluísio",
        "given": "Ra"
      }
    ],
    "container-title": "Proceedings of the LREC 2008 workshop on language technology for cultural heritage data (LaTeCH 2008)",
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "family": "Ribarov",
        "given": "Kiril"
      }
    ],
    "id": "Vale2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, portuguese",
    "language": "en-US",
    "page": "47-54",
    "title": "Building a large dictionary of abbreviations for named entity recognition in Portuguese historical corpora",
    "type": "paper-conference"
  },
  {
    "ISBN": "9782807302150",
    "abstract": "Démystifiez les humanités numériques au travers d’une introduction conceptuelle et technique. Pour maîtriser les méthodes et outils permettant de publier et de valoriser ses travaux de recherche en ligne.",
    "author": [
      {
        "family": "Van Hooland",
        "given": "Seth"
      },
      {
        "family": "Gillet",
        "given": "Florence"
      },
      {
        "family": "Hengchen",
        "given": "Simon"
      },
      {
        "family": "De Wilde",
        "given": "Max"
      }
    ],
    "id": "VanHooland2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "fr-FR",
    "publisher": "De Boeck Supérieur",
    "publisher-place": "Louvain-la-Neuve",
    "title": "Introduction aux humanités numériques: méthodes et pratiques",
    "title-short": "Introduction aux humanités numériques",
    "type": "book"
  },
  {
    "DOI": "10.1145/3219819.3219996",
    "ISBN": "9781450355520",
    "abstract": "Deep Learning (DL) methods have been transforming computer vision with innovative adaptations to other domains including climate change. For DL to pervade Science and Engineering (S&E) applications where risk management is a core component, well-characterized uncertainty estimates must accompany predictions. However, S&E observations and model-simulations often follow heavily skewed distributions and are not well modeled with DL approaches, since they usually optimize a Gaussian, or Euclidean, likelihood loss. Recent developments in Bayesian Deep Learning (BDL), which attempts to capture uncertainties from noisy observations, aleatoric, and from unknown model parameters, epistemic, provide us a foundation. Here we present a discrete-continuous BDL model with Gaussian and lognormal likelihoods for uncertainty quantification (UQ). We demonstrate the approach by developing UQ estimates on \"DeepSD”, a super-resolution based DL model for Statistical Downscaling (SD) in climate applied to precipitation, which follows an extremely skewed distribution. We find that the discrete-continuous models outperform a basic Gaussian distribution in terms of predictive accuracy and uncertainty calibration. Furthermore, we find that the lognormal distribution, which can handle skewed distributions, produces quality uncertainty estimates at the extremes. Such results may be important across S&E, as well as other domains such as finance and economics, where extremes are often of significant interest. Furthermore, to our knowledge, this is the first UQ model in SD where both aleatoric and epistemic uncertainties are characterized.",
    "author": [
      {
        "family": "Vandal",
        "given": "Thomas"
      },
      {
        "family": "Kodra",
        "given": "Evan"
      },
      {
        "family": "Dy",
        "given": "Jennifer"
      },
      {
        "family": "Ganguly",
        "given": "Sangram"
      },
      {
        "family": "Nemani",
        "given": "Ramakrishna"
      },
      {
        "family": "Ganguly",
        "given": "Auroop R."
      }
    ],
    "container-title": "Proceedings of the 24<sup>th</sup> ACM SIGKDD international conference on knowledge discovery & data mining (KDD ’18)",
    "id": "Vandal2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "keyword": "machine_learning, uncertainty",
    "language": "en-US",
    "page": "2377-2386",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Quantifying uncertainty in discrete-continuous and skewed data with Bayesian deep learning",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-540-85569-9_3",
    "ISBN": "978-3-540-85568-2",
    "abstract": "Currently almost all legislative bodies throughout Europe use general purpose word-processing software for the drafting of legal documents. These regular word processors do not provide specific support for legislative drafters and parliamentarians to facilitate the legislative process. Furthermore, they do not natively support metadata on regulations. This paper describes how the MetaLex regulation-drafting environment (MetaVex) aims to meet such requirements.",
    "author": [
      {
        "family": "Ven",
        "given": "Saskia"
      },
      {
        "family": "Hoekstra",
        "given": "Rinke"
      },
      {
        "family": "Winkels",
        "given": "Radboud"
      },
      {
        "family": "Maat",
        "given": "Emile"
      },
      {
        "family": "Kollár",
        "given": "Ádám"
      }
    ],
    "chapter-number": "3",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Computable models of the law",
    "editor": [
      {
        "family": "Casanovas",
        "given": "Pompeu"
      },
      {
        "family": "Sartor",
        "given": "Giovanni"
      },
      {
        "family": "Casellas",
        "given": "Núria"
      },
      {
        "family": "Rubino",
        "given": "Rossella"
      }
    ],
    "id": "Ven2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "authoring, interactive_editing, legal, rdf",
    "language": "en-US",
    "page": "42-55",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "MetaVex: Regulation drafting meets the semantic web",
    "title-short": "MetaVex",
    "type": "chapter",
    "volume": "4884"
  },
  {
    "ISBN": "978-0-230-50005-1",
    "author": [
      {
        "family": "Verga",
        "given": "Marcello"
      }
    ],
    "container-title": "Setting the standards. Institutions, networks and communities of national historiography",
    "editor": [
      {
        "family": "Porciani",
        "given": "Ilaria"
      },
      {
        "family": "Tollebeek",
        "given": "Jo"
      }
    ],
    "id": "Verga2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "history",
    "language": "en-US",
    "page": "89-104",
    "publisher": "Palgrave Macmillan",
    "publisher-place": "Basingstoke, UK",
    "title": "The dictionary is dead, long live the dictionary! Biographical collections in national contexts",
    "type": "chapter"
  },
  {
    "URL": "http://www.naccq.ac.nz/conference05/proceedings_04/verhaart-LO.pdf",
    "abstract": "The packaging and distribution of learning content into learning objects is increasingly being discussed as a natural evolution. Such packaging and distribution allows the distribution of a variety of learning materials with lower production and delivery costs. There is a view that the learning object will allow both educators and learners to individualise their material to best align with their teaching and learning styles, though customization raises some interesting issues. Groups and organisations are collecting learning objects in electronic repositories, thus enabling their sharing across the world. Products such as Blackboard allow learning objects to be integrated within its learning management system. This paper looks at the terminology and standards of learning objects and reviews some existing learning object repositories, discussing their usefulness in relation to providing course content.",
    "author": [
      {
        "family": "Verhaart",
        "given": "Michael"
      }
    ],
    "container-title": "Proceedings of the 17th annual conference of the national advisory committee on computer qualifications (NACCQ)",
    "editor": [
      {
        "family": "Mann",
        "given": "Samuel"
      },
      {
        "family": "Clear",
        "given": "Tony"
      }
    ],
    "id": "Verhaart2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "language": "en-US",
    "page": "458-462",
    "publisher": "New Zealand National Advisory Committee on Computing Qualifications",
    "publisher-place": "Christchurch, New Zealand",
    "title": "Learning object repositories: How useful are they?",
    "title-short": "Learning object repositories",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1111/j.1749-818X.2008.00116.x",
    "abstract": "The goal of temporal annotation is to mark up all elements of a text that are relevant to the temporal interpretation of that text. As such, temporal annotation reveals part of the semantics of a text. We describe recent approaches to temporal annotation, focusing on the recently developed annotation language TimeML. In addition, we describe a couple of systems aimed at temporally parsing texts. We motivate the use of temporal annotation from a question answering viewpoint and describe some earlier insights from theoretical linguistics on what the temporal properties of language constructs are.",
    "author": [
      {
        "family": "Verhagen",
        "given": "Marc"
      },
      {
        "family": "Moszkowicz",
        "given": "Jessica L."
      }
    ],
    "container-title": "Language and Linguistics Compass",
    "id": "Verhagen2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "annotations, temporal_information",
    "page": "517-536",
    "publisher-place": "Department of Computer Science, Brandeis University",
    "title": "Temporal annotation and representation",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "abstract": "During the last years the campaign of mass digitization made available catalogues and valuable rare manuscripts and old printed books vie the Internet. The Manuscriptorium digital library ingested hundreds of olumes and it is expected that the volume will grow up in the next years. Other European initiatives like Europeana and Monasterium have also as central activities the online presentation of cultural heritage. With the growing of the available on-line volumes, a special attention was paid to the management and retrieval of documents within digital libraries. Enabling semantic technologies and intelligent linking and search are a big step forward, but they still do not succeed in making the content of old rare books intelligible to the broad public or specialists in other domains or languages. In this paper we will argue that multilingual language technologies have the potential to fill this gap. We overview the existent language resources for historical documents, and present an architecture which aims at presenting such texts to the normal user, without altering the character of the texts.",
    "author": [
      {
        "family": "Vertan",
        "given": "Cristina"
      }
    ],
    "container-title": "Proceedings of the seventh conference on international language resources and evaluation (LREC’10)",
    "editor": [
      {
        "family": "Calzolari",
        "given": "Nicoletta"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Maegaard",
        "given": "Bente"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Odijk",
        "given": "Jan"
      },
      {
        "family": "Piperidis",
        "given": "Stelios"
      },
      {
        "family": "Rosner",
        "given": "Mike"
      },
      {
        "family": "Tapias",
        "given": "Daniel"
      }
    ],
    "id": "Vertan2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "publisher": "European Language Resources Association (ELRA)",
    "publisher-place": "Valletta, Malta",
    "title": "Towards the integration of language tools within historical digital libraries",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/2184512.2184554",
    "ISBN": "978-1-4503-1203-5",
    "abstract": "Storing historical data is not new. Data warehouses are filled with it. However, querying temporal data has only, in the past few years, become a realistic possibility. Yet the SQL standard has not had significant updates in temporal data support since 1992. The demands of the modern world are no longer satisfied by decades-old solutions. MySQL, an open-source database extremely popular in the sciences, has still not yet fully implemented SQL-92. Its approach to temporal needs is lacking. Too many queries are not answerable, and those that are answerable are often answered inconsistently.",
    "author": [
      {
        "family": "Vicknair",
        "given": "Chad"
      },
      {
        "family": "Wilkins",
        "given": "Dawn"
      },
      {
        "family": "Chen",
        "given": "Yixin"
      }
    ],
    "container-title": "Proceedings of the 50<sup>th</sup> annual southeast regional conference (ACM-SE ’12)",
    "id": "Vicknair2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "database, temporal_data",
    "language": "en-US",
    "page": "176-181",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "MySQL and the trouble with temporal data",
    "type": "paper-conference"
  },
  {
    "URL": "http://ceur-ws.org/Vol-258/paper06.pdf",
    "abstract": "This paper will describe the creation of an ontology to model information contained in historical documents. It will describe the structure of the RDF/OWL ontology and how the model facilitated the expression and analysis of implicit and hidden information/associations existing in the sources. Some aspects of data processing and delivery based on the ontology, like the generation of indices and the implementation of search mechanisms, will also be described.",
    "author": [
      {
        "family": "Vieira",
        "given": "Jose"
      },
      {
        "family": "Ciula",
        "given": "Arianna"
      }
    ],
    "container-title": "Proceedings of the OWLED 2007 workshop on OWL: Experiences and directions",
    "editor": [
      {
        "family": "Golbreich",
        "given": "Christine"
      },
      {
        "family": "Kalyanpur",
        "given": "Aditya"
      },
      {
        "family": "Parsia",
        "given": "Bijan"
      }
    ],
    "id": "Vieira2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "digital_edition, digital_humanities, rdf, semantic_web, tei",
    "language": "en-US",
    "title": "Implementing an RDF/OWL ontology on Henry the III Fine Rolls",
    "type": "paper-conference"
  },
  {
    "URL": "http://jstor.org/stable/40949974",
    "author": [
      {
        "family": "Vilar",
        "given": "Pierre"
      }
    ],
    "container-title": "Revue Historique",
    "id": "Vilar1965",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1965
        ]
      ]
    },
    "keyword": "formal_models, history",
    "language": "fr-FR",
    "page": "293-312",
    "title": "Pour une meilleure compréhension entre économistes et historiens. «Histoire quantitative» ou économétrie rétrospective?",
    "type": "article-journal",
    "volume": "233"
  },
  {
    "ISBN": "9782846708883",
    "abstract": "« Il ne s’agit pas d’une révolution numérique, mais d’une civilisation numérique ». Cette phrase prononcée par Irina Bokova, directrice de l’Unesco, résume à elle seule l’importance des humanités numériques. Contrairement à ce que l’on pourrait croire, celles-ci ne sont pas l’affaire de quelques geeks lettreux, mais, traitant de notre patrimoine et de nos identités, elles nous concernent tous et nous accompagnent déjà au quotidien. Dématérialisation du savoir, démocratisation de la culture, mais aussi mort du livre et création de nouvelles inégalités, les humanités numériques font débat : quel rapport voulons-nous entretenir avec notre passé et les autres communautés ? Quelles limites fixer à la collecte et à l’exploitation des données ? Comment réguler les usages délictueux ? Au travers d’une analyse précise et d’exemples concrets, Dominique Vinck montre que les humanités numériques vont bien au-delà de la diffusion de l’informatique. Elles sont un défi posé à notre société pour la nouvelle humanité que nous voulons construire.",
    "author": [
      {
        "family": "Vinck",
        "given": "Dominique"
      }
    ],
    "id": "Vinck2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "fr-FR",
    "publisher": "Le Cavalier Bleu",
    "publisher-place": "Paris",
    "title": "Humanités numériques: La culture face aux nouvelles technologies",
    "title-short": "Humanités numériques",
    "type": "book"
  },
  {
    "URL": "http://www.cedis.fu-berlin.de/aktuelles/cedis/memorandum_virtusd.html",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "Initiative VirtusD"
      }
    ],
    "id": "Virtusd2007",
    "issued": {
      "date-parts": [
        [
          2007,
          1
        ]
      ]
    },
    "title": "Berliner Memorandum ",
    "type": ""
  },
  {
    "DOI": "10.1145/1244002.1244139",
    "ISBN": "1-59593-480-4",
    "abstract": "The digitalization of historical text documents as a basis of data mining and information retrieval for the purpose of progress in the history sciences is urgently needed. We present a novel, specialist XML tool-suite supporting the working historian in the transcription of original medieval charters into a machine-readable form.",
    "author": [
      {
        "family": "Vogeler",
        "given": "Georg"
      },
      {
        "family": "Gruner",
        "given": "Stefan"
      },
      {
        "family": "Burkard",
        "given": "Benjamin"
      }
    ],
    "container-title": "Proceedings of the 2007 ACM symposium on applied computing (SAC ’07)",
    "id": "Vogeler2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "cultural_heritage, xml",
    "language": "en-US",
    "page": "594-599",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "New specialist tools for medieval document XML markup",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Volk",
        "given": "Martin"
      },
      {
        "family": "Bubenhofer",
        "given": "Noah"
      },
      {
        "family": "Althaus",
        "given": "Adrian"
      },
      {
        "family": "Bangerter",
        "given": "Maya"
      }
    ],
    "container-title": "Künstliche Intelligenz",
    "id": "Volk2009a",
    "issue": "4/2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "page": "40-43",
    "title": "Classifying named entities in an Alpine heritage corpus",
    "type": "article-journal"
  },
  {
    "abstract": "In the project “Text+Berg” we digitize all yearbooks of the Swiss Alpine Club from 1864 until today. The books comprise articles in German, French and Italian, a total of around 100,000 pages. This paper describes the corpus and the project phases towards its digitalization. We then focus on the classification of named entities, in particular geographic entities. We explore the usefulness of a large list of geographical names that is distributed by the Swiss Federal Office of Topography. A first experiment indicates that the recognition and classification of geographical names remains difficult despite the large gazetteer.",
    "author": [
      {
        "family": "Volk",
        "given": "Martin"
      }
    ],
    "container-title": "Searching answers – festschrift in honour of michael hess on the occasion of his 60th birthday",
    "editor": [
      {
        "family": "Clematide",
        "given": "Simon"
      },
      {
        "family": "Klenner",
        "given": "Manfred"
      },
      {
        "family": "Volk",
        "given": "Martin"
      }
    ],
    "id": "Volk2009b",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "morphology, mxp, nlp",
    "page": "127-140",
    "publisher": "MV-Verlag",
    "publisher-place": "Münster",
    "title": "How many mountains are there in Switzerland? Explorations of the SwissTopo name list",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Volk",
        "given": "Martin"
      },
      {
        "family": "Göhring",
        "given": "Anne"
      },
      {
        "family": "Marek",
        "given": "Torsten"
      }
    ],
    "container-title": "Fourth linguistic annotation workshop (LAW IV)",
    "id": "Volk2010a",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "title": "Combining parallel treebanks and geo-tagging",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2010/pdf/110_Paper.pdf",
    "abstract": "This paper describes our efforts to build a multilingual heritage corpus of alpine texts. Currently we digitize the yearbooks of the Swiss Alpine Club which contain articles in French, German, Italian and Romansch. Articles comprise mountaineering reports from all corners of the earth, but also scientific topics such as topography, geology or glacierology as well as occasional poetry and lyrics. We have already scanned close to 70,000 pages which has resulted in a corpus of 25 million words, 10% of which is a parallel French-German corpus. We have solved a number of challenges in automatic language identification and text structure recognition. Our next goal is to identify the great variety of toponyms (e.g. names of mountains and valleys, glaciers and rivers, trails and cabins) in this corpus, and we sketch how a large gazetteer of Swiss topographical names can be exploited for this purpose. Despite the size of the resource, exact matching leads to a low recall because of spelling variations, language mixtures and partial repetitions.",
    "author": [
      {
        "family": "Volk",
        "given": "Martin"
      },
      {
        "family": "Bubenhofer",
        "given": "Noah"
      },
      {
        "family": "Althaus",
        "given": "Adrian"
      },
      {
        "family": "Bangerter",
        "given": "Maya"
      },
      {
        "family": "Furrer",
        "given": "Lenz"
      },
      {
        "family": "Ruef",
        "given": "Beni"
      }
    ],
    "container-title": "Proceedings of the seventh international conference on language resources and evaluation (LREC’10)",
    "editor": [
      {
        "family": "Calzolari",
        "given": "Nicoletta"
      },
      {
        "family": "Choukri",
        "given": "Khalid"
      },
      {
        "family": "Maegaard",
        "given": "Bente"
      },
      {
        "family": "Mariani",
        "given": "Joseph"
      },
      {
        "family": "Odijk",
        "given": "Jan"
      },
      {
        "family": "Piperidis",
        "given": "Stelios"
      },
      {
        "family": "Rosner",
        "given": "Mike"
      },
      {
        "family": "Tapias",
        "given": "Daniel"
      }
    ],
    "id": "Volk2010b",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, french, german, italian, language_identification, ocr",
    "language": "en-US",
    "page": "1653-1659",
    "publisher": "European Language Resources Association (ELRA)",
    "title": "Challenges in building a multilingual Alpine heritage corpus",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-20227-8_1",
    "ISBN": "978-3-642-20226-1",
    "abstract": "In this paper we describe our efforts in reducing and correcting OCR errors in the context of building a large multilingual heritage corpus of Alpine texts which is based on digitizing the publications of various Alpine clubs. We have already digitized the yearbooks of the Swiss Alpine Club from its start in 1864 until 1995 with more than 75,000 pages resulting in 29 million running words. Since these books have come out continuously, they represent a unique basis for historical, cultural and linguistic research. We used commercial OCR systems for the conversion from the scanned images to searchable text. This poses several challenges. For example, the built-in lexicons of the OCR systems do not cover the 19th century German spelling, the Swiss German spelling variants and the plethora of toponyms that are characteristic of our text genre.We also realized that different OCR systems make different recognition errors. We therefore run two OCR systems over all our scanned pages and merge the output. Merging is especially tricky at spots where both systems result in partially correct word groups. We describe our strategies for reducing OCR errors by enlarging the systems’ lexicons and by two post-correction methods namely merging the output of two OCR systems and auto-correction based on additional lexical resources.",
    "author": [
      {
        "family": "Volk",
        "given": "Martin"
      },
      {
        "family": "Furrer",
        "given": "Lenz"
      },
      {
        "family": "Sennrich",
        "given": "Rico"
      }
    ],
    "chapter-number": "1",
    "collection-title": "Theory and applications of natural language processing",
    "container-title": "Language technology for cultural heritage",
    "editor": [
      {
        "family": "Sporleder",
        "given": "Caroline"
      },
      {
        "family": "Bosch",
        "given": "Antal"
      },
      {
        "family": "Zervanou",
        "given": "Kalliopi"
      }
    ],
    "id": "Volk2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, ocr, spelling_correction",
    "language": "en-US",
    "page": "3-22",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Strategies for reducing and correcting OCR errors",
    "type": "chapter"
  },
  {
    "URL": "http://www.clomedia.com/departments/2003/June/223/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Vollmer",
        "given": "Jennifer"
      }
    ],
    "container-title": "Chief Learning Officer magazine",
    "id": "Vollmer2003",
    "issued": {
      "date-parts": [
        [
          2003,
          6
        ]
      ]
    },
    "title": "Debunking the LCMS myth",
    "type": "article-journal"
  },
  {
    "abstract": "The Text REtrieval Conference (TREC), a yearly workshop hosted by the US government’s National Institute of Standards and Technology, provides the infrastructure necessary for large-scale evaluation of text retrieval methodologies. With the goal of accelerating research in this area, TREC created the first large test collections of full-text documents and standardized retrieval evaluation. The impact has been significant; since TREC’s beginning in 1992, retrieval effectiveness has approximately doubled. TREC has built a variety of large test collections, including collections for such specialized retrieval tasks as cross-language retrieval and retrieval of speech. Moreover, TREC has accelerated the transfer of research ideas into commercial systems, as demonstrated in the number of retrieval techniques developed in TREC that are now used in Web search engines. This book provides a comprehensive review of TREC research, summarizing the variety of TREC results, documenting the best practices in experimental information retrieval, and suggesting areas for further research. The first part of the book describes TREC’s history, test collections, and retrieval methodology. Next, the book provides “track” reports—describing the evaluations of specific tasks, including routing and filtering, interactive retrieval, and retrieving noisy text. The final part of the book offers perspectives on TREC from such participants as Microsoft Research, University of Massachusetts, Cornell University, University of Waterloo, City University of New York, and IBM. The book will be of interest to researchers in information retrieval and related technologies, including natural language processing.",
    "editor": [
      {
        "family": "Voorhees",
        "given": "Ellen M."
      },
      {
        "family": "Harman",
        "given": "Donna K."
      }
    ],
    "id": "Voorhees2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "evaluation, ir",
    "language": "en-US",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "TREC: Experiment and evaluation in information retrieval",
    "title-short": "TREC",
    "type": "book"
  },
  {
    "DOI": "10.1016/j.edurev.2007.02.004",
    "abstract": "The adoption of Course Management Systems (CMSs) for web-based instruction continues to increase in today’s higher education. A CMS is a software program or integrated platform that contains a series of web-based tools to support a number of activities and course management procedures (Severson, 2004). Examples of Course Management Systems are Blackboard, WebCT, eCollege, Moodle, Desire2Learn, Angel, etc. An argument for the adoption of e-learning environments using CMSs is the flexibility of such environments when reaching out to potential learners in remote areas where brick and mortar institutions are non-existent. It is also believed that e-learning environments can have potential added learning benefits and can improve students’ and educators’ self-regulation skills, in particular their metacognitive skills. In spite of this potential to improve learning by means of using a CMS for the delivery of e-learning, the features and functionalities that have been built into these systems are often underutilized. As a consequence, the created learning environments in CMSs do not adequately scaffold learners to improve their self-regulation skills. In order to support the improvement of both the learners’ subject matter knowledge and learning strategy application, the e-learning environments within CMSs should be designed to address learners’ diversity in terms of learning styles, prior knowledge, culture, and self-regulation skills. Self-regulative learners are learners who can demonstrate “personal initiative, perseverance and adaptive skill in pursuing learning” (Zimmerman, 2002). Self-regulation requires adequate monitoring strategies and metacognitive skills. The created e-learning environments should encourage the application of learners’ metacognitive skills by prompting learners to plan, attend to relevant content, and monitor and evaluate their learning. This position paper sets out to inform policy makers, educators, researchers, and others of the importance of a metacognitive e-learning approach when designing instruction using Course Management Systems. Such a metacognitive approach will improve the utilization of CMSs to support learners on their path to self-regulation. We argue that a powerful CMS incorporates features and functionalities that can provide extensive scaffolding to learners and support them in becoming self-regulated learners. Finally, we believe that extensive training and support is essential if educators are expected to develop and implement CMSs as powerful learning tools.",
    "author": [
      {
        "family": "Vovides",
        "given": "Yianna"
      },
      {
        "family": "Sanchez-Alonso",
        "given": "Salvador"
      },
      {
        "family": "Mitropoulou",
        "given": "Vasiliki"
      },
      {
        "family": "Nickmans",
        "given": "Goele"
      }
    ],
    "container-title": "Educational Research Review",
    "id": "Vovides2007",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "e-learning, pedagogy",
    "language": "en-US",
    "page": "64-74",
    "title": "The use of e-learning course management systems to support learning strategies and to improve self-regulated learning",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "ISBN": "978-1-932432-88-6",
    "URL": "http://portal.acm.org/citation.cfm?id=2002736.2002832",
    "abstract": "A topic model outputs a set of multinomial distributions over words for each topic. In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources. Experiments on a document-aligned English-Italian Wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space. The best results, obtained by combining knowledge from word-topic distributions with similarity measures in the original space, are also reported.",
    "author": [
      {
        "family": "Vulić",
        "given": "Ivan"
      },
      {
        "family": "De Smet",
        "given": "Wim"
      },
      {
        "family": "Moens",
        "given": "Marie F."
      }
    ],
    "container-title": "Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies (HLT ’11)",
    "id": "Vulic2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "topic_modeling, wikipedia",
    "language": "en-US",
    "page": "479-484",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Identifying word translations from comparable corpora using latent topic models",
    "type": "paper-conference",
    "volume": "2"
  },
  {
    "URL": "http://www.w3.org/TR/CSS2/",
    "accessed": {
      "date-parts": [
        [
          2011,
          4,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "World Wide Web Consortium"
      }
    ],
    "id": "W3CCSS",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "title": "Cascading Style Sheets Level 2 Revision 1 (CSS 2.1) Specification",
    "type": ""
  },
  {
    "URL": "http://www.w3.org/TR/DOM-Level-2-Core/",
    "accessed": {
      "date-parts": [
        [
          2011,
          4,
          6
        ]
      ]
    },
    "author": [
      {
        "literal": "World Wide Web Consortium"
      }
    ],
    "id": "W3CDOM",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "title": "Document Object Model (DOM) Level 2 Core Specification",
    "type": ""
  },
  {
    "URL": "https://www.w3.org/TR/owl2-overview/",
    "author": [
      {
        "literal": "W3C OWL Working Group"
      }
    ],
    "genre": "{W3C Recommendation}",
    "id": "W3COWL",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "publisher": "World Wide Web Consortium",
    "title": "OWL 2 web ontology language document overview",
    "type": "report"
  },
  {
    "URL": "http://www.w3.org/TR/rdf-concepts/",
    "abstract": "The Resource Description Framework (RDF) is a framework for representing information in the Web. RDF Concepts and Abstract Syntax defines an abstract syntax on which RDF is based, and which serves to link its concrete syntax to its formal semantics. It also includes discussion of design goals, key concepts, datatyping, character normalization and handling of URI references.",
    "editor": [
      {
        "family": "Klyne",
        "given": "Graham"
      },
      {
        "family": "Carroll",
        "given": "Jeremy J."
      }
    ],
    "genre": "{W3C Recommendation}",
    "id": "W3CRDF",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "rdf",
    "language": "en-US",
    "publisher": "World Wide Web Consortium",
    "title": "Resource Description Framework (RDF): Concepts and abstract syntax",
    "title-short": "Resource Description Framework (RDF)",
    "type": "report"
  },
  {
    "URL": "http://www.w3.org/TR/skos-reference",
    "abstract": "This document defines the Simple Knowledge Organization System (SKOS), a common data model for sharing and linking knowledge organization systems via the Web. Many knowledge organization systems, such as thesauri, taxonomies, classification schemes and subject heading systems, share a similar structure, and are used in similar applications. SKOS captures much of this similarity and makes it explicit, to enable data and technology sharing across diverse applications. The SKOS data model provides a standard, low-cost migration path for porting existing knowledge organization systems to the Semantic Web. SKOS also provides a lightweight, intuitive language for developing and sharing new knowledge organization systems. It may be used on its own, or in combination with formal knowledge representation languages such as the Web Ontology language (OWL). This document is the normative specification of the Simple Knowledge Organization System. It is intended for readers who are involved in the design and implementation of information systems, and who already have a good understanding of Semantic Web technology, especially RDF and OWL.",
    "author": [
      {
        "family": "Miles",
        "given": "Alistair"
      },
      {
        "family": "Bechhofer",
        "given": "Sean"
      }
    ],
    "genre": "{W3C Recommendation}",
    "id": "W3CSKOS",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "publisher": "World Wide Web Consortium",
    "title": "SKOS Simple Knowledge Organization System Reference",
    "type": "report"
  },
  {
    "URL": "http://www.w3.org/TR/soap/",
    "accessed": {
      "date-parts": [
        [
          2011,
          3,
          30
        ]
      ]
    },
    "author": [
      {
        "literal": "World Wide Web Consortium"
      }
    ],
    "id": "W3CSOAP",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "title": "Simple Object Access Protocol (SOAP)",
    "type": ""
  },
  {
    "URL": "http://www.w3.org/TR/sparql11-query/",
    "author": [
      {
        "family": "Harris",
        "given": "Steve"
      },
      {
        "family": "Seaborne",
        "given": "Andy"
      }
    ],
    "genre": "{W3C Recommendation}",
    "id": "W3CSPARQL",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "publisher": "World Wide Web Consortium",
    "title": "SPARQL 1.1 Query Language",
    "type": "report"
  },
  {
    "URL": "http://www.w3.org/TR/xml/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "World Wide Web Consortium"
      }
    ],
    "edition": "4",
    "id": "W3CXML",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "title": "Extensible Markup Language (XML)",
    "type": ""
  },
  {
    "URL": "http://www.w3.org/TR/1999/REC-xml-names-19990114/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "World Wide Web Consortium"
      }
    ],
    "id": "W3CXMLNS",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "title": "Namespaces in XML",
    "type": ""
  },
  {
    "URL": "http://www.w3.org/TR/xmlschema-1/",
    "accessed": {
      "date-parts": [
        [
          2011,
          4,
          17
        ]
      ]
    },
    "author": [
      {
        "literal": "World Wide Web Consortium"
      }
    ],
    "id": "W3CXMLSchema",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "title": "XML Schema Part 1: Structures",
    "type": ""
  },
  {
    "URL": "http://www.w3.org/TR/xsl/",
    "accessed": {
      "date-parts": [
        [
          2011,
          4,
          1
        ]
      ]
    },
    "author": [
      {
        "literal": "World Wide Web Consortium"
      }
    ],
    "id": "W3CXSL",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "title": "Extensible Stylesheet Language 1.1",
    "type": ""
  },
  {
    "URL": "http://www.cs.cityu.edu.hk/~wbl2007/WBL2007_Proceedings_HTML/WBL2007_Proceedings.pdf",
    "abstract": "Since 2003 we have successively introduced the use of e-learning and computer-assisted assessment (CAA) components into all of our courses, namely online multiple-choice tests, electronic submission of assignments, and automatic testing of programs. We originally did not intend to make major changes to the courses; our primary motivation was just to make them more efficient and more effective by freeing teachers from administrative burdens and by offering more flexibility and interactivity for students. After several semesters of usage we have noticed, however, that the courses have changed much more radically than originally envisaged. The electronic support of face-to-face courses offers many new possibilities, but it also opens up new questions. This paper describes our system and our experience, and discusses some of the questions we have encountered.",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "container-title": "Proceedings of workshop on blended learning 2007",
    "editor": [
      {
        "family": "Fong",
        "given": "Joseph"
      },
      {
        "family": "Wang",
        "given": "Fu L."
      }
    ],
    "id": "WBL2007",
    "issued": {
      "date-parts": [
        [
          2007,
          8
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "language": "en-US",
    "page": "257-266",
    "publisher": "online",
    "publisher-place": "Edinburgh, U.K.",
    "title": "Large-scale computer-assisted assessment in computer science education: New possibilities, new questions",
    "title-short": "Large-scale computer-assisted assessment in computer science education",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/321796.321811",
    "ISSN": "0004-5411",
    "abstract": "The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of “edit operations” needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings.",
    "author": [
      {
        "family": "Wagner",
        "given": "Robert A."
      },
      {
        "family": "Fischer",
        "given": "Michael J."
      }
    ],
    "container-title": "Journal of the ACM",
    "id": "Wagner1974",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1974
        ]
      ]
    },
    "keyword": "approximate_matching, spelling_correction",
    "language": "en-US",
    "page": "168-173",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The String-to-String correction problem",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "author": [
      {
        "family": "Waldrop",
        "given": "Mitchell M."
      }
    ],
    "id": "Waldrop2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Penguin",
    "title": "The dream machine: J.C.R. Licklider and the revolution that made computing personal",
    "title-short": "The dream machine",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Walker",
        "given": "Sue"
      }
    ],
    "collection-title": "Language in social life",
    "id": "Walker2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "document_research, typography",
    "language": "en-US",
    "publisher": "Paperback; Longman",
    "publisher-place": "Harlow, UK",
    "title": "Typography and language in everyday life",
    "type": "book"
  },
  {
    "DOI": "10.1076/iaij.4.1.5.16466",
    "ISSN": "1389-5176",
    "abstract": "The aim of this paper is to provide a conceptual basis for the systematic treatment of uncertainty in model-based decision support activities such as policy analysis, integrated assessment and risk assessment. It focuses on the uncertainty perceived from the point of view of those providing information to support policy decisions (i.e., the modellers? view on uncertainty) ? uncertainty regarding the analytical outcomes and conclusions of the decision support exercise. Within the regulatory and management sciences, there is neither commonly shared terminology nor full agreement on a typology of uncertainties. Our aim is to synthesise a wide variety of contributions on uncertainty in model-based decision support in order to provide an interdisciplinary theoretical framework for systematic uncertainty analysis. To that end we adopt a general definition of uncertainty as being any deviation from the unachievable ideal of completely deterministic knowledge of the relevant system. We further propose to discriminate among three dimensions of uncertainty: location, level and nature of uncertainty, and we harmonise existing typologies to further detail the concepts behind these three dimensions of uncertainty. We propose an uncertainty matrix as a heuristic tool to classify and report the various dimensions of uncertainty, thereby providing a conceptual framework for better communication among analysts as well as between them and policymakers and stakeholders. Understanding the various dimensions of uncertainty helps in identifying, articulating, and prioritising critical uncertainties, which is a crucial step to more adequate acknowledgement and treatment of uncertainty in decision support endeavours and more focused research on complex, inherently uncertain, policy issues.",
    "author": [
      {
        "family": "Walker",
        "given": "W. E."
      },
      {
        "family": "Harremoës",
        "given": "P."
      },
      {
        "family": "Rotmans",
        "given": "J."
      },
      {
        "dropping-particle": "van der",
        "family": "Sluijs",
        "given": "J. P."
      },
      {
        "dropping-particle": "van",
        "family": "Asselt",
        "given": "M. B. A."
      },
      {
        "family": "Janssen",
        "given": "P."
      },
      {
        "dropping-particle": "Krayer von",
        "family": "Krauss",
        "given": "M. P."
      }
    ],
    "container-title": "Integrated Assessment",
    "id": "Walker2003",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "philosophy_of_science, uncertainty",
    "language": "en-US",
    "page": "5-17",
    "title": "Defining uncertainty: A conceptual basis for uncertainty management in Model-Based decision support",
    "title-short": "Defining uncertainty",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "URL": "http://er.educause.edu/articles/2014/10/the-virtual-pauls-cross-project-digital-modelings-uneasy-approximations",
    "abstract": "Multimedia modeling has begun to fulfill digital humanities’ promise of providing what David Berry deemed a \"new way of working with representation and mediation,\" enabling us \"to approach culture in a radically new way.\" One aspect of this \"new way\" of working is the restoration of multisensory and real-time experience. Using this capability transforms our understanding of the past from the print or static word to the word spoken and heard, the word performed, the word as interchange, situated in the specifics of a particular time and place. As we begin to re-create, and therefore in some sense re-experience, cultural events as they unfolded in real time, digital modeling engages us in an approximation of the event as it occurred in the past. The Virtual Paul’s Cross Project takes viewers back to experience John Donne preaching his sermon for Guy Fawkes Day on November 5, 1622. More accurately, the virtual environment allows us to experience as close an approximation as we can create of the event given the technology available today — and the limits of our knowledge.",
    "author": [
      {
        "family": "Wall",
        "given": "John N."
      }
    ],
    "container-title": "EDUCAUSE Review Online",
    "id": "Wall2014",
    "issued": {
      "date-parts": [
        [
          2014,
          10
        ]
      ]
    },
    "keyword": "digital_humanities, simulation",
    "language": "en-US",
    "title": "The Virtual Paul’s Cross Project: Digital modeling’s uneasy approximations",
    "title-short": "The Virtual Paul’s Cross Project",
    "type": "article-journal"
  },
  {
    "DOI": "10.1145/3132698",
    "ISSN": "0001-0782",
    "abstract": "This viewpoint is about differences between computer science and social science, and their implications for computational social science. Spoiler alert: The punchline is simple. Despite all the hype, machine learning is not a be-all and end-all solution. We still need social scientists if we are going to use machine learning to study social phenomena in a responsible and ethical manner.",
    "author": [
      {
        "family": "Wallach",
        "given": "Hanna"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Wallach2018",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "keyword": "formal_models, social_science",
    "language": "en-US",
    "page": "42-44",
    "title": "Computational social science ≠ computer science + social data",
    "type": "article-journal",
    "volume": "61"
  },
  {
    "abstract": "A case is made for the inclusion of graphic and spatial factors in the linguistic analysis of text, and in common rules and guidelines for clear writing. Some conceptual problems are considered and a parallel is drawn between the roles of punctuation and typography at the micro- and macro-levels of texts. The gradual codification of punctuation, from the original vernacular through an elocutionary or stylistic role to a well-specified syntactic role, is suggested as an indication of a direction for future typographic analysis.",
    "author": [
      {
        "family": "Waller",
        "given": "Robert H. W."
      }
    ],
    "container-title": "Processing of visible language",
    "editor": [
      {
        "family": "Kolers",
        "given": "Paul A."
      },
      {
        "family": "Wrolstad",
        "given": "Merald E."
      },
      {
        "family": "Bouma",
        "given": "Herman"
      }
    ],
    "id": "Waller1980",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "keyword": "document_research, typography, visual_languages",
    "language": "en-US",
    "page": "241-253",
    "publisher": "Plenum Press",
    "publisher-place": "New York, NY, USA",
    "title": "Graphic aspects of complex texts: Typography as macropunctuation",
    "title-short": "Graphic aspects of complex texts",
    "type": "chapter",
    "volume": "2"
  },
  {
    "author": [
      {
        "family": "Waller",
        "given": "Robert H. W."
      }
    ],
    "container-title": "The technology of text",
    "editor": [
      {
        "family": "Jonassen",
        "given": "David H."
      }
    ],
    "id": "Waller1985",
    "issued": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "keyword": "document_research, layout, visual_languages",
    "language": "en-US",
    "publisher": "Educational Technology Publications",
    "publisher-place": "Englewood Cliffs, NJ, USA",
    "title": "Using typography to structure arguments: A critical analysis of some examples",
    "title-short": "Using typography to structure arguments",
    "type": "chapter",
    "volume": "2"
  },
  {
    "URL": "http://www.robwaller.org/RobWaller_thesis87.pdf",
    "abstract": "A trawl through a range of relevant disciplines in search of an explanation of the role of typography in language. Concludes with a genre model.",
    "author": [
      {
        "family": "Waller",
        "given": "Robert H. W."
      }
    ],
    "genre": "PhD thesis",
    "id": "Waller1987",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "keyword": "document_analysis, document_research, layout, typography, visual_languages",
    "language": "en-US",
    "publisher": "University of Reading, Department of Typography & Graphic Communication",
    "publisher-place": "Reading, UK",
    "title": "The typographic contribution to language",
    "type": "thesis"
  },
  {
    "abstract": "These working notes summarise a genre theory that accounts for document layout in a three part communication model that recognises not only the effort of the writer to set out a topic and the purposeful effort by a reader to access information, but also the professional and manufacturing processes that intervene. I suggest that layout genres use conventions that at some historical point are rooted in functionality (of document generation, manufacture or use) but which have become conventionalised. It follows that genres will shift over time as reasons to generate or access information change, and as text technologies develop. Case studies are described that illustrate key aspects of the model and offer insight into the way designers think about layout.",
    "author": [
      {
        "family": "Waller",
        "given": "Robert H. W."
      }
    ],
    "container-title": "Proceedings of the 1999 autumn symposium",
    "id": "Waller1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "document_research, layout, typography",
    "language": "en-US",
    "publisher": "American Association for Artificial Intelligence",
    "title": "Making connections: Typography, layout and language",
    "title-short": "Making connections",
    "type": "paper-conference"
  },
  {
    "URL": "http://oasis-open.org/docbook/documentation/reference/",
    "author": [
      {
        "family": "Walsh",
        "given": "Norman"
      },
      {
        "family": "Muellner",
        "given": "Leonard"
      }
    ],
    "id": "Walsh1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "publisher": "O’Reilly",
    "publisher-place": "Sebastopol, CA",
    "title": "DocBook: The definitive guide",
    "title-short": "DocBook",
    "type": "book"
  },
  {
    "URL": "http://journals.tdl.org/jodi/article/view/79/78",
    "accessed": {
      "date-parts": [
        [
          2011,
          4,
          15
        ]
      ]
    },
    "author": [
      {
        "family": "Walsh",
        "given": "Norman"
      }
    ],
    "container-title": "Journal of Digital Information",
    "id": "Walsh2002",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "document_research, xml",
    "language": "en-US",
    "title": "XML: One input – many outputs: A response to Hillesund",
    "title-short": "XML",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "URL": "http://docbook.org/tdg5/",
    "author": [
      {
        "family": "Walsh",
        "given": "Norman"
      },
      {
        "family": "Muellner",
        "given": "Leonard"
      }
    ],
    "id": "Walsh2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "publisher": "O’Reilly",
    "publisher-place": "Sebastopol, CA",
    "title": "DocBook 5: The definitive guide",
    "title-short": "DocBook 5",
    "type": "book"
  },
  {
    "DOI": "10.1093/llc/fqr038",
    "ISSN": "1477-4615",
    "abstract": "The Chymistry of Isaac Newton project, an online scholarly edition of Newton’s alchemical manuscripts, has engaged in a process to include a number of core alchemical symbols into the Unicode standard, a standard for digital representation of characters and symbols from the world’s languages, scripts, and writing systems. Our article explores the relationship between information technology standardization and humanities research. We discuss Newton’s engagement with alchemy and explore the graphic dimensions of alchemical discourse. We illustrate this discussion with examples of Newton’s use of alchemical symbols. We examine Unicode itself, particularly a core Unicode principle distinguishing between the abstract character and the image or glyph of the character, and we discuss the tensions between this core principle and the representation of graphic, symbolic, and pictorial discourse. We describe our experience with the Unicode proposal process and illustrate again—this time with an organizational scheme for the symbols—how the technical standardization process forced a reexamination of our historical materials. Our conclusions reemphasize the potential for mutually beneficial relationships between certain types of information technology standardization and humanities research and suggest that study of the graphic qualities of alchemical discourse, especially in light of competing theories of text represented by standards like Unicode, may contribute to our understanding of the increasingly graphic, iconic, and pictorial nature of information and communication.",
    "author": [
      {
        "family": "Walsh",
        "given": "John A."
      },
      {
        "family": "Hooper",
        "given": "Wallace E."
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Walsh2012",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "55-79",
    "title": "The liberty of invention: Alchemical discourse and information technology standardization",
    "title-short": "The liberty of invention",
    "type": "article-journal",
    "volume": "27"
  },
  {
    "DOI": "10.1109/HICSS.2004.1265643",
    "abstract": "Object-Oriented Architecture (OOA), Component-Based Architecture (CBA), and Service-Based Architecture (SBA) represent three technical architecture paradigms in current software systems. Object, Component, and Service are three key concepts in distributed software systems. From implementation point of view, a service is implemented by one or more components, which in turn are often implemented in object-oriented programming languages like C++ and Java. Distributed component-based software systems can be structured in any of the architecture paradigms, some have more advantages than others depending on business requirements. Understanding the characteristics, features, benefits, and concerns of the architecture paradigms is crucial to the successful design, implementation and operation of a distributed system. In this paper, we describe the characteristics of the three architecture paradigms and the business drivers for their applications. The parallel evolution of architecture paradigms and software development methodologies is discussed in the context of practical business needs. Component-based software developers for distributed systems should decide on the architecture paradigms based on business requirements. The evolution of architecture paradigms and the selection of architecture paradigms have profound influences and impacts on component-based distributed systems, in the way components are designed and in the way component interactions are implemented. We discuss these influences and impacts with the goal of deriving some practical principles and strategies to best deal with them in software engineering practices.",
    "author": [
      {
        "family": "Wang",
        "given": "Guijun"
      },
      {
        "family": "Fung",
        "given": "Casey K."
      }
    ],
    "container-title": "Proceedings of the 37<sup>th</sup> hawaii international conference on system sciences",
    "id": "Wang2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "soa, software_components",
    "language": "en-US",
    "publisher": "IEEE Computer Society",
    "publisher-place": "Los Alamitos, CA, USA",
    "title": "Architecture paradigms and their influences and impacts on component-based software systems",
    "type": "paper-conference",
    "volume": "9"
  },
  {
    "DOI": "10.1145/1743384.1743441",
    "abstract": "Concept ontology has been used in the area of artificial intelligence, biomedical informatics and library science and it has been shown as an effective approach to better understand data in the respective domains. One main difficulty that hedge against the development of ontology approaches is the extra work required in ontology construction and annotation. With the emergent lexical dictionaries and encyclopedias such as WordNet, Wikipedia, innovations from different directions have been proposed to automatically extract concept ontologies. Unfortunately, many of the proposed ontologies are not fully exploited according to the general human knowledge. We study the various knowledge sources and aim to build a construct scalable concept thesaurus suitable for better understanding of media in the World Wide Web from Wikipedia. With its wide concept coverage, finely organized categories, diverse concept relations, and up-to-date information, the collaborative encyclopedia Wikipedia has almost all the requisite attributes to contribute to a well-defined concept ontology. Besides the explicit concept relations such as disambiguation, synonymy, Wikipedia also provides implicit concept relations through cross-references between articles. In our previous work, we have built ontology with explicit relations from Wikipedia page contents. Even though the method works, mining explicit semantic relations from every Wikipedia concept page content has unsolved scalable issue when more concepts are involved. This paper describes our attempt to automatically build a concept thesaurus, which encodes both explicit and implicit semantic relations for a large-scale of concepts from Wikipedia. Our proposed thesaurus construction takes advantage of both structure and content features of the downloaded Wikipedia database, and defines concept entries with its related concepts and relations. This thesaurus is further used to exploit semantics from web page context to build a more semantic meaningful space. We move a step forward to combine the similarity distance from the image feature space to boost the performance. We evaluate our approach through application of the constructed concept thesaurus to web image retrieval. The results show that it is possible to use implicit semantic relations to improve the retrieval performance.",
    "author": [
      {
        "family": "Wang",
        "given": "Huan"
      },
      {
        "family": "Chia",
        "given": "Liang-Tien"
      },
      {
        "family": "Gao",
        "given": "Shenghua"
      }
    ],
    "container-title": "Proceedings of the international conference on multimedia information retrieval (MIR ’10)",
    "id": "Wang2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "ir, topic_modeling",
    "language": "en-US",
    "page": "349+",
    "title": "Wikipedia-assisted concept thesaurus for better web media understanding",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/dexa.2008.120",
    "ISBN": "978-0-7695-3299-8",
    "ISSN": "1529-4188",
    "abstract": "We consider topic detection without any prior knowledge of category structure or possible categories. Keywords are extracted and clustered based on different similarity measures using the induced k-bisecting clustering algorithm. Evaluation on Wikipedia articles shows that clusters of keywords correlate strongly with the Wikipedia categories of the articles. In addition, we find that a distance measure based on the Jensen-Shannon divergence of probability distributions outperforms the cosine similarity. In particular, a newly proposed term distribution taking co-occurrence of terms into account gives best results.",
    "author": [
      {
        "family": "Wartena",
        "given": "Christian"
      },
      {
        "family": "Brussee",
        "given": "Rogier"
      }
    ],
    "container-title": "19th international workshop on database and expert systems application, 2008 (DEXA ’08)",
    "id": "Wartena2008",
    "issued": {
      "date-parts": [
        [
          2008,
          9
        ]
      ]
    },
    "keyword": "clustering, topic_modeling",
    "language": "en-US",
    "page": "54-58",
    "publisher": "IEEE",
    "title": "Topic detection by clustering keywords",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1002/9781118680605.ch37",
    "author": [
      {
        "family": "Warwick",
        "given": "Clair"
      }
    ],
    "chapter-number": "37",
    "container-title": "A new companion to digital humanities",
    "editor": [
      {
        "family": "Schreibman",
        "given": "Susan"
      },
      {
        "family": "Siemens",
        "given": "Ray"
      },
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "id": "Warwick2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "538-552",
    "publisher": "Wiley",
    "publisher-place": "Chichester",
    "title": "Building theories or theories of building? A tension at the heart of digital humanities",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/11780991_16",
    "abstract": "In recent years, there has been an increased need for the use of active systems – systems that include substantial processing which should be triggered by events. In many cases, however, there is an information gap between the actual occurrences of events to which such a system must respond, and the data generated by monitoring tools regarding these events. For example, some events, by their very nature, may not be signaled by any monitoring tools, or the inaccuracy of monitoring tools may incorrectly reflect the information associated with events. The result is that in many cases, there is uncertainty in the active system associated with event occurrence. In this paper, we provide a taxonomy of the sources of this uncertainty. Furthermore, we provide a formal way to represent this uncertainty, which is the first step towards addressing the aforementioned information gap.",
    "author": [
      {
        "family": "Wasserkrug",
        "given": "Segev"
      },
      {
        "family": "Gal",
        "given": "Avigdor"
      },
      {
        "family": "Etzion",
        "given": "Opher"
      }
    ],
    "collection-title": "Lecture notes in computer science",
    "container-title": "Next generation information technologies and systems (proceedings of NGITS 2006)",
    "editor": [
      {
        "family": "Etzion",
        "given": "Opher"
      },
      {
        "family": "Kuflik",
        "given": "Tsvi"
      },
      {
        "family": "Motro",
        "given": "Amihai"
      }
    ],
    "id": "Wasserkrug2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "174-185",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "A taxonomy and representation of sources of uncertainty in active systems",
    "type": "paper-conference",
    "volume": "4032"
  },
  {
    "URL": "http://dhdebates.gc.cuny.edu/debates/text/79",
    "abstract": "There has been much discussion about “the big tent” as the metaphor that defines and delineates the boundaries of the digital humanities. In some cases, such as at the University College London Centre for Digital Humanities (Warwick et al.), the “big tent” is framed quite broadly, defined not by traditional disciplinary boundaries but by practice. Kathleen Fitzpatrick,1 on the other hand, defines the “big tent” as “a nexus of fields within which scholars use computing technologies to investigate the kinds of questions that are traditional to the humanities, or, as is more true of [her] own work, who ask traditional kinds of humanities-oriented questions about computing technologies.” Whatever the perspective on the “big tent,” the metaphor has inevitably led to debate as to who is in this “tent” and who is not.",
    "author": [
      {
        "family": "Watrall",
        "given": "Ethan"
      }
    ],
    "container-title": "Debates in the digital humanities 2016",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      },
      {
        "family": "Klein",
        "given": "Lauren F."
      }
    ],
    "id": "Watrall2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "archeology, digital_humanities",
    "language": "en-US",
    "page": "345-358",
    "publisher": "University of Minnesota Press",
    "publisher-place": "Minneapolis, MN, USA",
    "title": "Archaeology, the digital humanities, and the “big tent”",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Weber",
        "given": "Max"
      }
    ],
    "container-title": "Gesammelte Aufsätze zur Wissenschaftslehre",
    "id": "Weber1904-1922",
    "issued": {
      "date-parts": [
        [
          1922
        ]
      ]
    },
    "language": "de-DE",
    "page": "146-214",
    "publisher": "Mohr",
    "publisher-place": "Tübingen",
    "title": "Die “Objektivität” sozialwissenschaftlicher und sozialpolitischer Erkenntnis",
    "type": "chapter"
  },
  {
    "URL": "https://search.proquest.com/docview/226976526",
    "abstract": "Public administration is a diverse and complex discipline which finds itself awashed in an abundance of paradigms which have led to multiple theories, models, and techniques, few of which ever originated within public administration. If public administration is truly to begin understanding decision-making, organizational behavior, the political and social environments or any of the other areas within which it operates, it needs to understand uncertainty. A general theory of uncertainty is needed which can serve as the basis for research into the influence of uncertainty within specialized areas.",
    "author": [
      {
        "family": "Weber",
        "given": "Jeffrey A."
      }
    ],
    "container-title": "Public Administration Quarterly",
    "id": "Weber1999",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "18-45",
    "title": "A response to public administration’s lack of a general theory of uncertainty: A theoretical vision of uncertainty",
    "title-short": "A response to public administration’s lack of a general theory of uncertainty",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.1007/s002870050104",
    "ISSN": "1432-122X",
    "author": [
      {
        "family": "Wedekind",
        "given": "Hartmut"
      },
      {
        "family": "Görz",
        "given": "Günter"
      },
      {
        "family": "Kötter",
        "given": "Rudolf"
      },
      {
        "family": "Inhetveen",
        "given": "Rüdiger"
      }
    ],
    "container-title": "Informatik-Spektrum",
    "id": "Wedekind1998",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          1998,
          10
        ]
      ]
    },
    "language": "de-DE",
    "page": "265-272",
    "title": "Modellierung, Simulation, Visualisierung: Zu aktuellen Aufgaben der Informatik",
    "title-short": "Modellierung, Simulation, Visualisierung",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "DOI": "10.1109/MIC.2002.1003134",
    "abstract": "To provide a consistent experience for customers, most Web site developers must integrate existing content with new content, server-side applications, and Web-based services. Increasingly, even basic sites are becoming more like traditional portals-a single, integrated point of access to information, applications, and people. Portals integrate diverse interaction channels at a central point, providing comprehensive context and an aggregated view across all information. Portals are largely based on existing Web application technology, such as Web servers and Java 2 Platform Enterprise Edition (J2EE). I present an overview of portal types and services, followed by a more detailed examination of portal-specific components and architectures",
    "author": [
      {
        "family": "Wege",
        "given": "Christian"
      }
    ],
    "container-title": "IEEE Internet Computing",
    "id": "Wege2002",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "cms",
    "language": "en-US",
    "page": "73-77",
    "title": "Portal server technology",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "author": [
      {
        "family": "Weick",
        "given": "Karl E."
      }
    ],
    "container-title": "Administrative Science Quarterly",
    "id": "Weick1976",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1976,
          3
        ]
      ]
    },
    "page": "1-19",
    "title": "Educational organizations as loosely coupled systems",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "author": [
      {
        "family": "Weicker",
        "given": "Nicole"
      },
      {
        "family": "Weicker",
        "given": "Karsten"
      }
    ],
    "container-title": "DeLFI2005: 3. Deutsche e-learning fachtagung informatik der gesellschaft für informatik e.v.",
    "edition": "Volume P-66",
    "editor": [
      {
        "family": "Haake",
        "given": "Jörg M."
      },
      {
        "family": "Lucke",
        "given": "Ulrike"
      },
      {
        "family": "Tavangarian",
        "given": "Djamshid"
      }
    ],
    "id": "Weicker2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "page": "435-446",
    "publisher": "GI-Verlag",
    "publisher-place": "Bonn",
    "title": "Didaktische Anmerkungen zur Unterstützung der Programmierlehre durch E-Learning",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/BF00993121",
    "abstract": "The following criteria were used to document program enhancement after the implementation of a microcomputer laboratory: faculty and student attitudes toward computer-assisted instruction (CAI); student anxiety scores toward state board examinations; increased visibility of the college (number of authored CAI modules, CAI grants, computer committee memberships, faculty attendance at computer courses); and relationship involving learning style, attitude, and student learning. Ninety-two of 112 students and 39 of 55 faculty responded to the request for data collection. Postimplementation, both faculty and students showed mean increases in their attitude toward CA1, although not statistically significant. Senior students had significantly decreased anxiety scores toward the state RN licensure examination after using Mosby’s NURSESTAR for review (p < .001). Increased visibility was also documented, but data collection continues regarding the relationship involving learning style, attitude, and student learning. It was concluded that the innovation of computers in an established nursing curriculum served to enhance program excellence.",
    "author": [
      {
        "family": "Weiner",
        "given": "Elizabeth E."
      },
      {
        "family": "Emerson",
        "given": "Lou Ann"
      }
    ],
    "container-title": "Journal of Medical Systems",
    "id": "Weiner1986",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1986,
          4
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "language": "en-US",
    "page": "151-162",
    "publisher": "Plenum Press",
    "publisher-place": "New York, NY, USA",
    "title": "The impact of microcomputers on program excellence",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "ISSN": "0891-2017",
    "URL": "http://portal.acm.org/citation.cfm?id=972442",
    "abstract": "All natural language systems are likely to receive inputs for which they are unprepared. The system must be able to respond to such inputs by explicitly indicating the reasons the input could not be understood, so that the user will have precise information for trying to rephrase the input. If natural language communication to data bases, to expert consultant systems, or to any other practical system is to be accepted by other than computer personnel, this is an absolute necessity. This paper presents several ideas for dealing with parts of this broad problem. One is the use of presupposition to detect user assumptions. The second is relaxation of tests while parsing. The third is a general technique for responding intelligently when no parse can be found. All of these ideas have been implemented and tested in one of two natural language systems. Some of the ideas are heuristics that might be employed by humans; others are engineering solutions for the problem of practical natural language systems.",
    "author": [
      {
        "family": "Weischedel",
        "given": "Ralph M."
      },
      {
        "family": "Black",
        "given": "John E."
      }
    ],
    "container-title": "Computational Linguistics",
    "id": "Weischedel1980",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "keyword": "interactive_parsing",
    "language": "en-US",
    "page": "97-109",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Responding intelligently to unparsable inputs",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "URL": "https://www.chronicle.com/article/There-Is-No-Such-Thing-as/241633",
    "abstract": "Timothy Brennan’s \"The Digital-Humanities Bust\" is the latest in a long line of essays criticizing a new field or approach known as \"the digital humanities.\" \"What exactly have the digital humanities accomplished?\" Brennan wants to know. He concludes that \"the digital humanities is a wedge separating the humanities from its reason to exist — namely, to think against prevailing norms.\" The problem with these hit pieces is that they are swinging at air. There is no such thing as \"the digital humanities.\"",
    "author": [
      {
        "family": "Weiskott",
        "given": "Eric"
      }
    ],
    "container-title": "The Chronicle of Higher Education",
    "id": "Weiskott2017",
    "issued": {
      "date-parts": [
        [
          2017,
          11,
          1
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "title": "There is no such thing as the ",
    "type": "article-journal"
  },
  {
    "DOI": "10.1145/365153.365168",
    "ISSN": "0001-0782",
    "abstract": "ELIZA is a program operating within the MAC time-sharing system at MIT which makes certain kinds of natural language conversation between man and computer possible. Input sentences are analyzed on the basis of decomposition rules which are triggered by key words appearing in the input text. Responses are generated by reassembly rules associated with selected decomposition rules. The fundamental technical problems with which ELIZA is concerned are: (1) the identification of key words, (2) the discovery of minimal context, (3) the choice of appropriate transformations, (4) generation of responses in the absence of key words, and (5) the provision of an editing capability for ELIZA \"scripts\". A discussion of some psychological issues relevant to the ELIZA approach as well as of future developments concludes the paper.",
    "author": [
      {
        "family": "Weizenbaum",
        "given": "Joseph"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Weizenbaum1966",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1966
        ]
      ]
    },
    "keyword": "classic, computational_linguistics",
    "language": "en-US",
    "page": "36-45",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "ELIZA—a computer program for the study of natural language communication between man and machine",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "author": [
      {
        "family": "Weizenbaum",
        "given": "Joseph"
      }
    ],
    "id": "Weizenbaum1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "language": "en-US",
    "publisher": "W.H. Freeman",
    "publisher-place": "San Francisco, CA, USA",
    "title": "Computer power and human reason",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Weizenbaum",
        "given": "Joseph"
      }
    ],
    "container-title": "Computer power and human reason",
    "id": "Weizenbaum1976-intro",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "keyword": "classic, hci",
    "language": "en-US",
    "page": "1-16",
    "publisher": "W.H. Freeman",
    "publisher-place": "San Francisco, CA, USA",
    "title": "Introduction",
    "type": "chapter"
  },
  {
    "ISBN": "0140225358",
    "author": [
      {
        "family": "Weizenbaum",
        "given": "Joseph"
      }
    ],
    "edition": "with new preface",
    "id": "Weizenbaum1984",
    "issued": {
      "date-parts": [
        [
          1984
        ]
      ]
    },
    "keyword": "ai, classic, cogsci",
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "publisher": "Penguin",
    "publisher-place": "Harmondsworth",
    "title": "Computer power and human reason: From judgement to calculation",
    "title-short": "Computer power and human reason",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Weizenbaum",
        "given": "Joseph"
      }
    ],
    "id": "Weizenbaum1981",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "original-date": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "publisher": "Éd. d’informatique",
    "title": "Puissance de l’ordinateur et raison de l’homme: Du jugement au calcul",
    "title-short": "Puissance de l’ordinateur et raison de l’homme",
    "translator": [
      {
        "family": "Margulici",
        "given": "Marie-Thérèse"
      }
    ],
    "type": "book"
  },
  {
    "abstract": "This article traces the fate of historical comparison in the discipline of history and discusses its function, capacities, and theoretical and methodological foundations. So long as the discipline of history was informed by historicism and thus emphasized individuality, it was profoundly sceptical about comparisons, but pioneers from the neighbouring social sciences eventually opened up to the comparative perspective. This meant at the same time an attempt at closer relations with the natural sciences. Comparison was introduced by way of modernisation theory into social history, which in recent decades has been criticized by competing approaches. From a systematic perspective, however, historical comparison is richer in possibilities than is generally assumed. It also encourages more circumspect modelling and typification.",
    "author": [
      {
        "family": "Welskopp",
        "given": "Thomas"
      }
    ],
    "container-title": "European history online (EGO)",
    "id": "Welskopp2010-en",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "formal_models, history",
    "language": "en-US",
    "publisher": "Leibniz-Institut für Europäische Geschichte",
    "publisher-place": "Mainz",
    "title": "Comparative history",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/s11614-013-0076-4",
    "author": [
      {
        "family": "Welz",
        "given": "Frank"
      }
    ],
    "id": "Welz2013",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "philosophy_of_science",
    "language": "de-DE",
    "page": "1-4",
    "publisher": "Springer Fachmedien Wiesbaden",
    "title": "Was heißt und zu welchem Ende studiert man Soziologie?",
    "type": "article-journal",
    "volume": "38"
  },
  {
    "DOI": "10.1145/248448.248467",
    "author": [
      {
        "family": "West",
        "given": "Dave"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "West1997",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "hermeneutics, theory, computational_thinking",
    "page": "115-116",
    "title": "Hermeneutic computer science",
    "type": "article-journal",
    "volume": "40"
  },
  {
    "DOI": "10.1145/1871437.1871556",
    "ISBN": "978-1-4503-0099-5",
    "abstract": "We present a method for automated topic suggestion. Given a plain-text input document, our algorithm produces a ranking of novel topics that could enrich the input document in a meaningful way. It can thus be used to assist human authors, who often fail to identify important topics relevant to the context of the documents they are writing. Our approach marries two algorithms originally designed for linking documents to Wikipedia articles, proposed by Milne and Witten [15] and West et al. [22]. While neither of them can suggest novel topics by itself, their combination does have this capability. The key step towards finding missing topics consists in generalizing from a large background corpus using principal component analysis. In a quantitative evaluation we conclude that our method achieves the precision of human editors when input documents are Wikipedia articles, and we complement this result with a qualitative analysis showing that the approach also works well on other types of input documents.",
    "author": [
      {
        "family": "West",
        "given": "Robert"
      },
      {
        "family": "Precup",
        "given": "Doina"
      },
      {
        "family": "Pineau",
        "given": "Joelle"
      }
    ],
    "collection-title": "CIKM ’10",
    "container-title": "Proceedings of the 19th ACM international conference on information and knowledge management (CIKM 2010)",
    "id": "West2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "topic_modeling, wikipedia",
    "language": "en-US",
    "page": "929-938",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Automatically suggesting topics for augmenting text documents",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Wetzel",
        "given": "Christian"
      }
    ],
    "genre": "Studienarbeit",
    "id": "Wetzel1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
    "title": "Erstellung einer Morphologie für Italienisch in Malaga",
    "type": "thesis"
  },
  {
    "DOI": "10.1093/llc/fqy085",
    "ISSN": "2055-7671",
    "abstract": "Digital humanities research has focused primarily on the analysis of texts. This emphasis stems from the availability of technology to study digitized text. Optical character recognition allows researchers to use keywords to search and analyze digitized texts. However, archives of digitized sources also contain large numbers of images. This article shows how convolutional neural networks (CNNs) can be used to categorize and analyze digitized historical visual sources. We present three different approaches to using CNNs for gaining a deeper understanding of visual trends in an archive of digitized Dutch newspapers. These include detecting medium-specific features (separating photographs from illustrations), querying images based on abstract visual aspects (clustering visually similar advertisements), and training a neural network based on visual categories developed by domain experts. We argue that CNNs allow researchers to explore the visual side of the digital turn. They allow archivists and researchers to classify and spot trends in large collections of digitized visual sources in radically new ways.",
    "author": [
      {
        "family": "Wevers",
        "given": "Melvin"
      },
      {
        "family": "Smits",
        "given": "Thomas"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Wevers2019",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "194-207",
    "title": "The visual digital turn: Using neural networks to study historical images",
    "title-short": "The visual digital turn",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "author": [
      {
        "family": "White",
        "given": "Morton"
      }
    ],
    "container-title": "Philosophy and history",
    "editor": [
      {
        "family": "Hook",
        "given": "Sidney"
      }
    ],
    "id": "White1963",
    "issued": {
      "date-parts": [
        [
          1963
        ]
      ]
    },
    "page": "3-4",
    "publisher": "New York University Press",
    "publisher-place": "New York, NY, USA",
    "title": "The logic of historical narration",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/966873.806474",
    "author": [
      {
        "family": "Whiteside",
        "given": "John"
      },
      {
        "family": "Archer",
        "given": "Norman"
      },
      {
        "family": "Wixon",
        "given": "Dennis"
      },
      {
        "family": "Good",
        "given": "Michael"
      }
    ],
    "container-title": "ACM SIGOA Newsletter",
    "id": "Whiteside1983",
    "issue": "1-2",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "29-40",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "How do people really use text editors?",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.1023/B:CHUM.0000031171.79841.02",
    "abstract": "Abstract This essay traces a distinction between computer-mediated writing environments that are tools for correcting student prose and those that are media for communication. This distinction has its roots in the influence of behavioral science on teaching machines and computer-aided writing instruction during the 1960s and 1970s. By looking at the development of the time-shared, interactive, computer-controlled, information television (TICCIT) and early human?computer interaction (HCI) research, this essay demonstrates that hardware and software systems had the potential to work as both tools and media. The influence of this double logic is not only historical but also has implications for post-secondary writing instruction in the age of Microsoft Word, ETS’s e-rater, and the reading/assessment software tools being developed by Knowledge Analysis Technologies (KAT). This essay challenges composition researchers and computational linguists to develop pedagogies and software systems that acknowledge writing environments as situated within the logic of both tools for correction and media for communication.",
    "author": [
      {
        "family": "Whithaus",
        "given": "Carl"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Whithaus2004",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2004,
          5
        ]
      ]
    },
    "keyword": "e-learning, interactive_editing, plato_system",
    "language": "en-US",
    "page": "149-162",
    "title": "The development of early computer-assisted writing instruction (1960–1978): The double logic of media and tools",
    "title-short": "The development of early computer-assisted writing instruction (1960–1978)",
    "type": "article-journal",
    "volume": "38"
  },
  {
    "author": [
      {
        "family": "Widera",
        "given": "Manfred"
      }
    ],
    "container-title": "Trends in functional programming",
    "editor": [
      {
        "family": "Gilmore",
        "given": "Stephen"
      }
    ],
    "id": "Widera2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "publisher": "Intellect",
    "title": "Testing scheme programming assignments automatically",
    "type": "chapter",
    "volume": "4"
  },
  {
    "author": [
      {
        "family": "Wiener",
        "given": "Norbert"
      }
    ],
    "id": "Wiener1950",
    "issued": {
      "date-parts": [
        [
          1950
        ]
      ]
    },
    "publisher": "Houghton Mifflin",
    "publisher-place": "Boston, MA, USA",
    "title": "The human use of human beings: Cybernetics and society",
    "title-short": "The human use of human beings",
    "type": "book"
  },
  {
    "DOI": "10.2200/S00194ED1V01Y200905HLT003",
    "abstract": "Linguistic annotation and text analytics are active areas of research and development, with academic conferences and industry events such as the Linguistic Annotation Workshops and the annual Text Analytics Summits. This book provides a basic introduction to both fields, and aims to show that good linguistic annotations are the essential foundation for good text analytics. After briefly reviewing the basics of XML, with practical exercises illustrating in-line and stand-off annotations, a chapter is devoted to explaining the different levels of linguistic annotations. The reader is encouraged to create example annotations using the WordFreak linguistic annotation tool. The next chapter shows how annotations can be created automatically using statistical NLP tools, and compares two sets of tools, the OpenNLP and Stanford NLP tools. The second half of the book describes different annotation formats and gives practical examples of how to interchange annotations between different formats using XSLT transformations. The two main text analytics architectures, GATE and UIMA, are then described and compared, with practical exercises showing how to configure and customize them. The final chapter is an introduction to text analytics, describing the main applications and functions including named entity recognition, coreference resolution and information extraction, with practical examples using both open source and commercial tools. Copies of the example files, scripts, and stylesheets used in the book are available from the companion website, located at the book website.",
    "author": [
      {
        "family": "Wilcock",
        "given": "Graham"
      }
    ],
    "collection-number": "3",
    "collection-title": "Synthesis lectures on human language technologies",
    "id": "Wilcock2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "computational_linguistics, nlp",
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "publisher-place": "San Rafael, CA, USA",
    "title": "Introduction to linguistic annotation and text analytics",
    "type": "book",
    "volume": "2"
  },
  {
    "DOI": "10.1007/978-3-540-78135-6_52",
    "abstract": "The trigram-based noisy-channel model of real-word spelling-error correction that was presented by Mays, Damerau, and Mercer in 1991 has never been adequately evaluated or compared with other methods. We analyze the advantages and limitations of the method, and present a new evaluation that enables a meaningful comparison with the WordNet-based method of Hirst and Budanitsky. The trigram method is found to be superior, even on content words. We then improve the method further and experiment with a new variation that optimizes over fixed-length windows instead of over sentences.",
    "author": [
      {
        "family": "Wilcox-O’Hearn",
        "given": "L. Amber"
      },
      {
        "family": "Hirst",
        "given": "Graeme"
      },
      {
        "family": "Budanitsky",
        "given": "Alexander"
      }
    ],
    "collection-number": "4919",
    "collection-title": "Lecture notes in computer science",
    "container-title": "Computational linguistics and intelligent text processing. 9<sup>th</sup> international conference, CICLing 2008",
    "editor": [
      {
        "family": "Gelbukh",
        "given": "Alexander"
      }
    ],
    "id": "Wilcox2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "interactive_editing, spelling_correction",
    "language": "en-US",
    "page": "605-616",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Real-word spelling correction with trigrams: A reconsideration of the mays, damerau, and mercer model",
    "title-short": "Real-word spelling correction with trigrams",
    "type": "paper-conference"
  },
  {
    "URL": "http://reusability.org/read/chapters/wiley.doc",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Wiley",
        "given": "David A."
      }
    ],
    "container-title": "The instructional use of learning objects: Online version",
    "editor": [
      {
        "family": "Wiley",
        "given": "David A."
      }
    ],
    "id": "Wiley2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "page": "1-35",
    "publisher": "Agency for Instructional Technology and Association for Educational Communications and Technology",
    "title": "Connecting learning objects to instructional design theory: A definition, a metaphor, and a taxonomy",
    "title-short": "Connecting learning objects to instructional design theory",
    "type": "chapter"
  },
  {
    "ISBN": "0784208921",
    "URL": "http://www.reusability.org/read/",
    "abstract": "The Agency for Instructional Technology and The Association for Educational Communications and Technology’s new book The Instructional Use of Learning Objects presents the cutting edge of modern instructional design theory. Learning objects ” ...may provide the foundation for an adaptive, generative, scalable learning architecture... [wherein both] teaching and learning as we know them are certain to be revolutionized” (Wiley, p. 20).",
    "editor": [
      {
        "family": "Wiley",
        "given": "David A."
      }
    ],
    "id": "Wiley2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "e-learning, learning_objects",
    "language": "en-US",
    "publisher": "Agency for Instructional Technology and Association for Educational Communications and Technology",
    "publisher-place": "Bloomington, IN",
    "title": "Instructional use of learning objects",
    "type": "book"
  },
  {
    "URL": "http://www.kenbenoit.net/pdfs/NDATAD2013/PolicyIdeas2013TextasData.pdf",
    "abstract": "We propose a method for tracking policy ideas in legislation. In the US Congress, only a very small proportion of bills become law. Surviving bills likely serve as vehicles for policy ideas originating in other bills, but there is currently no reliable way to learn when this occurs. Using the legislative history of the Patient Protection and Affordable Care Act as our test bed, we investigate whether ”text reuse” methods can help to shed additional light on policy development and lawmaking. In particular we ask whether lawmaking is more inclusive when judged in terms of the progress of ideas rather than the progress of bills.",
    "author": [
      {
        "family": "Wilkerson",
        "given": "John"
      },
      {
        "family": "Smith",
        "given": "David"
      },
      {
        "family": "Stramp",
        "given": "Nick"
      }
    ],
    "container-title": "New directions in analyzing text as data workshop (sept. 27–29, 2013)",
    "id": "Wilkerson2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "plagiarism",
    "language": "en-US",
    "title": "Tracing the flow of policy ideas in legislatures: A text reuse approach",
    "title-short": "Tracing the flow of policy ideas in legislatures",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/345966.346024",
    "ISSN": "0360-0300",
    "URL": "http://doi.acm.org/10.1145/345966.346024",
    "author": [
      {
        "family": "Wilkinson",
        "given": "Ross"
      },
      {
        "family": "Smeaton",
        "given": "Alan F."
      }
    ],
    "container-title": "ACM Computing Surveys",
    "id": "Wilkinson1999",
    "issue": "4es",
    "issued": {
      "date-parts": [
        [
          1999,
          12
        ]
      ]
    },
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Automatic link generation",
    "type": "article-journal",
    "volume": "31"
  },
  {
    "DOI": "10.1561/1800000002",
    "ISSN": "1555-077X",
    "abstract": "The main argument of this paper is that Natural Language Processing (NLP) does, and will continue to, underlie the Semantic Web (SW), including its initial construction from unstructured sources like the World Wide Web (WWW), whether its advocates realise this or not. Chiefly, we argue, such NLP activity is the only way up to a defensible notion of meaning at conceptual levels (in the original SW diagram) based on lower level empirical computations over usage. Our aim is definitely not to claim logic-bad, NLP-good in any simple-minded way, but to argue that the SW will be a fascinating interaction of these two methodologies, again like the WWW (which has been basically a field for statistical NLP research) but with deeper content. Only NLP technologies (and chiefly information extraction) will be able to provide the requisite RDF knowledge stores for the SW from existing unstructured text databases in the WWW, and in the vast quantities needed. There is no alternative at this point, since a wholly or mostly hand-crafted SW is also unthinkable, as is a SW built from scratch and without reference to the WWW. We also assume that, whatever the limitations on current SW representational power we have drawn attention to here, the SW will continue to grow in a distributed manner so as to serve the needs of scientists, even if it is not perfect. The WWW has already shown how an imperfect artefact can become indispensable.",
    "author": [
      {
        "family": "Wilks",
        "given": "Yorick"
      },
      {
        "family": "Brewster",
        "given": "Christopher"
      }
    ],
    "container-title": "Foundations and Trends in Web Science",
    "id": "Wilks2009",
    "issue": "3–4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "nlp, semantic_web",
    "language": "en-US",
    "page": "199-327",
    "publisher": "Now Publishers",
    "publisher-place": "Hanover, MA, USA",
    "title": "Natural language processing as a foundation of the semantic web",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "author": [
      {
        "family": "Williams",
        "given": "Noel"
      }
    ],
    "chapter-number": "1",
    "container-title": "Computers and writing: Models and tools",
    "editor": [
      {
        "family": "Williams",
        "given": "Noel"
      },
      {
        "family": "Holt",
        "given": "Patrick O’Brian"
      }
    ],
    "id": "Williams1989a",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "post-writing",
    "page": "1-16",
    "publisher": "Intellect",
    "publisher-place": "Bristol, UK",
    "title": "Computer assisted writing software: RUSKIN",
    "title-short": "Computer assisted writing software",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/1118178.1118215",
    "ISSN": "0001-0782",
    "abstract": "It represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use.",
    "author": [
      {
        "family": "Wing",
        "given": "Jeannette M."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Wing2006",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "computational_thinking",
    "language": "en-US",
    "page": "33-35",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Computational thinking",
    "type": "article-journal",
    "volume": "49"
  },
  {
    "DOI": "10.1016/0010-0285%2872%2990002-3",
    "ISSN": "0010-0285",
    "abstract": "This paper describes a computer system for understanding English. The system answers questions, executes commands, and accepts information in an interactive English dialog. It is based on the belief that in modeling language understanding, we must deal in an integrated way with all of the aspects of language—syntax, semantics, and inference. The system contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system. We assume that a computer cannot deal reasonably with language unless it can understand the subject it is discussing. Therefore, the program is given a detailed model of a particular domain. In addition, the system has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carrying them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, asking for clarification when its heuristic programs cannot understand a sentence through the use of syntactic, semantic, contextual, and physical knowledge. Knowledge in the system is represented in the form of procedures, rather than tables of rules or lists of patterns. By developing special procedural representations for syntax, semantics, and inference, we gain flexibility and power. Since each piece of knowledge can be a procedure, it can call directly on any other piece of knowledge in the system.",
    "author": [
      {
        "family": "Winograd",
        "given": "Terry"
      }
    ],
    "container-title": "Cognitive Psychology",
    "id": "Winograd1972a",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "keyword": "classic, computational_linguistics",
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "page": "1-191",
    "title": "Understanding natural language",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "ISBN": "0127597506",
    "URL": "http://portal.acm.org/citation.cfm?id=540414",
    "abstract": "This paper describes a computer system for understanding English. The system answers questions, executes commands, and accepts information in an interactive English dialog. It is based on the belief that in modeling language understanding, we must deal in an integrated way with all of the aspects of language—syntax, semantics, and inference. The system contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system. We assume that a computer cannot deal reasonably with language unless it can understand the subject it is discussing. Therefore, the program is given a detailed model of a particular domain. In addition, the system has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carrying them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, asking for clarification when its heuristic programs cannot understand a sentence through the use of syntactic, semantic, contextual, and physical knowledge. Knowledge in the system is represented in the form of procedures, rather than tables of rules or lists of patterns. By developing special procedural representations for syntax, semantics, and inference, we gain flexibility and power. Since each piece of knowledge can be a procedure, it can call directly on any other piece of knowledge in the system.",
    "author": [
      {
        "family": "Winograd",
        "given": "Terry"
      }
    ],
    "id": "Winograd1972b",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "keyword": "classic, computational_linguistics",
    "language": "en-US",
    "publisher": "Academic Press",
    "publisher-place": "Orlando, FL, USA",
    "title": "Understanding natural language",
    "type": "book"
  },
  {
    "ISBN": "0893910716",
    "author": [
      {
        "family": "Winograd",
        "given": "Terry"
      }
    ],
    "chapter-number": "10",
    "container-title": "Perspectives on cognitive science",
    "editor": [
      {
        "family": "Norman",
        "given": "Donald A."
      }
    ],
    "id": "Winograd1981",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "page": "231-263",
    "publisher": "ABLEX",
    "publisher-place": "Norwood, NJ, USA",
    "title": "What does it mean to understand language",
    "type": "chapter"
  },
  {
    "URL": "http://digitalcommons.unl.edu/classicsfacpub/70/",
    "abstract": "This is a story from early in the technological revolution, when the application was out searching for the hardware, from a time before the Internet, a time before the PC, before the chip, before the mainframe. From a time even before programming itself. Tasman’s 1957 prophecy was no shot in the dark. His view of the future was a projection from his recent past. Thomas J. Watson, Sr. had assigned him in 1949 to be IBM liaison and support person for a young Jesuit’s daring project to produce an index to the complete writings of St. Thomas Aquinas. First, Tasman’s thesis, as subsequent history turned out, was a huge understatement; and second, it essentially defines the first large invention of Father Roberto Busa, S. J., namely, to look at \"tools developed primarily for science and commerce\" and to see other uses for them. As will be seen, this was a case of fortune favoring the prepared mind. Redirecting scholarship, he essentially invented the machine-generated concordance, the first of which he had published in 1951. Father Busa, of course, is best known as the producer of the landmark 56-volume Index Thomisticus. As he began this work in 1946, and produced a sample proof-of-concept, machine-generated concordance in 1951, his professional life spans the entire computing chapter in the history of scholarship. Emphasis in this article will be on the early steps.",
    "author": [
      {
        "family": "Winter",
        "given": "Thomas N."
      }
    ],
    "container-title": "The Classical Bulletin",
    "id": "Winter1999",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "cultural_heritage, latin",
    "language": "en-US",
    "page": "3-20",
    "title": "Roberto Busa, S.J., And the invention of the machine-generated concordance",
    "type": "article-journal",
    "volume": "75"
  },
  {
    "DOI": "10.3115/976815.976848",
    "abstract": "This paper presents an algorithm for incremental chart parsing, outlines how this could be embedded in an interactive parsing system, and discusses why this might be useful. Incremental parsing here means that input is analysed in a piecemeal fashion, in particular allowing arbitrary changes of previous input without exhaustive reanalysis, Interactive parsing means that the analysis process is prompted immediately at the onset of new input, and possibly that the system then may interact with the user in order to resolve problems that occur. The combination of these techniques could be used as a parsing kernel for highly interactive and \"reactive\" natural language processors, such as parsers for dialogue systems, interactive computer-aided translation systems, and language-sensitive text editors. An incremental chart parser embodying the ideas put forward in this paper has been implemented, and an embedding of this in an interactive parsing system is near completion.",
    "author": [
      {
        "family": "Wirén",
        "given": "Mats"
      }
    ],
    "container-title": "Proceedings of the fourth conference of the european chapter of the association for computational linguistics",
    "id": "Wiren1989",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "interactive_editing, interactive_parsing",
    "language": "en-US",
    "page": "241-248",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Morristown, NJ, USA",
    "title": "Interactive incremental chart parsing",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Wirén",
        "given": "Mats"
      }
    ],
    "genre": "PhD thesis",
    "id": "Wiren1992",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "publisher": "Linköping University, Department of Computer and Information Science",
    "publisher-place": "Linköping, Sweden",
    "title": "Studies in incremental natural-language analysis",
    "type": "thesis"
  },
  {
    "author": [
      {
        "family": "Wirth",
        "given": "Niklaus"
      }
    ],
    "id": "Wirth1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Prentice-Hall",
    "title": "Algorithms + data structures = programs",
    "type": "book"
  },
  {
    "DOI": "10.1093/llc/fqp017",
    "abstract": "The TEI Consortium has taken on the task of maintaining the Guidelines for Electronic Text Encoding and Interchange. This article describes how the latest major revision to these Guidelines was developed over the course of >6 years by the members of the TEI Technical Council and workgroups charged and overseen by the Council and gives background information and reasoning for the decisions taken. Among the new additions for P5, two of the most outstanding, the chapters on Names, Dates, People, and Places and on digital facsimiles are treated in some more detail. The article concludes with a brief account of the decisions made with respect to customization and conformance.",
    "author": [
      {
        "family": "Wittern",
        "given": "Christian"
      },
      {
        "family": "Ciula",
        "given": "Arianna"
      },
      {
        "family": "Tuohy",
        "given": "Conal"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Wittern2009",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2009,
          5
        ]
      ]
    },
    "keyword": "cultural_heritage, markup, tei, xml",
    "language": "en-US",
    "page": "281-296",
    "title": "The making of TEI P5",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "ISBN": "9789042004573",
    "URL": "http://www.worldcat.org/isbn/9789042004573",
    "abstract": "This volume is a pioneering study in the theory and history of the imitation of music in fiction and constitutes an important contribution to current intermediality research. Starting with a comparison of basic similarities and differences between literature and music, the study goes on to provide outlines of a general theory of intermediality and its fundamental forms, in which a more specialized theory of the musicalization of (narrative) literature based on contemporary narratology and a typology of the forms of musico-literary intermediality are embedded. It also addresses the question of how to recognize a musicalized fiction when reading one and why Sterne’s Tristram Shandy, contrary to what has been previously said, is not to be regarded as a musicalized fiction.In its historical part, the study explores forms and functions of experiments with the musicalization of fiction in English literature. After a survey of the major preconditions for musicalization – the increasing appreciation of music in 18th and 19th-century aesthetics and its main causes – exemplary fictional texts from romanticism to postmodernism are analyzed. Authors interpreted are De Quincey, Joyce, Woolf, A. Huxley, Beckett, Burgess and Josipovici. Whilst the limitations of a transposition of music into fiction remain apparent, experiments in this field yield valuable insights into mainly a-mimetic and formalist aesthetic tendencies in the development of more recent fiction as a whole and also show to what extent traditional conceptions of music continue to influence the use of this medium in literature.The volume is of relevance for students and scholars of English, comparative and general literature as well as for readers who take an interest in intermediality or interart research.",
    "author": [
      {
        "family": "Wolf",
        "given": "Werner"
      }
    ],
    "id": "Wolf1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "intertextuality",
    "language": "en-US",
    "publisher": "Rodopi",
    "publisher-place": "Amsterdam",
    "title": "The musicalization of fiction. A study in the theory and history of intermediality",
    "type": "book"
  },
  {
    "DOI": "10.2200/S00211ED1V01Y200909HLT004",
    "ISSN": "1947-4040",
    "abstract": "This book introduces Chinese language-processing issues and techniques to readers who already have a basic background in natural language processing (NLP). Since the major difference between Chinese and Western languages is at the word level, the book primarily focuses on Chinese morphological analysis and introduces the concept, structure, and interword semantics of Chinese words. The following topics are covered: a general introduction to Chinese NLP; Chinese characters, morphemes, and words and the characteristics of Chinese words that have to be considered in NLP applications; Chinese word segmentation; unknown word detection; word meaning and Chinese linguistic resources; interword semantics based on word collocation and NLP techniques for collocation extraction. Table of Contents: Introduction / Words in Chinese / Challenges in Chinese Morphological Processing / Chinese Word Segmentation / Unknown Word Identification / Word Meaning / Chinese Collocations / Automatic Chinese Collocation Extraction / Appendix / References / Author Biographies Review \"This book focuses on the unique properties of Chinese language and Chinese NLP, offering careful comparisons with English wherever necessary. Not only will it benefit Chinese speakers, including a better understanding of Chinese NLP, but it also offers a good way for non-Chinese speakers to grasp the essence of the subject. The book contains a concise and insightful linguistic analysis of the problems faced by Chinese NLP, establishing foundations for the techniques introduced in later chapters. The technical details are described in appropriate depth with illustrative examples. In addition, this book is written in a consistent and didactic manner, and many insightful discussions in the book are guaranteed to inform and enlighten. This introduction of Chinese NLP is a valuable resource for students and researchers of theoretical and computational linguistics, as well as NLP practitioners and language engineers, who are interested in, and/or working on, Chinese NLP.\" –Dr. Yanjun Ma.",
    "author": [
      {
        "family": "Wong",
        "given": "Kam-Fai"
      },
      {
        "family": "Li",
        "given": "Wenjie"
      },
      {
        "family": "Xu",
        "given": "Ruifeng"
      },
      {
        "family": "Zhang",
        "given": "Zheng-sheng"
      }
    ],
    "collection-title": "Synthesis lectures on human language technologies",
    "id": "Wong2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "chinese, nlp",
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "publisher-place": "San Rafael, CA, USA",
    "title": "Introduction to Chinese natural language processing",
    "type": "book",
    "volume": "4"
  },
  {
    "DOI": "10.1145/800209.806447",
    "ISBN": "0-89791-050-8",
    "abstract": "Recently much attention has been focused on structure-oriented program editors that have specific knowledge about the syntax and semantics of a particular programming language [1, 4, 5, 18]. These editors provide many desirable features for editing programs. However, the user interface is constrained by the syntax and semantics of the target language, and editing operations that are simple in a text editor can be quite complicated in a structure-oriented editor. In addition, the user has an editor that is limited to a single language and must use a different editor for text editing. Existing implementations of structure-oriented editors use a parse-tree representation for a program along with a supporting lexical analyzer, parser, and pretty-printer; this representation significantly complicates the implementation of an editor.We believe that the most natural representation of programs is text and that the editor should be able to take advantage of the same visual cues that programmers use to understand their programs. With a text-oriented model of program structure, the editor is both a program editor and a document editor. As a program editor it provides features to support many different programming languages, such as LISP, APL, PASCAL, and BLISS. As a document editor it provides basic word-processing functions such as text justification and spelling correction. A text orientation considerably simplifies the design of the editor and presents the user with a simple but powerful model of program structure. This paper describes a text-oriented display editor called Z. Z is the production editor in the Yale Computer Science Department.",
    "author": [
      {
        "family": "Wood",
        "given": "Steven R."
      }
    ],
    "container-title": "Proceedings of the ACM SIGPLAN SIGOA symposium on text manipulation",
    "id": "Wood1981",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "1-7",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Z – the 95% program editor",
    "type": "paper-conference"
  },
  {
    "URL": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.4813",
    "abstract": "In this paper we present an algorithm which automatically extracts geopositional coordinate index terms from text to support georeferenced document indexing and retrieval. Under this algorithm, words and phrases containing geographic place names or characteristics are extracted from a text document and used as input to database functions which use spatial reasoning to approximate statistically the geoposition being referenced in the text. We conclude with a discussion of preliminary results and future work. 1",
    "author": [
      {
        "family": "Woodruff",
        "given": "Allison G."
      },
      {
        "family": "Plaunt",
        "given": "Christian"
      }
    ],
    "container-title": "Journal of the American Society for Information Science",
    "id": "Woodruff1994",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "geocoding",
    "language": "en-US",
    "page": "645-655",
    "title": "GIPSY: Automated geographic indexing of text documents",
    "title-short": "GIPSY",
    "type": "article-journal",
    "volume": "45"
  },
  {
    "URL": "http://thinkofit.com/plato/dwplato.htm",
    "abstract": "The PLATO system was designed for Computer-Based Education. But for many people, PLATO’s most enduring legacy is the online community spawned by its communication features. PLATO originated in the early 1960’s at the Urbana campus of the University of Illinois. Professor Don Bitzer became interested in using computers for teaching, and with some colleagues founded the Computer-based Education Research Laboratory (CERL). Bitzer, an electrical engineer, collaborated with a few other engineers to design the PLATO hardware. To write the software, he collected a staff of creative eccentrics ranging from university professors to high school students, few of whom had any computer background. Together they built a system that was at least a decade ahead of its time in many ways. PLATO is a timesharing system. (It was, in fact, one of the first timesharing systems to be operated in public.) Both courseware authors and their students use the same high-resolution graphics display terminals, which are connected to a central mainframe. A special-purpose programming language called TUTOR is used to write educational software. Throughout the 1960’s, PLATO remained a small system, supporting only a single classroom of terminals. About 1972, PLATO began a transition to a new generation of mainframes that would eventually support up to one thousand users simultaneously.",
    "author": [
      {
        "family": "Woolley",
        "given": "David R."
      }
    ],
    "id": "Woolley1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "e-learning, plato_system",
    "language": "en-US",
    "title": "PLATO: The emergence of online community",
    "title-short": "PLATO",
    "type": ""
  },
  {
    "URL": "http://go.worldbank.org/GX5J0A0KK0",
    "abstract": "This report describes how tertiary education contributes to building up a country’ s capacity for participation in an increasingly knowledge-based world economy and investigates policy options for tertiary education that have the potential to enhance economic growth and reduce poverty. It examines the following questions: What is the importance of tertiary education for economic and social development? How should developing and transition countries position themselves to take full advantage of the potential contribution of tertiary education? How can the World Bank and other development agencies assist in this process? The report draws on ongoing Bank research and analysis on the dynamics of knowledge economies and on science and technology development.",
    "author": [
      {
        "literal": "World Bank"
      }
    ],
    "genre": "{World Bank Report}",
    "id": "WorldBank2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "The World Bank",
    "publisher-place": "Washington, D.C., USA",
    "title": "Constructing knowledge societies: New challenges for tertiary education",
    "title-short": "Constructing knowledge societies",
    "type": "report"
  },
  {
    "URL": "http://www.gutenberg.org/ebooks/22636",
    "author": [
      {
        "family": "Wright",
        "given": "Joseph"
      }
    ],
    "id": "Wright1917",
    "issued": {
      "date-parts": [
        [
          1917
        ]
      ]
    },
    "keyword": "cultural_heritage, german",
    "language": "en-US",
    "publisher": "Clarendon",
    "publisher-place": "Oxford, UK",
    "title": "A middle high german primer",
    "type": "book"
  },
  {
    "DOI": "10.1016/0020-0190(90)90035-V",
    "ISSN": "0020-0190",
    "abstract": "Let A and B be two sequences of length M and N respectively, where without loss of generality N \\geq M, and let D be the length of a shortest edit script (consisting of insertions and deletions) between them. A parameter related to D is the number of deletions in such a script, P =\n                  \\frac{1}{2}D - \\frac{1}{2}(N - M). We present an algorithm for finding a shortest edit distance of A and B whose worst-case running time is O(NP) and whose expected running time is O(N + PD). The algorithm is simple and is very efficient whenever A is similar to a subsequence of B. It is nearly twice as fast as the O(ND) algorithm of Myers, and much more efficient when A and B differ substantially in length.",
    "author": [
      {
        "family": "Wu",
        "given": "Sun"
      },
      {
        "family": "Manber",
        "given": "Udi"
      },
      {
        "family": "Myers",
        "given": "Gene"
      },
      {
        "family": "Miller",
        "given": "Webb"
      }
    ],
    "container-title": "Information Processing Letters",
    "id": "Wu1990",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "approximate_matching",
    "language": "en-US",
    "page": "317-323",
    "publisher": "Elsevier",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "An O(NP) sequence comparison algorithm",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "DOI": "10.1524/dzph.1963.11.12.1504",
    "author": [
      {
        "family": "Wüstneck",
        "given": "Klaus-Dieter"
      }
    ],
    "container-title": "Deutsche Zeitschrift für Philosophie",
    "id": "Wuestneck1963",
    "issued": {
      "date-parts": [
        [
          1963
        ]
      ]
    },
    "language": "de-DE",
    "page": "1504-1523",
    "title": "Zur philosophischen Verallgemeinerung und Bestimmung des Modellbegriffs",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "URL": "https://dl.acm.org/citation.cfm?id=3237858",
    "abstract": "Advanced collision avoidance and driver hand-off systems can benefit from the ability to accurately predict, in real time, the probability a vehicle will be involved in a collision within an intermediate horizon of 10 to 20 seconds. The rarity of collisions in real-world data poses a significant challenge to developing this capability because, as we demonstrate empirically, intermediate-horizon risk prediction depends heavily on high-dimensional driver behavioral features. As a result, a large amount of data is required to fit an effective predictive model. In this paper, we assess whether simulated data can help alleviate this issue. Focusing on highway driving, we present a three-step approach for generating data and fitting a predictive model capable of real-time prediction. First, high-risk automotive scenes are generated using importance sampling on a learned Bayesian network scene model. Second, collision risk is estimated through Monte Carlo simulation. Third, a neural network domain adaptation model is trained on real and simulated data to address discrepancies between the two domains. Experiments indicate that simulated data can mitigate issues resulting from collision rarity, thereby improving risk prediction in real-world data.",
    "author": [
      {
        "family": "Wulfe",
        "given": "Blake"
      },
      {
        "family": "Chintakindi",
        "given": "Sunil"
      },
      {
        "family": "Choi",
        "given": "Sou-Cheng T."
      },
      {
        "family": "Hartong-Redden",
        "given": "Rory"
      },
      {
        "family": "Kodali",
        "given": "Anuradha"
      },
      {
        "family": "Kochenderfer",
        "given": "Mykel J."
      }
    ],
    "container-title": "Proceedings of the 17<sup>th</sup> international conference on autonomous agents and MultiAgent systems (AAMAS ’18)",
    "id": "Wulfe2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "keyword": "formal_models, machine_learning",
    "language": "en-US",
    "page": "1087-1096",
    "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
    "publisher-place": "Richland, SC, USA",
    "title": "Real-Time prediction of Intermediate-Horizon automotive collision risk",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/2494266.2494324",
    "ISBN": "978-1-4503-1789-4",
    "abstract": "This workshop asks how we might reimagine digital publishing for technical documents and proposes to investigate new adaptive approaches to document reading with flexible navigation and where contextual information—figures, references, definitions, etc—might be displayed dynamically at the point they are referred to. The workshop ultimately seeks to answer the question of what needs to happen for reading and annotation of technical documents on digital devices to become more comfortable and productive than on paper?",
    "author": [
      {
        "family": "Wybrow",
        "given": "Michael"
      }
    ],
    "collection-title": "DocEng ’13",
    "container-title": "Proceedings of the 2013 ACM symposium on document engineering",
    "id": "Wybrow2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "document_engineering, scientific_publishing",
    "language": "en-US",
    "page": "285-286",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Reimagining digital publishing for technical documents",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1037/h0028769",
    "ISSN": "0033-295X",
    "abstract": "Outlines an approach to the analysis of inference and classification processes, based on the assumption that subjective probabilities are related according to the laws of objective probability, and suggests certain of its implications for the study of relationships among attitudes and beliefs. Some preliminary empirical support for the formulation is reported. The consistency theories proposed by W. McGuire, by F. Heider, and by R. Abelson and M. Rosenberg are compared to the formulation proposed, and possible differences in their assumptions and implications are identified. Implications of the proposed formulation for impression-formation processes are discussed.",
    "author": [
      {
        "family": "Wyer",
        "given": "Robert S."
      },
      {
        "family": "Goldberg",
        "given": "Lee"
      }
    ],
    "container-title": "Psychological Review",
    "id": "Wyer1970",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1970
        ]
      ]
    },
    "keyword": "mental_models, uncertainty",
    "language": "en-US",
    "page": "100-120",
    "title": "A probabilistic analysis of the relationships among belief and attitudes",
    "type": "article-journal",
    "volume": "77"
  },
  {
    "URL": "http://www.w3.org/TR/html",
    "accessed": {
      "date-parts": [
        [
          2011,
          3,
          30
        ]
      ]
    },
    "author": [
      {
        "literal": "World Wide Web Consortium"
      }
    ],
    "edition": "2",
    "id": "XHTML",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "title": "XHTML 1.0 The Extensible HyperText Markup Language",
    "type": ""
  },
  {
    "URL": "http://www.w3.org/TR/xml",
    "author": [
      {
        "family": "Bray",
        "given": "Tim"
      },
      {
        "family": "Paoli",
        "given": "Jean"
      },
      {
        "family": "Sperberg-McQueen",
        "given": "C. M."
      },
      {
        "family": "Maler",
        "given": "Eve"
      },
      {
        "family": "Yergeau",
        "given": "François"
      }
    ],
    "genre": "{W3C Recommendation}",
    "id": "XML",
    "issued": {
      "date-parts": [
        [
          2006,
          9
        ]
      ]
    },
    "publisher": "World Wide Web Consortium",
    "title": "Extensible Markup Language (XML) 1.0",
    "type": "report"
  },
  {
    "URL": "http://www.w3.org/TR/xquery/",
    "author": [
      {
        "family": "Boag",
        "given": "Scott"
      },
      {
        "family": "Chamberlin",
        "given": "Don"
      },
      {
        "family": "Fernández",
        "given": "Mary F."
      },
      {
        "family": "Florescu",
        "given": "Daniela"
      },
      {
        "family": "Robie",
        "given": "Jonathan"
      },
      {
        "family": "Siméon",
        "given": "Jérôme"
      }
    ],
    "genre": "{W3C Recommendation}",
    "id": "XQuery",
    "issued": {
      "date-parts": [
        [
          2010,
          12
        ]
      ]
    },
    "publisher": "World Wide Web Consortium",
    "title": "XQuery 1.0: An XML Query Language",
    "type": "report"
  },
  {
    "URL": "http://www.w3.org/TR/xslt20/",
    "author": [
      {
        "family": "Kay",
        "given": "Michael"
      }
    ],
    "genre": "{W3C Recommendation}",
    "id": "XSLT",
    "issued": {
      "date-parts": [
        [
          2007,
          1
        ]
      ]
    },
    "publisher": "World Wide Web Consortium",
    "title": "XSL Transformations (XSLT) Version 2.0",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Hawisher",
        "given": "Gail E."
      }
    ],
    "container-title": "Computers and Composition",
    "id": "XXXHawisher1988",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1988,
          4
        ]
      ]
    },
    "page": "7-27",
    "title": "Research update: Writing and word processing",
    "title-short": "Research update",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.1145/1806596.1806622",
    "ISBN": "978-1-4503-0019-3",
    "abstract": "An ad hoc data format is any nonstandard, semi-structured data format for which robust data processing tools are not easily available. In this paper, we present ANNE, a new kind of markup language designed to help users generate documentation and data processing tools for ad hoc text data. More specifically, given a new ad hoc data source, an ANNE programmer edits the document to add a number of simple annotations, which serve to specify its syntactic structure. Annotations include elements that specify constants, optional data, alternatives, enumerations, sequences, tabular data, and recursive patterns. The ANNE system uses a combination of user annotations and the raw data itself to extract a context-free grammar from the document. This context-free grammar can then be used to parse the data and transform it into an XML parse tree, which may be viewed through a browser for analysis or debugging purposes. In addition, the ANNE system generates a PADS/ML description, which may be saved as lasting documentation of the data format or compiled into a host of useful data processing tools. In addition to designing and implementing ANNE, we have devised a semantic theory for the core elements of the language. This semantic theory describes the editing process, which translates a raw, unannotated text document into an annotated document, and the grammar extraction process, which generates a context-free grammar from an annotated document. We also present an alternative characterization of system behavior by drawing upon ideas from the field of relevance logic. This secondary characterization, which we call relevance analysis, specifies a direct relationship between unannotated documents and the context-free grammars that our system can generate from them. Relevance analysis allows us to prove important theorems concerning the expressiveness and utility of our system.",
    "author": [
      {
        "family": "Xi",
        "given": "Qian"
      },
      {
        "family": "Walker",
        "given": "David"
      }
    ],
    "collection-title": "PLDI ’10",
    "container-title": "Proceedings of the 2010 ACM SIGPLAN conference on programming language design and implementation",
    "id": "Xi2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "annotation, markup",
    "language": "en-US",
    "page": "221-232",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A context-free markup language for semi-structured text",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/CISW.2007.4425458",
    "ISBN": "978-0-7695-3073-4",
    "abstract": "In order to find the longest common subsequence (LCS) as soon as possible, we, with the method of match pairs, propose the new algorithm of the sequence of DNA, which is efficient both in time and in space on the basis of the improved dynamic programming theorem.",
    "author": [
      {
        "family": "Xiang",
        "given": "Xuyu"
      },
      {
        "family": "Zhang",
        "given": "Dafang"
      },
      {
        "family": "Qin",
        "given": "Jiaohua"
      }
    ],
    "container-title": "Computational intelligence and security workshops (CISW 2007)",
    "id": "Xiang2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "approximate_matching",
    "language": "en-US",
    "page": "112-115",
    "publisher": "IEEE",
    "publisher-place": "New York, NY, USA",
    "title": "A new algorithm for the longest common subsequence problem",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1453856.1453957",
    "ISSN": "2150-8097",
    "abstract": "There has been considerable interest in similarity join in the research community recently. Similarity join is a fundamental operation in many application areas, such as data integration and cleaning, bioinformatics, and pattern recognition. We focus on efficient algorithms for similarity join with edit distance constraints. Existing approaches are mainly based on converting the edit distance constraint to a weaker constraint on the number of matching q-grams between pair of strings. In this paper, we propose the novel perspective of investigating mismatching q-grams. Technically, we derive two new edit distance lower bounds by analyzing the locations and contents of mismatching q-grams. A new algorithm, Ed-Join, is proposed that exploits the new mismatch-based filtering methods; it achieves substantial reduction of the candidate sizes and hence saves computation time. We demonstrate experimentally that the new algorithm outperforms alternative methods on large-scale real datasets under a wide range of parameter settings.",
    "author": [
      {
        "family": "Xiao",
        "given": "Chuan"
      },
      {
        "family": "Wang",
        "given": "Wei"
      },
      {
        "family": "Lin",
        "given": "Xuemin"
      }
    ],
    "container-title": "Proceedings of the VLDB Endowment",
    "id": "Xiao2008",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "approximate_matching",
    "language": "en-US",
    "page": "933-944",
    "publisher": "VLDB Endowment",
    "title": "Ed-Join: An efficient algorithm for similarity joins with edit distance constraints",
    "title-short": "Ed-Join",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "URL": "http://zope.org/Members/gillou/XQuizz",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "id": "Xquizz",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "title": "XQuizz product for Zope",
    "type": ""
  },
  {
    "DOI": "10.1109/tnn.2005.845141",
    "ISSN": "1045-9227",
    "abstract": "Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.",
    "author": [
      {
        "family": "Xu",
        "given": "Rui"
      },
      {
        "family": "Wunsch",
        "given": "D."
      }
    ],
    "container-title": "IEEE Transactions on Neural Networks",
    "id": "Xu2005",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "clustering",
    "language": "en-US",
    "page": "645-678",
    "publisher": "IEEE",
    "publisher-place": "New York, NY, USA",
    "title": "Survey of clustering algorithms",
    "type": "article-journal",
    "volume": "16"
  },
  {
    "URL": "http://www.aaai.org/ocs/index.php/WS/AAAIW11/paper/view/3987",
    "abstract": "The use of computer mediated communication has resulted in a new form of written text–Microtext–which is very different from well-written text. Tweets and SMS messages, which have limited length and may contain misspellings, slang, or abbreviations, are two typical examples of microtext. Microtext poses new challenges to standard natural language processing tools which are usually designed for well-written text. The objective of this work is to normalize microtext, in order to produce text that could be suitable for further treatment. We propose a normalization approach based on the source channel model, which incorporates four factors, namely an orthographic factor, a phonetic factor, a contextual factor and acronym expansion. Experiments show that our approach can normalize Twitter messages reasonably well, and it outperforms existing algorithms on a public SMS data set.",
    "author": [
      {
        "family": "Xue",
        "given": "Zhenzhen"
      },
      {
        "family": "Yin",
        "given": "Dawei"
      },
      {
        "family": "Davison",
        "given": "Brian D."
      }
    ],
    "collection-title": "AAAI workshops",
    "container-title": "Analyzing microtext: Papers from the 2011 AAAI workshop",
    "id": "Xue2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "microtext, nlp, spelling_normalization",
    "language": "en-US",
    "page": "74-79",
    "publisher": "AAAI",
    "publisher-place": "Palo Alto, CA, USA",
    "title": "Normalizing microtext",
    "type": "paper-conference",
    "volume": "WS-11-05"
  },
  {
    "author": [
      {
        "family": "Yager",
        "given": "Tom"
      }
    ],
    "container-title": "BYTE",
    "id": "Yager:1990:WNA",
    "issue": "10",
    "issued": {
      "date-parts": [
        [
          1990,
          10
        ]
      ]
    },
    "keyword": "tables",
    "language": "en-US",
    "page": "147-149",
    "title": "What’s NeXT after 1-2-3?",
    "type": "article-journal",
    "volume": "15"
  },
  {
    "DOI": "10.1145/1993036.1993039",
    "ISSN": "10468188",
    "author": [
      {
        "family": "Yan",
        "given": "Xin"
      },
      {
        "family": "Lau",
        "given": "Raymond Y. K."
      },
      {
        "family": "Song",
        "given": "Dawei"
      },
      {
        "family": "Li",
        "given": "Xue"
      },
      {
        "family": "Ma",
        "given": "Jian"
      }
    ],
    "container-title": "ACM Transactions on Information Systems",
    "id": "Yan2011",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2011,
          7,
          1
        ]
      ]
    },
    "keyword": "ir, topic_modeling",
    "language": "en-US",
    "page": "1-46",
    "title": "Toward a semantic granularity model for domain-specific information retrieval",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "DOI": "10.1016/0020-7373(92)90053-n",
    "ISSN": "00207373",
    "abstract": "This paper presents decision making elements in an anatomy of the design of undo support in the GNU Emacs environment. Apart from providing design guidelines for undo support, it illustrates how to bring a design from an abstract conception to a concrete realization and how to balance trade-offs in the process. Undo support is a usability feature of interactive computer systems which allows a user to reverse the effects of executed commands. GNU Emacs was chosen as a suitable environment to demonstrate how to design undo support because of its sophistication and practical significance. User’s opinions about which aspects of the existing undo support facility in Emacs need to be improved were solicited by conducting an informal survey among Emacs users. The results of the survey are discussed and were used to tailor a proposal for an improved undo support facility for Emacs. In order to test the adequacy of the proposal, it was subjected to an informal expert walk-through and a review of Emacs users opinions was conducted through a computer network. These evaluations are discussed and revisions to the proposal elicited. After the revised prototype of the design was implemented, a post-mortem evaluation was carried out and its results were incorporated in the final implementation.",
    "author": [
      {
        "family": "Yang",
        "given": "Yiya"
      }
    ],
    "container-title": "International Journal of Man-Machine Studies",
    "id": "Yang1992",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "authoring, emacs, hci, interactive_editing",
    "language": "en-US",
    "page": "81-95",
    "title": "Anatomy of the design of an undo support facility",
    "type": "article-journal",
    "volume": "36"
  },
  {
    "URL": "http://aclweb.org/anthology/W11-1513",
    "abstract": "In this paper, we explore the task of automatic text processing applied to collections of historical newspapers, with the aim of assisting historical research. In particular, in this first stage of our project, we experiment with the use of topical models as a means to identify potential issues of interest for historians.",
    "author": [
      {
        "family": "Yang",
        "given": "Tze-I"
      },
      {
        "family": "Torget",
        "given": "Andrew"
      },
      {
        "family": "Mihalcea",
        "given": "Rada"
      }
    ],
    "container-title": "Proceedings of the 5th ACL-HLT workshop on language technology for cultural heritage, social sciences, and humanities",
    "id": "Yang2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "cultural_heritage, topic_modeling",
    "language": "en-US",
    "page": "96-104",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Portland, OR, USA",
    "title": "Topic modeling on historical newspapers",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1108/00220410910998906",
    "ISSN": "0022-0418",
    "abstract": "<B>Purpose</B> - The purpose of this paper is to investigate the linking of a folksonomy (user vocabulary) and LCSH (controlled vocabulary) on the basis of word matching, for the potential use of LCSH in bringing order to folksonomies. <B>Design/methodology/approach</B> - A selected sample of a folksonomy from a popular collaborative tagging system, Delicious, was word-matched with LCSH. LCSH was transformed into a tree structure called an LCSH tree for the matching. A close examination was conducted on the characteristics of folksonomies, the overlap of folksonomies with LCSH, and the distribution of folksonomies over the LCSH tree. <B>Findings</B> - The experimental results showed that the total proportion of tags being matched with LC subject headings constituted approximately two-thirds of all tags involved, with an additional 10 percent of the remaining tags having potential matches. A number of barriers for the linking as well as two areas in need of improving the matching are identified and described. Three important tag distribution patterns over the LCSH tree were identified and supported: skewedness, multifacet, and Zipfian-pattern. <B>Research limitations/implications</B> - The results of the study can be adopted for the development of innovative methods of mapping between folksonomy and LCSH, which directly contributes to effective access and retrieval of tagged web resources and to the integration of multiple information repositories based on the two vocabularies. <B>Practical implications</B> - The linking of controlled vocabularies can be applicable to enhance information retrieval capability within collaborative tagging systems as well as across various tagging system information depositories and bibliographic databases. <B>Originality/value</B> - This is among frontier works that examines the potential of linking a folksonomy, extracted from a collaborative tagging system, to an authority-maintained subject heading system. It provides exploratory data to support further advanced mapping methods for linking the two vocabularies.",
    "author": [
      {
        "family": "Yi",
        "given": "Kwan"
      },
      {
        "family": "Chan",
        "given": "Lois M."
      }
    ],
    "container-title": "Journal of Documentation",
    "id": "Yi2009",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "collaborative_tagging, terminology",
    "language": "en-US",
    "page": "872-900",
    "title": "Linking folksonomy to Library of Congress subject headings: An exploratory study",
    "title-short": "Linking folksonomy to Library of Congress subject headings",
    "type": "article-journal",
    "volume": "65"
  },
  {
    "DOI": "10.1145/1330311.1330324",
    "abstract": "The goal of an LMS, devised by a growing number of universities, is to offer faculty instructional support. The actual use of these programs, however, suggests thatsupport is elusive. An experience at National Taiwan University illustrates how a university can increase faculty usage through better LMS design.",
    "author": [
      {
        "family": "Yueh",
        "given": "Hsiu P."
      },
      {
        "family": "Hsu",
        "given": "Shihkuan"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Yueh2008",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "59-63",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Designing a learning management system to support instruction",
    "type": "article-journal",
    "volume": "51"
  },
  {
    "DOI": "10.18352/bmgn-lchr.9344",
    "ISSN": "2211-2898",
    "abstract": "Digital humanities seem to be omnipresent these days and the discipline of history is no exception. This introduction is concerned with the changing practice of ’doing’ history in the digital age, seen within a broader historical context of developments in the digital humanities and ’digital history’. It argues that there is too much emphasis on tools and data while too little attention is being paid to how doing history in the digital age is changing as a result of the digital turn. This tendency towards technological determinism needs to be balanced by more attention to methodological and epistemological considerations. The article offers a short survey of history and computing since the 1960s with particular attention given to the situation in the Netherlands, considers various definitions of ’digital history’ and argues for an integrative view of historical practice in the digital age that underscores hybridity as its main characteristic. It then discusses some of the major changes in historical practice before outlining the three major themes that are explored by the various articles in this thematic issue – digitisation and the archive, digital historical analysis, and historical knowledge (re)presentation and audiences.",
    "author": [
      {
        "family": "Zaagsma",
        "given": "Gerben"
      }
    ],
    "container-title": "BMGN – Low Countries Historical Review",
    "id": "Zaagsma2013",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "3+",
    "title": "On digital history",
    "type": "article-journal",
    "volume": "128"
  },
  {
    "DOI": "10.1016/s0019-9958(65)90241-x",
    "ISSN": "00199958",
    "abstract": "A fuzzy set is a class of objects with a continuum of grades of membership. Such a set is characterized by a membership (characteristic) function which assigns to each object a grade of membership ranging between zero and one. The notions of inclusion, union, intersection, complement, relation, convexity, etc., are extended to such sets, and various properties of these notions in the context of fuzzy sets are established. In particular, a separation theorem for convex fuzzy sets is proved without requiring that the fuzzy sets be disjoint.",
    "author": [
      {
        "family": "Zadeh",
        "given": "Lotfi A."
      }
    ],
    "container-title": "Information and Control",
    "id": "Zadeh1965",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1965
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "338-353",
    "title": "Fuzzy sets",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "DOI": "10.1016/j.csda.2006.04.029",
    "ISSN": "01679473",
    "abstract": "Uncertainty is an attribute of information. The path-breaking work of Shannon has led to a universal acceptance of the thesis that information is statistical in nature. Concomitantly, existing theories of uncertainty are based on probability theory. The generalized theory of uncertainty (GTU) departs from existing theories in essential ways. First, the thesis that information is statistical in nature is replaced by a much more general thesis that information is a generalized constraint, with statistical uncertainty being a special, albeit important case. Equating information to a generalized constraint is the fundamental thesis of GTU. Second, bivalence is abandoned throughout GTU, and the foundation of GTU is shifted from bivalent logic to fuzzy logic. As a consequence, in GTU everything is or is allowed to be a matter of degree or, equivalently, fuzzy. Concomitantly, all variables are, or are allowed to be granular, with a granule being a clump of values drawn together by a generalized constraint. And third, one of the principal objectives of GTU is achievement of NL-capability, that is, the capability to operate on information described in natural language. NL-capability has high importance because much of human knowledge, including knowledge about probabilities, is described in natural language. NL-capability is the focus of attention in the present paper. The centerpiece of GTU is the concept of a generalized constraint. The concept of a generalized constraint is motivated by the fact that most real-world constraints are elastic rather than rigid, and have a complex structure even when simple in appearance. The paper concludes with examples of computation with uncertain information described in natural language.",
    "author": [
      {
        "family": "Zadeh",
        "given": "Lotfi A."
      }
    ],
    "container-title": "Computational Statistics & Data Analysis",
    "id": "Zadeh2006",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "uncertainty",
    "language": "en-US",
    "page": "15-46",
    "title": "Generalized theory of uncertainty (GTU)—principal concepts and ideas",
    "type": "article-journal",
    "volume": "51"
  },
  {
    "DOI": "10.1007/bf02395110",
    "ISSN": "0010-4817",
    "author": [
      {
        "family": "Zampolli",
        "given": "Antonio"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Zampolli1973",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          1973
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "343-360",
    "title": "Humanities computing in italy",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "URL": "http://ietf.org/rfc/rfc4510.txt",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Zeilenga",
        "given": "Kurt D."
      }
    ],
    "genre": "RFC",
    "id": "Zeilenga2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "number": "4510",
    "publisher": "Internet Engineering Task Force",
    "title": "Lightweight Directory Access Protocol (LDAP): Technical Specification Road Map",
    "type": "report"
  },
  {
    "URL": "http://ucrel.lancs.ac.uk/publications/CL2007/paper/60_Paper.pdf",
    "abstract": "This paper explores methods for the extrapolation of correspondences in a small parallel diachronic corpus taken from the Modern and Middle Polish Bible, in an attempt to answer the question “can historical grammar and lexica be derived directly from a corpus?” The problem of extracting this data is approached from a machine translation point of view: by envisioning texts from different periods as language models for their respective language stages, and historical grammar as a translation model mapping one language stage onto another. This notion is explored using automatic extraction of morphological, lexical and syntactic correspondences.",
    "author": [
      {
        "family": "Zeldes",
        "given": "Amir"
      }
    ],
    "container-title": "Proceedings of the corpus linguistics conference CL2007",
    "editor": [
      {
        "family": "Davies",
        "given": "Matthew"
      },
      {
        "family": "Rayson",
        "given": "Paul"
      },
      {
        "family": "Hunston",
        "given": "Susan"
      },
      {
        "family": "Danielsson",
        "given": "Pernilla"
      }
    ],
    "id": "Zeldes2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "corpus_linguistics, cultural_heritage, polish",
    "language": "en-US",
    "publisher": "University of Birmingham",
    "title": "Machine translation between language stages: Extracting historical grammar from a parallel diachronic corpus of Polish",
    "title-short": "Machine translation between language stages",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.linguistik.hu-berlin.de/institut/professuren/korpuslinguistik/mitarbeiter-innen/amir/pdf/CL2009_ANNIS_pre.pdf",
    "abstract": "ANNIS (see Dipper & Götze 2005; Chiarcos et al. 2008) is a flexible web-based corpus architecture for search and visualization of multi-layer linguistic corpora. By multi-layer we mean that the same primary datum may be annotated independently with (i) annotations of different types (spans, DAGs with labelled edges and arbitrary pointing relations between terminals or non-terminals), and (ii) annotation structures that possibly overlap and/or conflict hierarchically. In this paper we present the different features of the architecture as well as actual use cases for corpus linguistic research on such diverse areas as information structure, learner language and discourse level phenomena.",
    "author": [
      {
        "family": "Zeldes",
        "given": "Amir"
      },
      {
        "family": "Ritz",
        "given": "Julia"
      },
      {
        "family": "Lüdeling",
        "given": "Anke"
      },
      {
        "family": "Chiarcos",
        "given": "Christian"
      }
    ],
    "container-title": "Proceedings of corpus linguistics 2009",
    "id": "Zeldes2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "corpus_linguistics",
    "language": "en-US",
    "title": "ANNIS: A search tool for multi-layer annotated corpora",
    "title-short": "ANNIS",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Zeller",
        "given": "Andreas"
      }
    ],
    "container-title": "ITiCSE ’00: Proceedings of the 5<sup>th</sup> annual SIGCSE/SIGCUE ITiCSE conference on innovation and technology in computer science education",
    "id": "Zeller2000",
    "issued": {
      "date-parts": [
        [
          2000,
          7
        ]
      ]
    },
    "page": "89-92",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Making students read and review code",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.allbusiness.com/services/educational-services/4275225-1.html",
    "abstract": "\"We need to wake up and recognize that information is not instruction. There is this belief that all you need for learning is information and collaboration: Put enough people and enough information on the Web, and learning will happen.... There isn’t enough guidance and structure there [on the Internet] for someone to learn a systematic body of knowledge.\" These and similar remarks were made by instructional technology pioneer M. David Merrill in a recently-published interview [\"Wake Up! (And Reclaim Instructional Design)\" by Ron Zemke in TRAINING, vol. 35, no. 6, June 1998, pp. 36-38, 40, 42]. Merrill, professor of instructional technology at Utah State University, has been a researcher, teacher, and practitioner in instructional design for over three decades. He is a critic of what he terms \"wild speculation and philosophical extremism\" in instructional design. Last year he (along with others from Utah State’s ID2 Research Group) wrote \"Reclaiming Instructional Design,\" a paper which attempts to steer the field of instructional design back to its scientific roots. The paper is available on the Web at http://www.coe.usu.edu/it/id2/reclaim.html",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Zemke",
        "given": "Ron"
      }
    ],
    "container-title": "Training",
    "id": "Zemke1998",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          1998,
          6
        ]
      ]
    },
    "note": "Interview with M. David Merrill",
    "page": "36-38, 40, 42",
    "title": "Wake up! (And reclaim instructional design)",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "URL": "http://elara.tk.informatik.tu-darmstadt.de/Publications/2007/gldv07_10p.pdf",
    "abstract": "We analyze Wikipedia as a lexical semantic resource and compare it with conventional resources, such as dictionaries, thesauri, semantic wordnets, etc. Different parts of Wikipedia reflect different aspects of these resources. We show that Wikipedia contains a vast amount of knowledge about, e.g., named entities, domain specific terms, and rare word senses. If Wikipedia is to be used as a lexical semantic resource in large-scale NLP tasks, efficient programmatic access to the knowledge therein is required. We review existing access mechanisms and show that they are limited with respect to performance and the provided access functions. Therefore, we introduce a general purpose, high performance Java-based Wikipedia API that overcomes these limitations. It is available for research purposes at http://www.ukp.tu-darmstadt.de/software/WikipediaAPI.",
    "author": [
      {
        "family": "Zesch",
        "given": "Torsten"
      },
      {
        "family": "Gurevych",
        "given": "Iryna"
      },
      {
        "family": "Mühlhäuser",
        "given": "Max"
      }
    ],
    "container-title": "Datenstrukturen für linguistische ressourcen und ihre anwendungen/data structures for linguistic resources and applications. Proceedings of the biennial GLDV conference 2007",
    "editor": [
      {
        "family": "Rehm",
        "given": "Georg"
      },
      {
        "family": "Witt",
        "given": "Andreas"
      },
      {
        "family": "Lemnitzer",
        "given": "Lothar"
      }
    ],
    "id": "Zesch2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "wikipedia",
    "language": "en-US",
    "page": "197-205",
    "publisher": "Gunter Narr",
    "publisher-place": "Tübingen, Germany",
    "title": "Analyzing and accessing Wikipedia as a lexical semantic resource",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.lrec-conf.org/proceedings/lrec2008/pdf/420_paper.pdf",
    "abstract": "Recently, collaboratively constructed resources such as Wikipedia and Wiktionary have been discovered as valuable lexical semantic knowledge bases with a high potential in diverse Natural Language Processing (NLP) tasks. Collaborative knowledge bases however significantly differ from traditional linguistic knowledge bases in various respects, and this constitutes both an asset and an impediment for research in NLP. This paper addresses one such major impediment, namely the lack of suitable programmatic access mechanisms to the knowledge stored in these large semantic knowledge bases. We present two application programming interfaces for Wikipedia and Wiktionary which are especially designed for mining the rich lexical semantic information dispersed in the knowledge bases, and provide efficient and structured access to the available knowledge. As we believe them to be of general interest to the NLP community, we have made them freely available for research purposes.",
    "author": [
      {
        "family": "Zesch",
        "given": "Torsten"
      },
      {
        "family": "Müller",
        "given": "Christof"
      },
      {
        "family": "Gurevych",
        "given": "Iryna"
      }
    ],
    "container-title": "Proceedings of the 6<sup>th</sup> international conference on language resources and evaluation (LREC 2008)",
    "id": "Zesch2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "wikipedia",
    "language": "en-US",
    "publisher": "European Language Resources Association (ELRA)",
    "title": "Extracting lexical semantic knowledge from Wikipedia and Wiktionary",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1276318.1276342",
    "ISBN": "978-1-59593-680-6",
    "abstract": "We describe and discuss the use of semantics-based citation networks in a new legal research tool. Such networks are generated based on citation relations between cases found in legal corpora as well as legal issues being discussed with these citations. Unlike traditional tools, the System allows legal professionals to efficiently study legal issues without having to go through whole cases or tedious manual citation search. This shift of focus from cases to individual issues within cases would greatly reduce time required for attorneys and legal scholars who have specific research problems in mind. The Systems User Interface (UI) allows users to easily navigate in the citation networks and study how citations are interrelated and how legal issues have evolved in the past. Various forms of natural language processing (NLP) technologies are used in building the metadata behind the prototype. Formal evaluation confirmed the Systems capability of accurately identifying citations relevant to given legal issues.",
    "author": [
      {
        "family": "Zhang",
        "given": "Paul"
      },
      {
        "family": "Koppaka",
        "given": "Lavanya"
      }
    ],
    "collection-title": "ICAIL ’07",
    "container-title": "Proceedings of the 11th international conference on artificial intelligence and law",
    "id": "Zhang2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "todo",
    "language": "en-US",
    "page": "123-130",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Semantics-based legal citation network",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/P10-1115",
    "abstract": "Probabilistic latent topic models have recently enjoyed much success in extracting and analyzing latent topics in text in an unsupervised way. One common deficiency of existing topic models, though, is that they would not work well for extracting cross-lingual latent topics simply because words in different languages generally do not co-occur with each other. In this paper, we propose a way to incorporate a bilingual dictionary into a probabilistic topic model so that we can apply topic models to extract shared latent topics in text data of different languages. Specifically, we propose a new topic model called Probabilistic Cross-Lingual Latent Semantic Analysis (PCLSA) which extends the Probabilistic Latent Semantic Analysis (PLSA) model by regularizing its likelihood function with soft constraints defined based on a bilingual dictionary. Both qualitative and quantitative experimental results show that the PCLSA model can effectively extract cross-lingual latent topics from multilingual text data.",
    "author": [
      {
        "family": "Zhang",
        "given": "Duo"
      },
      {
        "family": "Mei",
        "given": "Qiaozhu"
      },
      {
        "family": "Zhai",
        "given": "ChengXiang"
      }
    ],
    "container-title": "Proceedings of the 48th annual meeting of the association for computational linguistics (ACL ’10)",
    "id": "Zhang2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "topic_modeling",
    "language": "en-US",
    "page": "1128-1137",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Cross-lingual latent topic extraction",
    "type": "paper-conference"
  },
  {
    "ISBN": "978-1-932432-88-6",
    "URL": "http://aclweb.org/anthology/P/P11/P11-2085.pdf",
    "abstract": "Chinese Pinyin input method is very important for Chinese language information processing. Users may make errors when they are typing in Chinese words. In this paper, we are concerned with the reasons that cause the errors. Inspired by the observation that pressing backspace is one of the most common user behaviors to modify the errors, we collect 54,309,334 error-correction pairs from a real-world data set that contains 2,277,786 users via backspace operations. In addition, we present a comparative analysis of the data to achieve a better understanding of users’ input behaviors. Comparisons with English typos suggest that some language-specific properties result in a part of Chinese input errors.",
    "author": [
      {
        "family": "Zheng",
        "given": "Yabin"
      },
      {
        "family": "Xie",
        "given": "Lixing"
      },
      {
        "family": "Liu",
        "given": "Zhiyuan"
      },
      {
        "family": "Sun",
        "given": "Maosong"
      },
      {
        "family": "Zhang",
        "given": "Yang"
      },
      {
        "family": "Ru",
        "given": "Liyun"
      }
    ],
    "collection-title": "HLT ’11",
    "container-title": "Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies (HLT ’11)",
    "id": "Zheng2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "hci, interactive_editing",
    "language": "en-US",
    "page": "485-490",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Why press backspace?: Understanding user input behaviors in Chinese Pinyin input method",
    "title-short": "Why press backspace?",
    "type": "paper-conference",
    "volume": "2"
  },
  {
    "DOI": "10.1145/1148170.1148336",
    "ISBN": "1-59593-369-7",
    "abstract": "One challenging problem for biomedical text retrieval is to find accurate synonyms or name variants for biomedical entities. In this paper, we propose a new concept-based approach to tackle this problem. In this approach, a set of concepts instead of keywords will be extracted from a query first. Then these concepts will be used for retrieval purpose. The experiment results show that the proposed approach can boost the retrieval performance and it generates very good results on 2005 TREC Genomics data sets.",
    "author": [
      {
        "family": "Zhong",
        "given": "Ming"
      },
      {
        "family": "Huang",
        "given": "Xiangji"
      }
    ],
    "container-title": "Proceedings of the 29<sup>th</sup> annual international ACM SIGIR conference on research and development in information retrieval (SIGIR ’06)",
    "id": "Zhong2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "ir",
    "language": "en-US",
    "page": "723-724",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Concept-based biomedical text retrieval",
    "type": "paper-conference"
  },
  {
    "URL": "http://aclweb.org/anthology/C10-1150",
    "abstract": "Ambiguity of entity mentions and concept references is a challenge to mining text beyond surface-level keywords. We describe an effective method of disambiguating surface forms and resolving them to Wikipedia entities and concepts. Our method employs an extensive set of features mined from Wikipedia and other large data sources, and combines the features using a machine learning approach with automatically generated training data. Based on a manually labeled evaluation set containing over 1000 news articles, our resolution model has 85% precision and 87.8% recall. The performance is significantly better than three baselines based on traditional context similarities or sense commonness measurements. Our method can be applied to other languages and scales well to new entities and concepts.",
    "author": [
      {
        "family": "Zhou",
        "given": "Yiping"
      },
      {
        "family": "Nie",
        "given": "Lan"
      },
      {
        "family": "Kalleh",
        "given": "Omid R."
      },
      {
        "family": "Vasile",
        "given": "Flavian"
      },
      {
        "family": "Gaffney",
        "given": "Scott"
      }
    ],
    "collection-title": "COLING 2010",
    "container-title": "Proceedings of the 23rd international conference on computational linguistics",
    "id": "Zhou2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "topic_modeling, wikipedia",
    "language": "en-US",
    "page": "1335-1343",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Resolving surface forms to Wikipedia topics",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Zielinski",
        "given": "Andrea"
      },
      {
        "family": "Simon",
        "given": "Christian"
      }
    ],
    "container-title": "Seventh international workshop on finite-state methods and natural language processing",
    "id": "Zielinski2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "morphology, nlp",
    "page": "177-184",
    "title": "Morphisto: An open-source morphological analyzer for German",
    "title-short": "Morphisto",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/978-3-642-04131-0_5",
    "abstract": "This paper presents the current activities surrounding Morphisto, an open source morphological analyzer/generator for German, based on the SMOR-based SFST tools. Morphisto was designed as a user-friendly application within the eHumanities project TextGrid. It comprises a minimal lexicon component that works in tandem with SMOR and is now being extended within the ELEXIKO project. Additional tools for the management of lexical data and services built on top of the finite state transducer are also integrated as Web Services in the grid, so that all resources can be shared readily among lexicographers, linguists, and finite-state developers.",
    "author": [
      {
        "family": "Zielinski",
        "given": "Andrea"
      },
      {
        "family": "Simon",
        "given": "Christian"
      },
      {
        "family": "Wittl",
        "given": "Tilmann"
      }
    ],
    "collection-title": "Communications in computer and information science",
    "container-title": "State of the art in computational morphology",
    "editor": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "Zielinski2009a",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "morphology, mxp",
    "language": "en-US",
    "page": "64-75",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Morphisto: Service-oriented open source morphology for German",
    "title-short": "Morphisto",
    "type": "chapter",
    "volume": "41"
  },
  {
    "DOI": "10.1093/llc/fqp016",
    "abstract": "This article describes the life cycle of a TEI Document within TextGrid, an eHumanities platform for scholarly text processing, in which structured search is based on the TEI framework and metadata with restricted values. A workbench is provided that offers tools for handling TEI documents, TextGridLab, making it easier to annotate, process, search, and persistently store new digitized texts. The digitization and annotation of the Campe dictionary1 serves as a first test bed. The overall framework of TextGrid is very generic and can handle different types of text (literary editions, linguistic corpora, lexica) as well as heterogeneous data formats (plain text, XML/TEI, images). In fact, the TextGrid repository, TextGridRep, is designed as a digital virtual library over federated archives, where humanities projects are invited to participate. Sharing of data is enabled by means of a grid-based architecture. Specifically the middleware includes most of the treatment of authorization, search, and file management. TextGrid is entirely based on open source software including Eclipse2 and Globus Toolkit.",
    "author": [
      {
        "family": "Zielinski",
        "given": "Andrea"
      },
      {
        "family": "Pempe",
        "given": "Wolfgang"
      },
      {
        "family": "Gietz",
        "given": "Peter"
      },
      {
        "family": "Haase",
        "given": "Martin"
      },
      {
        "family": "Funk",
        "given": "Stefan"
      },
      {
        "family": "Simon",
        "given": "Christian"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "Zielinski2009b",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage, tei, xml",
    "language": "en-US",
    "page": "267-279",
    "title": "TEI documents in the grid",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "id": "Zope",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "title": "Welcome to zope.org, http://www.zope.org",
    "title-short": "Welcome to zope.org, http",
    "type": ""
  },
  {
    "URL": "http://zope.org/Members/upfront/zqti/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Systems",
        "given": "Upfront"
      }
    ],
    "id": "Zqti",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "note": "“vapour & some screenshots for now”",
    "title": "ZQTI",
    "type": ""
  },
  {
    "author": [
      {
        "family": "Züllighoven",
        "given": "Heinz"
      },
      {
        "family": "Beeger",
        "given": "Robert F."
      }
    ],
    "id": "Zuellighoven2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "publisher": "Elsevier",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "Object-oriented construction handbook: Developing application-oriented software with the tools & materials approach",
    "title-short": "Object-oriented construction handbook",
    "type": "book"
  },
  {
    "abstract": "Aleph is a programming language for batch typesetting systems. It has conventional programming-language features for implementing style packages, extensions, and even the typesetting engine itself. At the same time, an extensible syntax makes Aleph a convenient document input format. This paper introduces Aleph and demonstrates with examples the convenience of implementing typesetting algorithms in Aleph.",
    "author": [
      {
        "family": "Semenzato",
        "given": "Luigi"
      },
      {
        "family": "Wang",
        "given": "Edward"
      }
    ],
    "container-title": "EP92 (proceedings of electronic publishing 1992)",
    "editor": [
      {
        "family": "Vanoirbeek",
        "given": "Christine"
      },
      {
        "family": "Coray",
        "given": "G."
      }
    ],
    "id": "citeulike:1077291",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "lisp, typesetting",
    "language": "en-US",
    "page": "65-75",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge",
    "title": "Aleph – a language for typesetting",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Klöcker",
        "given": "Jens"
      },
      {
        "family": "Knappen",
        "given": "Jörg"
      }
    ],
    "container-title": "EuroTeX’99 proceedings",
    "editor": [
      {
        "family": "Partosch",
        "given": "Günter"
      },
      {
        "family": "Wilhelms",
        "given": "Gerhard"
      }
    ],
    "id": "citeulike:1080141",
    "issued": {
      "date-parts": [
        [
          1999,
          9
        ]
      ]
    },
    "keyword": "markup, mxp, typesetting",
    "language": "en-US",
    "page": "281-289",
    "publisher": "Ruprecht-Karls-Universität Heidelberg",
    "publisher-place": "Gießen, Augsburg",
    "title": "Is LaTeX 2e markup sufficient for scientific articles?",
    "type": "paper-conference"
  },
  {
    "abstract": "Übungen sind ein zentrales Element in der Informatiklehre. Ausgehend von didaktischen Überlegungen, wie der Übungsbetrieb durch Komponenten des ELearning, insbesondere durch Formen des Computer-Aided Assessment, intensiviert und effizienter gestaltet werden kann, haben wir die EduComponents entwickelt. Dabei handelt es sich um eine Sammlung von Erweiterungsmodulen, die ein allgemeines CMS (Plone) um E-Learning-Funktionalität ergänzen. Seit mehreren Semestern werden diese frei verfügbaren Module sowohl in allen Lehrveranstaltungen unserer Arbeitsgruppe als auch an anderen Institutionen erfolgreich eingesetzt.",
    "author": [
      {
        "family": "Rösner",
        "given": "Dietmar"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "collection-title": "Lecture notes in informatics",
    "container-title": "HDI2006: Hochschuldidaktik der informatik",
    "editor": [
      {
        "family": "Forbrig",
        "given": "Peter"
      },
      {
        "family": "Siegel",
        "given": "Günter"
      },
      {
        "family": "Schneider",
        "given": "Markus"
      }
    ],
    "id": "citeulike:1080157",
    "issued": {
      "date-parts": [
        [
          2006,
          12
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "language": "en-US",
    "page": "89-102",
    "publisher": "GI-Verlag",
    "publisher-place": "Bonn",
    "title": "E-Learning-komponenten zur intensivierung der übungen in der Informatik-Lehre – ein erfahrungsbericht",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/1140124.1140150",
    "ISBN": "1595930558",
    "author": [
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "container-title": "ITICSE ’06: Proceedings of the 11th annual SIGCSE conference on innovation and technology in computer science education",
    "id": "citeulike:1080161",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "language": "en-US",
    "page": "88-92",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "EduComponents: Experiences in e-assessment in computer science education",
    "title-short": "EduComponents",
    "type": "paper-conference"
  },
  {
    "ISSN": "1531-605X",
    "PMID": "11825186",
    "URL": "http://view.ncbi.nlm.nih.gov/pubmed/11825186",
    "abstract": "Many lexical items from medical sublanguages exhibit a complex morphological structure that is hard to account for by simple string matching (e.g., truncation). While inflection is usually easy to deal with, productive morphological processes in terms of derivation and (single-word) composition constitute a real challenge. We here propose an approach in which morphologically complex word forms are segmented into medically significant subwords. After segmentation, both query terms and document terms are submitted to the matching procedure. This way, problems arising from morphologically motivated word form alterations can be eliminated from the retrieval procedure. We provide empirical data which reveals that subword-based indexing and retrieval performs significantly better than conventional string matching approaches.",
    "author": [
      {
        "family": "Hahn",
        "given": "Udo"
      },
      {
        "family": "Honeck",
        "given": "Martin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Schulz",
        "given": "Stefan"
      }
    ],
    "container-title": "Proceedings of the 2001 AMIA annual symposium",
    "editor": [
      {
        "family": "Bakken",
        "given": "Suzanne"
      }
    ],
    "id": "citeulike:1080173",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "ir, mxp",
    "language": "en-US",
    "page": "229-233",
    "publisher": "AMIA",
    "publisher-place": "Bethesda, MD, USA",
    "title": "Subword segmentation—leveling out morphological variations for medical document retrieval",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "container-title": "Open source for knowledge and learning management: Strategies beyond tools",
    "editor": [
      {
        "family": "Lytras",
        "given": "Miltiadis"
      },
      {
        "family": "Naeve",
        "given": "Ambjörn"
      }
    ],
    "id": "citeulike:1080183",
    "issued": {
      "date-parts": [
        [
          2007,
          2
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "language": "en-US",
    "publisher": "Idea Group",
    "publisher-place": "Hershey, PA, USA",
    "title": "EduComponents: Educational components for plone",
    "title-short": "EduComponents",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:11018085",
    "issued": {
      "date-parts": [
        [
          2012,
          8,
          6
        ]
      ]
    },
    "keyword": "mxp, slides",
    "language": "en-US",
    "title": "NLP for historical Texts—Session 1",
    "type": ""
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:11021869",
    "issued": {
      "date-parts": [
        [
          2012,
          8,
          7
        ]
      ]
    },
    "keyword": "mxp, slides",
    "language": "en-US",
    "title": "NLP for historical Texts—Session 2",
    "type": ""
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:11029026",
    "issued": {
      "date-parts": [
        [
          2012,
          8,
          8
        ]
      ]
    },
    "keyword": "mxp, slides",
    "language": "en-US",
    "title": "NLP for historical Texts—Session 3",
    "type": ""
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:11033782",
    "issued": {
      "date-parts": [
        [
          2012,
          8,
          9
        ]
      ]
    },
    "keyword": "mxp, slides",
    "language": "en-US",
    "title": "NLP for historical Texts—Session 4",
    "type": ""
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:11037625",
    "issued": {
      "date-parts": [
        [
          2012,
          8,
          10
        ]
      ]
    },
    "keyword": "mxp, slides",
    "language": "en-US",
    "title": "NLP for historical Texts—Session 5",
    "type": ""
  },
  {
    "DOI": "10.1007/11431053_7",
    "ISBN": "978-3-540-26124-7",
    "abstract": "The Resource Description Framework (RDF) is a metadata model and language recommended by the W3C. This paper presents a framework to incorporate temporal reasoning into RDF, yielding temporal RDF graphs . We present a semantics for temporal RDF graphs, a syntax to incorporate temporality into standard RDF graphs, an inference system for temporal RDF graphs, complexity bounds showing that entailment in temporal RDF graphs does not yield extra asymptotic complexity with respect to standard RDF graphs and sketch a temporal query language for RDF.",
    "author": [
      {
        "family": "Gutierrez",
        "given": "Claudio"
      },
      {
        "family": "Hurtado",
        "given": "Carlos"
      },
      {
        "family": "Vaisman",
        "given": "Alejandro"
      }
    ],
    "chapter-number": "7",
    "collection-title": "Lecture notes in computer science",
    "editor": [
      {
        "family": "Gómez-Pérez",
        "given": "Asunción"
      },
      {
        "family": "Euzenat",
        "given": "Jérôme"
      }
    ],
    "id": "citeulike:11044376",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "rdf, temporal_data",
    "language": "en-US",
    "page": "167-199",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Temporal RDF",
    "type": "chapter",
    "volume": "3532"
  },
  {
    "abstract": "This paper presents our approach for supporting face-to-face courses with software components for e-learning based on a general-purpose content management system (CMS). These components—collectively named eduComponents—can be combined with other modules to create tailormade, sustainable learning environments, which help to make teaching and learning more efficient and effective. We give a short overview of these components, and we report on our practical experiences with the software in our courses.",
    "author": [
      {
        "family": "Rösner",
        "given": "Dietmar"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      }
    ],
    "container-title": "Proceedings of the german e-science conference (GES 2007)",
    "id": "citeulike:1291202",
    "issued": {
      "date-parts": [
        [
          2007,
          5
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "language": "en-US",
    "publisher": "Helmholtz-Gemeinschaft, Max-Planck-Gesellschaft, Hochschulrektorenkonferenz (HRK)",
    "title": "A sustainable learning environment based on an open source content management system",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1093/llc/fqh043",
    "ISSN": "0268-1145",
    "abstract": "The term document is used in various contexts, often referring to very different things. This article argues that we need to avoid a restrictive, essentialist definition of the concept and instead study the cognitive models that guide our way of viewing documents in different situations. Examples are drawn from the Library and Information field to show how the view of documents is influenced by different cognitive models and how more complex understandings may be described in terms of clusters of models. Such a set of tools for discussing the concept will be particularly useful as we are facing a whole range of new types of “documents” made possible by digital media.",
    "author": [
      {
        "family": "Francke",
        "given": "Helena"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "citeulike:137022",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2005,
          3
        ]
      ]
    },
    "keyword": "document_research, sslis",
    "language": "en-US",
    "page": "61-69",
    "publisher": "Oxford University Press",
    "title": "Whats in a name? Contextualizing the document concept",
    "type": "article-journal",
    "volume": "20"
  },
  {
    "DOI": "10.1145/1016850.1016880",
    "ISBN": "1-58113-905-5",
    "abstract": "Among slide-presentation systems, the dominant application offers essentially no abstraction capability. Slideshow, an extension of PLT Scheme, represents our effort over the last several years to build an abstraction-friendly slide system. We show how functional programming is well suited to the task of slide creation, we report on the programming abstractions that we have developed for slides, and we describe our solutions to practical problems in rendering slides. We also describe a prototype extension to DrScheme that supports a mixture of programmatic and WYSIWYG slide creation.",
    "author": [
      {
        "family": "Findler",
        "given": "Robert B."
      },
      {
        "family": "Flatt",
        "given": "Matthew"
      }
    ],
    "collection-number": "9",
    "collection-title": "ICFP ’04",
    "container-title": "Proceedings of the ninth ACM SIGPLAN international conference on functional programming",
    "id": "citeulike:1392",
    "issued": {
      "date-parts": [
        [
          2004,
          9
        ]
      ]
    },
    "keyword": "functional, lisp, typesetting",
    "language": "en-US",
    "page": "224-235",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Slideshow: Functional presentations",
    "title-short": "Slideshow",
    "type": "paper-conference",
    "volume": "39"
  },
  {
    "URL": "http://wdok.cs.uni-magdeburg.de/publikationen/alle-publikationen/inproceedingsreference.2007-06-12.8442215174",
    "abstract": "Die Erstellung qualitativ hochwertiger Tests ist aufwändig. Daher ist es wünschenswert, einmal erstellte Tests wieder- und weiterverwenden zu können. Um eine Abhängigkeit von einer einzelnen Testplattform zu vermeiden, werden standardisierte Austauschformate benötigt. In diesem Beitrag formulieren wir Desiderata für derartige Formate und untersuchen den derzeitigen De-Facto-Standard, die IMS Question & Test Interoperability Specification (QTI), auf seine Eignung. Das erklärte Ziel von QTI ist es, den Austausch von Tests zwischen verschiedenen Systemen zu ermöglichen. Nach der Analyse der Spezifikation und aufgrund unserer Erfahrungen bei der Implementierung von QTI im System ECQuiz kommen wir zu dem Schluss, dass QTI jedoch als Austauschformat ungeeignet ist.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Fenske",
        "given": "Wolfram"
      }
    ],
    "collection-title": "Lecture notes in informatics",
    "container-title": "DeLFI 2007: 5. E-learning fachtagung informatik",
    "id": "citeulike:1401732",
    "issued": {
      "date-parts": [
        [
          2007,
          9
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "language": "en-US",
    "page": "185-196",
    "publisher": "GI; GI-Verlag",
    "publisher-place": "Bonn",
    "title": "Interoperabilität von elektronischen tests",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/181550.181565",
    "ISSN": "0163-5808",
    "author": [
      {
        "family": "Loeffen",
        "given": "Arjan"
      }
    ],
    "container-title": "SIGMOD Rec.",
    "id": "citeulike:144034",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1994,
          3
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "page": "97-106",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Text databases: A survey of text models and systems",
    "title-short": "Text databases",
    "type": "article-journal",
    "volume": "23"
  },
  {
    "URL": "http://cogprints.org/3616/",
    "abstract": "Two studies using the methods of experimental psychology assessed the effects of two types of text presentation (page-by-page vs. scrolling) on participants’ performance while reading and revising texts. Greater facilitative effects of the page-by-page presentation were observed in both tasks. The participants’ reading task performance indicated that they built a better mental representation of the text as a whole and were better at locating relevant information and remembering the main ideas. Their revising task performance indicated a larger number of global corrections (which are the most difficult to make).",
    "author": [
      {
        "family": "Piolat",
        "given": "Annie"
      },
      {
        "family": "Roussey",
        "given": "Jean-Yves"
      },
      {
        "family": "Thunin",
        "given": "Olivier"
      }
    ],
    "container-title": "International Journal of Human-Computer Studies",
    "id": "citeulike:1596364",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "interactive_editing, psychology",
    "language": "en-US",
    "page": "565-589",
    "title": "Effects of screen presentation on text reading and revising",
    "type": "article-journal",
    "volume": "47"
  },
  {
    "DOI": "10.1145/142750.143055",
    "ISBN": "0897915135",
    "abstract": "A user study was conducted to investigate how people deal with the flow of information in their workspaces. Subjects reported that, in an attempt to quickly and informally manage their information, they created piles of documents. Piles were seen as complementary to the folder filing system, which was used for more formal archiving. A new desktop interface element–the pile– was developed and prototyped through an iterative process. The design includes direct manipulation techniques and support for browsing, and goes beyond physical world functionality by providing system assistance for automatic pile construction and reorganization. Preliminary user tests indicate the design is promising and raise issues that will be addressed in future work.",
    "author": [
      {
        "family": "Mander",
        "given": "Richard"
      },
      {
        "family": "Salomon",
        "given": "Gitta"
      },
      {
        "family": "Wong",
        "given": "Yin Y."
      }
    ],
    "container-title": "CHI ’92: Proceedings of the SIGCHI conference on human factors in computing systems",
    "id": "citeulike:176888",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "document_management",
    "language": "en-US",
    "page": "627-634",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A “pile” metaphor for supporting casual organization of information",
    "type": "paper-conference"
  },
  {
    "ISBN": "0415263468",
    "abstract": "There is a technological revolution taking place in higher education. The growth of ’e-learning’ is being described as explosive, unprecedented and disruptive. E-Learning in the 21st Century provides a framework for understanding the application and characteristics of e-learning in higher education. The authors discuss their extensive research from technological, pedagogical and organizational perspectives in order to create practical models and release the full potential of e-learning. This in-depth understanding will give direction and guidance to educators who wish to facilitate critical discourse and higher-order learning through the use of electronic technologies in a networked learning context.",
    "author": [
      {
        "family": "Garrison",
        "given": "Randy D."
      },
      {
        "family": "Anderson",
        "given": "Terry"
      }
    ],
    "id": "citeulike:1890367",
    "issued": {
      "date-parts": [
        [
          2002,
          12,
          26
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "Paperback; RoutledgeFalmer",
    "title": "E-Learning in the 21st century: A framework for research and practice",
    "title-short": "E-Learning in the 21st century",
    "type": "book"
  },
  {
    "DOI": "10.1108/00400910110399247",
    "abstract": "The focus of much e-learning activity is upon the development of courses and their resources. Successful e-learning takes place within a complex system involving the student experience of learning, teachers’ strategies, teachers’ planning and thinking, and the teaching/ learning context. Staff development for e-learning focuses around the level of technological delivery strategies when other issues such as the teachers’ conception of learning has a major influence on the planning of courses, development of teaching strategies and what students learn. This article proposes a more comprehensive framework for the design, development and implementation of e-learning systems in higher education.",
    "author": [
      {
        "family": "Alexander",
        "given": "Shirley"
      }
    ],
    "container-title": "Education + Training",
    "id": "citeulike:1890432",
    "issue": "4/5",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "240-248",
    "title": "E-learning developments and experiences",
    "type": "article-journal",
    "volume": "43"
  },
  {
    "URL": "http://educause.edu/educatingthenetgen",
    "abstract": "The Net Generation has grown up with information technology. The aptitudes, attitudes, expectations, and learning styles of Net Gen students reflect the environment in which they were raised—one that is decidedly different from that which existed when faculty and administrators were growing up. This collection explores the Net Gen and the implications for institutions in areas such as teaching, service, learning space design, faculty development, and curriculum. Contributions by educators and students are included.",
    "editor": [
      {
        "family": "Oblinger",
        "given": "Diana G."
      },
      {
        "family": "Oblinger",
        "given": "James L."
      }
    ],
    "id": "citeulike:1895763",
    "issued": {
      "date-parts": [
        [
          2005,
          6
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "Educause",
    "title": "Educating the net generation",
    "type": "book"
  },
  {
    "ISBN": "0415408741",
    "abstract": "Incorporating a variety of contexts face-to-face, self-directed, blended and distance learning modes this book examines different perspectives on effectively designing and delivering learning activities to ensure that future development is pedagogically sound, learner focused and accessible. It considers key topics including: * specific activities for achieving learning outcomes * classifies and clarifies technologies uses for learning and role in educational design * current systems and future developments * learners competencies and approaches * designing for mobile technologies * practitioner development * sustainability, organisational barriers and learning communities. Illustrated by case studies from the disciplines and with a helpful appendix of tools and resources for researchers, practitioners and teachers, this book is an essential guide to effective design and implementation of sound e-learning activities.",
    "edition": "New edition",
    "editor": [
      {
        "family": "Beetham",
        "given": "Helen"
      },
      {
        "family": "Sharpe",
        "given": "Rhona"
      }
    ],
    "id": "citeulike:2114200",
    "issued": {
      "date-parts": [
        [
          2007,
          6,
          16
        ]
      ]
    },
    "keyword": "e-learning, pedagogy",
    "language": "en-US",
    "publisher": "Paperback; Routledge",
    "title": "Rethinking pedagogy for a digital age: Designing and delivering E-Learning",
    "title-short": "Rethinking pedagogy for a digital age",
    "type": "book"
  },
  {
    "ISBN": "0415414318",
    "abstract": "<P>Virtual learning environments are widely spread in higher education, yet they are often under utilised by the institutions that employ them. This book addresses the need to move beyond thinking about the VLE simply in terms of the particular package that an institution has adopted, and viewing it as a significant educational technology that will shape much of the teaching and learning process in the coming years.</P><P></P><P>Considering how virtual learning environments can be successfully deployed and used for effective teaching, it sets out a model for effective use, focussing on pedagogic application rather than a specific technology, and seeks to provide a bridge between pedagogical approaches and the tools educators have at their disposal.</P><P>It contains essential advice for those choosing a VLE and encourages all those involved in the deployment of a VLEs to use them more productively in order to create engaging learning experiences. </P>",
    "author": [
      {
        "family": "Weller",
        "given": "Martin"
      }
    ],
    "edition": "New edition",
    "id": "citeulike:2114209",
    "issued": {
      "date-parts": [
        [
          2007,
          4,
          12
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "Paperback; Routledge",
    "title": "Virtual learning environments: Using, choosing and developing your VLE",
    "title-short": "Virtual learning environments",
    "type": "book"
  },
  {
    "ISBN": "0415373077",
    "abstract": "E-Learning has long been touted as the brave new frontier of education, offering new challenges to teachers, students and, indeed, the whole of the education system. This timely book is the perfect reference for anyone seeking to navigate the myriad of names, concepts and applications associated with this new era of teaching, training and learning.<br><i>E-Learning: the Key Concepts </i>takes you from A to Z through a range of topics, including:<br><br>· Blogging<br>· Course Design<br>· Plagiarism<br>· Search Engines<br>· Self-Directed Learning<br>· Tutoring<br>· Virtual Learning Environments (VLEs).<br><br>Fully cross-referenced, the book also includes a substantial introduction exploring the development of e-learning and putting these new challenges in context, and provides extensive guides to further reading, making this an invaluable guide to a vital field.",
    "author": [
      {
        "family": "Mason"
      }
    ],
    "id": "citeulike:2114438",
    "issued": {
      "date-parts": [
        [
          2006,
          7,
          27
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "Paperback; Routledge",
    "title": "E-Learning: The key concepts (routledge key guides)",
    "title-short": "E-Learning",
    "type": "book"
  },
  {
    "ISBN": "0787984256",
    "abstract": "From William Horton – a world renowned expert with more than thirty-five years of hands-on experience creating networked-based educational systems – comes the next-step resource for e-learning training professionals. Like his best-selling book <I>Designing Web-Based Training</I>, this book is a comprehensive resource that provides practical guidance for making the thousand and one decisions needed to design effective e-learning. <p> e-Learning by Design includes a systematic, flexible, and rapid design process covering every phase of designing e-learning. Free of academic jargon and confusing theory, this down-to-earth, hands-on book is filled with hundreds of real-world examples and case studies from dozens of fields. <p> \"Like the book’s predecessor (<I>Designing Web-based Training</I>), it deserves four stars and is a must read for anyone not selling an expensive solution. – From Training Media Review, by Jon Aleckson, www.tmreview.com, 2007",
    "author": [
      {
        "family": "Horton",
        "given": "William"
      }
    ],
    "id": "citeulike:2114882",
    "issued": {
      "date-parts": [
        [
          2006,
          7,
          28
        ]
      ]
    },
    "keyword": "e-learning, pedagogy",
    "language": "en-US",
    "publisher": "Paperback; Pfeiffer",
    "title": "e-Learning by design",
    "type": "book"
  },
  {
    "ISBN": "1844450767",
    "author": [
      {
        "family": "Gillespie",
        "given": "Helena"
      },
      {
        "family": "Boulton",
        "given": "Helen"
      },
      {
        "family": "Hramiak",
        "given": "Alison"
      },
      {
        "family": "Williamson",
        "given": "Richard"
      }
    ],
    "id": "citeulike:2114907",
    "issued": {
      "date-parts": [
        [
          2007,
          2,
          13
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "Paperback; Learning Matters",
    "title": "Learning and teaching with virtual learning environments (practial handbooks)",
    "type": "book"
  },
  {
    "DOI": "10.2307/357381",
    "ISSN": "0010096X",
    "author": [
      {
        "family": "Flower",
        "given": "Linda"
      },
      {
        "family": "Hayes",
        "given": "John R."
      },
      {
        "family": "Carey",
        "given": "Linda"
      },
      {
        "family": "Schriver",
        "given": "Karen"
      },
      {
        "family": "Stratman",
        "given": "James"
      }
    ],
    "container-title": "College Composition and Communication",
    "id": "citeulike:2255037",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1986
        ]
      ]
    },
    "keyword": "authoring, psychology",
    "language": "en-US",
    "page": "16-55",
    "publisher": "National Council of Teachers of English",
    "title": "Detection, diagnosis, and the strategies of revision",
    "type": "article-journal",
    "volume": "37"
  },
  {
    "DOI": "10.1145/3864.3867",
    "ISSN": "1046-8188",
    "author": [
      {
        "family": "Gould",
        "given": "John D."
      },
      {
        "family": "Lewis",
        "given": "Clayton"
      },
      {
        "family": "Barnes",
        "given": "Vincent"
      }
    ],
    "container-title": "ACM Trans. Inf. Syst.",
    "id": "citeulike:2349833",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1985,
          1
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "22-34",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Cursor movement during text editing",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.1145/357431.357434",
    "ISSN": "1046-8188",
    "abstract": "Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.",
    "author": [
      {
        "family": "Allen",
        "given": "Robert B."
      },
      {
        "family": "Scerbo",
        "given": "M. W."
      }
    ],
    "container-title": "ACM Trans. Inf. Syst.",
    "id": "citeulike:2349849",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1983,
          4
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "159-178",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Details of command-language keystrokes",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.1145/800049.801773",
    "author": [
      {
        "family": "Embley",
        "given": "David W."
      },
      {
        "family": "Nagy",
        "given": "George"
      }
    ],
    "container-title": "Proceedings of the 1982 conference on human factors in computing systems",
    "id": "citeulike:2349867",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "keyword": "interactive_editing, psychology",
    "language": "en-US",
    "page": "152-156",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Can we expect to improve text editing performance?",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1007/bf01464075",
    "abstract": "Abstract&nbsp;&nbsp;To assess progress in understanding text revision, we review research reported since 1980, when process analyses of writing were beginning (Fitzgerald, 1987). A modernized version of the revision model by Flower, Hayes, Carey, Schriver, and Stratman (1986) was used to organize findings about how revision is influenced by environmentally posed rhetorical problems and actual text variables; by cognitive knowledge, strategies, and representations of the text being revised; by metacognitive understanding, monitoring, and control of knowledge and strategies; by interactions among these environmental, cognitive, and metacognitive influences; and by how working memory limits those interactive influences. These influences have been studied with a rich diversity of research approaches, and even though no part of the modernized model has been studied fully, and even though interactions of the model’s parts have been examined minimally, clearly interpretable results have been reported about all of the model’s parts. Substantial and encouraging progress has been made toward understanding text revision, and the stage has been set for more progress. We suggest investigations to increase understanding of revision and to promote integration of research and theory about reading and writing.",
    "author": [
      {
        "family": "Butterfield",
        "given": "Earl"
      },
      {
        "family": "Hacker",
        "given": "Douglas"
      },
      {
        "family": "Albertson",
        "given": "Luann"
      }
    ],
    "container-title": "Educational Psychology Review",
    "id": "citeulike:2349922",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "authoring, interactive_editing",
    "language": "en-US",
    "page": "239-297",
    "title": "Environmental, cognitive, and metacognitive influences on text revision: Assessing the evidence",
    "title-short": "Environmental, cognitive, and metacognitive influences on text revision",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "DOI": "10.1145/1165385.317474",
    "ISSN": "0736-6906",
    "author": [
      {
        "family": "Good",
        "given": "Michael"
      }
    ],
    "container-title": "SIGCHI Bull.",
    "id": "citeulike:2349938",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1985,
          4
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "93-97",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The use of logging data in the design of a new text editor",
    "type": "article-journal",
    "volume": "16"
  },
  {
    "DOI": "10.1002/acp.2350040203",
    "abstract": "A series of five experiments examined the effects of irrelevant speech on proofreading and memory. Four of the experiments used a proofreading task and showed that the deleterious effects of irrelevant speech: (1) depend on the speech being meaningful, (2) are only present when the burden on short-term memory is low and (3) are manifested in a lower detection rate for non-contextual as opposed to contextual errors. Neither the spatial location of the speech (either in terms of spatial dispersion of sources or spatial movement of a single source) nor the intensity of the speech (in a range bounded by 50 dB(A) and 70 dB(A)) had any effect on proofreading. Late selection models of attention are favoured by the results in preference to models having arousal, short-term memory or early selection in attention as their basis. A final experiment showed serial recall for visual lists to be impaired by the presence of any speech-like sound (including reversed speech and speech in an unfamiliar language) which suggests a set of phenomena qualitatively different from those associated with proofreading. Throughout the article the practical consequences of the findings are emphasized.",
    "author": [
      {
        "family": "Jones",
        "given": "Dylan M."
      },
      {
        "family": "Miles",
        "given": "Christopher"
      },
      {
        "family": "Page",
        "given": "Jean"
      }
    ],
    "container-title": "Applied Cognitive Psychology",
    "id": "citeulike:2349959",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "keyword": "authoring, psychology",
    "language": "en-US",
    "page": "89-108",
    "publisher-place": "School of Psychology, University of Wales College of Cardiff, P.O. Box 104, Cardiff CFI 2YG, U. K.",
    "title": "Disruption of proofreading by irrelevant speech: Effects of attention, arousal or memory?",
    "title-short": "Disruption of proofreading by irrelevant speech",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "DOI": "10.1007/s10648-007-9051-6",
    "abstract": "Abstract&nbsp;&nbsp;This concluding paper summarizes the main points and recommendations of the previous papers in this Special issue within a conceptual framework of cognitive load theory. Design of efficient interactive learning environments should take into account main features and limitations of our cognitive architecture. The paper provides a brief overview of this architecture and sources of cognitive load, considers their instructional implications for interactive e-learning environments, and analyzes methods for managing cognitive load and enhancing instructional efficiency of such environments.",
    "author": [
      {
        "family": "Kalyuga",
        "given": "Slava"
      }
    ],
    "container-title": "Educational Psychology Review",
    "id": "citeulike:2349963",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "keyword": "e-learning, psychology",
    "language": "en-US",
    "page": "387-399",
    "title": "Enhancing instructional efficiency of interactive e-learning environments: A cognitive load perspective",
    "title-short": "Enhancing instructional efficiency of interactive e-learning environments",
    "type": "article-journal",
    "volume": "19"
  },
  {
    "DOI": "10.1007/bf01464076",
    "abstract": "Abstract The review examines ways in which working memory contributes to individual and particularly to developmental differences in writing skill. It begins with a brief definition of working memory and then summarizes current debates regarding working memory and capacity theories in the field of reading. It is argued that a capacity theory of writing can provide a framework within which to consider the development of writing skill, and relevant data are discussed. Effects of capacity limitations are documented in all three component writing processes: planning, translating, and reviewing.",
    "author": [
      {
        "family": "McCutchen",
        "given": "Deborah"
      }
    ],
    "container-title": "Educational Psychology Review",
    "id": "citeulike:2349968",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "authoring, psychology",
    "language": "en-US",
    "page": "299-325",
    "title": "A capacity theory of writing: Working memory in composition",
    "title-short": "A capacity theory of writing",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "DOI": "10.1145/872730.806453",
    "ISSN": "0362-1340",
    "author": [
      {
        "family": "Walker",
        "given": "Janet H."
      }
    ],
    "container-title": "SIGPLAN Not.",
    "id": "citeulike:2350069",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          1981,
          6
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "44-50",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The document editor: A support environment for preparing technical documents",
    "title-short": "The document editor",
    "type": "article-journal",
    "volume": "16"
  },
  {
    "DOI": "10.1145/1159890.806455",
    "author": [
      {
        "family": "Cherry",
        "given": "Lorinda"
      }
    ],
    "container-title": "ACM SIGOA Newsletter",
    "id": "citeulike:2350079",
    "issue": "1-2",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "keyword": "authoring, interactive_editing",
    "language": "en-US",
    "page": "61-67",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Computer aids for writers",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.1145/872730.806466",
    "ISSN": "0362-1340",
    "author": [
      {
        "family": "Stallman",
        "given": "Richard M."
      }
    ],
    "container-title": "SIGPLAN Not.",
    "id": "citeulike:2350092",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          1981,
          6
        ]
      ]
    },
    "keyword": "emacs, interactive_editing",
    "language": "en-US",
    "page": "147-156",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "EMACS the extensible, customizable self-documenting display editor",
    "type": "article-journal",
    "volume": "16"
  },
  {
    "DOI": "10.1145/1159890.806467",
    "author": [
      {
        "family": "Reid",
        "given": "Brian K."
      },
      {
        "family": "Hanson",
        "given": "David"
      }
    ],
    "container-title": "ACM SIGOA Newsletter",
    "id": "citeulike:2350098",
    "issue": "1-2",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "keyword": "interactive_editing, typesetting",
    "language": "en-US",
    "page": "157-160",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "An annotated bibliography of background material on text manipulation",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.1145/800175.809841",
    "ISBN": "0897910494",
    "author": [
      {
        "family": "Daiute",
        "given": "Colette A."
      },
      {
        "family": "Taylor",
        "given": "Robert"
      }
    ],
    "container-title": "ACM 81: Proceedings of the ACM ’81 conference",
    "id": "citeulike:2350138",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "keyword": "authoring, interactive_editing",
    "language": "en-US",
    "page": "83-88",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Computers and the improvement of writing",
    "type": "paper-conference"
  },
  {
    "DOI": "10.2307/357400",
    "author": [
      {
        "family": "Daiute",
        "given": "Colette A."
      }
    ],
    "container-title": "College Composition and Communication",
    "id": "citeulike:2350143",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1983
        ]
      ]
    },
    "keyword": "authoring, interactive_editing",
    "language": "en-US",
    "page": "134-145",
    "title": "The computer as stylus and audience",
    "type": "article-journal",
    "volume": "34"
  },
  {
    "DOI": "10.1145/1054972.1054999",
    "ISBN": "1581139985",
    "URL": "http://www.cc.gatech.edu/\\~{}amyvoida/vita.html\\#publications",
    "author": [
      {
        "family": "Voida",
        "given": "Amy"
      },
      {
        "family": "Grinter",
        "given": "Rebecca E."
      },
      {
        "family": "Ducheneaut",
        "given": "Nicolas"
      },
      {
        "family": "Edwards",
        "given": "W. Keith"
      },
      {
        "family": "Newman",
        "given": "Mark W."
      }
    ],
    "container-title": "CHI ’05: Proceedings of the SIGCHI conference on human factors in computing systems",
    "id": "citeulike:235048",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "awareness, collaboration",
    "language": "en-US",
    "page": "191-200",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Listening in: Practices surrounding iTunes music sharing",
    "title-short": "Listening in",
    "type": "paper-conference"
  },
  {
    "URL": "http://cogprints.org/3621/",
    "abstract": "Revising is an evaluating and editing process that is an essential part of text production. Is text revising facilitated by the use of word processors? After examining the related research, it is difficult to conclude with certainty that the use of word processors is always effective in improving writers’ revising skills, or that their use necessarily leads to the production of higher quality texts. Their effectiveness depends on a large number of parameters (computer equipment, writing skills, task execution conditions) which psychologists are now starting to measure.",
    "author": [
      {
        "family": "Piolat",
        "given": "Annie"
      }
    ],
    "container-title": "Language and Education",
    "id": "citeulike:2353080",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "keyword": "authoring, interactive_editing, psychology",
    "language": "en-US",
    "page": "255-272",
    "title": "Effects of word processing on text revision",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.2307/356600",
    "ISSN": "0010096X",
    "author": [
      {
        "family": "Flower",
        "given": "Linda"
      },
      {
        "family": "Hayes",
        "given": "John R."
      }
    ],
    "container-title": "College Composition and Communication",
    "id": "citeulike:2353099",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "keyword": "authoring, psychology",
    "language": "en-US",
    "page": "365-387",
    "publisher": "National Council of Teachers of English",
    "title": "A cognitive process theory of writing",
    "type": "article-journal",
    "volume": "32"
  },
  {
    "DOI": "10.1109/2.222115",
    "abstract": "A logical history of document editing mechanisms is presented. The design space for document style mechanisms is analyzed. Six primary design issues and the subsidiary issues they raise are discussed. Some major style issues that are seen as the subject of future research are identified.",
    "author": [
      {
        "family": "Johnson",
        "given": "J."
      },
      {
        "family": "Beach",
        "given": "R. J."
      }
    ],
    "container-title": "Computer",
    "id": "citeulike:2354543",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "interactive_editing, typesetting",
    "language": "en-US",
    "page": "32-43",
    "title": "Styles in document editing systems",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "URL": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1095396",
    "abstract": "For many people, writing is painful and editing one’s own prose is difficult, tedious, and error-prone. It is often hard to see which parts of a document are difficult to read or how to transform a wordy sentence into a more concise one. It is even harder to discover that one overuses a particular linguistic construct. The system of programs described here helps writers to evaluate documents and to produce better written and more readable prose. The system consists of programs to measure surface features of text that are important to good writing style, as well as programs to do some of the tedious jobs of a copy editor. Some of the surface features measured are readability, sentence and word length, sentence type, word usage, and sentence openers. The copy editing programs find spelling errors, wordy phrases, bad diction, some punctuation errors, double words, and split infinitives.",
    "author": [
      {
        "family": "Cherry",
        "given": "Lorinda"
      }
    ],
    "container-title": "IEEE Transactions on Communications",
    "id": "citeulike:2354547",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "100-105",
    "title": "Writing tools",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "DOI": "10.1109/cie.2002.1185961",
    "abstract": "Providing timely and high-quality feedback has been identified as one of the critical factors in learning. In courses with large class sizes the amount of time required for critiquing makes it prohibitive to give feedback to students on more than two or three assignments per semester. As a result, students have limited opportunities to learn from their mistakes, and it is difficult for lecturers to monitor student progress. The alternative to manual critiquing is to automate the process. The authors expect automated program critiquing to deliver two major benefits to students: improved feedback and more useful access to lecturers. This paper introduces their project and the expected benefits.",
    "author": [
      {
        "family": "Pisan",
        "given": "Y."
      },
      {
        "family": "Sloane",
        "given": "A."
      },
      {
        "family": "Richards",
        "given": "D."
      },
      {
        "family": "Dale",
        "given": "R."
      }
    ],
    "container-title": "Computers in Education, 2002. Proceedings. International Conference on",
    "id": "citeulike:2354666",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "413-414 vol.1",
    "title": "Providing timely feedback to large classes",
    "type": "book"
  },
  {
    "ISBN": "3875124146",
    "URL": "http://www.worldcat.org/isbn/3875124146",
    "author": [
      {
        "family": "Renner",
        "given": "Paul"
      }
    ],
    "id": "citeulike:2358201",
    "issued": {
      "date-parts": [
        [
          2003,
          8,
          31
        ]
      ]
    },
    "keyword": "classic, layout, typography",
    "language": "en-US",
    "publisher": "Hardcover; Maro Verlag",
    "title": "Die kunst der typographie",
    "type": "book"
  },
  {
    "DOI": "10.1145/567446.567449",
    "ISBN": "0897910117",
    "author": [
      {
        "family": "Reid",
        "given": "Brian K."
      }
    ],
    "container-title": "POPL ’80: Proceedings of the 7th ACM SIGPLAN-SIGACT symposium on principles of programming languages",
    "id": "citeulike:261498",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "keyword": "typesetting",
    "language": "en-US",
    "page": "24-31",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A high-level approach to computer document formatting",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/872730.806456",
    "ISSN": "0362-1340",
    "author": [
      {
        "family": "Goldfarb",
        "given": "C. F."
      }
    ],
    "collection-number": "6",
    "container-title": "Proceedings of the ACM SIGPLAN SIGOA symposium on text manipulation",
    "id": "citeulike:261499",
    "issued": {
      "date-parts": [
        [
          1981,
          6
        ]
      ]
    },
    "keyword": "markup",
    "language": "en-US",
    "page": "68-73",
    "publisher": "ACM",
    "title": "A generalized approach to document markup",
    "type": "paper-conference",
    "volume": "16"
  },
  {
    "DOI": "10.1145/345966.346029",
    "ISSN": "0360-0300",
    "author": [
      {
        "family": "Brailsford",
        "given": "David F."
      }
    ],
    "container-title": "ACM Comput. Surv.",
    "id": "citeulike:261504",
    "issue": "4es",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "hypertext",
    "language": "en-US",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Separable hyperstructure and delayed link binding",
    "type": "article-journal",
    "volume": "31"
  },
  {
    "DOI": "10.1145/585058.585077",
    "ISBN": "1581135947",
    "author": [
      {
        "family": "Hardy",
        "given": "Matthew R. B."
      },
      {
        "family": "Brailsford",
        "given": "David F."
      }
    ],
    "container-title": "DocEng ’02: Proceedings of the 2002 ACM symposium on document engineering",
    "id": "citeulike:261505",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "hypertext, typesetting",
    "language": "en-US",
    "page": "95-102",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Mapping and displaying structural transformations between XML and PDF",
    "type": "paper-conference"
  },
  {
    "URL": "http://archivesic.ccsd.cnrs.fr/sic_00000594.html",
    "author": [
      {
        "family": "Pédauque",
        "given": "Roger T."
      }
    ],
    "id": "citeulike:261514",
    "keyword": "document_research",
    "language": "en-US",
    "publisher": "CNRS",
    "title": "Document: Form, sign and medium, as reformulated for electronic documents",
    "title-short": "Document",
    "type": "manuscript"
  },
  {
    "ISSN": "1099-6621",
    "URL": "http://www.ingentaconnect.com/content/mitpress/mlang/2000/00000002/00000004/art00006",
    "abstract": "The traditional distinction between descriptive and procedural markup is flawed; it conflates two different dimensions - mood and domain - which in fact can vary independently. An adequate markup taxonomy must, among other things, incorporate distinctions such as those developed in contemporary \"speech-act theory\". This will substantially complicate, although in interesting ways, the development of an adequate theory of markup semantics, as formalization will require modal operators and additional axiomatic relationships. In addition, these reflections reveal that there are foundational issues in markup theory that are not yet resolved, in particular the precise relationship between markup and text.",
    "author": [
      {
        "family": "Renear",
        "given": "Allen H."
      }
    ],
    "container-title": "Markup Languages: Theory and Practice",
    "id": "citeulike:261523",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2000,
          9,
          1
        ]
      ]
    },
    "keyword": "document_research, markup",
    "language": "en-US",
    "page": "411-420",
    "title": "The descriptive/procedural distinction is flawed",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "author": [
      {
        "family": "Stiff",
        "given": "Paul"
      }
    ],
    "container-title": "Information Design Journal",
    "id": "citeulike:261525",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "page": "227-241",
    "title": "Structuralists, stylists and forgotten readers",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "ISSN": "0268-1145",
    "URL": "http://www.ingentaconnect.com/content/oup/litlin/2003/00000018/00000001/art00039",
    "abstract": "Markup licenses inferences about a text. But the information warranting such inferences may not be entirely explicit in the syntax of the markup language used to encode the text. This paper describes a Prolog environment for exploring alternative approaches to representing facts and rules of inference about structured documents. It builds on earlier work proposing an account of how markup licenses inferences, and of what is needed in a specification of the meaning of a markup language. Our system permits an analyst to specify facts and rules of inference about domain entities and properties as well as facts about the markup syntax, and to construct and test alternative approaches to translation between representation layers. The system provides a level of abstraction at which the performative or interpretive meaning of the markup can be explicitly represented in machine-readable and executable form.",
    "author": [
      {
        "family": "Dubin",
        "given": "David"
      },
      {
        "family": "Renear",
        "given": "Allen H."
      },
      {
        "family": "Sperberg-McQueen",
        "given": "C. M."
      },
      {
        "family": "Huitfeldt",
        "given": "Claus"
      }
    ],
    "container-title": "Literary and Linguistic Computing",
    "id": "citeulike:261526",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2003,
          4
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "page": "39-47",
    "title": "A logic programming environment for document semantics and inference",
    "type": "article-journal",
    "volume": "18"
  },
  {
    "ISSN": "1099-6621",
    "URL": "http://www.w3.org/People/cmsmcq/2000/mim.html",
    "abstract": "SGML and XML markup allows the user to make (licenses) certain inferences about passages in the marked-up material; in particular, markup signals the occurrence of specific features in a document. Some features are distributed, and their occurrences are logically non-countable (italic font is a simple example); others are non-distributed (paragraphs and other standard textual structures, for example). Formally, the inferences licensed by markup may be expressed as open sentences, whose blanks are to be filled by the contents of an element, by an attribute value, by an individual token of an attribute value, etc. The task of interpreting the meaning of the markup at a particular location in a document may then be formulated as finding the set of inferences about that location which may be drawn on the basis of the markup in the document. Several different approaches to this problem are outlined; one measure of their relative complexity is the complexity of the expressions which are allowed to fill the slots in the open sentences which formally specify the meaning of a markup language.",
    "author": [
      {
        "family": "Sperberg-McQueen",
        "given": "C. M."
      },
      {
        "family": "Huitfeldt",
        "given": "Claus"
      },
      {
        "family": "Renear",
        "given": "Allen H."
      }
    ],
    "container-title": "Markup Languages: Theory and Practice",
    "id": "citeulike:261529",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2000,
          8,
          1
        ]
      ]
    },
    "keyword": "document_research, markup",
    "language": "en-US",
    "page": "215-234",
    "title": "Meaning and interpretation of markup",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "URL": "http://archivesic.ccsd.cnrs.fr/sic_00001401.html",
    "abstract": "Ce document de travail, réalisé collectivement dans le cadre du RTP-DOC, attire l’attention de la communauté scientifique sur les risques qu’il y aurait à ne pas préciser la notion de texte et sa relation avec celle de document, afin de mieux adapter les développements du numérique à une communication humaine. La brutalité des évolutions actuelles s’appuie sur des implicites, séparant structure et contenu et rapprochant les activités de lecture et d’écriture. Trois modélisations informatiques, successives et distinctes, sous-tendent les développements récents des outils autour du document codé en XML : les DTD, les Schémas et le Web sémantique. Ces modélisations négligent des réflexions multidisciplinaires indispensables : 1. Les développements informatiques utilisent des disciplines comme la linguistique qui n’ont pas analysé clairement la notion de texte. Pour imaginer une structure indépendante d’un contenu, il faut rendre compte des multiples dimensions, intellectuelles et matérielles, de la textualité. 2. La différence entre texte et document, déjà intégrée par exemple dans le raisonnement juridique, peut se décliner dans les dispositifs techniques par des Schémas en repérant les invariants pour préserver le statut documentaire dans la transformation du document d’un état à un autre. Néanmoins, la notion d’invariance montre ses limites dans la traduction. 3. Enfin la tentative de dépassement par le Web sémantique peut conduire à un appauvrissement ou une confusion de l’ordre des savoirs humains si une analyse correcte n’est pas faite du rôle des ontologies, refusant le « nominalisme ». Un retour sur les notions fondamentales devrait faciliter un développement des outils allié à une meilleure maîtrise par les communautés humaines des conséquences de leur utilisation. En particulier, il paraîtrait utile de préciser ou modifier la notion de texte dans un environnement documentaire de plus en plus multimédia et de mieux analyser le rôle de la médiation, humaine ou automatisée sans la confondre avec une « écriture automatique ».",
    "author": [
      {
        "family": "Pédauque",
        "given": "Roger T."
      }
    ],
    "id": "citeulike:261530",
    "keyword": "document_research",
    "language": "en-US",
    "publisher": "CNRS",
    "title": "Le texte en jeu permanence et transformations du document",
    "type": "manuscript"
  },
  {
    "ISSN": "0894-3982",
    "URL": "http://cajun.cs.nott.ac.uk/compsci/epo/papers/volume2/issue3/epja023.pdf",
    "author": [
      {
        "family": "André",
        "given": "Jacques"
      }
    ],
    "container-title": "Electronic Publishing—Origination, Dissemination and Design",
    "id": "Andre1989",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1989,
          10
        ]
      ]
    },
    "keyword": "markup, typesetting",
    "language": "en-US",
    "page": "169-173",
    "publisher": "John Wiley and Sons Ltd.",
    "publisher-place": "Chichester",
    "title": "Can structured formatters prevent train crashes?",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "URL": "http://xml.coverpages.org/DurandWhatShouldTextBe-ALLC1996.pdf",
    "author": [
      {
        "family": "Durand",
        "given": "David G."
      },
      {
        "family": "Mylonas",
        "given": "Elli"
      },
      {
        "family": "DeRose",
        "given": "Steven J."
      }
    ],
    "container-title": "ALLC/ACH conference, june 1996, bergen",
    "id": "citeulike:261532",
    "issued": {
      "date-parts": [
        [
          1996,
          6
        ]
      ]
    },
    "keyword": "document_research, markup",
    "language": "en-US",
    "page": "67-70",
    "title": "What should markup really be? Applying theories of text to the design of markup systems",
    "type": "paper-conference"
  },
  {
    "URL": "http://cajun.cs.nott.ac.uk/compsci/epo/papers/volume5/issue3/ep067fh.pdf",
    "author": [
      {
        "family": "Heeman",
        "given": "Frans C."
      }
    ],
    "container-title": "Electronic Publishing—Origination, Dissemination and Design",
    "id": "citeulike:261553",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1992,
          9
        ]
      ]
    },
    "keyword": "document_research, markup",
    "language": "en-US",
    "page": "143-155",
    "title": "Granularity in structured documents",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "URL": "http://www.mulberrytech.com/Extreme/Proceedings/xslfo-pdf/2004/DeRose01/EML2004DeRose01.pdf",
    "abstract": "“Overlap” is the common term for cases where some markup structures do not nest neatly into others, such as when a quotation starts in the middle of one paragraph and ends in the middle of the next. OSIS [Duru03], a standard XML schema for Biblical and related materials, has to deal with extreme amounts of overlap. The simplest case involves book/chapter/verse and book/story/paragraph hierarchies that pervasively diverge; but many types of overlap are more complicated than this. The basic options for dealing with overlap in the context of SGML [ISO 8879] or XML [Bray98] are described in the TEI Guidelines [TEI]. I summarize these with their strengths and weaknesses. Previous proposals for expressing overlap, or at least kinds of overlap, don’t work well enough for the severe and frequent cases found in OSIS. Thus, I present a variation on TEI milestone markup that has several advantages. This is now the normative way of encoding non-hierarchical structures in OSIS documents.",
    "author": [
      {
        "family": "DeRose",
        "given": "Steven J."
      }
    ],
    "container-title": "Extreme markup languages 2004, montréal, québec",
    "id": "citeulike:261556",
    "keyword": "markup",
    "language": "en-US",
    "title": "Markup overlap: A review and a horse",
    "title-short": "Markup overlap",
    "type": "paper-conference"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=962201",
    "author": [
      {
        "family": "Raymond",
        "given": "Darrell R."
      }
    ],
    "container-title": "CASCON ’92: Proceedings of the 1992 conference of the centre for advanced studies on collaborative research",
    "id": "citeulike:261574",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "document_research, markup",
    "language": "en-US",
    "page": "19-28",
    "publisher": "IBM Press",
    "title": "Evolutions in typesetting systems",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.mulberrytech.com/Extreme/Proceedings/html/2002/Quin01/EML2002Quin01.html",
    "abstract": "The Extensible Markup Language (XML) 1.0 Specification has a number of features that are rarely used, or are poorly defined, or that interact badly with common implementation strategies. The widespread deployment of XML 1.0 has led to its adoption in many new areas. As new needs have arisen, new features have been added in ad hoc ways, beyond the scope of the original specification. This has led to incompatible approaches to solving similar problems. This additional complexity becomes a burden upon both users and implementors. New specifications building upon XML find that the areas in which the Extensible Markup Language is not extensible – for example, permitting only a single link to a DTD, or requiring the use of DTD syntax for entities – to be limiting and frustrating. There have been a number of proposals to form a subset of XML 1.0, and to label that subset XML 2.0. This proposal takes an alternative approach: it implements all of the functionality of XML 1.0, but with a reduced syntax. The additional semantics of linking are expressed using a subset of the Resource Description Format, RDF. The result of combining some built-in support for metadata processing along with a simplified XML specification will be, it is claimed, specifications that are more powerful and higher-level, and will facilitate the creation of more powerful applications. In particular, widespread adoption of infrastructure with direct support for explicit handling of metadata will lead to human and business benefits in fields as diverse as web services, Topic Maps, metadata searching and the semantic web of the future.",
    "author": [
      {
        "family": "Quin",
        "given": "Liam"
      }
    ],
    "container-title": "Extreme markup languages 2002, montréal, québec",
    "id": "citeulike:261584",
    "issued": {
      "date-parts": [
        [
          2002,
          8
        ]
      ]
    },
    "keyword": "markup",
    "language": "en-US",
    "title": "XMLR: XML reduced",
    "title-short": "XMLR",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/585058.585081",
    "ISBN": "1581135947",
    "author": [
      {
        "family": "Renear",
        "given": "Allen H."
      },
      {
        "family": "Dubin",
        "given": "David"
      },
      {
        "family": "Sperberg-McQueen",
        "given": "C. M."
      }
    ],
    "container-title": "DocEng ’02: Proceedings of the 2002 ACM symposium on document engineering",
    "id": "citeulike:261593",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "document_research, markup",
    "language": "en-US",
    "page": "119-126",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Towards a semantics for XML markup",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.cs.nott.ac.uk/\\~{}dfb/Publications/Download/1993/groves.pdf",
    "abstract": "This paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. A block-based document model is described which allows for separate compilation of various portions of a document. These portions are brought together and merged by a linker program, called dlink, whose pilot implementation is based on ditroff and on its underlying intermediate code. In the light of experiences with dlink the requirements for a universal “object-module language” for documents are discussed. These requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are “executed”.",
    "author": [
      {
        "family": "Groves",
        "given": "Michael J."
      },
      {
        "family": "Brailsford",
        "given": "David F."
      }
    ],
    "container-title": "Electronic Publishing—Origination, Dissemination and Design",
    "id": "citeulike:261635",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1993,
          12
        ]
      ]
    },
    "keyword": "typesetting",
    "language": "en-US",
    "page": "315-326",
    "title": "Separate compilation of structured documents",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "URL": "http://www.cs.usyd.edu.au/\\~{}jeff/nonpareil/future.ps",
    "author": [
      {
        "family": "Kingston",
        "given": "Jeffrey H."
      }
    ],
    "id": "citeulike:261636",
    "keyword": "interactive_editing, typesetting",
    "language": "en-US",
    "title": "The future of document formatting",
    "type": "manuscript"
  },
  {
    "URL": "http://www.mulberrytech.com/Extreme/Proceedings/xslfo-pdf/2003/Gessa01/EML2003Gessa01.pdf",
    "abstract": "DTDs and XML Schema are important validation languages for XML documents. They lie at opposite ends of a spectrum of validation languages in terms of expressive power and readability. Differently from other proposals for validation languages, DTD++ provides a DTD-like syntax to XML Schema constructs, thereby enriching the ease of use and reading of DTDs with the expressive power of XML Schema. An implementation as a pre-processor of a Schema-validating XML parser aids in ensuring wide support for the language.",
    "author": [
      {
        "family": "Vitali",
        "given": "Fabio"
      },
      {
        "family": "Amorosi",
        "given": "Nicola"
      },
      {
        "family": "Gessa",
        "given": "Nicola"
      }
    ],
    "container-title": "Extreme markup languages 2003, montréal, québec",
    "id": "citeulike:261638",
    "issued": {
      "date-parts": [
        [
          2003,
          8
        ]
      ]
    },
    "keyword": "markup, xml",
    "language": "en-US",
    "title": "Datatype- and namespace-aware DTDs: A minimal extension",
    "title-short": "Datatype- and namespace-aware DTDs",
    "type": "paper-conference"
  },
  {
    "URL": "http://xml.sys-con.com/read/40561.htm",
    "abstract": "When the content of an XML document is intended for human eyes, rather than software digestion, it’s necessary to consider the possible limitations of rendering applications, including typesetting systems, DTP packages, and on-screen document presentation tools such as Web browsers. Simple text structures, like headings, paragraphs, and list items can be presented easily enough, but intricate structures, such as chemical and multiline mathematical formulas, are so difficult to render that they’re often preprocessed into images. Between these extremes we have tabular structures.",
    "author": [
      {
        "family": "Bradley",
        "given": "Neil"
      }
    ],
    "container-title": "XML Journal",
    "id": "citeulike:261642",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "markup, tables, xml",
    "language": "en-US",
    "title": "The trouble with tables",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "URL": "http://www.xml.com/pub/a/w3j/s3.nelson.html",
    "author": [
      {
        "family": "Nelson",
        "given": "Theodor H."
      }
    ],
    "container-title": "XML.com",
    "id": "Nelson1997",
    "issued": {
      "date-parts": [
        [
          1997,
          10,
          2
        ]
      ]
    },
    "keyword": "hypertext, markup",
    "language": "en-US",
    "title": "Embedded markup considered harmful",
    "type": "article-journal"
  },
  {
    "URL": "http://www.siderean.com/dc2003/503_Paper71.pdf",
    "abstract": "By \"identity conditions\" we mean a method for determining whether an object x and an object y are the same object. Identity conditions are arguably an essential feature of any rigorously developed conceptual framework for information modeling. Surprisingly, the concept of same document, which is fundamental to many aspects of library and information science, and to digital libraries in particular, has received little systematic analysis. As a result, not only is the concept of a document itself under-theorized, but progress on a number of important practical problems has been hindered. We review the importance of document identity conditions, demonstrate problems with current approaches, and discuss the general form a solution must take.",
    "author": [
      {
        "family": "Renear",
        "given": "Allen H."
      },
      {
        "family": "Dubin",
        "given": "David"
      }
    ],
    "container-title": "2003 dublin core conference, seattle, WA",
    "id": "Renear2003",
    "issued": {
      "date-parts": [
        [
          2003,
          10
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "publisher": "University of Washington",
    "title": "Identity conditions for digital documents",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.hum.aau.dk/\\~{}ulrikp/pdf/petersen-emdros-COLING-2004.pdf",
    "abstract": "Emdros is a text database engine for linguistic analysis or annotation of text. It is appliccable especially in corpus linguistics for storing and retrieving linguistic analyses of text, at any linguistic level. Emdros implements the EMdF text database model and the MQL query language. In this paper, I present both, and give an example of how Emdros can be useful in computational linguistics.",
    "author": [
      {
        "family": "Petersen",
        "given": "Ulrik"
      }
    ],
    "container-title": "Proceedings of COLING 2004, geneva",
    "id": "Petersen2004",
    "issued": {
      "date-parts": [
        [
          2004,
          8
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "title": "Emdros – a text database engine for analyzed or annotated text",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.mulberrytech.com/Extreme/Proceedings/html/2004/Witt01/EML2004Witt01.html",
    "abstract": "Overlap in markup occurs where some markup structures do not nest, such as where the structural division of the text into lists, sections, etc., differs from the syntactic division of the text into sentences and phrases. The Multiple Annotation solution to this problem (redundant encoding in multiple forms) has many advantages: it is based on XML, the modeling of alternative annotations is possible, each level can be viewed separately, and new levels can be added at any time. But it has the significant disadvantage of independence of the separate files. These multiply annotated files can be regarded as an interrelated unit, with the text serving as the implicit link. Two representations of the information contained in the multiple files (one in Prolog and one in XML) can be programmatically derived and used together for editing, for inference, or for unification of the multiply annotated documents.",
    "author": [
      {
        "family": "Witt",
        "given": "Andreas"
      }
    ],
    "container-title": "Extreme markup languages 2004, montréal, québec",
    "id": "Witt2004",
    "issued": {
      "date-parts": [
        [
          2004,
          8
        ]
      ]
    },
    "keyword": "markup, xml",
    "language": "en-US",
    "title": "Multiple hierarchies: New aspects of an old solution",
    "title-short": "Multiple hierarchies",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.mulberrytech.com/Extreme/Proceedings/html/2005/Witt01/EML2005Witt01.xml",
    "abstract": "This paper describes ways to approach the functionality of the SGML-feature CONCUR. This work is based on two Master’s Theses. The origin of these theses was a paper originally given at Extreme Markup Languages 2004 [Witt (2005)],where it was argued that the redundant encoding of information in multiple forms, as described by the TEI-Guidelines (see [ACH/ACL/ALLC (1994)] and [Barnard et al. (1995)]), has a lot of advantages over other methods of encoding multiply structured text. The multiple encoding of text allows to use all the available techniques and software products for XML documents. The MuLaX-Format was developed as an integrated format for editing. This format is strongly influenced by the SGML option CONCUR. The XML-conformance is achieved by the processing model: The processing is conservative because the concurrent annotations are kept separately. In other words, a non XML-syntax as a sequentialization of multiple incompatible hierarchies is used on the one hand, and on the other hand, all the processing is done on (hierarchical/XML conform) trees.",
    "author": [
      {
        "family": "Hilbert",
        "given": "Mirco"
      },
      {
        "family": "Schonefeld",
        "given": "Oliver"
      },
      {
        "family": "Witt",
        "given": "Andreas"
      }
    ],
    "container-title": "Extreme markup languages 2005, montréal, québec",
    "id": "Hilbert2005",
    "issued": {
      "date-parts": [
        [
          2005,
          8
        ]
      ]
    },
    "keyword": "markup, xml",
    "language": "en-US",
    "title": "Making CONCUR work",
    "type": "paper-conference"
  },
  {
    "abstract": "This paper presents a comprehensive survey of the typographic issues for laying out inforamtion within two-dimensional tables. Early typesetting systems formatted tables by coding the tyble style and lyout into the program, and later systems provided a limited range of typographic features. The typographic issues include table structure, alignment of rows and columns simultaneously, formatting styles, treatment of whitespace within a table, graphical embellishments, placement of footnotes, various readability issues, and the problems of breaking large tables. Extending the table formatting problem to both page layout and arrangement of mathematical notation is highlighted, as is the need for interactive design tools for table layout.",
    "author": [
      {
        "family": "Beach",
        "given": "Richard J."
      }
    ],
    "container-title": "Text processing and document manipulation. Proceedings of the international conference",
    "id": "Beach1986",
    "issued": {
      "date-parts": [
        [
          1986,
          4
        ]
      ]
    },
    "keyword": "tables",
    "language": "en-US",
    "page": "18-33",
    "title": "Tabular typography",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-33.html",
    "abstract": "Lilac is an experimental document preparation system which combines the best features of batch-style document formatters and WYSIWYG editiors. To do this, it offers the user two views of the document: a WYSIWYG view and a formatter-like source view. Changes in either view are rapidly propagated to the other. This report describes both the user interface design and the implementation mechanisms used to build Lilac.",
    "author": [
      {
        "family": "Brooks",
        "given": "Kenneth P."
      }
    ],
    "genre": "PhD thesis",
    "id": "Brooks1988",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "typesetting",
    "language": "en-US",
    "publisher": "Stanford University",
    "title": "A two-view document editor with user-definable document structure",
    "type": "thesis"
  },
  {
    "URL": "http://csdl.computer.org/comp/mags/co/1991/06/r6007abs.htm",
    "abstract": "A description is given of Lilac, an experimental document preparation system designed to provide the best of both the WYSIWYG (what you see is what you get) and the document compiler approaches. It does this by offering both WYSIWYG editing and language-based document description as two views side by side on the screen. The page view is a WYSIWYG editor showing a close approximation to the printed output. The source view shows a programlike description of the document in a special-purpose language. This language supports subroutines, variables, and conditional execution, and is designed to encourage the use of subroutines to embody structure. Both views are editable, but Lilac is designed with the expectation that most editing will occur in the page view.",
    "author": [
      {
        "family": "Brooks",
        "given": "Kenneth P."
      }
    ],
    "container-title": "IEEE Computer",
    "id": "Brooks1991",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          1991,
          6
        ]
      ]
    },
    "keyword": "interactive_editing, typesetting",
    "language": "en-US",
    "page": "7-19",
    "title": "Lilac: A Two-View document editor",
    "title-short": "Lilac",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "URL": "http://www.edcom.com/\\~{}edward/tug91.ps.Z",
    "abstract": "Historically, typesetting languages have been designed for the entry of text. An embedded command language has since become important, indeed essential, but has remained a second-class citizen, sometimes masquerading as text, invariably clumsy and inadequate. We have designed a language that is a full-function programming language with *embedded text*. This shift in emphasis results in a level of consistency, flexibility, and power not otherwise possible.",
    "author": [
      {
        "family": "Semenzato",
        "given": "Luigi"
      },
      {
        "family": "Wang",
        "given": "Edward"
      }
    ],
    "container-title": "Proceedings of the 1991 TeX users group annual meeting",
    "id": "Semenzato1991",
    "issued": {
      "date-parts": [
        [
          1991,
          6
        ]
      ]
    },
    "keyword": "functional, lisp, typesetting",
    "language": "en-US",
    "publisher": "TeX Users Group",
    "title": "A text-processing language should be first a programming language",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.edcom.com/\\~{}edward/ep92.ps.Z",
    "abstract": "Aleph is a programming language for batch typesetting systems. It has conventional programming-language features for implementing style packages, extensions, and even the typesetting engine itself. At the same time, an extensible syntax makes Aleph a convenient document input format. This paper introduces Aleph and demonstrates with examples the convenience of implementing typesetting algorithms in Aleph.",
    "author": [
      {
        "family": "Semenzato",
        "given": "Luigi"
      },
      {
        "family": "Wang",
        "given": "Edward"
      }
    ],
    "container-title": "Proceedings of electronic publishing 1992",
    "id": "Semenzato1992",
    "issued": {
      "date-parts": [
        [
          1992,
          4
        ]
      ]
    },
    "keyword": "functional, lisp, typesetting",
    "language": "en-US",
    "title": "Aleph - a language for typesetting",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.cas.mcmaster.ca/\\~{}kahl/Haskell/VGalleys.html",
    "abstract": "Galleys have been introduced by Jeff Kingston as one of the key concepts underlying his advanced document formatting system Lout. Although Lout is built on a lazy functional programming language, galley concepts are implemented as part of that language and defined only informally. In this paper we present a first formalisation of document formatting combinators using galley concepts in the purely functional programming language Haskell.",
    "author": [
      {
        "family": "Kahl",
        "given": "Wolfram"
      }
    ],
    "container-title": "Lecture Notes in Computer Science",
    "id": "Kahl1998",
    "issued": {
      "date-parts": [
        [
          1998,
          1
        ]
      ]
    },
    "keyword": "functional, haskell, monads, typesetting",
    "language": "en-US",
    "page": "76+",
    "title": "Beyond Pretty-Printing: Galley concepts in document formatting combinators",
    "title-short": "Beyond Pretty-Printing",
    "type": "book",
    "volume": "1551"
  },
  {
    "DOI": "10.1002/spe.485",
    "ISSN": "1097-024X",
    "abstract": "TEX allows users to define a macro that abstracts a sequence of typesetting commands. However, defining macros is not easy for most users, because the mechanism of macro expansion in TEX is complicated. As a remedy for this situation, a new system that enables users to define macros for TEX documents as Lisp programs has been developed. The system acts as a preprocessor for TEX; given a document that contains Lisp programs as S-expressions, the system expands each S-expression on the basis of Lisp’s evaluation rules, thus generating an ordinary TEX document. The system is very flexible and easy-to-use, thanks to the underlying language’s general-purpose data structure, i.e. the S-expression, applicative order evaluation, and rich set of predefined functions. This paper also demonstrates that the proposed system is really effective for practical use by giving some concrete examples of Lisp macros, some of which are difficult to define in terms of TEX commands. The system is currently implemented on the Emacs Lisp, and a user-friendly environment is thus available in the Emacs text editor. Copyright &copy; 2002 John Wiley &amp; Sons, Ltd.",
    "author": [
      {
        "family": "Iwasaki",
        "given": "Hideya"
      }
    ],
    "container-title": "Software: Practice and Experience",
    "id": "Iwasaki2002",
    "issue": "14",
    "issued": {
      "date-parts": [
        [
          2002,
          9,
          27
        ]
      ]
    },
    "keyword": "functional, lisp, typesetting",
    "language": "en-US",
    "page": "1345-1363",
    "title": "Developing a lisp-based preprocessor for TEX documents",
    "type": "article-journal",
    "volume": "32"
  },
  {
    "DOI": "10.1002/spe.653",
    "ISSN": "1097-024X",
    "abstract": "Arabic is a Semitic language that is rich in its morphology and syntax. The very numerous and complex grammar rules of the language may be confusing for the average user of a word processor. In this paper, we report our attempt at developing a grammar checker program for Modern Standard Arabic, called Arabic GramCheck. Arabic GramCheck can help the average user by checking his/her writing for certain common grammatical errors; it describes the problem for him/her and offers suggestions for improvement. The use of the Arabic grammatical checker can increase productivity and improve the quality of the text for anyone who writes Arabic. Arabic GramCheck has been successfully implemented using SICStus Prolog on an IBM PC. The current implementation covers a well-formed subset of Arabic and focuses on people trying to write in a formal style. Successful tests have been performed using a set of Arabic sentences. It is concluded that the approach is promising by observing the results as compared to the output of a commercially available Arabic grammar checker. Copyright &copy; 2005 John Wiley &amp; Sons, Ltd.",
    "author": [
      {
        "family": "Shaalan",
        "given": "Khaled F."
      }
    ],
    "container-title": "Software: Practice and Experience",
    "id": "Shaalan2005",
    "issue": "7",
    "issued": {
      "date-parts": [
        [
          2005,
          3,
          11
        ]
      ]
    },
    "keyword": "nlp",
    "language": "en-US",
    "page": "643-665",
    "title": "Arabic GramCheck: A grammar checker for arabic",
    "title-short": "Arabic GramCheck",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "DOI": "10.1002/spe.621",
    "ISSN": "1097-024X",
    "abstract": "In name and in practice, the World-Wide Web (hereafter Web) is used around the World beyond English-speaking areas. This creates a tremendous need to internationalize standard terminology used in the technologies that make the Web possible. Existing efforts on XML internationalization (i18n) and localization (i10n) have focused on the content of XML documents instead of the terms used in markup (annotations) such as elements and attributes. The SGML standard ISO 8879 supports the use of Unicode (ISO 10646) throughout a document, including markups. However, most elements and attributes of XML documents are still defined in English, thereby limiting their use among non-English speakers. This paper presents an XSLT-based method that can completely localize the markup of XML documents into different natural languages. We also describe how the proposed technique can be applied to translation problems in programming (e.g. C and Java) or documentation (e.g. LAT<SUB >E</SUB >X or other formatting languages) so that a program or a document can be converted to and from an XML format. Copyright &copy; 2004 John Wiley &amp; Sons, Ltd.",
    "author": [
      {
        "family": "Yu",
        "given": "Yijun"
      },
      {
        "family": "Lu",
        "given": "Jianguo"
      },
      {
        "family": "Mylopoulos",
        "given": "John"
      },
      {
        "family": "Sun",
        "given": "Weiwei"
      },
      {
        "family": "Xue",
        "given": "Jing-Hao"
      },
      {
        "family": "D’Hollander",
        "given": "Erik H."
      }
    ],
    "container-title": "Software: Practice and Experience",
    "id": "Yu2004",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "xml",
    "language": "en-US",
    "page": "1-14",
    "title": "Making XML document markup international",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "DOI": "10.1002/spe.603",
    "ISSN": "1097-024X",
    "abstract": "The most popular bibliographical standards, which prescribe the exchange of bibliographical data in machine readable form, are MARC (Machine Readable Cataloguing) and UNIMARC (Universal Machine Readable Cataloguing). This paper presents two schemas, both written in the XML schema language: the UNIMARC format schema and the XML bibliographical records schema. The instance of the UNIMARC format schema is an XML document of UNIMARC format. This schema provides easier updating and adjusting of the UNIMARC format at the national or regional level. It was also the basis for the development of the <I >XML editor for UNIMARC format description</I >. The instance of the XML bibliographical records schema is an XML bibliographic record. This schema provides very thorough control of the contents and structure of bibliographical records, whether they were directly created as XML records or converted to XML from existing UNIMARC records. The validation process is implemented in the Java environment, with the Sun Multi-Schema XML Validator (MSV) package. The author is aware of the inevitable question of redundant information contained in these two schemas and proposes some possible solutions. Copyright © 2004 John Wiley & Sons, Ltd.",
    "author": [
      {
        "family": "Jakšić",
        "given": "Mirjana"
      }
    ],
    "container-title": "Software: Practice and Experience",
    "id": "Jaksic2004",
    "issue": "11",
    "issued": {
      "date-parts": [
        [
          2004,
          6,
          7
        ]
      ]
    },
    "keyword": "bibliography, xml",
    "language": "en-US",
    "page": "1051-1064",
    "title": "Mapping of bibliographical standards into XML",
    "type": "article-journal",
    "volume": "34"
  },
  {
    "URL": "http://www.cs.usyd.edu.au/\\~{}jeff/nonpareil/",
    "abstract": "Nonpareil is a functional object-oriented programming language, currently under development, which is intended for use in interactive document formatting. This document is a formal specification of parts of Nonpareil. As the implementation proceeds, this document will be updated. A separate document, “Prospectus for Nonpareil,” discusses the whole project less formally.",
    "author": [
      {
        "family": "Kingston",
        "given": "Jeffrey H."
      }
    ],
    "edition": "1.1",
    "id": "Kingston2003",
    "issued": {
      "date-parts": [
        [
          2003,
          8
        ]
      ]
    },
    "keyword": "functional, typesetting",
    "language": "en-US",
    "title": "Nonpareil language specification",
    "type": "report"
  },
  {
    "DOI": "10.1145/32206.32209",
    "ISSN": "0001-0782",
    "author": [
      {
        "family": "Coombs",
        "given": "James H."
      },
      {
        "family": "Renear",
        "given": "Allen H."
      },
      {
        "family": "DeRose",
        "given": "Steven J."
      }
    ],
    "container-title": "Commun. ACM",
    "id": "Coombs1987",
    "issue": "11",
    "issued": {
      "date-parts": [
        [
          1987,
          11
        ]
      ]
    },
    "keyword": "document_research, markup",
    "language": "en-US",
    "page": "933-947",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Markup systems and the future of scholarly text processing",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "DOI": "10.1145/958220.958262",
    "ISBN": "1581137249",
    "author": [
      {
        "family": "Janssen",
        "given": "William C."
      },
      {
        "family": "Popat",
        "given": "Kris"
      }
    ],
    "container-title": "DocEng ’03: Proceedings of the 2003 ACM symposium on document engineering",
    "id": "citeulike:261957",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "bibliography, digital_library",
    "language": "en-US",
    "page": "234-242",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "UpLib: A universal personal digital library system",
    "title-short": "UpLib",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/5401.5402",
    "ISSN": "1046-8188",
    "author": [
      {
        "family": "Trigg",
        "given": "Randall H."
      },
      {
        "family": "Weiser",
        "given": "Mark"
      }
    ],
    "container-title": "ACM Trans. Inf. Syst.",
    "id": "citeulike:261960",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1986,
          1
        ]
      ]
    },
    "keyword": "document_research, hypertext",
    "language": "en-US",
    "page": "1-23",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "TEXTNET: A network-based approach to text handling",
    "title-short": "TEXTNET",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "DOI": "10.1145/62506.62524",
    "ISBN": "0897912918",
    "author": [
      {
        "family": "Chamberlin",
        "given": "Donald D."
      }
    ],
    "container-title": "DOCPROCS ’88: Proceedings of the ACM conference on document processing systems",
    "id": "citeulike:261961",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "interactive_editing, typesetting",
    "language": "en-US",
    "page": "101-109",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "An adaptation of dataflow methods for WYSIWYG document processing",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/158976.159000",
    "ISBN": "0897916069",
    "author": [
      {
        "family": "Lauritsen",
        "given": "Marc"
      }
    ],
    "container-title": "ICAIL ’93: Proceedings of the 4th international conference on artificial intelligence and law",
    "id": "citeulike:261962",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "page": "184-191",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Knowing documents",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.google.com/url?sa=U;start=4;q=http://cajun.cs.nott.ac.uk/compsci/epo/papers/volume5/issue4/ep072fc.pdf;e=9888",
    "abstract": "Creating structured documents–where every document element belongs to a class–has many well-known advantages. Using generic document styles to define and constrain the hierarchical relationships between the different classes of element also has many advantages, but causes significant problems in interactive editing. The recent EP-odd paper on Rita [1] provided new insights into the possibilities and problems of editing structured documents. This “EP-odds and ends” sketches some additional problems and suggests alternative solutions based on the idea of fall-back classes.",
    "author": [
      {
        "family": "Cole",
        "given": "Fred"
      },
      {
        "family": "Brown",
        "given": "Heather"
      }
    ],
    "container-title": "Electronic Publishing—Origination, Dissemination and Design",
    "id": "citeulike:261963",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1992,
          12
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "209-216",
    "title": "Editing structured documents–problems and solutions",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.1145/133160.133186",
    "ISBN": "0897915232",
    "author": [
      {
        "family": "Deerwester",
        "given": "Scott C."
      },
      {
        "family": "Waclena",
        "given": "Keith"
      },
      {
        "family": "Lamar",
        "given": "Michelle"
      }
    ],
    "container-title": "SIGIR ’92: Proceedings of the 15th annual international ACM SIGIR conference on research and development in information retrieval",
    "id": "citeulike:261965",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "document_management, document_research",
    "language": "en-US",
    "page": "126-139",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "A textual object management system",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/383962.383978",
    "ISBN": "1581133839",
    "author": [
      {
        "family": "Rowley",
        "given": "Chris"
      }
    ],
    "container-title": "PODC ’01: Proceedings of the twentieth annual ACM symposium on principles of distributed computing",
    "id": "citeulike:261966",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "typesetting",
    "language": "en-US",
    "page": "17-25",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The LaTeX legacy: 2.09 and all that",
    "title-short": "The LaTeX legacy",
    "type": "paper-conference"
  },
  {
    "URL": "http://mustard.tapor.uvic.ca/cocoon/ach_abstracts/xq/pdf.xq?id=158",
    "abstract": "The recent digitisation of the papers and lecture notes of the Australian realist philosopher, Challis Professor of Philosophy John Anderson, has given us cause to reflect upon, on the one hand, the suitability of the TEI model for encoding digital documents and, on the other, the possibility that Anderson’s philosophy itself may be relevant to some of the issues and debates in contemporary markup theory and practice.",
    "author": [
      {
        "family": "Cole",
        "given": "Creagh"
      },
      {
        "family": "Scifleet",
        "given": "Paul"
      }
    ],
    "id": "citeulike:261967",
    "issued": {
      "date-parts": [
        [
          2005,
          6
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "title": "In the philosophy room: Australian realism and the digital content object",
    "title-short": "In the philosophy room",
    "type": "manuscript"
  },
  {
    "URL": "http://www.tsi.enst.fr/\\~{}cfaure/articles/CIDE01_f.pdf",
    "author": [
      {
        "family": "Faure",
        "given": "Claudie"
      }
    ],
    "container-title": "Actes de CIDE2001, Toulouse",
    "id": "citeulike:261968",
    "issued": {
      "date-parts": [
        [
          2001,
          10
        ]
      ]
    },
    "keyword": "visual_languages",
    "language": "fr-FR",
    "page": "85-97",
    "title": "Étude expérimentale de l’équivalence sémantique des configurations graphiques",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.tsi.enst.fr/\\~{}cfaure/articles/cide03.pdf",
    "abstract": "La détection des colonnes dans des documents représentés sous des formats où leur structure n’est pas ou peu explicite (format image, format d’échange) est une question qui est généralement traitée au niveau de la structuration physique. De nombreux algorithmes existent pour répondre à cette question. On constate néanmoins la difficulté à obtenir des résultats sans erreur avec des OCRs et certains logiciels d’extraction du texte des documents PDF. La détection des colonnes a pour conséquence principale de reconstruire l’ordre de lecture en éliminant les ruptures de colonnes non significatives du point de vue logique. Cette linéarisation du texte ne couvre qu’un sous- ensemble des cas où il est fait usage des colonnes. Une typologie des cas s’appuyant sur des corpus sera introduite. La question qui se pose alors à l’analyse des documents est d’affecter une catégorie logique aux colonnes, ou aux éléments d’une colonne, et de déterminer leurs liens logiques. Cette interprétation est nécessaire pour reconstruire un document sémantiquement \"équivalent\" à l’original mais dont la présentation diffère de l’original pour s’adapter à la lecture sur écran. Des règles de traduction seront proposées pour passer des mises en page aux mises en écran.",
    "author": [
      {
        "family": "Faure",
        "given": "Claudie"
      },
      {
        "family": "Vincent",
        "given": "Nicole"
      }
    ],
    "container-title": "Actes de Cide6",
    "id": "citeulike:261977",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "document_analysis, document_research",
    "language": "fr-FR",
    "title": "De la mise en page à la mise en écran: le cas des colonnes",
    "title-short": "De la mise en page à la mise en écran",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/378965.378968",
    "ISSN": "0736-6906",
    "author": [
      {
        "family": "Rubinstein",
        "given": "Richard"
      },
      {
        "family": "Bigelow",
        "given": "Charles"
      },
      {
        "family": "Baudin",
        "given": "Fernand"
      },
      {
        "family": "Lynch",
        "given": "Eugene"
      },
      {
        "family": "Levy",
        "given": "David M."
      }
    ],
    "container-title": "SIGCHI Bulletin",
    "id": "citeulike:261978",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1985,
          7
        ]
      ]
    },
    "keyword": "typesetting, typography",
    "language": "en-US",
    "page": "9-15",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Proceedings of the typography interest group ACM CHI’85",
    "type": "article-journal",
    "volume": "17"
  },
  {
    "URL": "http://mustard.tapor.uvic.ca/cocoon/ach_abstracts/xq/pdf.xq?id=174",
    "author": [
      {
        "family": "Iacob",
        "given": "Ionut E."
      },
      {
        "family": "Dekhtyar",
        "given": "Alex"
      }
    ],
    "id": "citeulike:262011",
    "issued": {
      "date-parts": [
        [
          2005,
          6
        ]
      ]
    },
    "keyword": "markup",
    "language": "en-US",
    "title": "Concurrent markup hierarchies: A computer science approach",
    "title-short": "Concurrent markup hierarchies",
    "type": "manuscript"
  },
  {
    "URL": "http://mustard.tapor.uvic.ca/cocoon/ach_abstracts/xq/pdf.xq?id=176",
    "author": [
      {
        "family": "Kiernan",
        "given": "Kevin"
      },
      {
        "family": "Porter",
        "given": "Dorothy"
      },
      {
        "family": "Dekhtyar",
        "given": "Alex"
      },
      {
        "family": "Iacob",
        "given": "Ionut E."
      },
      {
        "family": "Jaromczyk",
        "given": "Jerzy W."
      },
      {
        "family": "Moore",
        "given": "Neil"
      }
    ],
    "id": "citeulike:262012",
    "issued": {
      "date-parts": [
        [
          2005,
          6
        ]
      ]
    },
    "keyword": "markup",
    "language": "en-US",
    "title": "The edition production technology (EPT) and the ARCHway and electronic boethius projects",
    "type": "manuscript"
  },
  {
    "URL": "http://mustard.tapor.uvic.ca/cocoon/ach_abstracts/xq/pdf.xq?id=167",
    "author": [
      {
        "family": "Flanders",
        "given": "Julia"
      },
      {
        "family": "Bauman",
        "given": "Syd"
      },
      {
        "family": "Romary",
        "given": "Laurent"
      },
      {
        "family": "Birnbaum",
        "given": "David J."
      },
      {
        "family": "Zimmerman",
        "given": "Matthew"
      }
    ],
    "id": "citeulike:262013",
    "issued": {
      "date-parts": [
        [
          2005,
          6
        ]
      ]
    },
    "keyword": "markup",
    "language": "en-US",
    "title": "Advanced topics in TEI",
    "type": "manuscript"
  },
  {
    "URL": "http://mustard.tapor.uvic.ca/cocoon/ach_abstracts/xq/pdf.xq?id=205",
    "author": [
      {
        "family": "Renear",
        "given": "Allen H."
      },
      {
        "family": "Lee",
        "given": "Jin H."
      },
      {
        "family": "Choi",
        "given": "Yunseon"
      },
      {
        "family": "Xiang",
        "given": "Xin"
      }
    ],
    "id": "citeulike:262014",
    "issued": {
      "date-parts": [
        [
          2005,
          6
        ]
      ]
    },
    "keyword": "markup",
    "language": "en-US",
    "title": "Exhibition: A problem for conceptual modeling in the humanities",
    "title-short": "Exhibition",
    "type": "manuscript"
  },
  {
    "URL": "http://mustard.tapor.uvic.ca/cocoon/ach_abstracts/xq/pdf.xq?id=165",
    "author": [
      {
        "family": "Cayless",
        "given": "Hugh"
      }
    ],
    "id": "citeulike:262015",
    "issued": {
      "date-parts": [
        [
          2005,
          6
        ]
      ]
    },
    "keyword": "document_management",
    "language": "en-US",
    "title": "DocScapes: Visualizing document structures with SVG",
    "title-short": "DocScapes",
    "type": "manuscript"
  },
  {
    "URL": "http://www.blnz.com/lml/",
    "author": [
      {
        "family": "Lindsey",
        "given": "William D."
      }
    ],
    "id": "citeulike:262017",
    "keyword": "functional, lisp, markup, sgml",
    "language": "en-US",
    "title": "Lml – lambda markup language",
    "type": "webpage"
  },
  {
    "DOI": "10.1002/spe.532",
    "ISSN": "1097-024X",
    "URL": "http://www.cs.nott.ac.uk/\\~{}dfb/Publications/Download/2003/Probets03.pdf",
    "abstract": "As collections of archived digital documents continue to grow the maintenance of an archive, and the quality of reproduction from the archived format, become important long-term considerations. In particular, Adobe’s portable document format (PDF) is now an important <IMG SRC=\"/giflibrary/12/lsquo.gif\" BORDER=\"0\">final form<IMG SRC=\"/giflibrary/12/rsquo.gif\" BORDER=\"0\"> standard for archiving and distributing electronic versions of technical documents. It is important that all embedded images in the PDF, and any fonts used for text rendering, should at the very minimum be easily readable on screen. Unfortunately, because PDF is based on PostScript technology, it allows the embedding of bitmap fonts in Adobe Type 3 format as well as higher-quality outline fonts in TrueType or Adobe Type 1 formats. Bitmap fonts do not generally perform well when they are scaled and rendered on low-resolution devices such as workstation screens.</TD ></TR ><TR ><TD >The work described here investigates how a plug-in to Adobe Acrobat enables bitmap fonts to be substituted by corresponding outline fonts using a checksum matching technique against a canonical set of bitmap fonts, as originally distributed. The target documents for our initial investigations are those PDF files produced by LATEX systems when set up in a default (bitmap font) configuration. For all bitmap fonts where recognition exceeds a certain confidence threshold replacement fonts in Adobe Type 1 (outline) format can be substituted with consequent improvements in file size, screen display quality and rendering speed. The accuracy of font recognition is discussed together with the prospects of extending these methods to bitmap-font PDF files from sources other than LATEX. Copyright &copy; 2003 John Wiley &amp; Sons, Ltd.",
    "author": [
      {
        "family": "Probets",
        "given": "S. G."
      },
      {
        "family": "Brailsford",
        "given": "D. F."
      }
    ],
    "container-title": "Software: Practice and Experience",
    "id": "citeulike:262294",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          2003,
          6,
          25
        ]
      ]
    },
    "keyword": "typesetting, typography",
    "language": "en-US",
    "page": "885-899",
    "title": "Substituting outline fonts for bitmap fonts in archived PDF files",
    "type": "article-journal",
    "volume": "33"
  },
  {
    "URL": "http://www-sop.inria.fr/mimosa/fp/Scribe/doc/scribe.html",
    "abstract": "This paper presents Scribe, a functional programming language for authoring documents. Even if it is a general purpose tool, it best suits the writing of technical documents such as web pages or technical reports, API documentations, etc. Executing Scribe programs can produce documents of various formats such as PostScript, PDF, HTML, Texinfo or Unix man pages. That is, the very same program can be used to produce documents in different formats. Scribe is a full featured programming language but it looks like a markup language à la HTML.",
    "author": [
      {
        "family": "Seranno",
        "given": "Manuel"
      },
      {
        "family": "Gallesio",
        "given": "Erick"
      }
    ],
    "container-title": "Third workshop on scheme and functional programming, pittsburg, PA",
    "id": "citeulike:262335",
    "issued": {
      "date-parts": [
        [
          2002,
          10
        ]
      ]
    },
    "keyword": "functional, lisp, typesetting",
    "language": "en-US",
    "page": "31-40",
    "title": "This is scribe!",
    "type": "paper-conference"
  },
  {
    "URL": "http://www-sop.inria.fr/mimosa/fp/Skribe/",
    "abstract": "Skribe is a programming language designed for implementing electronic documents. It is mainly designed for the writing of technical documents such as the documentation of computer programs. With Skribe these documents can be rendered using various tools and technologies. For instance, a Skribe document can be compiled to an HTML file that suits Web browser, it can be compiled to a TeX file in order to produce a high-quality printed document, and so on.",
    "author": [
      {
        "family": "Serrano",
        "given": "Manuel"
      },
      {
        "family": "Gallesio",
        "given": "Erick"
      }
    ],
    "id": "citeulike:262337",
    "keyword": "functional, lisp, typesetting",
    "language": "en-US",
    "title": "Skribe home page",
    "type": "webpage"
  },
  {
    "URL": "http://isbn.nu/0306405768",
    "collection-title": "NATO conference series: III, human factors; v. 13",
    "editor": [
      {
        "family": "Kolers",
        "given": "Paul A."
      },
      {
        "family": "Wrolstad",
        "given": "Merald E."
      },
      {
        "family": "Bouma",
        "given": "Herman"
      }
    ],
    "id": "citeulike:264919",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "keyword": "typography, visual_languages",
    "language": "en-US",
    "publisher": "NATO Scientific Affairs Division; Plenum Press",
    "publisher-place": "New York, London",
    "title": "Processing of visible language 2",
    "type": "book"
  },
  {
    "ISBN": "0521325927",
    "abstract": "This book covers all aspects of computer document preparation text processing and printing. Computers are being used increasingly in the processing of documents, from simple textual material, such as letters and memos, to complete books with mathematical formulae and graphics. The material may be extensively edited and manipulated on the computer before subsequent output on media such as typewriters, laser printers or photocomposers. This volume contains contributions from several established leaders in the field, and a number of research articles referred by an international programme committee. As such, the book gives a good impression of the state-of-the art in this area, which is of major importance in this ’electronic age’ where on-line information retrieval and electronic publishing will increasingly affect our everyday life.",
    "author": [
      {
        "family": "Van Vliet",
        "given": "J. C."
      }
    ],
    "collection-title": "British computer society workshop series",
    "editor": [
      {
        "family": "Van Vliet",
        "given": "J. C."
      }
    ],
    "id": "citeulike:264944",
    "issued": {
      "date-parts": [
        [
          1986,
          3,
          31
        ]
      ]
    },
    "keyword": "document_research, markup, tables, typesetting",
    "language": "en-US",
    "publisher": "Hardcover; Cambridge University Press",
    "title": "Text processing and document manipulation",
    "type": "book"
  },
  {
    "ISBN": "0792383575",
    "abstract": "Document Computing: Technologies for Managing Electronic Document Collections discusses the important aspects of document computing and recommends technologies and techniques for document management, with an emphasis on the processes that are appropriate when computers are used to create, access, and publish documents. This book includes descriptions of the nature of documents, their components and structure, and how they can be represented; examines how documents are used and controlled; explores the issues and factors affecting design and implementation of a document management strategy; and gives a detailed case study. The analysis and recommendations are grounded in the findings of the latest research. Document Computing: Technologies for Managing Electronic Document Collections brings together concepts, research, and practice from diverse areas including document computing, information retrieval, librarianship, records management, and business process re-engineering. It will be of value to anyone working in these areas, whether as a researcher, a developer, or a user. Document Computing: Technologies for Managing Electronic Document Collections can be used for graduate classes in document computing and related fields, by developers and integrators of document management systems and document management applications, and by anyone wishing to understand the processes of document management.",
    "author": [
      {
        "family": "Wilkinson",
        "given": "Ross"
      },
      {
        "family": "Arnold-Moore",
        "given": "Timothy"
      },
      {
        "family": "Fuller",
        "given": "Michael"
      },
      {
        "family": "Sacks-Davis",
        "given": "Ron"
      },
      {
        "family": "Thom",
        "given": "James"
      },
      {
        "family": "Zobel",
        "given": "Justin"
      }
    ],
    "collection-title": "The kluwer international series on information retrieval, 5",
    "id": "citeulike:264945",
    "issued": {
      "date-parts": [
        [
          1998,
          11,
          1
        ]
      ]
    },
    "keyword": "document_management, document_research",
    "language": "en-US",
    "publisher": "Hardcover; Kluwer Academic Publishers",
    "title": "Document computing: Technologies for managing electronic document collections",
    "title-short": "Document computing",
    "type": "book"
  },
  {
    "DOI": "10.1017/S1351324903003073",
    "URL": "http://www.cambridge.org/uk/journals/journal_article.asp?mnemonic=NLE;pii=S1351324903003073",
    "abstract": "Hyphenation is the task of identifying potential hyphenation points in words. In this paper, three nite-state hyphenation methods for Dutch are presented and compared in terms of accuracy and size of the resulting automata.",
    "author": [
      {
        "family": "Bouma",
        "given": "Gosse"
      }
    ],
    "container-title": "Natural Language Engineering",
    "id": "citeulike:267653",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "nlp, typesetting",
    "language": "en-US",
    "page": "5-20",
    "title": "Finite state methods for hyphenation",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "URL": "http://www.fi.muni.cz/usr/sojka/papers/sojka-thesis.pdf",
    "abstract": "The goal of this dissertation is to explore models, methods and methodologies for machine learning of the compact and effective storage of empirical data in the areas of language engineering and computer typesetting, with a focus on the massive exception handling. Research has focused on the pattern-driven approach. The whole methodology of so called competing patterns capable of handling exceptions to be found so widely in natural language data and computer typesetting, is further developed. Competing patterns can store context dependent information and can be learnt from data, or written by experts, or combined together. In the first part of the thesis, the theory of competing patterns is built; competing patterns are defined, cornerstones of methodology based on stratified sampling, bootstrapping and problem modeling by competing patterns are described. Segmentation problems (hyphenation) and problems of disambiguation of tagged data in corpus linguistics are used as examples when developing formal model of the competing patterns method. The second part consist of a series of seven published papers that describe problems addressed by the proposed methods: applications of competing patterns and related learning methods in areas of hyphenation, hyphenation of compound words and, for example, the segmentation of Thai texts.",
    "author": [
      {
        "family": "Sojka",
        "given": "Petr"
      }
    ],
    "genre": "PhD thesis",
    "id": "citeulike:268305",
    "issued": {
      "date-parts": [
        [
          2005,
          1
        ]
      ]
    },
    "keyword": "nlp, typesetting",
    "language": "en-US",
    "publisher": "Masaryk University in Brno",
    "title": "Competing patterns in language engineering and computer typesetting",
    "type": "thesis"
  },
  {
    "URL": "https://www.pragma-pod.com/pdftex/thesis.pdf",
    "abstract": "This thesis investigates the possibility to improve the quality of text composition. Two typographic extensions were examined: margin kerning and composing with font expansion. Margin kerning is the adjustments of the characters at the margins of a typeset text. A simplified employment of margin kerning is hanging punctuation. Margin kerning is needed for optical alignment of the margins of a typeset text, because mechanical justification of the margins makes them look rather ragged. Some characters can make a line appear shorter to the human eye than others. Shifting such characters by an appropriate amount into the margins would greatly improve the appearance of a typeset text. Composing with font expansion is the method to use a wider or narrower variant of a font to make interword spacing more even. A font in a loose line can be substituted by a wider variant so the interword spaces are stretched by a smaller amount. Similarly, a font in a tight line can be replaced by a narrower variant to reduce the amount that the interword spaces are shrunk by. There is certainly a potential danger of font distortion when using such manipulations, thus they must be used with extreme care. The potentiality to adjust a line width by font expansion can be taken into consideration while a paragraph is being broken into lines, in order to choose better breakpoints. These typographic extensions were implemented in pdfTEX, a derivation of TEX. Heavy experiments have been done to examine the influence of the extensions on the quality of typesetting. The extensions turned out to noticeably improve the appearance of a typeset text. A number of “real-world” documents have been typeset using these typographic extensions, including this thesis.",
    "author": [
      {
        "family": "Thành",
        "given": "Hàn T."
      }
    ],
    "genre": "PhD thesis",
    "id": "citeulike:268306",
    "issued": {
      "date-parts": [
        [
          2000,
          10
        ]
      ]
    },
    "keyword": "typesetting, typography",
    "language": "en-US",
    "publisher": "Masaryk University Brno",
    "title": "Micro-typographic extensions to the TeX typesetting system",
    "type": "thesis"
  },
  {
    "DOI": "10.1002/(sici)1097-4571(199709)48:9\\%3C804::aid-asi5\\%3E3.0.co;2-v",
    "ISSN": "1097-4571",
    "abstract": "Ordinarily the word “document” denotes a textual record. Increasingly sophisticated attempts to provide access to the rapidly growing quantity of available documents raised questions about what should be considered a “document.” The answer is important for any definition of the scope of Information Science. Paul Otlet and others developed a functional view of “document” and discussed whether, for example, sculpture, museum objects, and live animals, could be considered “documents.” Suzanne Briet equated “document” with organized physical evidence. These ideas appear to resemble notions of “material culture” in cultural anthropology and “object-as-sign” in semiotics. Others, especially in the U.S.A. (e.g., Jesse Shera and Louis Shores) took a narrower view. New digital technology renews old questions and also old confusions between medium, message, and meaning.",
    "author": [
      {
        "family": "Buckland",
        "given": "Michael K."
      }
    ],
    "container-title": "Journal of the American Society for Information Science",
    "id": "Buckland1997",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "page": "804-809",
    "title": "What is a “document”?",
    "type": "article-journal",
    "volume": "48"
  },
  {
    "DOI": "10.1002/(sici)1097-4571(199106)42:5<351::aid-asi5>3.0.co;2-3",
    "ISSN": "1097-4571",
    "author": [
      {
        "family": "Buckland",
        "given": "Michael K."
      }
    ],
    "container-title": "Journal of the American Society for Information Science",
    "id": "Buckland1991",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "language": "en-US",
    "page": "351-360",
    "title": "Information as thing",
    "type": "article-journal",
    "volume": "42"
  },
  {
    "DOI": "10.1002/(sici)1097-4571(199609)47:9\\%3C669::aid-asi3\\%3E3.0.co;2-q",
    "ISSN": "1097-4571",
    "abstract": "Rapid growth in electronic publishing seems to have provoked a sense of uneasiness among information professionals. This essay describes the challenge of redefining the fundamental concept of ” document” that underlies their apprehensions about changes in information generation, control, and access, along with their apprehensions about the future of the information professions.",
    "author": [
      {
        "family": "Schamber",
        "given": "Linda"
      }
    ],
    "container-title": "Journal of the American Society for Information Science",
    "id": "citeulike:270763",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          1998,
          12,
          7
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "page": "669-671",
    "title": "What is a document? Rethinking the concept in uneasy times",
    "type": "article-journal",
    "volume": "47"
  },
  {
    "DOI": "10.1145/958220.958250",
    "ISBN": "1581137249",
    "author": [
      {
        "family": "Bayerl",
        "given": "Petra S."
      },
      {
        "family": "Lüngen",
        "given": "Harald"
      },
      {
        "family": "Goecke",
        "given": "Daniela"
      },
      {
        "family": "Witt",
        "given": "Andreas"
      },
      {
        "family": "Naber",
        "given": "Daniel"
      }
    ],
    "container-title": "DocEng ’03: Proceedings of the 2003 ACM symposium on document engineering",
    "id": "citeulike:271572",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "document_research, markup",
    "language": "en-US",
    "page": "161-170",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Methods for the semantic analysis of document markup",
    "type": "paper-conference"
  },
  {
    "URL": "http://mulberrytech.com/Extreme/Proceedings/html/2001/Piez01/EML2001Piez01-toc.html",
    "abstract": "There has come to be a consensus that the ” procedural vs. declarative” distinction is useful, if only as a rough guide, in the design of markup languages. To understand how and why this is the case, we need to ask questions that are usually left unasked when this principle is proposed, such as ” is it the model (the schema) that we consider to be descriptive, or the tagged document?” or, more deeply, ” why do we validate our markup anyway?” A number of our fundamental assumptions are not always true. Sometimes a schema might be more than a ” go/no-go gauge”, becoming a diagnostic and investigatory instrument. Sometimes marked-up documents look backward (as representations of something pre-existing), not just forward to processing. Sometimes semantic opacity is a feature, not a bug. In order to understand the power of markup languages, it is helpful to keep in mind that they are both technologies, and a species of rhetoric. New characterizations and categories of markup languages may help focus our design efforts. Keywords: Theory: Markup as a species of rhetoric; Theory: Varieties of markup principle; validation; interoperability; Contrasting Approaches: Descriptive, procedural, retrospective, prospective, proleptic, metaleptic markup; gauge and jig",
    "author": [
      {
        "family": "Piez",
        "given": "Wendell"
      }
    ],
    "container-title": "Extreme markup languages 2001",
    "id": "citeulike:271588",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "document_research, markup",
    "language": "en-US",
    "title": "Beyond the ” descriptive vs. Procedural” distinction",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.mulberrytech.com/Extreme/Proceedings/html/2003/Lawton01/EML2003Lawton01-toc.html",
    "abstract": "The FRBR [Functional Requirements for Bibliographic Records], released by the International Federation of Library Associations and Institutions in 1998, generalizes and refines current practices and theory in library cataloging, presenting a compelling natural ontology of entities, attributes, and relationships for representing the ” bibliographic universe”. The FRBR framework is extremely influential and increasingly accepted as a conceptual foundation for cataloging practice and technology in libraries and elsewhere. XML documents as defined in the W3C XML 1.0 specification, are now an important part of this bibliographic universe and it is natural to ask to which of FRBR’s ” Group 1” entities does the XML document correspond. Curiously, there seem to be conflicting arguments for assigning the XML document to either of the two plausible entity categories: manifestation and expression. We believe these difficulties illuminate both the nature of the FRBR entities, and the nature of markup. We explore a conjecture that an XML document has a double aspect and that whether it is a FRBR manifestation or a FRBR expression depends upon context and intention. Such a double-aspected nature would not only be consistent with previous arguments that the meaning of XML markup varies in ” illocutionary force” according to context of use, but might also help resolve an old puzzle in the humanities computing community as to whether markup is ” part of” the text [buzzetti02] . However, there are alternative resolutions to explore as well and we seem to still be some distance from a full understanding of the issues. Keywords: BECHAMEL; Contrasting Approaches: XML document represents manifestation, expression; Functional Requirements for Bibliographic Records (FRBR); Theory: Double aspect of XML document; XML document, definitions of; bibliographic records; context of use, role in markup decisions; markup, dual-aspect theory of; semantic integration",
    "author": [
      {
        "family": "Renear",
        "given": "Allen H."
      },
      {
        "family": "Phillippe",
        "given": "Christopher"
      },
      {
        "family": "Lawton",
        "given": "Pat"
      },
      {
        "family": "Dubin",
        "given": "David"
      }
    ],
    "container-title": "Proceedings of extreme markup languages 2003",
    "id": "citeulike:271589",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "document_research, xml",
    "language": "en-US",
    "title": "An XML document corresponds to which FRBR group 1 entity?",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.dynalabs.de/mxp/magister/ma-as-report.pdf",
    "abstract": "Although information retrieval systems mainly deal with natural language, linguistic methods are rarely used. Most systems only use stemming, i.e., the mechanical cutting off of inflectional and derivational suffixes to better match index terms to query terms. Since most research on information retrieval is done for English, which has a relatively weak morphology, this is seldom regarded as problematic. Some researchers even consider stemming as completely unnecessary. There is, however, considerable evidence that stemming and more linguistically motivated methods do have a positive impact on retrieval performance for languages such as Dutch, German, Italian, or Slovene, which are morphologically richer than English. Morphologic phenomena like compounds and changes of the stem are still not handled by conventional stemmers. As German, for example, makes extensive use of these morphologic processes (consider compounds like Bundesverfassungsgericht, and changes of the stem like in Häuser, the plural of Haus), the application of full morphologic analysis to the information retrieval task intuitively seems to be promising. This thesis sets out to determine the usefulness of morphologic analysis in information retrieval systems, particularly for the retrieval of German-language documents. An experimental retrieval system called IRF/1 was developed as a test bed. It is described in this thesis. IRF/1 is used to compare the retrieval effectiveness of different text processing methods for a test collection of about 300 magazine articles. The evaluated methods are: 1. stemming (as a baseline), 2. base form reduction using morphologic analysis 3. same as (2) but compounds are split into the base forms of their constituents, and 4. same as (3) but the base forms of compounds are kept along with their parts. Using the standard information retrieval measures of recall and precision, the comparison finds morphologic analysis to be generally more effective than stemming. While morphologic base form reduction only provides relatively little improvement over stemming, decomposition of compounds results in a decisive increase in retrieval effectiveness for German. It can be concluded that morphologic analysis with decomposition of compounds is a very promising approach to improving information retrieval for German and should be further investigated.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "genre": "Master’s thesis",
    "id": "citeulike:271609",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "ir, mxp, nlp",
    "language": "en-US",
    "publisher": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
    "title": "NLP-supported full-text retrieval",
    "type": "thesis"
  },
  {
    "abstract": "This article analyzes the usefulness of NLP techniques for text retrieval and presents a new research project, which tries to take into account the results of the analysis, namely by replacing stemming with full morphological analysis.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Revue. Informatique et Statistique dans les Sciences Humaines",
    "id": "citeulike:271621",
    "issue": "1–4",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "ir, mxp, nlp",
    "language": "en-US",
    "page": "285-298",
    "publisher": "Université de Liège",
    "title": "NLP-supported full text retrieval",
    "type": "article-journal",
    "volume": "33"
  },
  {
    "URL": "http://citeseer.ist.psu.edu/chali96text.html",
    "abstract": "This paper is focused on the first aim : we propose a model of text that is, at the same time, constrained by and needed for the second aim. This framework led us to examine the notion of text structure in an original way, following from the nature of the project: a class of neuropsycholinguistic experimental protocols, whose purpose",
    "author": [
      {
        "family": "Chali",
        "given": "Yllias"
      },
      {
        "family": "Pascual",
        "given": "Elsa"
      },
      {
        "family": "Virbel",
        "given": "Jacques"
      }
    ],
    "id": "citeulike:271625",
    "keyword": "document_research",
    "language": "en-US",
    "title": "Text structure modelling and language comprehension processes",
    "type": ""
  },
  {
    "URL": "http://www.blackwell-synergy.com/doi/abs/10.1111/j.1365-2729.2004.00091.x",
    "abstract": "With the rapid development of distance learning and the XML technology, metadata play an important role in e-Learning. Nowadays, many distance learning standards, such as SCORM, AICC CMI, IEEE LTSC LOM and IMS, use metadata to tag learning materials. However, most metadata models are used to define learning materials and test problems. Few metadata models are dedicated to assessment. In this paper, the authors propose an assessment metadata model for e-Learning operations. With support from assessment metadata, we can incorporate measured aspects of the following list into the metadata description at the question cognition level, the item difficulty index, the item discrimination index, the questionnaire style and the question style. The assessment analysis model provides analytical suggestions for individual questions, summary of test results and cognition analysis. Analytical suggestions provide teachers information about why a question is not appropriate. Summary of test results improves the teacher’s view of student learning status immediately. Items missing from the teaching materials can be identified by cognition analysis. In this research, the authors propose an enhanced metadata model and an implemented system based on our model. With metadata support, metadata can help teachers in authoring examination.",
    "author": [
      {
        "family": "Chang",
        "given": "Wen-Chih"
      },
      {
        "family": "Hsu",
        "given": "Hui-Huang"
      },
      {
        "family": "Smith",
        "given": "Timothy K."
      },
      {
        "family": "Wang",
        "given": "Chun-Chia"
      }
    ],
    "container-title": "Journal of Computer Assisted Learning",
    "id": "citeulike:271627",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2004,
          8
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "page": "305-316",
    "title": "Enhancing SCORM metadata for assessment authoring in e-Learning",
    "type": "article-journal",
    "volume": "20"
  },
  {
    "DOI": "10.1145/143457.143473",
    "ISBN": "0897915429",
    "abstract": "An important activity in collaborative writing is communicating about changes to texts. This paper reports on a software system, flexible diff, that finds and reports differences (\"diffs\") between versions of texts. The system is flexible, allowing users to control several aspects of its operation including what changes are reported and how they are shown when they are reported. We argue that such flexibility is necessary to support users’ different social and cognitive needs. KEYWORDS: Text comparison, collaborative writing, flexible differencing",
    "author": [
      {
        "family": "Neuwirth",
        "given": "Christine M."
      },
      {
        "family": "Chandhok",
        "given": "Ravinder"
      },
      {
        "family": "Kaufer",
        "given": "David S."
      },
      {
        "family": "Erion",
        "given": "Paul"
      },
      {
        "family": "Morris",
        "given": "James"
      },
      {
        "family": "Miller",
        "given": "Dale"
      }
    ],
    "container-title": "CSCW ’92: Proceedings of the 1992 ACM conference on computer-supported cooperative work",
    "id": "citeulike:271642",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "document_management, interactive_editing",
    "language": "en-US",
    "page": "147-154",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Flexible diff-ing in a collaborative writing system",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.adm.hb.se/\\~{}mg/gslt/da2.pdf",
    "abstract": "The study of document architectures concerns among many things, the logical and physical structure of textual artefacts. This paper tries to sketch out a theoretical framework in which studies of how document architectures are construed in scholarly electronic publishing on the web may be thoroughly grounded. The framework represents an attempt to pull theories from sociology, linguistics and technology into a coherent model for text production that may function in document structure recognition tasks.",
    "author": [
      {
        "family": "Gunnarsson",
        "given": "Mikael"
      }
    ],
    "id": "citeulike:271717",
    "issued": {
      "date-parts": [
        [
          2003,
          3,
          24
        ]
      ]
    },
    "keyword": "document_analysis, document_research",
    "language": "en-US",
    "publisher": "Swedish School of Library and Information Science at Högskolan i Borås; A paper for a PhD course on Theory Development",
    "title": "Toward a theory of document architectures",
    "type": "manuscript"
  },
  {
    "URL": "http://asi-www.informatik.uni-hamburg.de/studiendiplomarbeiten/tim_wohlberg/diplomarbeit.pdf",
    "abstract": "Ein wichtiges Konzept bei der computerunterstützten Erzeugung und Verwaltung von Dokumenten ist die explizite Trennung zwischen der logischen und grafischen Struktur. Seit Mitte der achtziger Jahre existiert der ISO-Standard SGML (vgl. ISO 8879, 1986) zur Beschreibung logischer Strukturen von Dokumenten. Die Hypertext Markup Language (vgl. W3C, REC-html) des World Wide Web dürfte die populärste Anwendung dieser Metasprache sein. Daneben gibt es eine Vielzahl anderer Beschreibungssprachen in SGML, die in der Industrie, Forschung und beim Militär eingesetzt werden. Viele dieser SGML-Anwendungen enthalten Möglichkeiten zur Beschreibung von Tabellen. Auffallend an diesen Beschreibungssprachen ist die Tatsache, daß die meisten von ihnen sich an der geometrischen Form von Tabellen orientieren. Sie Aspekte der grafischen Struktur und nicht die logischen Zusammenhänge zwischen den Elementen der Tabellen. Mit XML (vgl. W3C, REC-xml) wurde 1998 ein weiterer Standard zur Beschreibung logischer Strukturen von SGML abgeleitet. Zielrichtung der Entwicklung war die Vereinfachung von SGML für den Einsatz im World Wide Web. In dieser Arbeit soll überprüft werden, ob mit der neu geschaffenen Metasprache XML die logischen Strukturen von Tabellen sinnvoll beschrieben werden können. Dafür soll eine entsprechende Beschreibungssprache in Form einer Dokument-Typ-Definition entwickelt werden.",
    "author": [
      {
        "family": "Wohlberg",
        "given": "Tim"
      }
    ],
    "genre": "Master’s thesis",
    "id": "citeulike:271718",
    "issued": {
      "date-parts": [
        [
          1999,
          6
        ]
      ]
    },
    "keyword": "document_research, tables, xml",
    "language": "en-US",
    "publisher": "Fachbereich Informatik, Universität Hamburg",
    "title": "Hypertables: Entwicklung einer strukturbeschreibungssprache für tabellen in XML",
    "title-short": "Hypertables",
    "type": "thesis"
  },
  {
    "URL": "http://xml.coverpages.org/bib-ce.html\\#durandTextShould",
    "author": [
      {
        "family": "Durand",
        "given": "David G."
      },
      {
        "family": "DeRose",
        "given": "Steven J."
      },
      {
        "family": "Mylonas",
        "given": "Elli"
      }
    ],
    "container-title": "ALLC/ACH ’96 (june 25 - 29, 1996. University of bergen, norway)",
    "id": "citeulike:271720",
    "issued": {
      "date-parts": [
        [
          1996,
          6
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "title": "What should markup really be? Applying theories of text to the design of markup systems",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.univ-tlse2.fr/erss/textes/theses_hdr/hdr_mppw.pdf",
    "author": [
      {
        "family": "Péry-Woodley",
        "given": "Marie-Paule"
      }
    ],
    "id": "Pery-Woodley2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "document_research, nlp",
    "language": "en-US",
    "publisher": "Université de Toulouse-Le Mirail; Habilitation à Diriger des Recherches",
    "title": "Une pragmatique à fleur de texte : Approche en corpus de l’organisation textuelle",
    "title-short": "Une pragmatique à fleur de texte ",
    "type": ""
  },
  {
    "URL": "http://www.research.ibm.com/journal/sj/213/ibmsj2103C.pdf",
    "abstract": "This paper describes the architecture of an experimental document composition system named JANUS, which is intended to support authors of complex documents containing mixtures of text and images. The JANUS system is highly interactive, providing authors with immediate feedback and direct electronic control over page layouts, using a special two-display workstation. Authors communicate with the system by marking up their documents with high-level descriptive ” tags.” A tag definition language is provided whereby new tags may be defined and the format of each tagged object may be controlled.",
    "author": [
      {
        "family": "Chamberlin",
        "given": "Donald D."
      },
      {
        "family": "Bertrand",
        "given": "O. P."
      },
      {
        "family": "Goodfellow",
        "given": "M. J."
      },
      {
        "family": "King",
        "given": "James C."
      },
      {
        "family": "Slutz",
        "given": "Donald R."
      },
      {
        "family": "Todd",
        "given": "Stephen J. P."
      },
      {
        "family": "Wade",
        "given": "Bradford W."
      }
    ],
    "container-title": "IBM Systems Journal",
    "id": "citeulike:271725",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "keyword": "interactive_editing, markup",
    "language": "en-US",
    "page": "250-271",
    "title": "JANUS: An interactive document formatter based on declarative tags",
    "title-short": "JANUS",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "abstract": "The good news is that non-linear or hypertext systems may dramatically increase the accessibility of information. The bad news is that this increased accessibility may magnify further an already severe problem of selection. Whether we are sending or receiving a body of information, we must take steps to distinguish its components on the basis of their potential importance or relevance. Current hypertext efforts have focused on the development of tools giving users direct control over the formation and traversal of links connecting units of information in a network structure. Such tools place considerable power and a considerable burden in the hands of the users. Information must be initially organized in ways that prove useful later on; links leading to relevant information must subsequently be distinguished from a potentially large number of others. These activities may be very difficult to accomplish in an expanding knowledge base. In this article we look at potential selection problems in hypertext and we examine some of the ways in which these problems may be remedied.",
    "author": [
      {
        "family": "Jones",
        "given": "William P."
      }
    ],
    "container-title": "Proceedings INTERACT ’87 – 2nd IFIP international conference on human-computer interaction. Stuttgart, germany",
    "editor": [
      {
        "family": "Bullinger",
        "given": "H. J."
      },
      {
        "family": "Schackel",
        "given": "B."
      }
    ],
    "id": "citeulike:271726",
    "issued": {
      "date-parts": [
        [
          1987,
          9
        ]
      ]
    },
    "keyword": "hypertext",
    "language": "en-US",
    "page": "1107-1113",
    "publisher": "Elsevier Science Publishers B.V. (North-Holland)",
    "title": "How do we distinguish the hyper from the hype in non-linear text?",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Renner",
        "given": "Paul"
      }
    ],
    "container-title": "Das moderne buch",
    "id": "citeulike:271728",
    "issued": {
      "date-parts": [
        [
          1947
        ]
      ]
    },
    "keyword": "orthography",
    "language": "en-US",
    "page": "12-25",
    "publisher": "Thorbecke",
    "publisher-place": "Lindau",
    "title": "Großschreibung oder kleinschreibung",
    "type": "chapter"
  },
  {
    "ISBN": "3870970944",
    "author": [
      {
        "family": "Sager",
        "given": "Juan C."
      },
      {
        "family": "Dungworth",
        "given": "David"
      },
      {
        "family": "Mcdonald",
        "given": "Peter F."
      }
    ],
    "container-title": "English special languages: Principles and practice in science and technology",
    "id": "citeulike:271729",
    "issued": {
      "date-parts": [
        [
          1980,
          6,
          1
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "page": "138-143",
    "publisher": "Hardcover; Brandstetter",
    "publisher-place": "Wiesbaden",
    "title": "Types of special reports",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/192593.192718",
    "ISBN": "0897916867",
    "abstract": "This paper describes an approach to the automatic presentation of multimedia documents based on parsing and syntax-directed translation using Relational Grammars. This translation is followed by a constraint solving mechanism to create the final layout. Grammatical rules provide the mechanism for mapping from a representation of the content of a presentation to forms that specify the media objects to be realized. These realization forms include sets of spatial and temporal constraints between elements of the presentation. Individual grammars encapsulate the “look and feel” of a presentation and can be used as generators of that style. By making the grammars sensitive to the requirements of the output medium, parsing can introduce flexibility into the information realization process. Keywords: Automatic design, grammar-directed design, visual languages, relational grammars, parsing, constraints",
    "author": [
      {
        "family": "Weitzman",
        "given": "Louis"
      },
      {
        "family": "Wittenburg",
        "given": "Kent"
      }
    ],
    "container-title": "MULTIMEDIA ’94: Proceedings of the second ACM international conference on multimedia",
    "id": "citeulike:271731",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "automatic_layout, layout, visual_languages",
    "language": "en-US",
    "page": "443-451",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Automatic presentation of multimedia documents using relational grammars",
    "type": "paper-conference"
  },
  {
    "URL": "http://acg.media.mit.edu/people/fry/thesis/",
    "abstract": "Design techniques for static information are well understood, their descriptions and discourse thorough and well-evolved. But these techniques fail when dynamic information is considered. There is a space of highly complex systems for which we lack deep understanding because few techniques exist for visualization of data whose structure and content are continually changing. To approach these problems, this thesis introduces a visualization process titled Organic Information Design. The resulting systems employ simulated organic properties in an interactive, visually refined environment to glean qualitative facts from large bodies of quantitative data generated by dynamic information sources.",
    "author": [
      {
        "family": "Fry",
        "given": "Benjamin J."
      }
    ],
    "genre": "Master’s thesis",
    "id": "citeulike:271735",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "keyword": "infodesign",
    "language": "en-US",
    "publisher": "MIT Media Lab Aesthetics & Computation Group",
    "title": "Organic information design",
    "type": "thesis"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=167487",
    "abstract": "Many distributed systems for wide-area networks can be built conveniently, and operate efficiently and correctly, using a weak consistency group communication mechanism. This mechanism organizes a set of principals into a single logical entity, and provides methods to multicast messages to the members. A weak consistency distributed system allows the principals in the group to differ on the value of shared state at any given instant, as long as they will eventually converge to a single, consistent value. A group containing many principals and using weak consistency can provide the reliability, performance, and scalability necessary for wide-area systems. I have developed a framework for constructing group communication systems, for classifying existing distributed system tools, and for constructing and reasoning about a particular group communication model. It has four components: message delivery, message ordering, group membership, and the application. Each component may have a different implementation, so that the group mechanism can be tailored to application requirements. The framework supports a new message delivery protocol, called timestamped anti-entropy, which provides reliable, eventual message delivery; is efficient; and tolerates most transient processor and network failures. It can be combined with message ordering implementations that provide orderikng guarantees ranging from unordered to total, causal delivery. A new group membership protocol completes the set, providing temporarily inconsistent membership views resilient to up to simultaneous principal failures. The Refdbms distributed bibliographic database system, which has been constructed using this framework, is used as an example. Refdbms databases can be replicated on many different sites, using the group communication system described here.",
    "author": [
      {
        "family": "Golding",
        "given": "Richard A."
      }
    ],
    "genre": "PhD thesis",
    "id": "citeulike:271737",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "bibliography, digital_library, distributed_systems",
    "language": "en-US",
    "publisher": "University of California at Santa Cruz; University of California at Santa Cruz",
    "publisher-place": "Santa Cruz, CA, USA",
    "title": "Weak-consistency group communication and membership",
    "type": "thesis"
  },
  {
    "URL": "http://citeseer.ifi.unizh.ch/pascual96semantic.html",
    "abstract": "Higher-level graphical and lexical punctuation (paragraphing, indentation, font changes, ...) must be taken into consideration in comprehension processes and text generation. In this paper, we analyse a class of text punctuation marks which includes lexical units (chapter, introduction). We give a method for the analysis of the semantics of these units, in terms of metalanguage. In addition, their syntactic properties are considered using the contrastive properties of their layout.",
    "author": [
      {
        "family": "Pascual",
        "given": "Elsa"
      },
      {
        "family": "Virbel",
        "given": "Jacques"
      }
    ],
    "container-title": "Proceedings of the association for computational linguistics workshop on punctuation, santa cruz, CA",
    "id": "citeulike:271741",
    "issued": {
      "date-parts": [
        [
          1996,
          6
        ]
      ]
    },
    "keyword": "document_research, layout, markup",
    "language": "en-US",
    "page": "41-48",
    "title": "Semantic and layout properties of text punctuation",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/604131.604132",
    "ISBN": "1-58113-628-5",
    "ISSN": "0362-1340",
    "abstract": "The World-Wide Web Consortium (W3C) promotes XML and related standards, including XML Schema, XQuery, and XPath. This paper describes a formalization of XML Schema. A formal semantics based on these ideas is part of the official XQuery and XPath specification, one of the first uses of formal methods by a standards body. XML Schema features both named and structural types, with structure based on tree grammars. While structural types and matching have been studied in other work (notably XDuce, Relax NG, and a previous formalization of XML Schema), this is the first work to study the relation between named types and structural types, and the relation between matching and validation.",
    "author": [
      {
        "family": "Siméon",
        "given": "Jérôme"
      },
      {
        "family": "Wadler",
        "given": "Philip"
      }
    ],
    "collection-number": "1",
    "collection-title": "POPL ’03",
    "container-title": "Proceedings of the 30th ACM SIGPLAN-SIGACT symposium on principles of programming languages",
    "id": "citeulike:271744",
    "issued": {
      "date-parts": [
        [
          2003,
          1
        ]
      ]
    },
    "keyword": "document_research, markup, xml",
    "language": "en-US",
    "page": "1-13",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "The essence of XML",
    "type": "paper-conference",
    "volume": "38"
  },
  {
    "URL": "http://citeseer.ist.psu.edu/307325.html",
    "author": [
      {
        "family": "Golding",
        "given": "Richard A."
      }
    ],
    "id": "citeulike:271747",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "bibliography, digital_library, distributed_systems",
    "language": "en-US",
    "publisher": "University of California at Santa Cruz",
    "publisher-place": "Santa Cruz, CA, USA",
    "title": "A WEAK-CONSISTENCY ARCHITECTURE FOR DISTRIBUTED INFORMATION SERVICES",
    "type": "report"
  },
  {
    "DOI": "10.1145/143457.143554",
    "ISBN": "0-89791-542-9",
    "abstract": "An abstract is not available.",
    "author": [
      {
        "family": "Musliner",
        "given": "David J."
      },
      {
        "family": "Dolter",
        "given": "James W."
      },
      {
        "family": "Shin",
        "given": "Kang G."
      }
    ],
    "collection-title": "CSCW ’92",
    "container-title": "Proceedings of the 1992 ACM conference on computer-supported cooperative work",
    "id": "citeulike:271757",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "bibliography, digital_library, distributed_systems",
    "language": "en-US",
    "page": "386-393",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "BIBDB: A bibliographic database for collaboration",
    "title-short": "BIBDB",
    "type": "paper-conference"
  },
  {
    "URL": "ftp://ftp.cse.ucsc.edu/pub/refdbms/usenix-paper.ps.Z",
    "abstract": "Refdbms is a database system for sharing bibliographic references among many users at sites on a wide-area network such as the Internet. This paper describes our experiences in building and using refdbms for the last two years. It summarizes the collection of facilities that refdbms provides, and gives detailed information on how well refdbms functions as a collaborative, wide-area, distributed information system.",
    "author": [
      {
        "family": "Golding",
        "given": "Richard A."
      },
      {
        "family": "Long",
        "given": "Darrell D. E."
      },
      {
        "family": "Wilkes",
        "given": "John"
      }
    ],
    "container-title": "Proceedings of the winter usenix conference, san francisco, california",
    "id": "citeulike:271761",
    "issued": {
      "date-parts": [
        [
          1994,
          1
        ]
      ]
    },
    "keyword": "bibliography, digital_library, distributed_systems",
    "language": "en-US",
    "page": "47-62",
    "title": "The refdbms distributed bibliographic database system",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.cs.ust.hk/faculty/dwood/preprints/thesis.ps",
    "abstract": "Supervisor: Derick Wood",
    "author": [
      {
        "family": "Wang",
        "given": "Xinxin"
      }
    ],
    "genre": "PhD thesis",
    "id": "citeulike:271762",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "tables",
    "language": "en-US",
    "publisher": "University of Waterloo",
    "title": "Tabular abstraction, editing, and formatting",
    "type": "thesis"
  },
  {
    "ISBN": "3540650865",
    "URL": "http://portal.acm.org/citation.cfm?id=647507.726045",
    "author": [
      {
        "family": "Wang",
        "given": "Xinxin"
      },
      {
        "family": "Wood",
        "given": "Derick"
      }
    ],
    "container-title": "PODDP ’98: Proceedings of the 4th international workshop on principles of digital document processing",
    "id": "citeulike:271774",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "tables",
    "language": "en-US",
    "page": "10-23",
    "publisher": "Springer-Verlag",
    "publisher-place": "London, UK",
    "title": "A conceptual model for tables",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/502187.502198",
    "ISBN": "1581134320",
    "author": [
      {
        "family": "Silberhorn",
        "given": "Horst"
      }
    ],
    "container-title": "DocEng ’01: Proceedings of the 2001 ACM symposium on document engineering",
    "id": "citeulike:271778",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "keyword": "tables",
    "language": "en-US",
    "page": "68-75",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "TabulaMagica: An integrated approach to manage complex tables",
    "title-short": "TabulaMagica",
    "type": "paper-conference"
  },
  {
    "ISSN": "0038-0644",
    "URL": "http://portal.acm.org/citation.cfm?id=46063.46064",
    "author": [
      {
        "family": "Aho",
        "given": "Alfred V."
      },
      {
        "family": "Sethi",
        "given": "Ravi"
      }
    ],
    "container-title": "Softw. Pract. Exper.",
    "id": "citeulike:271779",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1988,
          1
        ]
      ]
    },
    "keyword": "bibliography, classic",
    "language": "en-US",
    "page": "1-13",
    "publisher": "John Wiley & Sons, Inc.",
    "publisher-place": "New York, NY, USA",
    "title": "Maintaining cross reference in manuscripts",
    "type": "article-journal",
    "volume": "18"
  },
  {
    "URL": "http://informationr.net/ir/5-2/paper70.html",
    "abstract": "The architectural metaphor used in analyzing compound meta-objects such as referential databases might be applied also to the primary records these referential tools point to, thereby making way for the study of document architecture (DA). Library and information science has by and large been focussing on possible ways of determining the meanings of the objects of \"input\", whereas the materiality and the textual structure of the objects have been regarded as transparent, offering little or no room for problematization and discussion. The article argues for a revaluation of this somewhat delimiting perspective. Digital production and distribution reframe the ways in which objects and meta-objects might be construed. The mismatch of traditional library institutions and systems (where the printed codex book and its derivatives have been the standard of measurement) and digital carriers for bodies of text and the different architectures of these, suggests our great need for new fields of LIS research, where DA might prove a valuable tool. DA studies might also be useful in re-theorizing traditional reading and writing technologies and their conditioning of textual carriers.",
    "author": [
      {
        "family": "Dahlström",
        "given": "Mats"
      },
      {
        "family": "Gunnarsson",
        "given": "Mikael"
      }
    ],
    "container-title": "Information Research",
    "id": "citeulike:271780",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2000,
          1
        ]
      ]
    },
    "keyword": "document_research, sslis",
    "language": "en-US",
    "title": "Document architecture draws a circle: On document architecture and its relation to library and information science education and research",
    "title-short": "Document architecture draws a circle",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "ISBN": "0198236638",
    "abstract": "The electronic presentation of text has revolutionized the understanding and use of literary evidence. Formerly, readers and editors were obliged to choose one edition of a text in book form to work with and to treat other versions as ancillary. Now electronic editions of a text can incorporate all the various versions and revisions. This allows unconstrained access to a much greater range of information. This collection considers the role of computerized technology in contributing to the interpretation and editing of texts, from both practical and theoretical perspectives. The contributors investigate the ways in which the treatment of texts and the idea of a \"text\" are affected by current and prospective advances in electronic production and reproduction.",
    "author": [
      {
        "family": "Sutherland",
        "given": "Kathryn"
      }
    ],
    "editor": [
      {
        "family": "Sutherland",
        "given": "Kathryn"
      }
    ],
    "id": "citeulike:271784",
    "issued": {
      "date-parts": [
        [
          1998,
          4,
          1
        ]
      ]
    },
    "keyword": "document_research",
    "language": "en-US",
    "publisher": "Hardcover; Oxford University Press",
    "title": "Electronic text: Investigations in method and theory",
    "title-short": "Electronic text",
    "type": "book"
  },
  {
    "URL": "http://portal.acm.org/citation.cfm?id=1072314",
    "author": [
      {
        "family": "Lee",
        "given": "Kyung-Soon"
      },
      {
        "family": "Kageura",
        "given": "Kyo"
      },
      {
        "family": "Choi",
        "given": "Key-Sun"
      }
    ],
    "container-title": "Proceedings of the 19th international conference on computational linguistics",
    "id": "citeulike:272191",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "ir, nlp",
    "language": "en-US",
    "page": "1-7",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Morristown, NJ, USA",
    "title": "Implicit ambiguity resolution using incremental clustering in Korean-to-English cross-language information retrieval",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1145/356835.356838",
    "ISSN": "0360-0300",
    "URL": "http://www.ecse.rpi.edu/homepages/nagy/PDF_files/Embley81.pdf",
    "abstract": "Theoretical models and experimental results relevant to the study of behavioral issues in the use of text editors—including both those intended primarily for computer program development and those intended for manuscript preparation—are examined.",
    "author": [
      {
        "family": "Embley",
        "given": "David W."
      },
      {
        "family": "Nagy",
        "given": "George"
      }
    ],
    "container-title": "ACM Comput. Surv.",
    "id": "citeulike:272615",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1981,
          3
        ]
      ]
    },
    "keyword": "interactive_editing, psychology",
    "language": "en-US",
    "page": "33-70",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Behavioral aspects of text editors",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "ISBN": "0521432774",
    "abstract": "Electronic publishing encompasses all aspects of computer-assisted document preparation, presentation, transmittance, storage, and retrieval. It is a very active area of study, involving developments in academic and industrial arenas, with important commercial applications and implications. The papers in this volume, which are all refereed and carefully edited, reflect not only such recent trends as hypermedia and multimedia documents, but also the classical and ever-present problems of document structure and font design.",
    "author": [
      {
        "family": "Vanoirbeek",
        "given": "Christine"
      },
      {
        "family": "Coray",
        "given": "Giovanni"
      }
    ],
    "collection-title": "Cambridge series on electronic publishing",
    "editor": [
      {
        "family": "Vanoirbeek",
        "given": "Christine"
      },
      {
        "family": "Coray",
        "given": "Giovanni"
      }
    ],
    "id": "citeulike:272636",
    "issued": {
      "date-parts": [
        [
          1992,
          4,
          30
        ]
      ]
    },
    "keyword": "interactive_editing, ir, lisp, markup, sgml, typesetting",
    "language": "en-US",
    "publisher": "Hardcover; Cambridge University Press",
    "publisher-place": "Cambridge",
    "title": "EP92 (proceedings of electronic publishing 1992)",
    "type": "book"
  },
  {
    "DOI": "10.1006/jvlc.2002.0234",
    "ISSN": "1045926X",
    "abstract": "An important step in the design of visual languages is the specification of the graphical objects and the composition rules for constructing feasible visual sentences. The presence of different typologies of visual languages, each with specific graphical and structural characteristics, yields the need to have models and tools that unify the design steps for different types of visual languages. To this aim, in this paper we present a formal framework of visual language classes. Each class characterizes a family of visual languages based upon the nature of their graphical objects and composition rules. The framework has been embedded in the Visual Language Compiler–Compiler (VLCC), a graphical system for the automatic generation of visual programming environments.",
    "author": [
      {
        "family": "Costagliola",
        "given": "G."
      },
      {
        "family": "Delucia",
        "given": "A."
      },
      {
        "family": "Orefice",
        "given": "S."
      },
      {
        "family": "Polese",
        "given": "G."
      }
    ],
    "container-title": "Journal of Visual Languages & Computing",
    "id": "citeulike:27306",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          2002,
          12
        ]
      ]
    },
    "keyword": "visual_languages",
    "language": "en-US",
    "page": "573-600",
    "publisher": "Academic Press",
    "title": "A classification framework to support the design of visual languages",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "URL": "https://humanit.hb.se/article/view/134/0",
    "abstract": "In this paper I ask the question: what has literature to offer computer science? Can a bilateral programme of research be started with the aim of discovering the same kind of deep intertwining of ideas between computer science and literature, as already exists between computer science and linguistics? What practical use could such results yield? […]",
    "author": [
      {
        "family": "Dougherty",
        "given": "Mark"
      }
    ],
    "container-title": "Human IT",
    "id": "Dougherty2004",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "literature",
    "language": "en-US",
    "page": "74-91",
    "title": "What has literature to offer computer science?",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "URL": "http://www.hb.se/bhs/ith/4-02/md.pdf",
    "abstract": "The concepts, methods and perspectives of “material bibliography” and its various branches (analytical, descriptive, historical, social and textual) have as tools been specifically developed in order to deal with the artefacts of script and print culture. To whate extent, if at all, is it possible (or even desirable) to bring these tools of material bibliography into the worlds of new media and apply them in critical analysis of *digital* document and their texts? Does such a supposedly quaint scholarly practice really have anything to contribute to modern media studies? This article attempts a critical analysis of material bibliography versus new media, primarily by scrutinising its fundamental tools: the bibliographic concepts. A key question is whether the concepts are far too dependant on their original historical media ecology to be meaningfully exported to other media ecologies, or whether it is indeed possible to identify relatively context-free meanings in the concepts that enable them to cross media borders. Particular emphasis is put on the bibliographic concept of material document and its distinction from an immaterial work.",
    "author": [
      {
        "family": "Dahlström",
        "given": "Mats"
      }
    ],
    "container-title": "Human IT",
    "id": "citeulike:275099",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "document_research, sslis",
    "language": "en-US",
    "page": "71-116",
    "title": "Nya medier, gamla verktyg",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "DOI": "10.1093/aesthj/ayi030",
    "URL": "http://bjaesthetics.oxfordjournals.org/cgi/reprint/45/3/209.pdf",
    "abstract": "Could a work of art actually authored by one artist have been authored, instead, by another? This is the question of the necessity of authorship. After distinguishing this question from another, regarding individuation, with which it is often confused, this paper offers an argument that authorship is indeed a necessary feature of most artworks. The argument proceeds from “independence principles”, which govern the processes by which artworks are produced. Independence principles are motivated, in turn, by metaphysical reflections on what it takes to prevent an artist from producing a particular work of art.",
    "author": [
      {
        "family": "Rohrbaugh",
        "given": "Guy"
      }
    ],
    "container-title": "British Journal of Aesthetics",
    "id": "citeulike:275133",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2005,
          7
        ]
      ]
    },
    "keyword": "authoring, philosophy",
    "language": "en-US",
    "page": "209-228",
    "title": "I could have done that",
    "type": "article-journal",
    "volume": "45"
  },
  {
    "ISBN": "3922660231",
    "abstract": "This text is a theoretical document of Central European modernism between the World Wars and a source of working principles for the practising designer. It ranges from theoretical discussions of typography in the age of photography and mechanical standardization, to practical considerations in the design of business forms. The book should be of interest to designers, art historians, and all those concerned with the evolution of visual communication in the 20th century.",
    "author": [
      {
        "family": "Tschichold",
        "given": "Jan"
      }
    ],
    "edition": "zweite Auflage",
    "id": "citeulike:276788",
    "issued": {
      "date-parts": [
        [
          1987,
          1,
          31
        ]
      ]
    },
    "keyword": "classic, layout, typography",
    "language": "en-US",
    "publisher": "Die erste Auflage war 1928 im Verlag des Bildungsverbandes der Deutschen Buchdrucker erschienen; Brinkmann & Bose",
    "title": "Die neue typographie",
    "type": "book"
  },
  {
    "abstract": "Communication is the production, transmission, and consumption of messages. Messages, discrete units of content, can be as simple as a wink or as compex as the Encyclopedia Britannica. To prepare ourselves to function effectively in this new era requires structuring communications. We begin by identifying and defining key terms. Whereas engineers know the precise meanings of the terms they use (such as torsion, moment, velocity, and so on) the therms communicatios use are ambigous. For example, communicators still argue about the meaning of the elementary term “writing”; a final decision must be made if it means the actual marks on the page which people read or an activity carried on by writers as they conceive content. Once the key terms are defined, they must be related using diagrammatic models. This paper provides a classification and description of various pictorial and linguistic communicative techniques.",
    "author": [
      {
        "family": "Doblin",
        "given": "Jay"
      }
    ],
    "collection-title": "NATO conference series: III, human factors; v. 13",
    "container-title": "Processing of visible language 2",
    "editor": [
      {
        "family": "Kolers",
        "given": "Paul A."
      },
      {
        "family": "Wrolstad",
        "given": "Merald E."
      },
      {
        "family": "Bouma",
        "given": "Herman"
      }
    ],
    "id": "citeulike:276863",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "keyword": "document_research, visual_languages",
    "language": "en-US",
    "page": "89-111",
    "publisher": "NATO Scientific Affairs Division; Plenum Press",
    "publisher-place": "New York, London",
    "title": "A structure for nontextual communications",
    "type": "chapter"
  },
  {
    "abstract": "During on-screen writing and reading with a word processor, users often lack a global perspective ot the text. A writing system is presented, based on the metaphor of paper sheets, aiming to facilitate overview and give better support for the human spatial memory. The advantages and problems of the paper sheet metaphor for reading and writing are discussed, as well as additional overview functions compatible with the paper model.",
    "author": [
      {
        "family": "Eklundh",
        "given": "Kerstin S."
      },
      {
        "family": "Romberger",
        "given": "Staffan"
      },
      {
        "family": "Englund",
        "given": "Per"
      }
    ],
    "collection-title": "Cambridge series on electronic publishing",
    "container-title": "EP92 (proceedings of electronic publishing 1992)",
    "id": "citeulike:276887",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "keyword": "document_research, interactive_editing",
    "language": "en-US",
    "page": "143-151",
    "publisher": "Cambridge University Press",
    "title": "Writing on sheets of paper: A spatial metaphor for Computer-Based text handling",
    "title-short": "Writing on sheets of paper",
    "type": "chapter"
  },
  {
    "ISBN": "1878271423",
    "abstract": "The ABC’s of Bauhaus traces the origins and impact of the Bauhaus in relation to design, graphic design, and typography. The book, designed by the authors, invokes the Bauhaus ideal of synthesizing editorial concept, typography, and format. The essays address such issues as modernist design theory in relation to the nineteenth-century kindergarden movement and Bauhaus graphic design in relation to the idea of a universal \"language\" of vision. Additional essays address psychoanalysis, fractal geometry, and Weimar culture.",
    "author": [
      {
        "family": "Lupton",
        "given": "Ellen"
      },
      {
        "family": "Miller",
        "given": "Abbott J."
      }
    ],
    "editor": [
      {
        "family": "Lupton",
        "given": "Ellen"
      },
      {
        "family": "Miller",
        "given": "Abbott J."
      }
    ],
    "id": "citeulike:277216",
    "issued": {
      "date-parts": [
        [
          1991,
          6,
          15
        ]
      ]
    },
    "keyword": "bauhaus, typography, visual_languages",
    "language": "en-US",
    "publisher": "Paperback; Princeton Architectural Press",
    "title": "The ABC’s of ▲■●: The bauhaus and design theory",
    "title-short": "The ABC’s of ▲■●",
    "type": "book"
  },
  {
    "URL": "http://citeseer.ist.psu.edu/douglas96layout.html",
    "abstract": "In this paper, we describe some of the interactions between layout and language we have been dealing with in recent applied NLP projects. We present two complementary views of lists and tables, intended to bridge the gap between considering them as a type of running text (which linguistics knows how to deal with) and as a multi-dimensional relation represented in two dimensions, which may have many reading-paths (which linguistics doesn’t know how to deal with). Stated or inferred linguistic and world knowledge in the text surrounding tables and lists provides a context for the interpretation of a set of tuples extracted from tables or lists together with heuristics about how multi-dimensional information is projected on to two dimensions.",
    "author": [
      {
        "family": "Douglas",
        "given": "Shona"
      },
      {
        "family": "Hurst",
        "given": "Matthew"
      }
    ],
    "id": "citeulike:277304",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "document_analysis, nlp, tables",
    "language": "en-US",
    "page": "19-24",
    "title": "Layout and language: Lists and tables in technical documents",
    "title-short": "Layout and language",
    "type": ""
  },
  {
    "ISBN": "1581150822",
    "abstract": "When Transforming Words Into Print Becomes a Passion <P>Color’em, distort’em, move’em: changing fonts on a computer screen is almost as easy and fun as playing with a set of rubber stamps. And yet, creating and choosing type is an art that not only requires artistic skills, but a deep understanding of the many ways type is rooted in the communication patterns of our times. In \"Texts on Type: Critical Writings on Typography,\" acclaimed design authors Steven Heller and Philip B. Meggs open a window into the secret universe of present and past typography. <P>\"Texts on Type\" is an unprecedented opportunity to discover some of the most brilliant and sparkling minds of 20th century type design and typography. In fifty thought-provoking essays, readers encounter the evangelists and critics of a craft whose aesthetic frontiers have always been negotiated with unsurpassed fervor, from W.A. Dwiggins, Herman Zapf, and Paul Rand to the front-runners of contemporary typography. Practitioners of type and typophiles will perceive this craft with new eyes as they learn about the groundbreaking technological, aesthetic, and cultural changes that have propelled typography from a rarified, secluded craft into a dynamic activity practiced by millions. <P>The fifty essays cover topics such as <P>* Principals of designing and choosing the right typeface <br>* The Modernism versus Tradition debate <br>* The relationship between type form and expression <br>* The anatomy of typefaces across the 20th century <br>* Aesthetical reflections from classics to electronic and dynamic typography",
    "editor": [
      {
        "family": "Heller",
        "given": "Steven"
      },
      {
        "family": "Meggs",
        "given": "Philip B."
      }
    ],
    "id": "citeulike:293043",
    "issued": {
      "date-parts": [
        [
          2001,
          3,
          1
        ]
      ]
    },
    "keyword": "typography",
    "language": "en-US",
    "publisher": "Paperback; Watson-Guptill Publications",
    "title": "Texts on type: Critical writings on typography",
    "title-short": "Texts on type",
    "type": "book"
  },
  {
    "ISBN": "1568981589",
    "abstract": "German typographer Paul Renner is best known as the designer of the typeface Futura, which stands as a landmark of modern graphic design. This title is the first study in any language of Renner’s typographic career; it details his life and work to reveal the breadth of his accomplishment and influence. Renner was a central figure in the German artistic movements of the 1920s and 1930s, becoming an early and prominent member of the Deutscher Werkbund while creating his first book designs for various Munich-based publishers. As the author of numerous texts such as Typografie als Kunst (Typography as Art) and Die Kunst der Typographie (The Art of Typography) he created a new set of guidelines for balanced book design. Renner taught with Jan Tschichold in the 1930s and was a key participant in the heated ideological and artistic debates of that time. Arrested and dismissed from his post by the Nazis, he eventually emerged as a voice of experience and reason in the postwar years. Throughout this tumultuous period he produced a body of work of the highest distinction.",
    "author": [
      {
        "family": "Burke",
        "given": "Christopher"
      }
    ],
    "id": "citeulike:293044",
    "issued": {
      "date-parts": [
        [
          1999,
          1,
          1
        ]
      ]
    },
    "keyword": "typography",
    "language": "en-US",
    "publisher": "Paperback; Princeton Architectural Press",
    "title": "Paul renner: The art of typography",
    "title-short": "Paul renner",
    "type": "book"
  },
  {
    "ISBN": "1581150636",
    "abstract": "In this eclectic collection, thought-provoking essays from a wide range of perspectives explore the multifaceted interaction between graphic design and reading. The writings discuss various kinds of established reading patterns-from magazine browsing to online reading to contemplating a one-word poster. All of these settings for graphic design, and many more, offer springboards for investigating the complex relationship between typography and content-a fascinating exploration for graphic designers, teachers, and students.",
    "editor": [
      {
        "family": "Swanson",
        "given": "Gunnar"
      }
    ],
    "id": "citeulike:293045",
    "issued": {
      "date-parts": [
        [
          2000,
          7,
          15
        ]
      ]
    },
    "keyword": "document_research, layout, typography, visual_languages",
    "language": "en-US",
    "publisher": "Paperback; Watson-Guptill Publications",
    "title": "Graphic design and reading: Explorations of an uneasy relationship",
    "title-short": "Graphic design and reading",
    "type": "book"
  },
  {
    "ISBN": "0769521819",
    "URL": "http://portal.acm.org/citation.cfm?id=1020039",
    "author": [
      {
        "family": "Garcia-Robles",
        "given": "Rocio"
      },
      {
        "family": "Blat",
        "given": "Josep"
      },
      {
        "family": "Sayago",
        "given": "Sergio"
      },
      {
        "family": "Griffiths",
        "given": "Dai"
      },
      {
        "family": "Casado",
        "given": "Francis"
      },
      {
        "family": "Martinez",
        "given": "Juanjo"
      }
    ],
    "container-title": "ICALT ’04: Proceedings of the IEEE international conference on advanced learning technologies",
    "id": "citeulike:2945399",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "assessment, e-learning",
    "language": "en-US",
    "page": "495-499",
    "publisher": "IEEE Computer Society",
    "publisher-place": "Washington, DC, USA",
    "title": "Supporting usability and reusability based on eLearning standards",
    "type": "paper-conference"
  },
  {
    "ISBN": "1412911117",
    "abstract": "E-learning is now an essential component of education. Globalization, the proliferation of information and knowledge available on the Internet and the importance of our knowledge-based economy have added a whole new dimension to teaching and learning. As more teachers and students adopt online learning there is a need for resources that will examine and instruct this field.",
    "author": [
      {
        "family": "Gardner",
        "given": "John R."
      },
      {
        "family": "Holmes",
        "given": "Bryn"
      }
    ],
    "id": "citeulike:404940",
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "Paperback; SAGE Publications",
    "title": "E-Learning: Concepts and practice",
    "title-short": "E-Learning",
    "type": "book"
  },
  {
    "ISBN": "3830912862",
    "URL": "http://www.worldcat.org/isbn/3830912862",
    "author": [
      {
        "family": "Grotlüschen",
        "given": "Anke"
      }
    ],
    "id": "citeulike:405495",
    "issued": {
      "date-parts": [
        [
          2003,
          9,
          30
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "Paperback; Waxmann Verlag GmbH",
    "title": "Widerständiges lernen im web – virtuell selbstbestimmt?",
    "type": "book"
  },
  {
    "DOI": "10.1145/1526709.1526798",
    "ISBN": "978-1-60558-487-4",
    "abstract": "We present a novel method for key term extraction from text documents. In our method, document is modeled as a graph of semantic relationships between terms of that document. We exploit the following remarkable feature of the graph: the terms related to the main topics of the document tend to bunch up into densely interconnected subgraphs or communities, while non-important terms fall into weakly interconnected communities, or even become isolated vertices. We apply graph community detection techniques to partition the graph into thematically cohesive groups of terms. We introduce a criterion function to select groups that contain key terms discarding groups with unimportant terms. To weight terms and determine semantic relatedness between them we exploit information extracted from Wikipedia. Using such an approach gives us the following two advantages. First, it allows effectively processing multi-theme documents. Second, it is good at filtering out noise information in the document, such as, for example, navigational bars or headers in web pages. Evaluations of the method show that it outperforms existing methods producing key terms with higher precision and recall. Additional experiments on web pages prove that our method appears to be substantially more effective on noisy and multi-theme documents than existing methods.",
    "author": [
      {
        "family": "Grineva",
        "given": "Maria"
      },
      {
        "family": "Grinev",
        "given": "Maxim"
      },
      {
        "family": "Lizorkin",
        "given": "Dmitry"
      }
    ],
    "container-title": "Proceedings of the 18th international world wide web conference (WWW ’09)",
    "id": "citeulike:5790283",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "ir, topic_modeling, wikipedia",
    "language": "en-US",
    "page": "661-670",
    "publisher": "ACM",
    "publisher-place": "New York, NY, USA",
    "title": "Extracting key terms from noisy and multitheme documents",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1016/s0304-3975(02)00025-7",
    "ISSN": "0304-3975",
    "abstract": "This paper discusses a mathematical concept of language that models both artificial and natural languages and thus provides a framework for a unified language processing methodology. This concept of a language is regarded as a communication tool that allows language users to develop knowledges, while interacting with their universe of discourse, and to communicate with each other, while exchanging knowledges. Criteria for consistent usage of a language are established using a Galois connection between language syntax and language semantics. Solutions to ambiguity, paraphrase, attitude, and other problems concerning the relationship between syntax and semantics are addressed. A general schema for language specification is introduced and algorithms that perform language generation and language analysis are discussed as universal tools defined by the specification schema. Language transformations performed by various kinds of translators are examined and correctness criteria of these translators are defined using the language Galois connection. The paper is structured as follows: Section 1 introduces the framework and justifies the necessity of a unified methodology for language processing. Section 2 presents the mathematical concept of a language. Section 3 illustrates the mathematical concept of a language with three kinds of language structures: natural language, logical language, and programming language. Section 4 discusses the algebraic mechanism of language specification that unifies the methodology for language processing tool development. Section 5 formalizes the criterion for the consistency of the language usage, defines the architecture of a unified language processing system, and shows how the consistency criteria for language usage can be employed as correctness criteria for the algorithms performing various language transformations.",
    "author": [
      {
        "family": "Rus",
        "given": "Teodor"
      }
    ],
    "container-title": "Theor. Comput. Sci.",
    "id": "citeulike:591387",
    "issue": "1-2",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "language_theory, nlp",
    "language": "en-US",
    "page": "499-536",
    "publisher": "Elsevier Science Publishers Ltd.",
    "publisher-place": "Essex, UK",
    "title": "A unified language processing methodology",
    "type": "article-journal",
    "volume": "281"
  },
  {
    "DOI": "10.1016/0920-5489(96)00033-5",
    "ISSN": "0920-5489",
    "abstract": "The ISO SGML standard for document markup has had a tremendous impact on the electronic document community’s understanding of its data. The notion of a public, common representation for text suggests a level of data sharing and systems interoperability that was previously unknown. By focusing attention on the structure of a document rather than its appearance, SGML popularized a new approach to document management, one that treats documents as databases, rather than artifacts whose sole function is to be displayed. By shifting documents from display-centered media to database data, however, SGML has raised new problems that the standard itself was not intended to solve. These problems arise because a printed document is fundamentally different from a database. A printed document is essentially a closed world with a single product. A database, on the other hand, is an open world, with many potential products, and most importantly, with the possibility of change. In this paper we neither appraise the semantics of previous proposals, nor introduce our own. Instead, we discuss some issues that are “metasemantic”: issues that lie behind every semantics, but that are peculiar to none. We discuss three issues–equivalence, redundancy, and operations–that are implicit in current systems and approaches. By making these issues explicit, we hope to provide the beginnings of a framework for comparison of the semantics of existing systems, and the development of more advanced ones. Before considering meta-semantic issues, it will be useful to briefly recount the differing experiences of the document and data processing communities.",
    "author": [
      {
        "family": "Raymond",
        "given": "Darrell"
      },
      {
        "family": "Tompa",
        "given": "Frank"
      },
      {
        "family": "Wood",
        "given": "Derick"
      }
    ],
    "container-title": "Computer Standards & Interfaces",
    "id": "citeulike:591393",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1996,
          1
        ]
      ]
    },
    "keyword": "document_research, markup, sgml",
    "language": "en-US",
    "page": "25-36",
    "publisher": "Elsevier Science Publishers B. V.",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "From data representation to data model: Meta-semantic issues in the evolution of SGML",
    "title-short": "From data representation to data model",
    "type": "article-journal",
    "volume": "18"
  },
  {
    "ISBN": "3486580035",
    "author": [
      {
        "family": "Schulmeister",
        "given": "Rolf"
      }
    ],
    "id": "citeulike:701293",
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "Oldenbourg",
    "title": "eLearning: Einsichten und aussichten",
    "title-short": "eLearning",
    "type": "book"
  },
  {
    "DOI": "10.1111/j.1473-4192.1998.tb00128.x",
    "ISSN": "1473-4192",
    "abstract": "One of the conspicuous differences in academic discourse presentation is the preference for use of the first person singular ’I’ perspective in some languages versus the first person plural ’we’ perspective in others. This distinction is due not so much to purely linguistic reasons as to cultural ones. The purpose of this paper is to present some data concerning the employment of the two perspectives in English, German, French, Russian and Bulgarian research articles in linguistics and to suggest some explanations, as well as to point to certain cross-cultural misunderstandings which may result from the differences established here. An attempt is also made to investigate certain cross-linguistic and cross-cultural influences among the five languages. The analysis is based on 5 corpora consisting of 300 pages for each language.",
    "author": [
      {
        "family": "Vassileva",
        "given": "Irena"
      }
    ],
    "container-title": "International Journal of Applied Linguistics",
    "id": "citeulike:7858162",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "keyword": "authoring",
    "language": "en-US",
    "page": "163-185",
    "publisher": "Blackwell Publishing Ltd",
    "title": "Who am I/who are we in academic writing?",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "URL": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.6920",
    "abstract": "A Structured documents such as those developed for SGML, GML or LaTeX usually contain a combination of text and tags. Since various types of documents require tags with different placement, the creator of a document must learn and retain a large amount of knowledge. Rita consists of an editor and user interface which are controlled by a grammar or description of a document type and its tags, and which guide the user in preparing a document, thus avoiding the problems of tags being used or placed incorrectly. The user interface contains a display which is almost WYSIWYG so that the appearance of the document can be examined while it is being prepared. This paper describes Rita, its user interface and some of its internal structure and algorithms, and relates anecdotal user experiences. Comparisons are also made with other commercial and experimental systems.",
    "author": [
      {
        "family": "Cowan",
        "given": "Donald D."
      },
      {
        "family": "Mackie",
        "given": "E. W."
      },
      {
        "family": "Pianosi",
        "given": "G. M."
      },
      {
        "dropping-particle": "de",
        "family": "V. Smit",
        "given": "G"
      }
    ],
    "container-title": "Electronic Publishing—Origination, Dissemination and Design",
    "id": "citeulike:878986",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1991,
          9
        ]
      ]
    },
    "keyword": "interactive_editing, markup",
    "language": "en-US",
    "page": "125-150",
    "title": "Rita – an editor and user interface for manipulating structured documents",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "URL": "http://www-static.cc.gatech.edu/\\~{}amyvoida/listeningIn-chi05.pdf",
    "abstract": "This paper presents a descriptive account of the social practices surrounding the iTunes music sharing of 13 participants in one organizational setting. Specifically, we characterize adoption, critical mass, and privacy; impression management and access control; the musical impressions of others that are created as a result of music sharing; the ways in which participants attempted to make sense of the dynamic system; and implications of the overlaid technical, musical, and corporate topologies. We interleave design implications throughout our results and relate those results to broader themes in a music sharing design space.",
    "author": [
      {
        "family": "Voida",
        "given": "Amy"
      },
      {
        "family": "Grinter",
        "given": "Rebecca E."
      },
      {
        "family": "Ducheneaut",
        "given": "Nicolas"
      },
      {
        "family": "Edwards",
        "given": "Keith W."
      },
      {
        "family": "Newman",
        "given": "Mark W."
      }
    ],
    "container-title": "CHI 2005, april 2­–7, 2005, portland, oregon, USA",
    "id": "citeulike:878996",
    "keyword": "awareness, collaboration, music_sharing",
    "language": "en-US",
    "title": "Listening in: Practices surrounding iTunes music sharing",
    "title-short": "Listening in",
    "type": "paper-conference"
  },
  {
    "ISBN": "1402040318",
    "abstract": "Listening to, buying and sharing music is an immensely important part of everyday life. Yet recent technological developments are increasingly changing how we use and consume music. This book collects together the most recent studies of music consumption, and new developments in music technology. It combines the perspectives of both social scientists and technology designers, uncovering how new music technologies are actually being used, along with discussions of new music technologies still in development. With a specific focus on the social nature of music, the book breaks new ground in bringing together discussions of both the social and technological aspects of music use. Chapters cover topics such as the use of the iPod, music technologies which encourage social interaction in public places, and music sharing on the internet. Prof. Dorothy Miell, (Open University) and Associate Dean (Curriculum ad Awards) in the Social Sciences Faculty, Milton Keynes, UK says: \"A highly original and stimulating collection of contributions addressing aspects of our everyday music experiences in the modern world. The picture it paints of music as highly social and collaborative, yet deeply personal, is a rich and complex one which advances thinking about the many functions music plays in our lives. It is often the case that new ideas and exciting developments emerge at the boundaries between existing disciplines and bodies of knowledge, and in this text the editors have succeeded in bringing together work from music, technical and social science backgrounds to point out possibilities for researchers at these boundaries as they can be applied to a fast moving and exciting area of knowledge\". A valuable collection for anyone concerned with the future of music technology, this book will be of particular interest to those designing new music technologies, those working in the music industry, along with students of music and new technology.",
    "collection-title": "Computer supported cooperative work",
    "editor": [
      {
        "family": "Brown",
        "given": "Barry"
      }
    ],
    "id": "citeulike:879042",
    "issued": {
      "date-parts": [
        [
          2006,
          2,
          28
        ]
      ]
    },
    "keyword": "awareness, collaboration, music_sharing",
    "language": "en-US",
    "publisher": "Hardcover; Kluwer Academic Publishers",
    "title": "Consuming music together: Social and collaborative aspects of music consumption technologies",
    "title-short": "Consuming music together",
    "type": "book"
  },
  {
    "ISBN": "080131903X",
    "abstract": "B></I></B></U> This revision of Bloom’s taxonomy is designed to help teachers understand and implement standards-based curriculums. Cognitive psychologists, curriculum specialists, teacher educators, and researchers have developed a two-dimensional framework, focusing on knowledge and cognitive processes. In combination, these two define what students are expected to learn in school. Like no other text, it explores curriculums from three unique perspectives-cognitive psychologists (learning emphasis), curriculum specialists and teacher educators (C&I emphasis), and measurement and assessment experts (assessment emphasis). This \"revisited\" framework allows you to connect learning in all areas of curriculum. Educators, or others interested in Educational Psychology or Educational Methods for grades K-12.",
    "author": [
      {
        "family": "Anderson",
        "given": "Lorin W."
      },
      {
        "family": "Krathwohl",
        "given": "David R."
      },
      {
        "family": "Airasian",
        "given": "Peter W."
      },
      {
        "family": "Cruikshank",
        "given": "Kathleen A."
      },
      {
        "family": "Mayer",
        "given": "Richard E."
      },
      {
        "family": "Pintrich",
        "given": "Paul R."
      },
      {
        "family": "Raths",
        "given": "James"
      },
      {
        "family": "Wittrock",
        "given": "Merlin C."
      }
    ],
    "edition": "2",
    "id": "citeulike:961573",
    "issued": {
      "date-parts": [
        [
          2000,
          12,
          29
        ]
      ]
    },
    "keyword": "e-learning, pedagogy",
    "language": "en-US",
    "publisher": "Paperback; Pearson",
    "title": "A taxonomy for learning, teaching, and assessing: A revision of bloom’s taxonomy of educational objectives, abridged edition",
    "title-short": "A taxonomy for learning, teaching, and assessing",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:9724553",
    "issued": {
      "date-parts": [
        [
          2011,
          8,
          29
        ]
      ]
    },
    "keyword": "slides",
    "language": "en-US",
    "title": "Natural language processing for cultural heritage texts. Session 1",
    "type": "manuscript"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:9726294",
    "issued": {
      "date-parts": [
        [
          2011,
          8,
          30
        ]
      ]
    },
    "keyword": "slides",
    "language": "en-US",
    "title": "Natural language processing for cultural heritage texts. Session 2",
    "type": "manuscript"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:9730117",
    "issued": {
      "date-parts": [
        [
          2011,
          8,
          31
        ]
      ]
    },
    "keyword": "slides",
    "language": "en-US",
    "title": "Natural language processing for cultural heritage texts. Session 3",
    "type": "manuscript"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:9732731",
    "keyword": "slides",
    "language": "en-US",
    "title": "Natural language processing for cultural heritage texts. Session 4",
    "type": "manuscript"
  },
  {
    "URL": "http://www.cl.uzh.ch/people/team/mxp/template.zip",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:9733157",
    "keyword": "slides",
    "language": "en-US",
    "title": "LaTeX template for fall school papers",
    "type": "manuscript"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:9736345",
    "issued": {
      "date-parts": [
        [
          2011,
          9,
          2
        ]
      ]
    },
    "keyword": "slides",
    "language": "en-US",
    "title": "Natural language processing for cultural heritage texts. Session 5",
    "type": "manuscript"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:9740353",
    "keyword": "slides",
    "language": "en-US",
    "title": "Natural language processing for cultural heritage texts. Session 6",
    "type": "manuscript"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:9744287",
    "issued": {
      "date-parts": [
        [
          2011,
          9,
          6
        ]
      ]
    },
    "keyword": "slides",
    "language": "en-US",
    "title": "Natural language processing for cultural heritage texts. Session 7",
    "type": "manuscript"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:9750460",
    "issued": {
      "date-parts": [
        [
          2011,
          9,
          7
        ]
      ]
    },
    "keyword": "slides",
    "language": "en-US",
    "title": "Natural language processing for cultural heritage texts. Session 8",
    "type": "manuscript"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:9753371",
    "issued": {
      "date-parts": [
        [
          2011,
          9,
          8
        ]
      ]
    },
    "keyword": "slides",
    "language": "en-US",
    "title": "Natural language processing for cultural heritage texts. Session 9",
    "type": "manuscript"
  },
  {
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "citeulike:9755316",
    "issued": {
      "date-parts": [
        [
          2011,
          9,
          9
        ]
      ]
    },
    "keyword": "slides",
    "language": "en-US",
    "title": "Natural language processing for cultural heritage texts. Session 10",
    "type": "manuscript"
  },
  {
    "URL": "http://aclweb.org/anthology/W10-04",
    "editor": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Dale",
        "given": "Robert"
      }
    ],
    "id": "clw2010",
    "issued": {
      "date-parts": [
        [
          2010,
          6
        ]
      ]
    },
    "keyword": "authoring, interactive_editing, mxp",
    "language": "en-US",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Los Angeles, CA, USA",
    "title": "Proceedings of the NAACL HLT 2010 workshop on computational linguistics and writing: Writing processes and authoring aids",
    "title-short": "Proceedings of the NAACL HLT 2010 workshop on computational linguistics and writing",
    "type": "book"
  },
  {
    "URL": "https://aclanthology.info/pdf/W/W12/W12-0300.pdf",
    "editor": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Dale",
        "given": "Robert"
      }
    ],
    "id": "clw2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "authoring, mxp, writing_research",
    "language": "en-US",
    "publisher": "ACL",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Proceedings of the second workshop on computational linguistics and writing (CL&w 2012): Linguistic and cognitive aspects of document creation and document engineering",
    "title-short": "Proceedings of the second workshop on computational linguistics and writing (CL&w 2012)",
    "type": "book"
  },
  {
    "URL": "http://purl.org/utwente/41410",
    "abstract": "The research project Flexibility Support for a Changing University focused on the problem of how to identify underlying dimensions for change in higher education and how to help instructors via their use of technology for teaching and learning to respond to the change",
    "author": [
      {
        "dropping-particle": "de",
        "family": "Boer",
        "given": "Willem"
      }
    ],
    "genre": "PhD thesis",
    "id": "deBoer2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "keyword": "e-learning",
    "language": "en-US",
    "publisher": "University of Twente",
    "publisher-place": "Enschede, The Netherlands",
    "title": "Flexibility support for a changing university",
    "type": "thesis"
  },
  {
    "URL": "http://eprints.eemcs.utwente.nl/7266/",
    "abstract": "Historical and heritage collections consist for a considerable part of text and may incorporate diverse text types such as journals, archival documents, and catalogue descriptions. Because of the historical distance, access to this content is not straightforward. Historical variants of text are often more complex to identify and retrieve than modern variants. This is due to the less standardized spelling, the effect of on-going language change and different word (de)compounding principles. Moreover, more words are ambiguous because one or more meaning shifts may have occurred. Common full-text search tools can only be applied successfully by users who are able to formulate queries with (a) knowledge of historical language and (b) insight in the relevant time span from which the words have evolved. This paper explores techniques which may compensate for these linguistic obstacles: linking of contemporary search terms to their historical equivalents and ’dating’ of texts. We envisage to restore the diachronic relationship between terms which may be obscured by language evolution and usage, by applying statistical language models. These models may support the automatic detection of semantic similarities between words and word ambiguities, and they also allow to classify a text according to the time span from which it originates. This approach involves building temporal profiles of words as longitudinal sections in a reference corpus and temporal language models as cross sections.",
    "author": [
      {
        "dropping-particle": "de",
        "family": "Jong",
        "given": "Franciska"
      },
      {
        "family": "Rode",
        "given": "Henning"
      },
      {
        "family": "Hiemstra",
        "given": "Djoerd"
      }
    ],
    "container-title": "Humanities, computers and cultural heritage: Proceedings of the XVI<sup>th</sup> international conference of the association for history and computing (AHC 2005)",
    "id": "deJong2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "cultural_heritage, ir",
    "language": "en-US",
    "page": "161-168",
    "publisher": "Royal Netherlands Academy of Arts and Sciences",
    "title": "Temporal language models for the disclosure of historical text",
    "type": "paper-conference"
  },
  {
    "URL": "http://www.hd.uib.no/AcoHum/book/",
    "abstract": "In a three-year cooperation, leading experts from over one hundred universities in Europe have reflected on what is happening with humanities in the information age. Our private and professional lives have been thoroughly invaded by computer creations. The text you read now, the music you listen to on the radio, the words you speak in your mobile phone, have all at some point been represented and transmitted as digital signals consisting of bits - electronic ones and zeros. How is the information age affecting humanities scholars? How are students and staff coping with digital language, music, and culture? Are the humanities adopting the new technologies passively, or are they actively shaping new forms of computing? Such questions were taken up by a SOCRATES/ERASMUS thematic network project on Advanced Computing in the Humanities (ACO*HUM), which started in September 1996 and concluded its third year in August 1999. The project has proceeded through meetings, conferences, surveys, and other communication. This book presents an overview of the network partners’ analyses, proposals and recommendations.",
    "editor": [
      {
        "dropping-particle": "de",
        "family": "Smedt",
        "given": "Koenraad"
      },
      {
        "family": "Gardiner",
        "given": "Hazel"
      },
      {
        "family": "Ore",
        "given": "Espen"
      },
      {
        "family": "Orlandi",
        "given": "Tito"
      },
      {
        "family": "Short",
        "given": "Harold"
      },
      {
        "family": "Souillot",
        "given": "Jacques"
      },
      {
        "family": "Vaughan",
        "given": "William"
      }
    ],
    "id": "deSmedt_etal1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "publisher": "University of Bergen",
    "publisher-place": "Bergen",
    "title": "Computing in humanities education: A european perspective",
    "title-short": "Computing in humanities education",
    "type": "book"
  },
  {
    "author": [
      {
        "dropping-particle": "de",
        "family": "Solla Price",
        "given": "Derek J."
      }
    ],
    "chapter-number": "1",
    "container-title": "Computers in humanistic research: Readings and perspectives",
    "editor": [
      {
        "family": "Bowles",
        "given": "Edmund A."
      }
    ],
    "id": "deSollaPrice1967",
    "issued": {
      "date-parts": [
        [
          1967
        ]
      ]
    },
    "language": "en-US",
    "page": "3-7",
    "publisher": "Prentice-Hall",
    "publisher-place": "Englewood Cliffs, NJ, USA",
    "title": "Gods in black boxes",
    "type": "chapter"
  },
  {
    "URL": "http://ceur-ws.org/Vol-930/p2.pdf",
    "abstract": "To enable better representations of biomedical argumentation over collections of research papers, we propose a model and a lightweight ontology to represent interpersonal, discourse-based, data-driven reasoning. This model is applied to a collection of scientific documents, to show how it can be applied in practice. We present three biomedical applications for this work, and suggest connections with other, existing, ontologies and reasoning tools. Specifically, this model offers a lightweight way to connect nanopublication-like formal representations to scientific papers written in natural language.",
    "author": [
      {
        "dropping-particle": "de",
        "family": "Waard",
        "given": "Anita"
      },
      {
        "family": "Schneider",
        "given": "Jodi"
      }
    ],
    "container-title": "Proceedings of the joint workshop on semantic technologies applied to biomedical informatics and individualized medicine (SATBI+SWIM 2012)",
    "editor": [
      {
        "family": "Rodríguez González",
        "given": "Alejandro"
      },
      {
        "family": "Pathak",
        "given": "Jyotishman"
      },
      {
        "family": "Wilkinson",
        "given": "Mark"
      },
      {
        "family": "Shah",
        "given": "Nigam"
      },
      {
        "family": "Stevens",
        "given": "Robert"
      },
      {
        "family": "Boyce",
        "given": "Richard"
      },
      {
        "family": "Garcia-Crespo",
        "given": "Angel"
      }
    ],
    "id": "deWaard2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "nanopublications, rdf, semantic_web, uncertainty",
    "language": "en-US",
    "page": "8-15",
    "title": "Formalising uncertainty: An ontology of reasoning, certainty and attribution (ORCA)",
    "title-short": "Formalising uncertainty",
    "type": "paper-conference"
  },
  {
    "URL": "http://handle.dtic.mil/100.2/ADA315858",
    "abstract": "Surveys 28 expert technical editors regarding their views on editing and how computer-based tools could aid their work. Finds that the editing environment, the process of editing, the skills involved, and the errors found all influenced the types of tools need. Names tools the editors used and suggests others that should be developed.",
    "author": [
      {
        "family": "Duffy",
        "given": "Thomas M."
      },
      {
        "family": "Robinson",
        "given": "Carol A."
      }
    ],
    "id": "duffy1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "keyword": "document_management, interactive_editing",
    "language": "en-US",
    "publisher": "Navy Personnel Research and Development Center",
    "publisher-place": "San Diego, CA, USA",
    "title": "Designing tools to aid technical editors: A needs analysis",
    "title-short": "Designing tools to aid technical editors",
    "type": "report"
  },
  {
    "URL": "http://www.janus-projekte.de/exam/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "id": "eduploneexam",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "title": "Eduplone eXam",
    "type": ""
  },
  {
    "URL": "http://www.zope.org/Members/J.A.R.Williams/exam",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "id": "exam",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "title": "Exam product for Zope",
    "type": ""
  },
  {
    "author": [
      {
        "literal": "IEEE"
      }
    ],
    "id": "ieee-lom",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "title": "1484.12.1-2002 IEEE Standard for Learning Object Metadata",
    "type": ""
  },
  {
    "URL": "http://www.imsglobal.org/cc/",
    "accessed": {
      "date-parts": [
        [
          2009,
          8,
          6
        ]
      ]
    },
    "id": "imscc1.0",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "note": "Version 1.0 Final Specification",
    "publisher": "IMS Global Learning Consortium",
    "title": "IMS Common Cartridge Profile",
    "type": "book"
  },
  {
    "URL": "http://imsglobal.org/content/packaging/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "id": "imscp1.1.4",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "note": "Version 1.1.4",
    "publisher": "IMS Global Learning Consortium",
    "title": "IMS Content Packaging Specification",
    "type": "book"
  },
  {
    "URL": "http://imsglobal.org/question/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "id": "imsqti2.0",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "note": "Version 2.0 Final Specification",
    "publisher": "IMS Global Learning Consortium",
    "title": "IMS Question & Test Interoperability Specification",
    "type": "book"
  },
  {
    "URL": "http://www.imsglobal.org/vdex/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "id": "imsvdex1.0",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "note": "Version 1.0 Final Specification",
    "publisher": "IMS Global Learning Consortium",
    "title": "IMS Vocabulary Definition Exchange",
    "type": "book"
  },
  {
    "URL": "http://www.jisc.ac.uk/assessment",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Busuttil-Reynaud",
        "given": "Gavin"
      },
      {
        "family": "Winkley",
        "given": "John"
      }
    ],
    "genre": "Version 1.1",
    "id": "jisc-gloss",
    "issued": {
      "date-parts": [
        [
          2006,
          1
        ]
      ]
    },
    "publisher": "Joint Information Systems Committee (JISC) and Qualifications and Curriculum Authority (QCA)",
    "title": "e-Assessment Glossary (Extended)",
    "type": "report"
  },
  {
    "author": [
      {
        "family": "Jurish",
        "given": "Bryan"
      }
    ],
    "collection-title": "Studia grammatica",
    "container-title": "Language and logos: Studies in theoretical and computational linguistics",
    "editor": [
      {
        "family": "Hanneforth",
        "given": "Thomas"
      },
      {
        "family": "Fanselow",
        "given": "Gisbert"
      }
    ],
    "id": "jurish2010kbest",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "keyword": "cultural_heritage, german, spelling_normalization",
    "language": "en-US",
    "page": "313-327",
    "publisher": "Akademie Verlag",
    "publisher-place": "Berlin",
    "title": "Efficient online <i>k</i>-best lookup in weighted finite-state cascades",
    "type": "chapter",
    "volume": "72"
  },
  {
    "URL": "http://lawtec.net/projects/ltonlinetest/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "id": "ltonlinetest",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "title": "LTOnlineTest",
    "type": ""
  },
  {
    "DOI": "10.1109/IMCSIT.2009.5352721",
    "ISBN": "978-83-60810-22-4",
    "URL": "http://www.proceedings2009.imcsit.org/pliks/101.pdf",
    "abstract": "In this paper we compare the state of the art of language awareness in source code editors and word processors. Language awareness refers to functions operating on the elements and structures of a formal or natural language. Language-aware functions allow users to work with meaningful units, increasing efficiency and reducing errors. While all modern source code editors provide programmers with language-aware functions, similar functions for natural-language editing are almost nonexistent. Writers have to manipulate characters, which makes editing and revising challenging and results in typical errors. We describe the LingURed project, in which we implement language aware editing functions for German with the goal of supporting experienced writers. Our approach is based on the combination of standard editor functionality and shallow localized natural language processing. Prototypical functions demonstrate the feasibility of the approach. Based on our preliminary experience we discuss requirements for NLP components suitable for use in interactive editing environments.",
    "author": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Proceedings of the international multiconference on computer science and information technology",
    "id": "mahlow2009e",
    "issued": {
      "date-parts": [
        [
          2009,
          10
        ]
      ]
    },
    "keyword": "emacs, interactive_editing, mxp, nlp",
    "language": "en-US",
    "page": "243-250",
    "publisher": "Polish Information Processing Society",
    "title": "LingURed: Language-aware editing functions based on NLP resources",
    "title-short": "LingURed",
    "type": "paper-conference",
    "volume": "4"
  },
  {
    "abstract": "The Bologna Process requires, besides other changes, more frequent assessment of students, both during and after modules. As e-learning and e-teaching scenarios already play important roles in many curricula, instructors are now starting to consider e-assessment as well. To enable automated evaluation, selected-response items are typically used in e-assessment. However, current e-assessment platforms offer only a limited and rather arbitrary selection of item types. This means that the decision on which item types to use in a test is often based on purely technical issues instead of pedagogical considerations. In this paper, we argue that both implementers and users of e-assessment platforms should abstract from current implementations and base the selection of item types for e-assessment on a sound typology of test items. This would allow instructors to choose the item types best suited for a test and it would allow implementers to generalize the test facilities of their systems, reducing maintenance and development costs. As an example, we outline Rütter’s 1973 typology and discuss selected issues from the point of users and implementers of e-assessment.",
    "author": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Fenske",
        "given": "Wolfram"
      }
    ],
    "container-title": "Proceedings of the IADIS international conference e-learning 2010, freiburg, germany, july 26–29, 2010",
    "editor": [
      {
        "family": "Nunes",
        "given": "Miguel B."
      },
      {
        "family": "McPherson",
        "given": "Maggie"
      }
    ],
    "id": "mahlow2010b",
    "issued": {
      "date-parts": [
        [
          2010,
          7
        ]
      ]
    },
    "keyword": "assessment, e-learning, mxp",
    "language": "en-US",
    "page": "47-51",
    "publisher": "IADIS Press",
    "publisher-place": "Lisbon, Portugal",
    "title": "A solid foundation: Why e-assessment should be based on a systematic typology of test items",
    "title-short": "A solid foundation",
    "type": "paper-conference",
    "volume": "2"
  },
  {
    "DOI": "10.1007/978-3-642-40486-3",
    "ISBN": "978-3-642-40485-6",
    "abstract": "This book constitutes the refereed proceedings of the Third International Workshop on Systems and Frameworks for Computational Morphology, SFCM 2013, held in Berlin, in September 2013. The 7 full papers were carefully reviewed and selected from 15 submissions and are complemented with an invited talk. The papers discuss recent advances in the field of computational morphology.",
    "collection-title": "Communications in computer and information science",
    "editor": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "mahlow2013c",
    "issued": {
      "date-parts": [
        [
          2013,
          8,
          28
        ]
      ]
    },
    "keyword": "morphology, mxp, nlp, sfcm",
    "language": "en-US",
    "publisher": "Paperback; Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Systems and frameworks for computational morphology: Third international workshop, SFCM 2013, berlin, Germany, September 6, 2013, proceedings",
    "title-short": "Systems and frameworks for computational morphology",
    "type": "book",
    "volume": "380"
  },
  {
    "abstract": "Writing is a complex task and several computer systems have been developed in order to support writing. Most of these systems, however, are mainly designed with the purpose of supporting the processes of planning, organizing and connecting ideas. In general, these systems help writers to formulate external visual representations of their ideas and connections of the main topics that should be addressed in the paper, sequence of the sections, etc. With the advent of the world wide web, writing and finding information for the written text has become increasingly intertwined. Consequently, it is necessary to develop systems able to support the task of finding relevant information during writing, without interfering with the writing process proper. In this paper we present the Proactive Recommender System: ‘A propos. This system is being developed in order to support writers in the difficult task of finding appropriate relevant information during writing. We raise the question whether the tendency to interleave (re)search and writing implies a need for developing more comprehensive models of the cognitive processes involved in writing scientific and policy papers.",
    "author": [
      {
        "family": "Puerta Melguizo",
        "given": "Mari Carmen"
      },
      {
        "family": "Muñoz Ramos",
        "given": "Olga"
      },
      {
        "family": "Boves",
        "given": "Lou"
      },
      {
        "family": "Bogers",
        "given": "Toine"
      },
      {
        "dropping-particle": "van den",
        "family": "Bosch",
        "given": "Antal"
      }
    ],
    "container-title": "LREC 2008 workshop on NLP resources, algorithms and tools for authoring aids",
    "editor": [
      {
        "family": "Dale",
        "given": "Robert"
      },
      {
        "family": "Max",
        "given": "Aurélien"
      },
      {
        "family": "Zock",
        "given": "Michael"
      }
    ],
    "id": "melguizo2008a",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "interactive_editing, proactive, authoring, writing_research",
    "page": "21-26",
    "publisher": "ELRA",
    "publisher-place": "Paris",
    "title": "A personalized recommender system for writing in the Internet age",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1109/tai.1994.346488",
    "abstract": "Since Writer’s Workbench (Bell Telephone Laboratories, early 1980’s), software for writing assistance and style checking has evolved over the last decade (1984-94) to become more intelligent and interactive. During this period the author has been involved in the development of several software packages and has monitored the growth and sophistication of software solutions. Today certain characteristics have become standard; yet challenges remain. The author discusses trends and suggests areas where we might expect continued future development. Today writers have a variety of useful but limited style and grammar checkers available for most computer systems. Some even come bundled with the current generation of enhanced word processors and are used by many writers. Another approach to writing assistance is suggested by online interactive group writing systems like MediaLink. Possibilities for combining the best features of current systems exist, but further improvements in the quality of the knowledge offered by automated writing assistants will depend on research advances in other areas of natural language processing. The author examines some of these problem areas and suggests approaches from ongoing NLP research that we can expect the writing assistants and style checkers of the future to include among their resources",
    "author": [
      {
        "family": "Oakman",
        "given": "R. L."
      }
    ],
    "container-title": "Tools with Artificial Intelligence, 1994. Proceedings., Sixth International Conference on",
    "id": "oakman1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "233-234",
    "title": "The evolution of intelligent writing assistants: Trends and future prospects",
    "title-short": "The evolution of intelligent writing assistants",
    "type": "book"
  },
  {
    "abstract": "Progression Analysis (PA) is a computer-based method for research on writing in the workplace. As a multilevel-analysis, PA focuses (1) on the Situation of the writing process, (2) on the movement of writing throughout the growing text, and (3) on consciously applied strategies. In this chapter, the innovative procedures of PA are presented on behalf of a pedagogically motivated case study out of a national research project.",
    "author": [
      {
        "family": "Perrin",
        "given": "Daniel"
      }
    ],
    "collection-title": "Studies in writing",
    "container-title": "Contemporary tools and techniques for studying writing",
    "editor": [
      {
        "family": "Olive",
        "given": "Thierry"
      },
      {
        "family": "Levy",
        "given": "C. Michael"
      }
    ],
    "id": "perrin2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "keystroke-recording, writing_research, revising, writing, writing-process",
    "page": "105-117",
    "publisher": "Kluwer",
    "publisher-place": "Amsterdam, The Netherlands",
    "title": "Progression analysis (PA): Investigating writing strategies in the workplace",
    "title-short": "Progression analysis (PA)",
    "type": "chapter"
  },
  {
    "abstract": "Wir berichten über Entwurf, Implementierung und Einsatz des Systems LlsChecker. LlsChecker ist eine in ein Content-Management-System (CMS) für Lehr- und Lernmaterialien integrierte Komponente zur automatischen Überprüfung studentischer Lösungen für Programmieraufgaben in unterschiedlichen funktionalen Programmiersprachen. Das System ist so generisch organisiert, dass die Ausweitung der Dienste auf weitere Sprachen – zumindest für funktionale Programmiersprachen – allein durch eine XML-basierte Deklaration möglich ist.",
    "author": [
      {
        "family": "Rösner",
        "given": "Dietmar"
      },
      {
        "family": "Amelung",
        "given": "Mario"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "collection-title": "Lecture notes in informatics (LNI) - proceedings",
    "container-title": "DeLFI2005: 3. Deutsche e-learning fachtagung informatik der gesellschaft für informatik e.v.",
    "edition": "Volume P-66",
    "editor": [
      {
        "family": "Tavangarian",
        "given": "Djamshid"
      },
      {
        "family": "Haake",
        "given": "Jörg M."
      },
      {
        "family": "Lucke",
        "given": "Ulrike"
      }
    ],
    "id": "roesneramelungmxp2005delfi",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "language": "en-US",
    "page": "307-318",
    "publisher": "GI-Verlag",
    "publisher-place": "Bonn",
    "title": "LlsChecker – ein CAA-System für die Lehre im Bereich Programmiersprachen",
    "type": "paper-conference"
  },
  {
    "abstract": "Formative Tests können für Lehrende und Lernende gleichermaßen nützlich sein. Webbasierte Multiple-Choice-Tests können helfen, den Aufwand für formative Tests zu senken und somit einen breiteren und häufigeren Einsatz zu ermöglichen. Wir stellen ein Modul für das Content-Management-System Plone vor, das es erlaubt, MC-Tests genau wie andere Ressourcen einzusetzen und zu verwalten. Auf dieseWeise können vor allem in Präsenzveranstaltungen, für die üblicherweise keine Lernplattform verwendet wird, Tests eng mit den anderen online verfügbaren Lehr- und Lernmaterialien (z. B. Vorlesungsskripten oder Aufgabenblättern) verknüpft werden. Das Modul erlaubt auch den Import und Export von Aufgaben gemäß IMS QTI; in diesem Zusammenhang diskutieren wir auch unsere Erfahrungen mit dieser Spezifikation.",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Rösner",
        "given": "Dietmar"
      }
    ],
    "collection-title": "Lecture notes in informatics (LNI) - proceedings",
    "container-title": "DeLFI2005: 3. Deutsche e-learning fachtagung informatik der gesellschaft für informatik e.v.",
    "edition": "Volume P-66",
    "editor": [
      {
        "family": "Tavangarian",
        "given": "Djamshid"
      },
      {
        "family": "Haake",
        "given": "Jörg M."
      },
      {
        "family": "Lucke",
        "given": "Ulrike"
      }
    ],
    "id": "roesnermxp2005delfi",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "keyword": "e-learning, mxp",
    "language": "en-US",
    "page": "129-140",
    "publisher": "GI-Verlag",
    "publisher-place": "Bonn",
    "title": "Integration von E-Assessment und Content-Management",
    "type": "paper-conference"
  },
  {
    "abstract": "In this paper, a new probabilistic tagging method is presented which avoids problems that Markov Model based taggers face, when they have to estimate transition probabilities from sparse data. In this tagging method, transition probabilities are estimated using a decision tree. Based on this method, a part-of-speech tagger (called TreeTagger) has been implemented which achieves 96.36 % accuracy on Penn-Treebank data which is better than that of a trigram tagger (96.06 %) on the same data. Keywords: Corpus-based NLP, Statistical NLP, Part-of-Speech Tagging. 1 Introduction Word forms are often ambiguous in their part-of-speech (POS). The English word form store for example can be either a noun, a finite verb or an infinitive. In an utterance, this ambiguity is normally resolved by the context of a word: e.g. in the sentence \"The 1977 PCs could store two pages of data.\", store can only be an infinitive. The predictability of the part-of-speech from the context is used by automatic part-...",
    "author": [
      {
        "family": "Schmid",
        "given": "Helmut"
      }
    ],
    "container-title": "Proceedings of the international conference on new methods in language processing",
    "id": "schmid1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "pos_tagging",
    "language": "en-US",
    "page": "44-49",
    "title": "Probabilistic part-of-speech tagging using decision trees",
    "type": "paper-conference"
  },
  {
    "abstract": "This paper presents a couple of extensions to a basic Markov Model tagger (called TreeTagger) which improve its accuracy when trained on small corpora. The basic tagger was originally developed for English [Schmid, 1994]. The extensions together reduced error rates on a German test corpus by more than a third.",
    "author": [
      {
        "family": "Schmid",
        "given": "Helmut"
      }
    ],
    "container-title": "Proceedings of the ACL SIGDAT-workshop",
    "id": "schmid1995",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "keyword": "german, nlp, tagger, treetagger",
    "page": "47-50",
    "title": "Improvements in part-of-speech tagging with an application to German",
    "type": "paper-conference"
  },
  {
    "URL": "http://adlnet.gov/",
    "accessed": {
      "date-parts": [
        [
          2008,
          10,
          17
        ]
      ]
    },
    "edition": "3<sup>rd</sup>",
    "id": "scorm2004.3",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "publisher": "Advanced Distributed Learning",
    "title": "Sharable Content Object Reference (SCORM) 2004",
    "type": "book"
  },
  {
    "URL": "http://adlnet.gov/",
    "accessed": {
      "date-parts": [
        [
          2009,
          8,
          6
        ]
      ]
    },
    "edition": "4<sup>th</sup>, Version 1.0",
    "id": "scorm2004.4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "publisher": "Advanced Distributed Learning",
    "title": "Sharable Content Object Reference (SCORM) 2004",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-642-23138-4",
    "abstract": "This book constitutes the refereed proceedings of the Second International Workshop on Systems and Frameworks for Computational Morphology, SFCM 2011, held in Zurich, Switzerland in August 2011. The eight revised full papers presented together with one invited paper were carefully reviewed and selected from 13 submissions. The papers address various topics in computational morphology and the relevance of morphology to computational linguistics more broadly.",
    "collection-title": "Communications in computer and information science",
    "editor": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "sfcm2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "keyword": "computational_linguistics, morphology, mxp",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Systems and frameworks for computational morphology: Second international workshop, SFCM 2011, zurich, switzerland, august 26, 2011, proceedings",
    "title-short": "Systems and frameworks for computational morphology",
    "type": "book",
    "volume": "100"
  },
  {
    "DOI": "10.1007/978-3-642-40486-3",
    "abstract": "This book constitutes the refereed proceedings of the Third International Workshop on Systems and Frameworks for Computational Morphology, SFCM 2013, held in Berlin, Germany in September 2013.",
    "collection-title": "Communications in computer and information science",
    "editor": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "sfcm2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "computational_linguistics, morphology, mxp",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Systems and frameworks for computational morphology: Third international workshop, SFCM 2013, berlin, germany, september 6, 2013, proceedings",
    "title-short": "Systems and frameworks for computational morphology",
    "type": "book",
    "volume": "380"
  },
  {
    "DOI": "10.1007/978-3-319-23980-4",
    "ISBN": "978-3-319-23978-1",
    "collection-title": "Communications in computer and information science",
    "editor": [
      {
        "family": "Mahlow",
        "given": "Cerstin"
      },
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "id": "sfcm2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "computational_linguistics, morphology",
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Systems and frameworks for computational morphology: Fourth international workshop, SFCM 2015, stuttgart, germany, september 17–18, 2015, proceedings",
    "title-short": "Systems and frameworks for computational morphology",
    "type": "book",
    "volume": "537"
  },
  {
    "DOI": "10.1145/356589.356591",
    "ISSN": "0360-0300",
    "author": [
      {
        "dropping-particle": "van",
        "family": "Dam",
        "given": "Andries"
      },
      {
        "family": "Rice",
        "given": "David E."
      }
    ],
    "container-title": "ACM Comput. Surv.",
    "id": "vanDam1971",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1971,
          9
        ]
      ]
    },
    "keyword": "interactive_editing",
    "language": "en-US",
    "page": "93-114",
    "publisher": "ACM",
    "title": "On-line text editing: A survey",
    "title-short": "On-line text editing",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "URL": "http://adelheid.ruhosting.nl/download/Adelheid1_TagsetManual.pdf",
    "abstract": "This manual describes the tagging and lemmatization that is used in two resources relating to historical Dutch text: The Corpus van Reenen-Mulder and the Adelheid Tagger-Lemmatizer.",
    "author": [
      {
        "family": "Rem",
        "given": "Margit"
      },
      {
        "dropping-particle": "van",
        "family": "Halteren",
        "given": "Hans"
      }
    ],
    "id": "vanHalteren2012",
    "keyword": "corpus_linguistics, cultural_heritage, dutch, pos_tagging",
    "language": "en-US",
    "publisher": "Radboud University",
    "publisher-place": "Nijmegen, The Netherlands",
    "title": "Tagging and lemmatization manual for the Corpus van Reenen-Mulder and the Adelheid 1.0 tagger-lemmatizer",
    "type": "book"
  },
  {
    "DOI": "10.1002/9781118680605.ch23",
    "author": [
      {
        "dropping-particle": "van",
        "family": "Zundert",
        "given": "Joris J."
      }
    ],
    "chapter-number": "23",
    "collection-title": "A new companion to digital humanities",
    "container-title": "A new companion to digital humanities",
    "editor": [
      {
        "family": "Schreibman",
        "given": "Susan"
      },
      {
        "family": "Siemens",
        "given": "Ray"
      },
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "id": "vanZundert2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "keyword": "hermeneutics, digital_humanities",
    "page": "331-347",
    "publisher": "Wiley",
    "title": "Screwmeneutics and hermenumericals",
    "type": "chapter"
  },
  {
    "DOI": "10.1080/03080188.2017.1296265",
    "ISSN": "0308-0188",
    "abstract": "This paper is concerned with patterns in past human behaviour, what they are, and how this relates to the detection of patterns in data by means of computation. Theorists have not given patterns the attention they deserve. Therefore it is far from clear what patterns are and to what purpose scholars may use them. This paper presents eight propositions on patterns which hold true for patterns found ’by hand’ and patterns found ’by computation’. One such is that a pattern is discernible in behaviour when we subject it to the intentional stance, as the philosopher Daniel Dennett argues. Here behaviour is part of an intentional system. This paper’s argument is that the patterns found ’by computation’ too are part of an intentional system. To substantiate this claim this paper discusses two important examples of detecting computational patterns in the domain of the humanities.",
    "author": [
      {
        "dropping-particle": "van den",
        "family": "Akker",
        "given": "Chiel"
      }
    ],
    "container-title": "Interdisciplinary Science Reviews",
    "id": "vandenAkker2017",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "digital_humanities, formal_models",
    "language": "en-US",
    "page": "74-86",
    "title": "What are patterns in the humanities?",
    "type": "article-journal",
    "volume": "43"
  },
  {
    "DOI": "10.1109/MIS.2009.33",
    "abstract": "Digitization brings about new ways of analyzing data from cultural heritage areas. Automatic error detection, as input to semiautomatic error correction, is one type of analysis that can be found high on the priority list of cultural heritage data managers and researchers. We describe a general approach to cleaning cultural heritage databases. We present four case studies on databases from different cultural heritage institutions, and describe an information system in which we embed our error detector in a larger framework, enabling researchers to access, check, and correct their data more easily than before.",
    "author": [
      {
        "dropping-particle": "van den",
        "family": "Bosch",
        "given": "Antal"
      },
      {
        "dropping-particle": "van",
        "family": "Erp",
        "given": "Marieke"
      },
      {
        "family": "Sporleder",
        "given": "Caroline"
      }
    ],
    "container-title": "IEEE Intelligent Systems",
    "id": "vandenBosch2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "cultural_heritage",
    "language": "en-US",
    "page": "54-63",
    "publisher": "IEEE Computer Society",
    "publisher-place": "Los Alamitos, CA, USA",
    "title": "Making a clean sweep of cultural heritage",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "author": [
      {
        "dropping-particle": "van der",
        "family": "Aalst",
        "given": "Wil"
      },
      {
        "dropping-particle": "van",
        "family": "Hee",
        "given": "Kees"
      }
    ],
    "id": "vanderAalst2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA, USA",
    "title": "Workflow management: Models, methods, and systems",
    "title-short": "Workflow management",
    "type": "book"
  },
  {
    "DOI": "10.1126/science.1160379",
    "ISSN": "1095-9203",
    "PMID": "18703711",
    "abstract": "CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) are widespread security measures on the World Wide Web that prevent automated programs from abusing online services. They do so by asking humans to perform a task that computers cannot yet perform, such as deciphering distorted characters. Our research explored whether such human effort can be channeled into a useful purpose: helping to digitize old printed material by asking users to decipher scanned words from books that computerized optical character recognition failed to recognize. We showed that this method can transcribe text with a word accuracy exceeding 99%, matching the guarantee of professional human transcribers. Our apparatus is deployed in more than 40,000 Web sites and has transcribed over 440 million words.",
    "author": [
      {
        "dropping-particle": "von",
        "family": "Ahn",
        "given": "Luis"
      },
      {
        "family": "Maurer",
        "given": "Benjamin"
      },
      {
        "family": "McMillen",
        "given": "Colin"
      },
      {
        "family": "Abraham",
        "given": "David"
      },
      {
        "family": "Blum",
        "given": "Manuel"
      }
    ],
    "container-title": "Science",
    "id": "vonAhn2008",
    "issue": "5895",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "ocr",
    "language": "en-US",
    "page": "1465-1468",
    "publisher": "American Association for the Advancement of Science",
    "title": "reCAPTCHA: Human-based character recognition via web security measures",
    "title-short": "reCAPTCHA",
    "type": "article-journal",
    "volume": "321"
  },
  {
    "DOI": "10.1145/253671.253744",
    "ISSN": "0001-0782",
    "author": [
      {
        "family": "Wulf",
        "given": "William A."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Wulf1997",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "language": "en-US",
    "page": "109--111",
    "title": "Look in the spaces for tomorrow’s innovations",
    "type": "article-journal",
    "volume": "40"
  },
  {
    "DOI": "10.1080/0958822940070208",
    "ISSN": "1744-3210",
    "author": [
      {
        "family": "Butler",
        "given": "Terry"
      }
    ],
    "container-title": "Computer Assisted Language Learning",
    "id": "Butler1994",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "language": "en-US",
    "page": "184-189",
    "title": "Report on the Joint Conference of the Association for Computers in the Humanities and the Association for Literary and Linguistic Computing: 15–19 june 1993, georgetown university, washington DC",
    "title-short": "Report on the Joint Conference of the Association for Computers in the Humanities and the Association for Literary and Linguistic Computing",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "author": [
      {
        "family": "Mullen",
        "given": "Lincoln"
      }
    ],
    "chapter-number": "12",
    "container-title": "Defining digital humanities",
    "editor": [
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Vanhoutte",
        "given": "Edward"
      }
    ],
    "id": "Mullen2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "237-238",
    "publisher": "Ashgate",
    "publisher-place": "Farnham",
    "title": "Digital humanities is a spectrum, or ",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Courtieux",
        "given": "Gérard"
      }
    ],
    "container-title": "Représentation des connaissances et raisonnement dans les sciences de l’homme",
    "editor": [
      {
        "family": "Borillo",
        "given": "Mario"
      }
    ],
    "event": "Colloque de Saint Maximin",
    "event-date": {
      "date-parts": [
        [
          1979,
          9,
          17
        ],
        [
          1979,
          9,
          19
        ]
      ]
    },
    "event-place": "Saint Maximin",
    "id": "Courtieux1979",
    "issued": {
      "date-parts": [
        [
          1979
        ]
      ]
    },
    "language": "fr-FR",
    "page": "571-578",
    "publisher": "IRIA-LISH; IRIA",
    "title": "Informatique et idéologies",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1080/03080188.2016.1165455",
    "author": [
      {
        "family": "Xanthos",
        "given": "Aris"
      }
    ],
    "container-title": "Interdisciplinary Science Reviews",
    "id": "Xanthos2015",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "en-US",
    "page": "393-408",
    "publisher": "Taylor & Francis",
    "title": "Software as theory: A case study in the domain of text analysis",
    "title-short": "Software as theory",
    "type": "article-journal",
    "volume": "40"
  },
  {
    "DOI": "10.3898/newf:98.06.2019",
    "ISSN": "0950-2378",
    "author": [
      {
        "family": "Fazi",
        "given": "M. Beatrice"
      }
    ],
    "container-title": "New Formations",
    "id": "Fazi2019",
    "issue": "98",
    "issued": {
      "date-parts": [
        [
          2019,
          7
        ]
      ]
    },
    "language": "en-US",
    "page": "85--100",
    "publisher": "Lawrence and Wishart",
    "title": "Distraction machines? Augmentation, automation and attention in a computational age",
    "type": "article-journal"
  },
  {
    "ISBN": "9780262536561",
    "author": [
      {
        "family": "Denning",
        "given": "Peter J."
      },
      {
        "family": "Tedre",
        "given": "Matti"
      }
    ],
    "id": "Denning2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA",
    "title": "Computational thinking",
    "type": "book"
  },
  {
    "ISBN": "0465046274",
    "author": [
      {
        "family": "Papert",
        "given": "Seymour"
      }
    ],
    "id": "Papert1980",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Basic Books",
    "publisher-place": "New York, NY",
    "title": "Mindstorms: Children, computers, and powerful ideas",
    "title-short": "Mindstorms",
    "type": "book"
  },
  {
    "ISBN": "978-3-03778-609-3",
    "author": [
      {
        "family": "López-Pérez",
        "given": "Daniel"
      }
    ],
    "id": "Lopez-Perez2020",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Lars Müller",
    "title": "R. Buckminster Fuller—pattern thinking",
    "type": "book"
  },
  {
    "ISBN": "978-0-262-69191-8",
    "author": [
      {
        "family": "Simon",
        "given": "Herbert A."
      }
    ],
    "edition": "3",
    "id": "Simon1969-1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA",
    "title": "The sciences of the artificial",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Hess",
        "given": "Michael"
      }
    ],
    "genre": "Dissertation",
    "id": "Hess1977",
    "issued": {
      "date-parts": [
        [
          1977
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Universität Zürich",
    "title": "Kybernetik, Philosophie, dialektischer Widerspruch",
    "type": "thesis"
  },
  {
    "author": [
      {
        "family": "Hess",
        "given": "Michael"
      }
    ],
    "genre": "Habilitationsschrift",
    "id": "Hess1989",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Universität Zürich",
    "title": "Reference and quantification in discourse",
    "type": "thesis"
  },
  {
    "DOI": "10.1007/978-94-010-1123-5",
    "author": [
      {
        "family": "Topolski",
        "given": "Jerzy"
      }
    ],
    "collection-number": "88",
    "collection-title": "Synthese library",
    "id": "Topolski1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1973
        ]
      ]
    },
    "original-title": "Metodologia historii",
    "publisher": "Reidel",
    "title": "Methodology of history",
    "translator": [
      {
        "family": "Wojtasiewicz",
        "given": "Olgierd"
      }
    ],
    "type": "book"
  },
  {
    "DOI": "10.1017/cbo9780511621222",
    "ISBN": "9780511621222",
    "author": [
      {
        "family": "Hawthorn",
        "given": "Geoffrey"
      }
    ],
    "id": "Hawthorn1991",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Cambridge University Press",
    "title": "Plausible worlds: Possibility and understanding in history and the social sciences",
    "title-short": "Plausible worlds",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/9426.001.0001",
    "author": [
      {
        "family": "Ceruzzi",
        "given": "Paul E."
      }
    ],
    "id": "Ceruzzi2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Computing: A concise history",
    "title-short": "Computing",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/8009.001.0001",
    "editor": [
      {
        "family": "Copeland",
        "given": "B. Jack"
      },
      {
        "family": "Posy",
        "given": "Carl J."
      },
      {
        "family": "Shagir",
        "given": "Oron"
      }
    ],
    "id": "Copeland2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Computability: Turing, gödel, church, and beyond",
    "title-short": "Computability",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/9809.001.0001",
    "author": [
      {
        "family": "Denning",
        "given": "Peter J."
      },
      {
        "family": "Martell",
        "given": "Craig H."
      }
    ],
    "id": "Denning2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Great principles of computing",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/2034.001.0001",
    "editor": [
      {
        "family": "Dertouzos",
        "given": "Michael L."
      },
      {
        "family": "Moses",
        "given": "Joel"
      }
    ],
    "id": "Dertouzos1980",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "The computer age: A twenty-year view",
    "title-short": "The computer age",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/10786.001.0001",
    "author": [
      {
        "family": "Erwig",
        "given": "Martin"
      }
    ],
    "id": "Erwig2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Once upon an algorithm",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/5803.001.0001",
    "author": [
      {
        "family": "Fagin",
        "given": "Ronald"
      },
      {
        "family": "Halpern",
        "given": "Joseph Y."
      },
      {
        "family": "Moses",
        "given": "Yoram"
      },
      {
        "family": "Vardi",
        "given": "Moshe"
      }
    ],
    "id": "Fagin2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Reasoning about knowledge",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/12044.001.0001",
    "editor": [
      {
        "family": "Forlano",
        "given": "Laura"
      },
      {
        "family": "Wright Steenson",
        "given": "Molly"
      },
      {
        "family": "Ananny",
        "given": "Mike"
      }
    ],
    "id": "Forlano2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Bauhaus futures",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/9780262033985.001.0001",
    "abstract": "This book explores the conception, design, construction, use, and afterlife of ENIAC, the first general purpose digital electronic computer. ENIAC was created and tested at the University of Pennsylvania from 1943 to 1946, then used at the Ballistic Research Laboratory in Aberdeen, Maryland until 1955. Unlike most discussion of early computers, this book focuses on ways in which ENIAC was used, and the relationship of its design to computational practice, particularly its use between 1948 and 1950 to conduct the first computerized Monte Caro simulations for Los Alamos. ENIAC’s first team of operators were all women, and the book probes their contribution to the machine’s achievements and the development of computer programming practice. ENIAC’s users changed its hardware and transformed its configuration over time, so that it eventually became the first computer to execute a modern program, defined by the authors as one following the “modern code paradigm” introduced in John von Neumann’s seminal 1945 “First Draft of a Report on the EDVAC.” They draw on new archival evidence to document the development of this idea and its relationship to work on ENIAC. They also use ENIAC to probe the construction of historical memory, looking at ways in which a bitter succession of legal battles around patent rights shaped later perceptions.",
    "author": [
      {
        "family": "Haigh",
        "given": "Thomas"
      },
      {
        "family": "Priestly",
        "given": "Mark"
      },
      {
        "family": "Rope",
        "given": "Crispin"
      }
    ],
    "id": "Haigh2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "ENIAC in action: Making and remaking the modern computer",
    "title-short": "ENIAC in action",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/10909.001.0001",
    "author": [
      {
        "family": "Levesque",
        "given": "Hector J."
      }
    ],
    "id": "Levesque2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Common sense, the Turing test, and the quest for real AI",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/9022.001.0001",
    "author": [
      {
        "family": "Maher",
        "given": "Jimmy"
      }
    ],
    "collection-title": "Platform studies",
    "id": "Maher2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "The future was here: The Commodore Amiga",
    "title-short": "The future was here",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/12252.001.0001",
    "author": [
      {
        "family": "Mindrup",
        "given": "Matthew"
      }
    ],
    "id": "Mindrup2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "The architectural model: Histories of the miniature and the prototype, the exemplar and the muse",
    "title-short": "The architectural model",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/11301.001.0001",
    "author": [
      {
        "family": "Minsky",
        "given": "Marvin"
      },
      {
        "family": "Papert",
        "given": "Seymour A."
      }
    ],
    "edition": "Reissue of the 1988 Expanded Edition with a new foreword by Léon Bottou",
    "id": "Minsky2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "publisher": "MIT Press",
    "title": "Perceptrons: An introduction to computational geometry",
    "title-short": "Perceptrons",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Tedre",
        "given": "Matti"
      }
    ],
    "id": "Tedre2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "en-US",
    "publisher": "CRC Press",
    "title": "The science of computing: Shaping a discipline",
    "title-short": "The science of computing",
    "type": "book"
  },
  {
    "DOI": "10.31235/osf.io/d2kb6",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      }
    ],
    "container-title": "Wozu digitale geisteswissenschaften? Innovationen, revisionen, binnenkonflikte",
    "editor": [
      {
        "family": "Huber",
        "given": "Martin"
      },
      {
        "family": "Krämer",
        "given": "Sybille"
      },
      {
        "family": "Pias",
        "given": "Claus"
      }
    ],
    "event": "Wozu digitale geisteswissenschaften? Innovationen, revisionen, binnenkonflikte",
    "event-date": {
      "date-parts": [
        [
          2019,
          11,
          20
        ],
        [
          2019,
          11,
          22
        ]
      ]
    },
    "event-place": "Lüneburg",
    "id": "Piotrowski2020b",
    "language": "en-US",
    "status": "submitted",
    "title": "Ain’t no way around it: Why we need to be clear about what we mean by “digital humanities”",
    "title-short": "Ain’t no way around it",
    "type": "paper-conference"
  },
  {
    "DOI": "10.6092/UNIBO/AMSACTA/6316",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Neuwirth",
        "given": "Markus"
      }
    ],
    "container-title": "Atti del IX convegno annuale AIUCD: La svolta inevitabile: Sfide e prospettive per l’informatica umanistica",
    "editor": [
      {
        "family": "Marras",
        "given": "Cristina"
      },
      {
        "family": "Passarotti",
        "given": "Marco"
      },
      {
        "family": "Franzini",
        "given": "Greta"
      },
      {
        "family": "Litta",
        "given": "Eleonora"
      }
    ],
    "event-date": {
      "date-parts": [
        [
          2020,
          1,
          15
        ],
        [
          2020,
          1,
          17
        ]
      ]
    },
    "event-place": "Milan",
    "id": "Piotrowski2020a",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "204-209",
    "publisher": "Associazione per l’Informatica Umanistica e la Cultura Digitale (AIUCD)",
    "title": "Prospects for computational hermeneutics",
    "type": "paper-conference"
  },
  {
    "ISBN": "3-89785-032-X",
    "URL": "http://computerphilologie.digital-humanities.de/jg02/orlandi.html",
    "accessed": {
      "date-parts": [
        [
          2020,
          4,
          28
        ]
      ]
    },
    "author": [
      {
        "family": "Orlandi",
        "given": "Tito"
      }
    ],
    "collection-number": "4",
    "container-title": "Jahrbuch für computerphilologie",
    "editor": [
      {
        "family": "Braungart",
        "given": "Georg"
      },
      {
        "family": "Eibl",
        "given": "Karl"
      },
      {
        "family": "Jannidis",
        "given": "Fotis"
      }
    ],
    "id": "Orlandi2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "language": "en-US",
    "page": "51-58",
    "publisher": "mentis",
    "title": "Is humanities computing a discipline?",
    "type": "chapter"
  },
  {
    "URL": "https://iath.virginia.edu/hcs/rockwell.html",
    "accessed": {
      "date-parts": [
        [
          2020,
          4,
          28
        ]
      ]
    },
    "author": [
      {
        "family": "Rockwell",
        "given": "Geoffrey"
      }
    ],
    "id": "Rockwell1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "title": "Is humanities computing an academic discipline?",
    "type": "webpage"
  },
  {
    "DOI": "10.7551/mitpress/8200.001.0001",
    "ISBN": "9780262254847",
    "abstract": "How the simulation and visualization technologies so pervasive in science, engineering, and design have changed our way of seeing the world. Over the past twenty years, the technologies of simulation and visualization have changed our ways of looking at the world. In Simulation and Its Discontents, Sherry Turkle examines the now dominant medium of our working lives and finds that simulation has become its own sensibility. We hear it in Turkle’s description of architecture students who no longer design with a pencil, of science and engineering students who admit that computer models seem more “real” than experiments in physical laboratories. Echoing architect Louis Kahn’s famous question, “What does a brick want?”, Turkle asks, “What does simulation want?” Simulations want, even demand, immersion, and the benefits are clear. Architects create buildings unimaginable before virtual design; scientists determine the structure of molecules by manipulating them in virtual space; physicians practice anatomy on digitized humans. But immersed in simulation, we are vulnerable. There are losses as well as gains. Older scientists describe a younger generation as “drunk with code.” Young scientists, engineers, and designers, full citizens of the virtual, scramble to capture their mentors’ tacit knowledge of buildings and bodies. From both sides of a generational divide, there is anxiety that in simulation, something important is slipping away. Turkle’s examination of simulation over the past twenty years is followed by four in-depth investigations of contemporary simulation culture: space exploration, oceanography, architecture, and biology.",
    "author": [
      {
        "family": "Turkle",
        "given": "Sherry"
      }
    ],
    "id": "Turkle2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "keyword": "models_in_general, philosophy_of_science, formal_models",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Simulation and its discontents",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/3594.001.0001",
    "ISBN": "9780262270533",
    "author": [
      {
        "family": "Cohen",
        "given": "I. Bernard"
      }
    ],
    "id": "Cohen1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "keyword": "computing_history",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Howard aiken: Portrait of a computer pioneer",
    "title-short": "Howard aiken",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/9780262062749.001.0001",
    "ISBN": "9780262273343",
    "editor": [
      {
        "family": "Fuller",
        "given": "Matthew"
      }
    ],
    "id": "Fuller2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "software_studies",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Software studies: A lexicon",
    "title-short": "Software studies",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/3137.001.0001",
    "ISBN": "9780262273688",
    "author": [
      {
        "family": "Gerovitch",
        "given": "Slava"
      }
    ],
    "id": "Gerovitch2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "keyword": "cybernetics, computing_history",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "From newspeak to cyberspeak: A history of soviet cybernetics",
    "title-short": "From newspeak to cyberspeak",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/1020.001.0001",
    "ISBN": "9780262280402",
    "abstract": "A Small Matter of Programming asks why it has been so difficult for end users to command programming power and explores the problems of end user-driven application development that must be solved to afford end users greater computational power. Drawing on empirical research on existing end user systems, A Small Matter of Programming analyzes cognitive, social, and technical issues of end user programming. In particular, it examines the importance of task-specific programming languages, visual application frameworks, and collaborative work practices for end user computing, with the goal of helping designers and programmers understand and better satisfy the needs of end users who want the capability to create, customize, and extend their applications software. The ideas in the book are based on the author’s research on two successful end user programming systems - spreadsheets and CAD systems - as well as other empirical research. Nardi concentrates on broad issues in end user programming, especially end users’ strengths and problems, introducing tools and techniques as they are related to higher-level user issues. Bonnie A. Nardi is a Member of the Technical Staff at Hewlett Packard Laboratories.",
    "author": [
      {
        "family": "Nardi",
        "given": "Bonnie A."
      }
    ],
    "id": "Nardi1993",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "keyword": "programming",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "A small matter of programming: Perspectives on end user computing",
    "title-short": "A small matter of programming",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/5891.001.0001",
    "ISBN": "9780262281973",
    "abstract": "Hilary Putnam, who may have been the first philosopher to advance the notion that the computer is an apt model for the mind, takes a radically new view of his own theory of functionalism in this book. Putnam argues that in fact the computational analogy cannot answer the important questions about the nature of such mental states as belief, reasoning, rationality, and knowledge that lie at the heart of the philosophy of mind.",
    "author": [
      {
        "family": "Putnam",
        "given": "Hilary"
      }
    ],
    "id": "Putnam1991",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "keyword": "models_in_general, philosophy_of_science",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Representation and reality",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/1968.001.0001",
    "ISBN": "9780262284837",
    "abstract": "By applying research in artificial intelligence to problems in the philosophy of science, Paul Thagard develops an exciting new approach to the study of scientific reasoning. This approach uses computational ideas to shed light on how scientific theories are discovered, evaluated, and used in explanations. Thagard describes a detailed computational model of problem solving and discovery that provides a conceptually rich yet rigorous alternative to accounts of scientific knowledge based on formal logic, and he uses it to illuminate such topics as the nature of concepts, hypothesis formation, analogy, and theory justification.",
    "author": [
      {
        "family": "Thagard",
        "given": "Paul"
      }
    ],
    "id": "Thagard1988",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "keyword": "formal_models, philosophy_of_science",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Computational philosophy of science",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/4705.001.0001",
    "ISBN": "9780262285827",
    "abstract": "In this groundbreaking book, Jonathan Waskan challenges cognitive science’s dominant model of mental representation and proposes a novel, well-devised alternative. The traditional view in the cognitive sciences uses a linguistic (propositional) model of mental representation. This logic-based model of cognition informs and constrains both the classical tradition of artificial intelligence and modeling in the connectionist tradition. It falls short, however, when confronted by the frame problem—the lack of a principled way to determine which features of a representation must be updated when new information becomes available. Proposed alternatives, including the imagistic model, have not so far resolved this problem. Waskan proposes instead the Intrinsic Cognitive Models (ICM) hypothesis, which argues that representational states can be conceptualized as the cognitive equivalent of scale models. Waskan argues further that the proposal that humans harbor and manipulate these cognitive counterparts to scale models offers the only viable explanation for what most clearly differentiates humans from other creatures: their capacity to engage in truth-preserving manipulation of representations.",
    "author": [
      {
        "family": "Waskan",
        "given": "Jonathan A."
      }
    ],
    "id": "Waskan2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "keyword": "models_in_general",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Models and cognition",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/1970.001.0001",
    "ISBN": "9780262286817",
    "abstract": "This series is for people—adults and teenagers—who are interested in computer programming because it’s fun. The three volumes use the Logo programming language as the vehicle for an exploration of computer science from the perspective of symbolic computation and artificial intelligence. Logo is a dialect of Lisp, a language used in the most advanced research projects in computer science, especially in artificial intelligence. Throughout the series, functional programming techniques (including higher order functions and recursion) are emphasized, but traditional sequential programming is also used when appropriate.In the second edition, the first two volumes have been rearranged so that illustrative case studies appear with the techniques they demonstrate. Volume 1 includes a new chapter about higher order functions, and the recursion chapters have been reorganized for greater clarity. Volume 2 includes a new tutorial chapter about macros, an exclusive capability of Berkeley Logo, and two new projects. Throughout the series, the larger program examples have been rewritten for greater readability by more extensive use of data abstraction.In Volume 3 \"Beyond Programming\", the reader learns that computer science includes not just programming computers, but also more formal ways to think about computing, such as automata theory and discrete mathematics. In contrast to most books on those subjects, this volume presents the ideas in the form of concrete, usable computer programs rather than as abstract proofs. Examples include a program to translate from the declarative Regular Expression formalism into the executable Finite State Machine notation, and a Pascal compiler written in Logo.The Logo programs in these books and the author’s free Berkeley Logo interpreter are available via the Internet or on diskette.",
    "author": [
      {
        "family": "Harvey",
        "given": "Brian"
      }
    ],
    "edition": "2",
    "id": "Harvey1997-vol1",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "programming",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Computer science logo style",
    "type": "book",
    "volume": "1",
    "volume-title": "Symbolic computing"
  },
  {
    "DOI": "10.7551/mitpress/1972.001.0001",
    "ISBN": "9780262286824",
    "author": [
      {
        "family": "Harvey",
        "given": "Brian"
      }
    ],
    "edition": "2",
    "id": "Harvey1997-vol2",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "programming",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Computer science logo style",
    "type": "book",
    "volume": "2",
    "volume-title": "Advanced techniques"
  },
  {
    "DOI": "10.7551/mitpress/1974.001.0001",
    "ISBN": "9780262286831",
    "author": [
      {
        "family": "Harvey",
        "given": "Brian"
      }
    ],
    "edition": "2",
    "id": "Harvey1997-vol3",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "keyword": "programming",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Computer science logo style",
    "type": "book",
    "volume": "3",
    "volume-title": "Beyond programming"
  },
  {
    "DOI": "10.7551/mitpress/3629.001.0001",
    "ISBN": "9780262290944",
    "author": [
      {
        "family": "Pugh",
        "given": "Emerson W."
      },
      {
        "family": "Johnson",
        "given": "Lyle R."
      },
      {
        "family": "Palmer",
        "given": "John H."
      }
    ],
    "id": "Pugh2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "keyword": "computing_history",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "IBM’s 360 and early 370 systems",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/1170.001.0001",
    "ISBN": "9780262291149",
    "author": [
      {
        "family": "Haugeland",
        "given": "John"
      }
    ],
    "id": "Haugeland1989",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "keyword": "ai",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Artificial intelligence: The very idea",
    "title-short": "Artificial intelligence",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/8545.001.0001",
    "ISBN": "9780262305280",
    "abstract": "Computing is not simply about hardware or software, or calculation or applications. Computing, writes Paul Rosenbloom, is an exciting and diverse, yet remarkably coherent, scientific enterprise that is highly multidisciplinary yet maintains a unique core of its own. In On Computing, Rosenbloom proposes that computing is a great scientific domain on a par with the physical, life, and social sciences. Rosenbloom introduces a relational approach for understanding computing, conceptualizing it in terms of forms of interaction and implementation, to reveal the hidden structures and connections among its disciplines. He argues for the continuing vitality of computing, surveying the leading edge in computing’s combination with other domains, from biocomputing and brain-computer interfaces to crowdsourcing and virtual humans to robots and the intermingling of the real and the virtual. He explores forms of higher order coherence, or macrostructures, over complex computing topics and organizations. Finally, he examines the very notion of a great scientific domain in philosophical terms, honing his argument that computing should be considered the fourth great scientific domain. With On Computing, Rosenbloom, a key architect of the founding of University of Southern California’s Institute for Creative Technologies and former Deputy Director of USC’s Information Sciences Institute, offers a broader perspective on what computing is and what it can become.",
    "author": [
      {
        "family": "Rosenbloom",
        "given": "Paul S."
      }
    ],
    "id": "Rosenbloom2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "computing, philosophy_of_science",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "On computing: The fourth great scientific domain",
    "title-short": "On computing",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/10095.001.0001",
    "ISBN": "9780262331777",
    "author": [
      {
        "family": "Everett",
        "given": "H. R."
      }
    ],
    "id": "Everett2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "keyword": "history, computing_history, cybernetics",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Unmanned systems of world wars i and II",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/10803.001.0001",
    "ISBN": "9780262341172",
    "author": [
      {
        "family": "Tozzi",
        "given": "Christopher"
      }
    ],
    "id": "Tozzi2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "computing_history",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "For fun and profit: A history of the free and open source software revolution",
    "title-short": "For fun and profit",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/11250.001.0001",
    "ISBN": "9780262348201",
    "author": [
      {
        "family": "Barr",
        "given": "Adam"
      }
    ],
    "id": "Barr2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "keyword": "programming, software_studies",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "The problem with software: Why smart engineers write bad code",
    "title-short": "The problem with software",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/11578.001.0001",
    "ISBN": "9780262349802",
    "abstract": "In this book, Kenneth Forbus proposes that qualitative representations hold the key to one of the deepest mysteries of cognitive science: how we reason and learn about the continuous phenomena surrounding us. Forbus argues that qualitative representations—symbolic representations that carve continuous phenomena into meaningful units—are central to human cognition. Qualitative representations provide a basis for commonsense reasoning, because they enable practical reasoning with very little data; this makes qualitative representations a useful component of natural language semantics. Qualitative representations also provide a foundation for expert reasoning in science and engineering by making explicit the broad categories of things that might happen and enabling causal models that help guide the application of more quantitative knowledge as needed. Qualitative representations are important for creating more human-like artificial intelligence systems with capabilities for spatial reasoning, vision, question answering, and understanding natural language. Forbus discusses, among other topics, basic ideas of knowledge representation and reasoning; qualitative process theory; qualitative simulation and reasoning about change; compositional modeling; qualitative spatial reasoning; and learning and conceptual change. His argument is notable both for presenting an approach to qualitative reasoning in which analogical reasoning and learning play crucial roles and for marshaling a wide variety of evidence, including the performance of AI systems. Cognitive scientists will find Forbus’s account of qualitative representations illuminating; AI scientists will value Forbus’s new approach to qualitative representations and the overview he offers.",
    "author": [
      {
        "family": "Forbus",
        "given": "Kenneth D."
      }
    ],
    "id": "Forbus2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "keyword": "models_in_general, formal_models",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Qualitative representations: How people reason and learn about the continuous world",
    "title-short": "Qualitative representations",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/11171.001.0001",
    "ISBN": "9780262354899",
    "author": [
      {
        "family": "Kelleher",
        "given": "John D."
      }
    ],
    "id": "Kelleher2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "keyword": "machine_learning",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Deep learning",
    "type": "book"
  },
  {
    "DOI": "10.1145/1480828.1480846",
    "ISSN": "1558-1160",
    "author": [
      {
        "family": "Krishnamurthi",
        "given": "Shriram"
      }
    ],
    "container-title": "ACM SIGPLAN Notices",
    "id": "Krishnamurthi2008",
    "issue": "11",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "keyword": "modeling_frameworks",
    "language": "en-US",
    "page": "81-83",
    "title": "Teaching programming languages in a post-Linnaean age",
    "type": "article-journal",
    "volume": "43"
  },
  {
    "DOI": "10.1145/1478873.1478913",
    "URL": "http://dx.doi.org/10.1145/1478873.1478913",
    "author": [
      {
        "family": "Thompson",
        "given": "F. B."
      },
      {
        "family": "Dostert",
        "given": "B. H."
      }
    ],
    "container-title": "Proceedings of the November 16-18, 1971, fall joint computer conference on - AFIPS ’71 (Fall)",
    "id": "Thompson1971",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "keyword": "modeling_frameworks",
    "language": "en-US",
    "publisher": "ACM Press",
    "title": "The future of specialized languages",
    "type": "article-journal"
  },
  {
    "DOI": "10.1145/512976.512991",
    "URL": "http://dx.doi.org/10.1145/512976.512991",
    "author": [
      {
        "family": "Naur",
        "given": "Peter"
      }
    ],
    "container-title": "Proceedings of the 2nd ACM SIGACT-SIGPLAN symposium on Principles of programming languages - POPL ’75",
    "id": "Naur1975",
    "issued": {
      "date-parts": [
        [
          1975
        ]
      ]
    },
    "keyword": "modeling_frameworks",
    "language": "en-US",
    "publisher": "ACM Press",
    "title": "Programming languages, natural languages, and mathematics",
    "type": "article-journal"
  },
  {
    "DOI": "10.1145/800228.806936",
    "URL": "http://dx.doi.org/10.1145/800228.806936",
    "author": [
      {
        "family": "Hobbs",
        "given": "Jerry R."
      }
    ],
    "container-title": "Proceedings of the 1977 symposium on Artificial intelligence and programming languages -",
    "id": "Hobbs1977",
    "issued": {
      "date-parts": [
        [
          1977
        ]
      ]
    },
    "keyword": "modeling_frameworks",
    "language": "en-US",
    "publisher": "ACM Press",
    "title": "What the nature of natural language tells us about how to make natural-language-like programming languages more natural",
    "type": "article-journal"
  },
  {
    "DOI": "10.1145/800088.802823",
    "ISBN": "0897910249",
    "URL": "http://dx.doi.org/10.1145/800088.802823",
    "author": [
      {
        "family": "Wexelblat",
        "given": "Richard L."
      }
    ],
    "container-title": "Proceedings of the 3rd ACM SIGSMALL symposium and the first SIGPC symposium on Small systems - SIGSMALL ’80",
    "id": "Wexelblat1980",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "keyword": "modeling_frameworks",
    "language": "en-US",
    "publisher": "ACM Press",
    "title": "The consequences of one’s first programming language",
    "type": "article-journal"
  },
  {
    "DOI": "10.1145/800177.810072",
    "ISBN": "0897910087",
    "URL": "http://dx.doi.org/10.1145/800177.810072",
    "author": [
      {
        "family": "Ballard",
        "given": "Bruce W."
      },
      {
        "family": "Biermann",
        "given": "Alan W."
      }
    ],
    "container-title": "Proceedings of the 1979 annual conference on - ACM 79",
    "id": "Ballard1979",
    "issued": {
      "date-parts": [
        [
          1979
        ]
      ]
    },
    "keyword": "modeling_frameworks",
    "language": "en-US",
    "publisher": "ACM Press",
    "title": "Programming in natural language",
    "type": "article-journal"
  },
  {
    "URL": "https://z-i-g.de/pdf/ZIG_3_2012_gerovitch.pdf",
    "accessed": {
      "date-parts": [
        [
          2020,
          5,
          9
        ]
      ]
    },
    "author": [
      {
        "family": "Gerovitch",
        "given": "Slava"
      }
    ],
    "container-title": "Zeitschrift für Ideengeschichte",
    "id": "Gerovitch2012",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "cybernetics",
    "language": "de-DE",
    "page": "19-25",
    "title": "Die sowjetische Kybürokratie",
    "type": "article-journal",
    "volume": "VI"
  },
  {
    "DOI": "10.1126/science.177.4047.393",
    "ISSN": "1095-9203",
    "author": [
      {
        "family": "Anderson",
        "given": "Philip W."
      }
    ],
    "container-title": "Science",
    "id": "Anderson1972",
    "issue": "4047",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "keyword": "philosophy_of_science, models_in_general",
    "page": "393--396",
    "title": "More is different",
    "type": "article-journal",
    "volume": "177"
  },
  {
    "URL": "https://ikkm-weimar.de/site/assets/files/8279/zmk_1-2019_debatte.pdf",
    "accessed": {
      "date-parts": [
        [
          2020,
          5,
          9
        ]
      ]
    },
    "author": [
      {
        "family": "Jannidis",
        "given": "Fotis"
      }
    ],
    "container-title": "Zeitschrift für Medien- und Kulturforschung",
    "id": "Jannidis2019",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "de-DE",
    "page": "63-70",
    "title": "Digitale Geisteswissenschaften: Offene Fragen – schöne Aussichten",
    "title-short": "Digitale Geisteswissenschaften",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "URL": "https://ikkm-weimar.de/site/assets/files/8279/zmk_1-2019_debatte.pdf",
    "accessed": {
      "date-parts": [
        [
          2020,
          5,
          9
        ]
      ]
    },
    "author": [
      {
        "family": "Krajewski",
        "given": "Markus"
      }
    ],
    "container-title": "Zeitschrift für Medien- und Kulturforschung",
    "id": "Krajewski2019",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "de-DE",
    "page": "71-80",
    "title": "Hilfe für die digitale Hilfswissenschaft: Eine Positionsbestimmung",
    "title-short": "Hilfe für die digitale Hilfswissenschaft",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "ISBN": "90-5183-721-6",
    "collection-number": "41",
    "collection-title": "Poznań studies in the philosophy of the sciences and the humanities",
    "editor": [
      {
        "family": "Topolski",
        "given": "Jerzy"
      }
    ],
    "id": "Topolski1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "keyword": "history, theory",
    "language": "en-US",
    "publisher": "Rodopi",
    "title": "Historiography between modernism and postmodernism: Contributions to the methodology of the historical research",
    "title-short": "Historiography between modernism and postmodernism",
    "type": "book"
  },
  {
    "author": [
      {
        "dropping-particle": "Teilhard de",
        "family": "Chardin",
        "given": "Pierre"
      }
    ],
    "collection-title": "Œuvres de Teilhard de Chardin 3",
    "id": "TeilharddeChardin1957",
    "issued": {
      "date-parts": [
        [
          1957
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Seuil",
    "title": "La vision du passé",
    "type": "book",
    "volume": "III"
  },
  {
    "author": [
      {
        "dropping-particle": "Teilhard de",
        "family": "Chardin",
        "given": "Pierre"
      }
    ],
    "collection-title": "Œuvres de Teilhard de Chardin 3",
    "container-title": "La vision du passé",
    "id": "Teilhard1957-hominisation",
    "issued": {
      "date-parts": [
        [
          1957
        ]
      ]
    },
    "language": "fr-FR",
    "number-of-volumes": "III",
    "page": "75-111",
    "publisher": "Seuil",
    "title": "L’hominisation",
    "type": "chapter"
  },
  {
    "author": [
      {
        "dropping-particle": "Teilhard de",
        "family": "Chardin",
        "given": "Pierre"
      }
    ],
    "id": "TeilharddeChardin1966",
    "issued": {
      "date-parts": [
        [
          1966
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Harper & Row",
    "title": "The vision of the past",
    "translator": [
      {
        "family": "Cohen",
        "given": "J. M."
      }
    ],
    "type": "book",
    "volume": "III"
  },
  {
    "author": [
      {
        "dropping-particle": "Teilhard de",
        "family": "Chardin",
        "given": "Pierre"
      }
    ],
    "container-title": "The vision of the past",
    "id": "TeilharddeChardin1966-hominization",
    "issued": {
      "date-parts": [
        [
          1966
        ]
      ]
    },
    "language": "en-US",
    "page": "51-79",
    "publisher": "Harper & Row",
    "title": "Hominization",
    "translator": [
      {
        "family": "Cohen",
        "given": "J. M."
      }
    ],
    "type": "chapter",
    "volume": "III"
  },
  {
    "DOI": "10.7551/mitpress/10522.001.0001",
    "ISBN": "9780262344753",
    "abstract": "How the future has been imagined and made, through the work of writers, artists, inventors, and designers. The future is like an unwritten book. It is not something we see in a crystal ball, or can only hope to predict, like the weather. In this volume of the MIT Press’s Essential Knowledge series, Nick Montfort argues that the future is something to be made, not predicted. Montfort offers what he considers essential knowledge about the future, as seen in the work of writers, artists, inventors, and designers (mainly in Western culture) who developed and described the core components of the futures they envisioned. Montfort’s approach is not that of futurology or scenario planning; instead, he reports on the work of making the future—the thinkers who devoted themselves to writing pages in the unwritten book. Douglas Engelbart, Alan Kay, and Ted Nelson didn’t predict the future of computing, for instance. They were three of the people who made it. Montfort focuses on how the development of technologies—with an emphasis on digital technologies—has been bound up with ideas about the future. Readers learn about kitchens of the future and the vision behind them; literary utopias, from Plato’s Republic to Edward Bellamy’s Looking Backward and Charlotte Perkins Gilman’s Herland; the Futurama exhibit at the 1939 New York World’s Fair; and what led up to Tim Berners-Lee’s invention of the World Wide Web. Montfort describes the notebook computer as a human-centered alterative to the idea of the computer as a room-sized “giant brain”; speculative practice in design and science fiction; and, throughout, the best ways to imagine and build the future.",
    "author": [
      {
        "family": "Montfort",
        "given": "Nick"
      }
    ],
    "id": "Montfort2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "keyword": "computing_history",
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "The future",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Forsythe",
        "given": "George E."
      }
    ],
    "container-title": "Information processing 68: Proceedings of IFIP congress 1968",
    "editor": [
      {
        "family": "Morrel",
        "given": "A. J. H."
      }
    ],
    "event-date": {
      "date-parts": [
        [
          1968,
          8,
          5
        ],
        [
          1968,
          8,
          10
        ]
      ]
    },
    "id": "Forsythe1968",
    "issued": {
      "date-parts": [
        [
          1968
        ]
      ]
    },
    "language": "en-US",
    "page": "1025-1039",
    "publisher": "International Federation for Information Processing",
    "title": "Computer science and education",
    "type": "paper-conference",
    "volume": "2"
  },
  {
    "URL": "https://blogs.lse.ac.uk/impactofsocialsciences/2016/08/10/why-are-interdisciplinary-research-proposals-less-likely-to-be-funded/",
    "accessed": {
      "date-parts": [
        [
          2020,
          5,
          23
        ]
      ]
    },
    "author": [
      {
        "family": "Bammer",
        "given": "Gabriele"
      }
    ],
    "id": "Bammer2016",
    "issued": {
      "date-parts": [
        [
          2016,
          8,
          10
        ]
      ]
    },
    "language": "en-US",
    "publisher": "LSE Impact Blog",
    "title": "Why are interdisciplinary research proposals less likely to be funded? Lack of adequate peer review may be a factor.",
    "type": "webpage"
  },
  {
    "ISBN": "8842493368",
    "author": [
      {
        "family": "Topolski",
        "given": "Jerzy"
      }
    ],
    "editor": [
      {
        "family": "Righini",
        "given": "Raffaello"
      }
    ],
    "id": "Topolski1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "language": "it-IT",
    "publisher": "Mondadori",
    "publisher-place": "Milano",
    "title": "Narrare la storia: Nuovi principi di metodologia storica",
    "title-short": "Narrare la storia",
    "type": "book"
  },
  {
    "ISBN": "351828357X",
    "author": [
      {
        "family": "Koselleck",
        "given": "Reinhart"
      }
    ],
    "edition": "3",
    "id": "Koselleck1995",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Suhrkamp",
    "publisher-place": "Frankfurt am Main",
    "title": "Vergangene Zukunft: zur Semantik geschichtlicher Zeiten",
    "title-short": "Vergangene Zukunft",
    "type": "book"
  },
  {
    "DOI": "10.5195/errs.2014.228",
    "ISSN": "2156-7808",
    "author": [
      {
        "family": "Cox",
        "given": "Linda L."
      }
    ],
    "container-title": "Études Ricoeuriennes / Ricoeur Studies",
    "id": "Cox2014",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "page": "95--114",
    "title": "The convergence of Ricœur’s and Von Wright’s complex models of history",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.1163/18722636-12341425",
    "ISSN": "1872-2636",
    "author": [
      {
        "family": "Gangl",
        "given": "Georg"
      }
    ],
    "container-title": "Journal of the Philosophy of History",
    "id": "Gangl2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "1--25",
    "title": "Narrative explanations: The case for causality",
    "title-short": "Narrative explanations",
    "type": "article-journal"
  },
  {
    "DOI": "10.7551/mitpress/10809.001.0001",
    "ISBN": "9780262336611",
    "author": [
      {
        "family": "Halpern",
        "given": "Joseph Y."
      }
    ],
    "id": "Halpern2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Actual causality",
    "type": "book"
  },
  {
    "ISBN": "978-0-19-874691-1",
    "editor": [
      {
        "family": "Beebee",
        "given": "Helen"
      },
      {
        "family": "Hitchcock",
        "given": "Christopher"
      },
      {
        "family": "Price",
        "given": "Huw"
      }
    ],
    "id": "Beebee2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Oxford University Press",
    "title": "Making a difference: Essays on the philosophy of causation",
    "title-short": "Making a difference",
    "type": "book"
  },
  {
    "annote": "https://archive.org/details/numericalmethods0000hamm/page/n7/mode/2up",
    "author": [
      {
        "family": "Hamming",
        "given": "Richard W."
      }
    ],
    "id": "Hamming1962",
    "issued": {
      "date-parts": [
        [
          1962
        ]
      ]
    },
    "language": "en-US",
    "publisher": "McGraw-Hill",
    "title": "Numerical methods for scientists and engineers",
    "type": "book"
  },
  {
    "URL": "https://jdmdh.episciences.org/5574",
    "annote": "Bad paper, ↗ Twitter DM with Efthymis and Mateusz, 2020-04-06",
    "author": [
      {
        "family": "Diarra",
        "given": "Djibril"
      },
      {
        "family": "Clouzot",
        "given": "Martine"
      },
      {
        "family": "Nicolle",
        "given": "Christophe"
      }
    ],
    "container-title": "Journal of Data Mining & Digital Humanities",
    "id": "Diarra2019",
    "issued": {
      "date-parts": [
        [
          2019,
          7
        ]
      ]
    },
    "language": "en-US",
    "title": "Causal reasoning and symbolic relationships in medieval illuminations",
    "type": "article-journal",
    "volume": "Special Issue on Data Science and Digital Humanities EGC 2018"
  },
  {
    "DOI": "10.1049/jiee-3.1958.0227",
    "ISSN": "2054-0612",
    "annote": "Quoted by Bowles, Bowles1967, 13",
    "author": [
      {
        "family": "Ashby",
        "given": "Eric"
      }
    ],
    "container-title": "Journal of the Institution of Electrical Engineers",
    "id": "Ashby1958",
    "issue": "45",
    "issued": {
      "date-parts": [
        [
          1958
        ]
      ]
    },
    "language": "en-US",
    "page": "478-484",
    "publisher": "Institution of Engineering and Technology (IET)",
    "title": "Techonological humanism",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "annote": "Cited by Bowles, Bowles1967, 58f",
    "author": [
      {
        "family": "Aydelotte",
        "given": "William O."
      }
    ],
    "container-title": "Proceedings of the conference on the use of computers in humanistic research",
    "id": "Aydelotte1966",
    "issued": {
      "date-parts": [
        [
          1966
        ]
      ]
    },
    "page": "8f",
    "publisher": "Rutgers, The State University of New Jersey",
    "title": "Computers in historical research",
    "type": "paper-conference"
  },
  {
    "URL": "https://technologyreview.com/2020/05/11/1001563/",
    "accessed": {
      "date-parts": [
        [
          2020,
          6,
          16
        ]
      ]
    },
    "author": [
      {
        "family": "Heaven",
        "given": "Will Douglas"
      }
    ],
    "container-title": "MIT Technology Review",
    "id": "Heaven2020",
    "issued": {
      "date-parts": [
        [
          2020,
          5,
          11
        ]
      ]
    },
    "language": "en-US",
    "title": "Our weird behavior during the pandemic is messing with AI models",
    "type": "article-journal"
  },
  {
    "URL": "http://digitalhumanities.org/dhq/vol/12/2/000377/000377.html",
    "abstract": "Digital humanities have a long tradition of using advanced computational techniques and machine learning to aid humanistic enquiry. In this paper, we concentrate on a specific subfield of machine learning called predictive analytics and its use in digital humanities. Predictive analytics has evolved from descriptive analytics, which creates summaries of data, while predictive analytics predicts relationships within the data that also help to explain new data. Predictive analytics uses machine learning techniques but also traditional statistical methods. It uses properties (or features) of the data to predict another target feature in the data. Machine learning is used by predictive analytics to establish the rules that given a certain combination of features make the target more or less likely. Predictive analytics can thus be considered to be a technique to machine-read data. The paper discusses the background of predictive analytics, its use for predicting the past and finally presents a case study in predicting past gender relations in a historical dataset. Predicting the past is introduced as a method to explore relationships in past data.",
    "accessed": {
      "date-parts": [
        [
          2020,
          6,
          16
        ]
      ]
    },
    "author": [
      {
        "family": "Blanke",
        "given": "Tobias"
      }
    ],
    "container-title": "Digital Humanities Quarterly",
    "id": "Blanke2018",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "title": "Predicting the past",
    "type": "article-journal",
    "volume": "12"
  },
  {
    "DOI": "10.1515/hzhz-2020-0019",
    "ISSN": "0018-2613",
    "author": [
      {
        "family": "Buchner",
        "given": "Michael"
      },
      {
        "family": "Jopp",
        "given": "Tobias A."
      },
      {
        "family": "Spoerer",
        "given": "Mark"
      },
      {
        "family": "Wehrheim",
        "given": "Lino"
      }
    ],
    "container-title": "Historische Zeitschrift",
    "id": "Buchner2020",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "de-DE",
    "page": "580-621",
    "title": "Zur Konjunktur des Zählens – oder wie man Quantifizierung quantifiziert: Eine empirische Analyse der Anwendung quantitativer Methoden in der deutschen Geschichtswissenschaft",
    "title-short": "Zur Konjunktur des Zählens – oder wie man Quantifizierung quantifiziert",
    "type": "article-journal",
    "volume": "310"
  },
  {
    "DOI": "10.18452/19244",
    "author": [
      {
        "family": "Hohls",
        "given": "Rüdiger"
      }
    ],
    "collection-title": "Historisches Forum",
    "container-title": "Clio Guide: Ein Handbuch zu digitalen Ressourcen für die Geschichtswissenschaften",
    "edition": "2. erw. und aktualisierte Aufl.",
    "editor": [
      {
        "family": "Busse",
        "given": "Laura"
      },
      {
        "family": "Enderle",
        "given": "Wilfried"
      },
      {
        "family": "Hohls",
        "given": "Rüdiger"
      },
      {
        "family": "Meyer",
        "given": "Thomas"
      },
      {
        "family": "Prellwitz",
        "given": "Jens"
      },
      {
        "family": "Schuhmann",
        "given": "Annette"
      }
    ],
    "id": "Hohls2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "de-DE",
    "page": "A.1-1-B.1-34",
    "title": "Digital Humanities und digitale Geschichtswissenschaften",
    "type": "chapter",
    "volume": "23"
  },
  {
    "URL": "https://www.wired.com/2008/06/pb-theory/",
    "accessed": {
      "date-parts": [
        [
          2020,
          6,
          24
        ]
      ]
    },
    "author": [
      {
        "family": "Anderson",
        "given": "Chris"
      }
    ],
    "container-title": "Wired Magazine",
    "id": "Anderson2008",
    "issued": {
      "date-parts": [
        [
          2008,
          6,
          23
        ]
      ]
    },
    "language": "en-US",
    "title": "The end of theory: The data deluge makes the scientific method obsolete",
    "title-short": "The end of theory",
    "type": "article-journal"
  },
  {
    "DOI": "10.3917/deba.207.0119",
    "ISSN": "2111-4587",
    "annote": "See also other articles in this issue.",
    "author": [
      {
        "family": "Anderson",
        "given": "Chris"
      }
    ],
    "container-title": "Le Débat",
    "id": "Anderson2019",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "fr-FR",
    "page": "119-122",
    "title": "La fin de la théorie: Le déluge de data rend la méthode scientifique obsolète",
    "title-short": "La fin de la théorie",
    "translator": [
      {
        "family": "Dauzat",
        "given": "Pierre-Emmanuel"
      }
    ],
    "type": "article-journal",
    "volume": "207"
  },
  {
    "ISBN": "978-88-7885-665-3",
    "author": [
      {
        "family": "Ferraris",
        "given": "Maurizio"
      },
      {
        "family": "Paini",
        "given": "Germano"
      }
    ],
    "id": "Ferraris2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "it-IT",
    "publisher": "Rosenberg & Sellier",
    "title": "Scienza nuova: Ontologia della trasformazione digitale",
    "title-short": "Scienza nuova",
    "type": "book"
  },
  {
    "ISBN": "0-19-824675-7",
    "author": [
      {
        "family": "Hockey",
        "given": "Susan M."
      }
    ],
    "id": "Hockey1985",
    "issued": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Clarendon Press",
    "title": "Snobol programming for the humanities",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-97226-8",
    "ISBN": "978-3-319-97225-1",
    "collection-number": "133",
    "collection-title": "Philosophical studies series",
    "editor": [
      {
        "family": "De Mol",
        "given": "Liesbeth"
      },
      {
        "family": "Primiero",
        "given": "Giuseppe"
      }
    ],
    "id": "DeMol2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Reflections on programming systems",
    "type": "book"
  },
  {
    "DOI": "10.1093/esr/jcn072",
    "ISSN": "1468-2672",
    "author": [
      {
        "family": "Abell",
        "given": "Peter"
      }
    ],
    "container-title": "European Sociological Review",
    "id": "Abell2008",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "language": "en-US",
    "page": "561-567",
    "title": "History, case studies, statistics, and causal inference",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "DOI": "10.18452/8922",
    "author": [
      {
        "family": "Gradmann",
        "given": "Stefan"
      }
    ],
    "container-title": "LIBREAS",
    "id": "Gradmann2009",
    "issue": "14",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "de-DE",
    "page": "44-52",
    "title": "Signal. Information. Zeichen.",
    "type": "article-journal"
  },
  {
    "DOI": "10.1086/708368",
    "ISSN": "2473-6007",
    "author": [
      {
        "family": "Dong",
        "given": "Liu"
      }
    ],
    "container-title": "KNOW: A Journal on the Formation of Knowledge",
    "id": "Dong2020",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "25-61",
    "title": "On the subversive nature of historical materials",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "DOI": "10.1086/708257",
    "ISSN": "2473-6007",
    "author": [
      {
        "family": "Macuglia",
        "given": "Daniele"
      },
      {
        "family": "Roux",
        "given": "Benoît"
      },
      {
        "family": "Ciccotti",
        "given": "Giovanni"
      }
    ],
    "container-title": "KNOW: A Journal on the Formation of Knowledge",
    "id": "Macuglia2020",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "63-87",
    "title": "Sense experiences and “necessary simulations”: Four centuries of scientific change from Galileo to fundamental computer simulations",
    "title-short": "Sense experiences and “necessary simulations”",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "DOI": "10.1086/692134",
    "ISSN": "2473-599X",
    "author": [
      {
        "family": "Smail",
        "given": "Daniel Lord"
      }
    ],
    "container-title": "KNOW: A Journal on the Formation of Knowledge",
    "id": "Smail2017",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "page": "155-169",
    "title": "Pattern in history",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.1086/692519",
    "ISSN": "2473-599X",
    "author": [
      {
        "family": "Hansen",
        "given": "Lars Peter"
      }
    ],
    "container-title": "KNOW: A Journal on the Formation of Knowledge",
    "id": "Hansen2017",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "page": "171-197",
    "title": "Uncertainty in economic analysis and the economic analysis of uncertainty",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.1086/704849",
    "ISSN": "2379-3171",
    "author": [
      {
        "family": "Bod",
        "given": "Rens"
      }
    ],
    "container-title": "History of Humanities",
    "id": "Bod2019",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "325-328",
    "title": "Roberto Busa (1913–2011): Lemmatizing Aquinas automatically",
    "title-short": "Roberto Busa (1913–2011)",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "DOI": "10.1086/704850",
    "ISSN": "2379-3171",
    "author": [
      {
        "family": "Wimmer",
        "given": "Mario"
      }
    ],
    "container-title": "History of Humanities",
    "id": "Wimmer2019",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "329-334",
    "title": "Josephine Miles (1911–1985): Doing digital humanism with and without machines",
    "title-short": "Josephine Miles (1911–1985)",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "annote": "Available from the Internet Archive: https://archive.org/details/byte-magazine-1980-12/page/n173/mode/2up; the whole issue is on the topic of \"Adventure.\"",
    "author": [
      {
        "family": "Lebling",
        "given": "P. David"
      }
    ],
    "container-title": "BYTE",
    "id": "Lebling1980",
    "issue": "12",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "keyword": "interactive_fiction",
    "language": "en-US",
    "page": "172-182",
    "title": "Zork and the future of computerized fantasy simulations",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.7202/1004046ar",
    "ISSN": "1189-4563",
    "author": [
      {
        "family": "Dahan-Gaida",
        "given": "Laurence"
      }
    ],
    "container-title": "Tangence",
    "id": "Dahan_Gaida_2011",
    "issue": "95",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "language": "fr-FR",
    "page": "43-65",
    "title": "Pensée analogique et dynamiques de la forme chez Paul Valéry: modèles, forces, diagrammes",
    "title-short": "Pensée analogique et dynamiques de la forme chez Paul Valéry",
    "type": "article-journal"
  },
  {
    "URL": "https://arxiv.org/abs/2002.12327",
    "author": [
      {
        "family": "Rogers",
        "given": "Anna"
      },
      {
        "family": "Kovaleva",
        "given": "Olga"
      },
      {
        "family": "Rumshisky",
        "given": "Anna"
      }
    ],
    "id": "Rogers2020",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "title": "A primer in BERTology: What we know about how BERT works",
    "title-short": "A primer in BERTology",
    "type": ""
  },
  {
    "DOI": "10.18653/v1/W19-4828",
    "abstract": "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT’s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention.",
    "author": [
      {
        "family": "Clark",
        "given": "Kevin"
      },
      {
        "family": "Khandelwal",
        "given": "Urvashi"
      },
      {
        "family": "Levy",
        "given": "Omer"
      },
      {
        "family": "Manning",
        "given": "Christopher D."
      }
    ],
    "container-title": "Proceedings of the 2019 ACL workshop “BlackboxNLP: Analyzing and interpreting neural networks for NLP”",
    "id": "Clark2019",
    "issued": {
      "date-parts": [
        [
          2019,
          8
        ]
      ]
    },
    "language": "en-US",
    "page": "276-286",
    "publisher": "Association for Computational Linguistics",
    "title": "What does BERT look at? An analysis of BERT’s attention",
    "type": "paper-conference"
  },
  {
    "annote": "Internet Archive: https://archive.org/details/introductiontos00langgoog/",
    "author": [
      {
        "family": "Langlois",
        "given": "Charles-Victor"
      },
      {
        "family": "Seignobos",
        "given": "Charles"
      }
    ],
    "id": "Langlois1898",
    "issued": {
      "date-parts": [
        [
          1898
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Hachette",
    "title": "Introduction aux études historiques",
    "type": "book"
  },
  {
    "DOI": "10.1163/9789004393547",
    "ISBN": "978-90-04-39354-7",
    "collection-number": "124",
    "collection-title": "History of warfare",
    "editor": [
      {
        "family": "Pennell",
        "given": "Catriona"
      },
      {
        "dropping-particle": "de",
        "family": "Meneses",
        "given": "Filipe Ribeiro"
      }
    ],
    "id": "Pennell2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Brill",
    "title": "A world at war, 1911–1949: Explorations in the cultural history of war",
    "title-short": "A world at war, 1911–1949",
    "type": "book"
  },
  {
    "editor": [
      {
        "family": "Winks",
        "given": "Robin W."
      }
    ],
    "id": "Winks1969",
    "issued": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Harper & Row",
    "title": "The historian as detective: Essays on evidence",
    "title-short": "The historian as detective",
    "type": "book"
  },
  {
    "annote": "https://www.historians.org/about-aha-and-membership/aha-history-and-archives/presidential-addresses/carl-l-becker",
    "author": [
      {
        "family": "Becker",
        "given": "Carl L."
      }
    ],
    "chapter-number": "1",
    "container-title": "The historian as detective: Essays on evidence",
    "editor": [
      {
        "family": "Winks",
        "given": "Robin W."
      }
    ],
    "id": "Becker1932-1969",
    "issued": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1932
        ]
      ]
    },
    "page": "3-23",
    "publisher": "Harper & Row",
    "title": "Everyman his own historian: Essays on evidence",
    "title-short": "Everyman his own historian",
    "type": "chapter"
  },
  {
    "DOI": "10.2200/S00428ED1V01Y201207WEB002",
    "author": [
      {
        "family": "Börner",
        "given": "Katy"
      },
      {
        "family": "Conlon",
        "given": "Michael"
      },
      {
        "family": "Corson-Rikert",
        "given": "Jon"
      },
      {
        "family": "Ding",
        "given": "Ying"
      }
    ],
    "collection-number": "2",
    "collection-title": "Synthesis lectures on semantic web: Theory and technology",
    "id": "Boerner2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "title": "VIVO: A semantic approach to scholarly networking and discovery",
    "title-short": "VIVO",
    "type": "book"
  },
  {
    "DOI": "10.2200/S00481ED1V01Y201302WBE005",
    "author": [
      {
        "family": "Swartz",
        "given": "Aaron"
      }
    ],
    "collection-number": "5",
    "collection-title": "Synthesis lectures on semantic web: Theory and technology",
    "id": "Swartz2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "title": "Aaron swartz’s a programmable web: An unfinished work",
    "title-short": "Aaron swartz’s a programmable web",
    "type": "book"
  },
  {
    "DOI": "10.2200/S00834ED1V01Y201802WBE018",
    "author": [
      {
        "family": "Kendall",
        "given": "Elisa F."
      },
      {
        "family": "McGuinness",
        "given": "Deborah L."
      }
    ],
    "collection-number": "18",
    "collection-title": "Synthesis lectures on semantic web: Theory and technology",
    "id": "Kendall2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Morgan & Claypool",
    "title": "Ontology engineering",
    "type": "book"
  },
  {
    "editor": [
      {
        "family": "Gasteiner",
        "given": "Martin"
      },
      {
        "family": "Haber",
        "given": "Peter"
      }
    ],
    "id": "Gasteiner2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Böhlau",
    "title": "Digitale Arbeitstechniken für Geistes- und Kulturwissenschaften",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Sterling",
        "given": "Leon"
      },
      {
        "family": "Shapiro",
        "given": "Ehud"
      }
    ],
    "edition": "2",
    "id": "Sterling1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "The art of Prolog: Advanced programming techniques",
    "title-short": "The art of Prolog",
    "type": "book"
  },
  {
    "editor": [
      {
        "family": "Shanin",
        "given": "Teodor"
      }
    ],
    "id": "Shanin1972",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "keyword": "formal_models, models_in_general",
    "language": "en-US",
    "publisher": "Tavistock",
    "title": "The rules of the game: Cross-disciplinary essays on models in scholarly thought",
    "title-short": "The rules of the game",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Leff",
        "given": "Gordon"
      }
    ],
    "container-title": "The rules of the game: Cross-disciplinary essays on models in scholarly thought",
    "editor": [
      {
        "family": "Shanin",
        "given": "Teodor"
      }
    ],
    "id": "Leff1972",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "keyword": "formal_models, models_in_general",
    "language": "en-US",
    "page": "148-160",
    "publisher": "Tavistock",
    "title": "Models inherent in history",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/978-3-642-18732-2",
    "author": [
      {
        "family": "Broy",
        "given": "Manfred"
      },
      {
        "family": "Steinbrüggen",
        "given": "Ralf"
      }
    ],
    "id": "Broy2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Springer",
    "title": "Modellbildung in der Informatik",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-642-65191-5",
    "author": [
      {
        "family": "Goldscheider",
        "given": "Peter"
      },
      {
        "family": "Zemanek",
        "given": "Heinz"
      }
    ],
    "editor": [
      {
        "family": "Chladek",
        "given": "Heinz-Peter"
      },
      {
        "family": "Lenk",
        "given": "Franz"
      },
      {
        "family": "Pachl",
        "given": "Walter"
      },
      {
        "family": "Stadler",
        "given": "Manfred"
      }
    ],
    "id": "Goldscheider1971",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Springer",
    "title": "Computer: Werkzeug der Information",
    "title-short": "Computer",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-7091-8202-4",
    "editor": [
      {
        "family": "Gunzenhäuser",
        "given": "Rul"
      }
    ],
    "id": "Gunzenhaeuser1968",
    "issued": {
      "date-parts": [
        [
          1968
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Springer",
    "title": "Nicht-numerische Informationsverarbeitung",
    "type": "book"
  },
  {
    "DOI": "10.10071978-3-642-18631-8",
    "editor": [
      {
        "family": "Hellige",
        "given": "Hans Dieter"
      }
    ],
    "id": "Hellige2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Springer",
    "title": "Geschichten der Informatik: Visionen, Paradigmen, Leitmotive",
    "title-short": "Geschichten der Informatik",
    "type": "book"
  },
  {
    "DOI": "10.10071978-3-642-18631-8",
    "author": [
      {
        "family": "Coy",
        "given": "Wolfgang"
      }
    ],
    "container-title": "Geschichten der Informatik: Visionen, Paradigmen, Leitmotive",
    "editor": [
      {
        "family": "Hellige",
        "given": "Hans Dieter"
      }
    ],
    "id": "Coy2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "language": "de-DE",
    "page": "473-498",
    "publisher": "Springer",
    "title": "Was ist Informatik? Zur Entstehung des Faches an den deutschen Universitäten",
    "title-short": "Was ist Informatik?",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/978-3-476-05434-0",
    "author": [
      {
        "family": "Kamlah",
        "given": "Wilhelm"
      },
      {
        "family": "Lorenzen",
        "given": "Paul"
      }
    ],
    "edition": "3",
    "id": "Kamlah1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Metzler",
    "title": "Logische Propädeutik: Vorschule des vernünftigen Redens",
    "title-short": "Logische Propädeutik",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-663-02006-6",
    "author": [
      {
        "family": "Sachsse",
        "given": "Hans"
      }
    ],
    "id": "Sachsse1971",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Vieweg",
    "title": "Einführung in die Kybernetik",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-642-65070-3",
    "author": [
      {
        "family": "Steinbuch",
        "given": "Karl"
      }
    ],
    "edition": "4",
    "id": "Steinbuch1971",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "keyword": "cybernetics",
    "language": "de-DE",
    "publisher": "Springer",
    "title": "Automat und Mensch: Auf dem Weg zu einer kybernetischen Anthropologie",
    "title-short": "Automat und Mensch",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-322-91089-9",
    "author": [
      {
        "dropping-particle": "von",
        "family": "Glasersfeld",
        "given": "Ernst"
      }
    ],
    "id": "vonGlasersfeld1987",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Vieweg",
    "title": "Wissen, Sprache und Wirklichkeit",
    "translator": [
      {
        "family": "Köck",
        "given": "Wolfram K."
      }
    ],
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-20451-2",
    "editor": [
      {
        "family": "Arló-Costa",
        "given": "Hoaracio"
      },
      {
        "family": "Hendricks",
        "given": "Vincent F."
      },
      {
        "dropping-particle": "van",
        "family": "Benthem",
        "given": "Johan"
      }
    ],
    "id": "Arlo-Costa2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Readings in formal epistemology: Sourcebook",
    "title-short": "Readings in formal epistemology",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-030-03255-5",
    "author": [
      {
        "dropping-particle": "de",
        "family": "Swart",
        "given": "Harrie"
      }
    ],
    "id": "deSwart2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Philosophical and mathematical logic",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-58487-4",
    "author": [
      {
        "family": "Ertel",
        "given": "Wolfgang"
      }
    ],
    "edition": "2",
    "id": "Ertel2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Introduction to artificial intelligence",
    "translator": [
      {
        "family": "Black",
        "given": "Nathanael"
      }
    ],
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-64410-3",
    "author": [
      {
        "family": "Forsyth",
        "given": "David"
      }
    ],
    "id": "Forsyth2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Probability and statistics for computer science",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-77434-3",
    "editor": [
      {
        "family": "Hansson",
        "given": "Sven Ove"
      },
      {
        "family": "Hendricks",
        "given": "Vincent F."
      }
    ],
    "id": "Hansson2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Introduction to formal philosophy",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-94-024-1555-1",
    "editor": [
      {
        "family": "Hunsinger",
        "given": "Jeremy"
      },
      {
        "family": "Allen",
        "given": "Matthew M."
      },
      {
        "family": "Klastrup",
        "given": "Lisbeth"
      }
    ],
    "id": "Hunsinger2020",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Second international handbook of internet research",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-030-20290-3",
    "author": [
      {
        "family": "Hunt",
        "given": "John"
      }
    ],
    "id": "Hunt2019a",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "A beginners guide to python 3 programming",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-030-25943-3",
    "author": [
      {
        "family": "Hunt",
        "given": "John"
      }
    ],
    "id": "Hunt2019b",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Advanced guide to python 3 programming",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-26551-3",
    "author": [
      {
        "family": "Johansson",
        "given": "Lars-Göran"
      }
    ],
    "id": "Johansson2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Philosophy of science for scientists",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-70790-7",
    "author": [
      {
        "family": "Lee",
        "given": "Kent D."
      }
    ],
    "edition": "2",
    "id": "Lee_KD2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Foundations of programming languages",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-96713-4",
    "editor": [
      {
        "family": "Levenberg",
        "given": "Lewis"
      },
      {
        "family": "Neilson",
        "given": "Tai"
      },
      {
        "family": "David",
        "given": "Rheams"
      }
    ],
    "id": "Levenberg2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Palgrave Macmillan",
    "title": "Research methods for the digital humanities",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-96713-4_1",
    "author": [
      {
        "family": "Neilson",
        "given": "Tai"
      },
      {
        "family": "Levenberg",
        "given": "Lewis"
      },
      {
        "family": "Rheams",
        "given": "David"
      }
    ],
    "chapter-number": "1",
    "container-title": "Research methods for the digital humanities",
    "editor": [
      {
        "family": "Levenberg",
        "given": "Lewis"
      },
      {
        "family": "Neilson",
        "given": "Tai"
      },
      {
        "family": "David",
        "given": "Rheams"
      }
    ],
    "id": "Neilson2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "page": "1-14",
    "publisher": "Palgrave Macmillan",
    "title": "Introduction: Research methods for the digital humanities",
    "title-short": "Introduction",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/978-3-319-33405-9",
    "author": [
      {
        "family": "McCain",
        "given": "Kevin"
      }
    ],
    "id": "McCain2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "The nature of scientific knowledge: An explanatory approach",
    "title-short": "The nature of scientific knowledge",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-73004-2",
    "author": [
      {
        "family": "Skansi",
        "given": "Sandro"
      }
    ],
    "id": "Skansi2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Introduction to deep learning: From logical calculus to artificial intelligence",
    "title-short": "Introduction to deep learning",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-95381-6",
    "author": [
      {
        "dropping-particle": "van",
        "family": "Eemeren",
        "given": "Frans H."
      }
    ],
    "id": "vanEemeren2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Argumentation theory: A pragma-dialectical perspective",
    "title-short": "Argumentation theory",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "White",
        "given": "Hayden"
      }
    ],
    "id": "White1978",
    "issued": {
      "date-parts": [
        [
          1978
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Johns Hopkins University Press",
    "title": "Tropics of discourse: Essays in cultural criticism",
    "title-short": "Tropics of discourse",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Evans",
        "given": "Richard J."
      }
    ],
    "id": "Evans1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Granta",
    "title": "In defence of history",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Marcus",
        "given": "Gary"
      },
      {
        "family": "Davis",
        "given": "Ernest"
      }
    ],
    "id": "Marcus2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Pantheon",
    "title": "Rebooting AI: Building artificial intelligence we can trust",
    "title-short": "Rebooting AI",
    "type": "book"
  },
  {
    "ISBN": "3-492-21165-8",
    "collection-number": "5",
    "collection-title": "Veröffentlichungen der Carl Friedrich von Siemens Stiftung",
    "edition": "8",
    "editor": [
      {
        "family": "Gumin",
        "given": "Heinz"
      },
      {
        "family": "Meier",
        "given": "Heinrich"
      }
    ],
    "id": "Gumin2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "language": "de-DE",
    "original-date": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "publisher": "Piper",
    "title": "Einführung in den Konstruktivismus",
    "type": "book"
  },
  {
    "ISBN": "3-492-21165-8",
    "author": [
      {
        "dropping-particle": "von",
        "family": "Glasersfeld",
        "given": "Ernst"
      }
    ],
    "collection-number": "5",
    "collection-title": "Veröffentlichungen der Carl Friedrich von Siemens Stiftung",
    "container-title": "Einführung in den Konstruktivismus",
    "editor": [
      {
        "family": "Gumin",
        "given": "Heinz"
      },
      {
        "family": "Meier",
        "given": "Heinrich"
      }
    ],
    "id": "vonGlasersfeld2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "language": "de-DE",
    "original-date": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "page": "9-39",
    "publisher": "Piper",
    "title": "Konstruktion der Wirklichkeit und des Begriffs der Objektivität",
    "type": "chapter"
  },
  {
    "ISBN": "3-492-21165-8",
    "author": [
      {
        "dropping-particle": "von",
        "family": "Foerster",
        "given": "Heinz"
      }
    ],
    "collection-number": "5",
    "collection-title": "Veröffentlichungen der Carl Friedrich von Siemens Stiftung",
    "container-title": "Einführung in den Konstruktivismus",
    "editor": [
      {
        "family": "Gumin",
        "given": "Heinz"
      },
      {
        "family": "Meier",
        "given": "Heinrich"
      }
    ],
    "id": "vonFoerster2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "language": "de-DE",
    "original-date": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "page": "41-88",
    "publisher": "Piper",
    "title": "Entdecken oder Erfinden: Wie läßt sich Verstehen verstehen?",
    "title-short": "Entdecken oder Erfinden",
    "type": "chapter"
  },
  {
    "ISBN": "3-492-21165-8",
    "author": [
      {
        "family": "Watzlawick",
        "given": "Paul"
      }
    ],
    "collection-number": "5",
    "collection-title": "Veröffentlichungen der Carl Friedrich von Siemens Stiftung",
    "container-title": "Einführung in den Konstruktivismus",
    "editor": [
      {
        "family": "Gumin",
        "given": "Heinz"
      },
      {
        "family": "Meier",
        "given": "Heinrich"
      }
    ],
    "id": "Watzlawick2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "language": "de-DE",
    "original-date": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "page": "89-107",
    "publisher": "Piper",
    "title": "Wirklichkeitsanpassung oder angepaßte “Wirklichkeit”? Konstruktivismus und Psychotherapie",
    "title-short": "Wirklichkeitsanpassung oder angepaßte “Wirklichkeit”?",
    "type": "chapter"
  },
  {
    "ISBN": "3-492-21165-8",
    "author": [
      {
        "family": "Hejl",
        "given": "Peter M."
      }
    ],
    "collection-number": "5",
    "collection-title": "Veröffentlichungen der Carl Friedrich von Siemens Stiftung",
    "container-title": "Einführung in den Konstruktivismus",
    "editor": [
      {
        "family": "Gumin",
        "given": "Heinz"
      },
      {
        "family": "Meier",
        "given": "Heinrich"
      }
    ],
    "id": "Hejl2005",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "language": "de-DE",
    "original-date": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "page": "109-146",
    "publisher": "Piper",
    "title": "Konstruktion der sozialen Konstruktion: Grundlinien einer konstruktivistischen Sozialtheorie",
    "title-short": "Konstruktion der sozialen Konstruktion",
    "type": "chapter"
  },
  {
    "URL": "http://nbn-resolving.de/urn:nbn:de:bvb:12-bsb00106359-4",
    "author": [
      {
        "family": "Copernicus",
        "given": "Nicolaus"
      }
    ],
    "id": "Copernicus1543",
    "issued": {
      "date-parts": [
        [
          1543
        ]
      ]
    },
    "language": "la",
    "publisher": "Petreius",
    "title": "De Revolutionibus Orbium coelestium, Libri VI",
    "type": "book"
  },
  {
    "URL": "http://nbn-resolving.de/urn:nbn:de:bvb:12-bsb00106359-4",
    "annote": "The author name should really be in brackets, because the preface was published anonymously. There are a variety of hacks that work sometimes, depending on the citation style. There’s biblatex-realauthor for exactly this situation, but like all biblatex shit, it doesn’t work as advertised.",
    "author": [
      {
        "family": "Osiander",
        "given": "Andreas"
      }
    ],
    "container-author": [
      {
        "family": "Copernicus",
        "given": "Nicolaus"
      }
    ],
    "container-title": "De Revolutionibus Orbium coelestium, Libri VI",
    "id": "Osiander1543",
    "issued": {
      "date-parts": [
        [
          1543
        ]
      ]
    },
    "language": "la",
    "page": "I<sup>v</sup>-II<sup>r</sup>",
    "publisher": "Petreius",
    "title": "Ad lectorem de hypothesibus huius operis",
    "type": "chapter"
  },
  {
    "ISBN": "3-518-29155-6",
    "author": [
      {
        "family": "Weizenbaum",
        "given": "Joseph"
      }
    ],
    "id": "Weizenbaum2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Suhrkamp",
    "publisher-place": "Frankfurt am Main",
    "title": "Computermacht und Gesellschaft",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Weizenbaum",
        "given": "Joseph"
      }
    ],
    "chapter-number": "6",
    "container-title": "Computermacht und Gesellschaft",
    "id": "Weizenbaum2001-ch06",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "language": "de-DE",
    "original-date": {
      "date-parts": [
        [
          1992,
          9,
          10
        ]
      ]
    },
    "page": "72-79",
    "publisher": "Suhrkamp",
    "title": "Die Sprache des Lernens",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/978-3-322-97005-3",
    "ISBN": "978-3-322-97005-3",
    "author": [
      {
        "family": "Luhmann",
        "given": "Niklas"
      }
    ],
    "id": "Luhmann1990",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Westdeutscher Verlag",
    "title": "Soziologische Aufklärung 5: Konstruktivistische Perspektiven",
    "title-short": "Soziologische Aufklärung 5",
    "type": "book"
  },
  {
    "DOI": "10.1002/asi.5090140205",
    "ISSN": "1936-6108",
    "author": [
      {
        "family": "Barnett",
        "given": "Michael P."
      },
      {
        "family": "Kelley",
        "given": "K. L."
      }
    ],
    "container-title": "American Documentation",
    "id": "Barnett1963",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1963,
          4
        ]
      ]
    },
    "keyword": "interactive_editing, classic",
    "language": "en-US",
    "page": "99--108",
    "title": "Computer editing of verbal texts: Part I. The ES1 system",
    "title-short": "Computer editing of verbal texts",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "DOI": "10.1145/365230.365249",
    "ISSN": "0001-0782",
    "author": [
      {
        "family": "Zemanek",
        "given": "Heinz"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Zemanek1966",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1966,
          3
        ]
      ]
    },
    "language": "en-US",
    "page": "139--143",
    "title": "Semiotics and programming languages",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "DOI": "10.1093/analys/52.4.193",
    "ISSN": "1467-8284",
    "author": [
      {
        "family": "Edgington",
        "given": "Dorothy"
      }
    ],
    "container-title": "Analysis",
    "id": "Edgington1992",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1992,
          10
        ]
      ]
    },
    "language": "en-US",
    "page": "193--204",
    "title": "Validity, uncertainty and vagueness",
    "type": "article-journal",
    "volume": "52"
  },
  {
    "URL": "https://dhdebates.gc.cuny.edu/read/40de72d8-f153-43fa-836b-a41d241e949c/section/20df8acd-9ab9-4f35-8a5d-e91aa5f4a0ea#ch09",
    "accessed": {
      "date-parts": [
        [
          2020,
          9,
          21
        ]
      ]
    },
    "author": [
      {
        "family": "McPherson",
        "given": "Tara"
      }
    ],
    "chapter-number": "9",
    "container-title": "Debates in the digital humanities",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      }
    ],
    "id": "McPherson2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "page": "139-160",
    "publisher": "University of Minnesota Press",
    "title": "Why are the digital humanities so white? Or thinking the histories of race and computation",
    "type": "chapter"
  },
  {
    "URL": "https://www.jstor.org/stable/10.5749/j.ctttv8hq.8",
    "author": [
      {
        "family": "Ramsay",
        "given": "Stephen"
      },
      {
        "family": "Rockwell",
        "given": "Geoffrey"
      }
    ],
    "chapter-number": "5",
    "container-title": "Debates in the digital humanities",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      }
    ],
    "id": "Ramsay2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "page": "75-84",
    "publisher": "University of Minnesota Press",
    "title": "Developing things: Notes toward an epistemology of building in the digital humanities",
    "title-short": "Developing things",
    "type": "chapter"
  },
  {
    "ISBN": "0415283302",
    "author": [
      {
        "family": "Rocca",
        "given": "Michael"
      }
    ],
    "id": "Rocca2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Routledge",
    "publisher-place": "Abingdon",
    "title": "Spinoza",
    "type": "book"
  },
  {
    "ISBN": "978-0-02-928045-4",
    "author": [
      {
        "family": "Searle",
        "given": "John"
      }
    ],
    "id": "Searle1995",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Free Press",
    "publisher-place": "New York, NY",
    "title": "The construction of social reality",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Hartmann",
        "given": "Peter"
      }
    ],
    "container-title": "Studium Generale",
    "id": "Hartmann1965",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          1965
        ]
      ]
    },
    "language": "de-DE",
    "page": "364-379",
    "title": "Modellbildungen in der Sprachwissenschaft",
    "type": "article-journal",
    "volume": "18"
  },
  {
    "DOI": "10.1016/0024-3841(68)90048-x",
    "author": [
      {
        "family": "Hartmann",
        "given": "Peter"
      }
    ],
    "container-title": "Lingua",
    "id": "Hartmann1968",
    "issued": {
      "date-parts": [
        [
          1968,
          1
        ]
      ]
    },
    "language": "de-DE",
    "page": "197--215",
    "title": "Zur Aufgabe der Linguistik",
    "type": "article-journal",
    "volume": "21"
  },
  {
    "author": [
      {
        "family": "Foucault",
        "given": "Michel"
      }
    ],
    "id": "Foucault1971",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Gallimard",
    "title": "L’Ordre du discours",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Meyriat",
        "given": "Jean"
      }
    ],
    "container-title": "Inforcom 78: Actes du premier congrès",
    "event-place": "Compiègne",
    "id": "Meyriat1978",
    "issued": {
      "date-parts": [
        [
          1978
        ]
      ]
    },
    "language": "fr-FR",
    "page": "23-32",
    "publisher": "SFSIC",
    "title": "De l’écrit à l’information: la notion de document et la méthodologie de l’analyse du document",
    "title-short": "De l’écrit à l’information",
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "family": "Meyriat",
        "given": "Jean"
      }
    ],
    "container-title": "Schéma et schématisation",
    "id": "Meyriat1981",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "language": "fr-FR",
    "page": "51-63",
    "title": "Document, documentation, documentologie",
    "type": "article-journal"
  },
  {
    "author": [
      {
        "family": "Escarpit",
        "given": "Robert"
      }
    ],
    "id": "Escarpit1991",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "language": "fr-FR",
    "original-date": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "publisher": "Hachette",
    "title": "L’Information et la communication: Théorie générale",
    "title-short": "L’Information et la communication",
    "type": "book"
  },
  {
    "editor": [
      {
        "family": "Boure",
        "given": "Robert"
      }
    ],
    "id": "Boure2002",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Presses universitaires du Septentrion",
    "title": "Les Origines des sciences de l’information et de la communication: Regards croisés",
    "title-short": "Les Origines des sciences de l’information et de la communication",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Kittler",
        "given": "Friedrich"
      }
    ],
    "id": "Kittler1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1986
        ]
      ]
    },
    "publisher": "Stanford University Press",
    "title": "Gramophone, film, typewriter",
    "translator": [
      {
        "family": "Winthrop-Young",
        "given": "Geoffrey"
      },
      {
        "family": "Wutz",
        "given": "Michael"
      }
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Kittler",
        "given": "Friedrich"
      }
    ],
    "id": "Kittler1986",
    "issued": {
      "date-parts": [
        [
          1986
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Brinkmann & Bose",
    "title": "Grammophon, Film, Typewriter",
    "type": "book"
  },
  {
    "URL": "https://theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/",
    "annote": "http://web.mit.edu/STS.035/www/PDFs/think.pdf",
    "author": [
      {
        "family": "Bush",
        "given": "Vannevar"
      }
    ],
    "container-title": "The Atlantic Monthly",
    "id": "Bush1945",
    "issued": {
      "date-parts": [
        [
          1945,
          7
        ]
      ]
    },
    "language": "en-US",
    "page": "101-108",
    "title": "As we may think",
    "type": "article-journal",
    "volume": "176"
  },
  {
    "author": [
      {
        "family": "Rheingold",
        "given": "Howard"
      }
    ],
    "id": "Rheingold1985-2000",
    "issued": {
      "date-parts": [
        [
          2000
        ]
      ]
    },
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1985
        ]
      ]
    },
    "publisher": "MIT Press",
    "title": "Tools for thought: The history and future of mind-expanding technology",
    "title-short": "Tools for thought",
    "type": "book"
  },
  {
    "DOI": "10.11588/DCO.2017.0.48490",
    "author": [
      {
        "family": "Krämer",
        "given": "Sybille"
      }
    ],
    "container-title": "Digital Classics Online",
    "id": "Kraemer2018",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "de-DE",
    "page": "5-11",
    "title": "Der “Stachel des Digitalen” – ein Anreiz zur Selbstreflexion in den Geisteswissenschaften? Ein philosophischer Kommentar zu den Digital Humanities in neun Thesen",
    "title-short": "Der “Stachel des Digitalen” – ein Anreiz zur Selbstreflexion in den Geisteswissenschaften?",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "author": [
      {
        "family": "Granger",
        "given": "Gilles-Gaston"
      }
    ],
    "edition": "Nouvelle éd. augmentée d’une préface",
    "id": "Granger1967",
    "issued": {
      "date-parts": [
        [
          1967
        ]
      ]
    },
    "language": "fr-FR",
    "original-date": {
      "date-parts": [
        [
          1960
        ]
      ]
    },
    "publisher": "Aubier-Montaigne",
    "title": "Pensée formelle et sciences de l’homme",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Granger",
        "given": "Gilles-Gaston"
      }
    ],
    "id": "Granger1983",
    "issued": {
      "date-parts": [
        [
          1983
        ]
      ]
    },
    "language": "en-US",
    "note": "With the author’s <i>Postface</i> to the English edition (1982)",
    "publisher": "Reidel",
    "title": "Formal thought and the sciences of man",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Granger",
        "given": "Gilles-Gaston"
      }
    ],
    "container-title": "Formal thought and the sciences of man",
    "id": "Granger1982-postface",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "language": "en-US",
    "page": "181-193",
    "publisher": "Reidel",
    "title": "Postface: Form, logics and reasons",
    "title-short": "Postface",
    "type": "chapter"
  },
  {
    "URL": "http://sens-public.org/articles/1369/",
    "accessed": {
      "date-parts": [
        [
          2020,
          10,
          11
        ]
      ]
    },
    "author": [
      {
        "family": "Lévy",
        "given": "Pierre"
      }
    ],
    "container-title": "Sens Public",
    "id": "Levy2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "fr-FR",
    "title": "Le rôle des humanités numériques dans le nouvel espace politique",
    "type": "article-journal"
  },
  {
    "author": [
      {
        "family": "Biggs",
        "given": "John"
      },
      {
        "family": "Tang",
        "given": "Catherine"
      }
    ],
    "edition": "4",
    "id": "Biggs2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Open University Press",
    "publisher-place": "Maidenhead",
    "title": "Teaching for quality learning at university: What the student does",
    "title-short": "Teaching for quality learning at university",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Thom",
        "given": "René"
      }
    ],
    "edition": "2",
    "id": "Thom1991",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Flammarion",
    "title": "Prédire n’est pas expliquer: Entretiens avec Émile Noël",
    "title-short": "Prédire n’est pas expliquer",
    "type": "book"
  },
  {
    "DOI": "10.1093/0198250142.001.0001",
    "author": [
      {
        "family": "Resnik",
        "given": "Michael D."
      }
    ],
    "id": "Resnik1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "publisher": "Oxford University Press",
    "title": "Mathematics as a science of patterns",
    "type": "book"
  },
  {
    "annote": "https://archive.org/details/mathematiciansap0000hard/page/84/mode/2up",
    "author": [
      {
        "family": "Hardy",
        "given": "Godfrey Harold"
      }
    ],
    "id": "Hardy1967",
    "issued": {
      "date-parts": [
        [
          1967
        ]
      ]
    },
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1940
        ]
      ]
    },
    "publisher": "Cambridge University Press",
    "title": "A mathematician’s apology",
    "type": "book"
  },
  {
    "ISBN": "978-2-37361-127-4",
    "author": [
      {
        "family": "Varenne",
        "given": "Franck"
      }
    ],
    "id": "Varenne2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Éditions Matériologiques",
    "title": "Théories et modèles en sciences humaines: Le cas de la géographie",
    "title-short": "Théories et modèles en sciences humaines",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Gardin",
        "given": "Jean-Claude"
      }
    ],
    "id": "Gardin1979",
    "issued": {
      "date-parts": [
        [
          1979
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Hachette",
    "title": "Une archéologie théorique",
    "type": "book"
  },
  {
    "ISBN": "978-2-7132-0956-7",
    "author": [
      {
        "family": "Gardin",
        "given": "Jean-Claude"
      }
    ],
    "id": "Gardin1991",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Éd. de l’École des hautes études en sciences sociales",
    "publisher-place": "Paris",
    "title": "Le calcul et la raison: essais sur la formalisation du discours savant",
    "title-short": "Le calcul et la raison",
    "type": "book"
  },
  {
    "ISBN": "978-2-7132-0956-7",
    "author": [
      {
        "family": "Gardin",
        "given": "Jean-Claude"
      }
    ],
    "chapter-number": "1",
    "container-title": "Le calcul et la raison: essais sur la formalisation du discours savant",
    "id": "Gardin1991-ch1",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "language": "fr-FR",
    "original-date": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "page": "17-37",
    "publisher": "Éd. de l’École des hautes études en sciences sociales",
    "publisher-place": "Paris",
    "title": "Introduction: essais sur la formalisation du discours savant",
    "title-short": "Introduction",
    "type": "chapter"
  },
  {
    "ISBN": "978-2-7132-0956-7",
    "author": [
      {
        "family": "Gardin",
        "given": "Jean-Claude"
      }
    ],
    "chapter-number": "3",
    "container-title": "Le calcul et la raison: essais sur la formalisation du discours savant",
    "id": "Gardin1991-ch3",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "language": "fr-FR",
    "original-date": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "page": "59-89",
    "publisher": "Éd. de l’École des hautes études en sciences sociales",
    "publisher-place": "Paris",
    "title": "Questions d’épistémologie pratique dans les perspectives de l’intelligence artificielle: essais sur la formalisation du discours savant",
    "title-short": "Questions d’épistémologie pratique dans les perspectives de l’intelligence artificielle",
    "type": "chapter"
  },
  {
    "ISBN": "0-7123-3186-7",
    "author": [
      {
        "family": "Gardin",
        "given": "Jean-Claude"
      }
    ],
    "collection-number": "71",
    "collection-title": "Library and Information Research Report",
    "container-title": "Interpretation in the Humanities: Perspectives from Artificial Intelligence",
    "editor": [
      {
        "family": "Ennals",
        "given": "Richard"
      },
      {
        "family": "Gardin",
        "given": "Jean-Claude"
      }
    ],
    "id": "Gardin1990a",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "language": "fr-FR",
    "page": "22-59",
    "publisher": "British Library",
    "title": "L’interprétation dans les humanités: réflexions sur la troisième voie",
    "title-short": "L’interprétation dans les humanités",
    "type": "chapter"
  },
  {
    "ISBN": "0-06-131545-1",
    "author": [
      {
        "family": "Fischer",
        "given": "David Hacket"
      }
    ],
    "id": "Fischer1970",
    "issued": {
      "date-parts": [
        [
          1970
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Harper & Row",
    "publisher-place": "New York",
    "title": "Historians’ fallacies: Toward a logic of historical thought",
    "title-short": "Historians’ fallacies",
    "type": "book"
  },
  {
    "URL": "https://philpapers.org/rec/CREGAS",
    "author": [
      {
        "family": "Cremaschi",
        "given": "Sergio"
      }
    ],
    "container-title": "Manuscrito",
    "id": "Cremaschi1987",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "language": "en-US",
    "page": "111-136",
    "title": "Granger and science as network of models",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "DOI": "10.7202/006768ar",
    "ISSN": "0038-030X",
    "author": [
      {
        "family": "Granger",
        "given": "Gilles-Gaston"
      }
    ],
    "container-title": "Sociologie et sociétés",
    "id": "Granger1982",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "language": "fr-FR",
    "page": "7--13",
    "title": "Modèles qualitatifs, modèles quantitatifs dans la connaissance scientifique",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "URL": "http://jstor.org/stable/23940313",
    "author": [
      {
        "family": "Granger",
        "given": "Gilles-Gaston"
      }
    ],
    "container-title": "Revue Internationale de Philosophie",
    "id": "Granger1965b",
    "issue": "73/74 (3/4)",
    "issued": {
      "date-parts": [
        [
          1965
        ]
      ]
    },
    "language": "fr-FR",
    "page": "251-290",
    "publisher": "Revue Internationale de Philosophie",
    "title": "Objet, structures et significations",
    "type": "article-journal",
    "volume": "19"
  },
  {
    "URL": "https://n2t.net/ark:/13960/t7tm9t43z",
    "collection-title": "Information et cybernétique",
    "id": "Concept-d-information1965",
    "issued": {
      "date-parts": [
        [
          1965
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Cahiers de Royaumont; Gauthier-Villars/Minuit",
    "title": "Le concept d’information dans la science contemporaine",
    "type": "book"
  },
  {
    "URL": "https://n2t.net/ark:/13960/t7tm9t43z",
    "author": [
      {
        "family": "Granger",
        "given": "Gilles-Gaston"
      }
    ],
    "collection-title": "Information et cybernétique",
    "container-title": "Le concept d’information dans la science contemporaine",
    "id": "Granger1965a",
    "issued": {
      "date-parts": [
        [
          1965
        ]
      ]
    },
    "language": "fr-FR",
    "page": "389-401",
    "publisher": "Cahiers de Royaumont; Gauthier-Villars/Minuit",
    "title": "Information et connaissance de l’individuel",
    "type": "paper-conference"
  },
  {
    "URL": "http://jstor.org/stable/40881436",
    "author": [
      {
        "family": "Granger",
        "given": "Gilles-Gaston"
      }
    ],
    "container-title": "Tijdschrift voor Filosofie",
    "id": "Granger1967b",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1967
        ]
      ]
    },
    "language": "fr-FR",
    "page": "771-780",
    "title": "Science, philosophie, idéologies",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "URL": "https://www.jstor.org/stable/41680777",
    "author": [
      {
        "family": "Granger",
        "given": "Gilles-Gaston"
      }
    ],
    "container-title": "Langages",
    "id": "Granger1971",
    "issue": "21",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "language": "fr-FR",
    "page": "71-87",
    "title": "Langue et systèmes formels",
    "type": "article-journal"
  },
  {
    "DOI": "10.4000/books.pusl.9698",
    "ISBN": "9782802803010",
    "author": [
      {
        "family": "Granger",
        "given": "Gilles-Gaston"
      }
    ],
    "container-title": "Savoir, faire, espérer: Les limites de la raison",
    "editor": [
      {
        "family": "Van Camp",
        "given": "Henri"
      }
    ],
    "id": "Granger1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "language": "fr-FR",
    "page": "143-169",
    "publisher": "Presses de l’Université Saint-Louis",
    "title": "Sur le traitement comme objets des faits humains",
    "type": "chapter"
  },
  {
    "URL": "http://www.jstor.org/stable/43036314",
    "author": [
      {
        "family": "Robert",
        "given": "Jean-Dominique"
      }
    ],
    "container-title": "Archives de Philosophie",
    "id": "Robert1981",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "language": "fr-FR",
    "page": "415-434",
    "title": "Précisions nouvelles de Gilles-Gaston Granger en épistémologie des sciences de l’homme",
    "type": "article-journal",
    "volume": "44"
  },
  {
    "DOI": "10.1007/bf01252572",
    "ISSN": "1573-1103",
    "author": [
      {
        "family": "Granger",
        "given": "Gilles Gaston"
      }
    ],
    "container-title": "Man and World",
    "id": "Granger1969",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "language": "fr-FR",
    "page": "386--409",
    "title": "Propositions pour un positivisme",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.1016/j.shpsa.2018.09.009",
    "author": [
      {
        "family": "Fabry",
        "given": "Lucie"
      }
    ],
    "container-title": "Studies in History and Philosophy of Science Part A",
    "id": "Fabry2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "34-42",
    "title": "Phenomenotechnique: Bachelard’s critical inheritance of conventionalism",
    "title-short": "Phenomenotechnique",
    "type": "article-journal",
    "volume": "75"
  },
  {
    "DOI": "10.1086/289838",
    "author": [
      {
        "family": "Castelão-Lawless",
        "given": "Teresa"
      }
    ],
    "container-title": "Philosophy of Science",
    "id": "Castelao-Lawless1995",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "language": "en-US",
    "page": "44-59",
    "title": "Phenomenotechnique in historical perspective: Its origins and implications for philosophy of science",
    "title-short": "Phenomenotechnique in historical perspective",
    "type": "article-journal",
    "volume": "62"
  },
  {
    "ISBN": "978-0-262-51791-1",
    "author": [
      {
        "family": "Wouters",
        "given": "Paul"
      },
      {
        "family": "Beaulieu",
        "given": "Anne"
      },
      {
        "family": "Scharnhorst",
        "given": "Andrea"
      },
      {
        "family": "Wyatt",
        "given": "Sally"
      }
    ],
    "id": "Wouters2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Virtual knowledge: Experimenting in the humanities and the social sciences",
    "title-short": "Virtual knowledge",
    "type": "book"
  },
  {
    "DOI": "10.1007/s00287-020-01245-8",
    "ISSN": "1432-122X",
    "author": [
      {
        "family": "Picca",
        "given": "Davide"
      }
    ],
    "container-title": "Informatik Spektrum",
    "id": "Picca2020",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "28--39",
    "title": "From intelligent to wise machines: Why a poem is worth more than 1 million tweets",
    "title-short": "From intelligent to wise machines",
    "type": "article-journal",
    "volume": "43"
  },
  {
    "DOI": "10.2307/j.ctv12sdvf1",
    "ISBN": "9789048537419",
    "URL": "http://hdl.handle.net/20.500.12657/39371",
    "annote": "Open access",
    "author": [
      {
        "family": "Rieder",
        "given": "Bernhard"
      }
    ],
    "id": "Rieder2020",
    "issued": {
      "date-parts": [
        [
          2020,
          5
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Amsterdam University Press",
    "title": "Engines of order",
    "type": "article-journal"
  },
  {
    "DOI": "10.17613/R92N-T207",
    "URL": "https://doi.org/10.17613/R92N-T207",
    "annote": "This article appeared in The Chronicle of Higher Education on March 27, 2019, under the title “Dear Humanists: Fear Not the Digital Revolution.” (https://www.chronicle.com/article/dear-humanists-fear-not-the-digital-revolution/)",
    "author": [
      {
        "family": "Underwood",
        "given": "Ted"
      }
    ],
    "id": "Underwood2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "title": "Reclaiming ground for the humanities",
    "type": "webpage"
  },
  {
    "DOI": "10.3998/mpub.11657120",
    "ISBN": "9780472127269",
    "author": [
      {
        "family": "Lockridge",
        "given": "Tim"
      },
      {
        "family": "Van Ittersum",
        "given": "Derek"
      }
    ],
    "id": "Lockridge2020",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "publisher": "University of Michigan Press",
    "publisher-place": "Ann Arbor, MI",
    "title": "Writing workflows: Beyond word processing",
    "title-short": "Writing workflows",
    "type": "book"
  },
  {
    "DOI": "10.48197/fot2020a",
    "editor": [
      {
        "family": "Hegland",
        "given": "Frode Alexander"
      }
    ],
    "id": "Hegland2020",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Future Text Publishing",
    "title": "The future of text",
    "type": "book"
  },
  {
    "DOI": "10.4000/enquete.1043",
    "author": [
      {
        "family": "Gardin",
        "given": "Jean-Claude"
      }
    ],
    "container-title": "Enquête",
    "id": "Gardin1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "language": "fr-FR",
    "page": "35-54",
    "title": "Le questionnement logiciste et les conflits d’interprétation",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.7202/800018ar",
    "ISSN": "1920-1346",
    "author": [
      {
        "family": "Nattiez",
        "given": "Jean-Jacques"
      }
    ],
    "container-title": "Cahier de linguistique",
    "id": "Nattiez1973",
    "issued": {
      "date-parts": [
        [
          1973
        ]
      ]
    },
    "language": "fr-FR",
    "page": "219-240",
    "title": "De la sémiologie à la sémantique",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "ISSN": "0011-5266",
    "URL": "http://jstor.org/stable/20026529",
    "author": [
      {
        "family": "Kemeny",
        "given": "John G."
      }
    ],
    "container-title": "Daedalus",
    "id": "Kemeny1959",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1959
        ]
      ]
    },
    "language": "en-US",
    "page": "577-591",
    "title": "Mathematics without numbers",
    "type": "article-journal",
    "volume": "88"
  },
  {
    "DOI": "10.4000/msh.2962",
    "ISSN": "1950-6821",
    "URL": "http://dx.doi.org/10.4000/msh.2962",
    "author": [
      {
        "family": "Armatte",
        "given": "Michel"
      }
    ],
    "container-title": "Mathématiques et sciences humaines",
    "id": "Armatte2005",
    "issue": "172",
    "issued": {
      "date-parts": [
        [
          2005,
          12
        ]
      ]
    },
    "language": "fr-FR",
    "page": "91-123",
    "title": "La notion de modèle dans les sciences sociales: anciennes et nouvelles significations",
    "title-short": "La notion de modèle dans les sciences sociales",
    "type": "article-journal"
  },
  {
    "DOI": "10.3917/puf.berth.2012.01.0407",
    "URL": "https://www.cairn.info/epistemologie-des-sciences-sociales--9782130607243-page-407.htm",
    "author": [
      {
        "family": "Gardin",
        "given": "Jean-Claude"
      }
    ],
    "chapter-number": "4",
    "container-title": "Épistémologie des sciences sociales",
    "editor": [
      {
        "family": "Berthelot",
        "given": "Jean-Michel"
      }
    ],
    "id": "Gardin2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "fr-FR",
    "page": "407-454",
    "publisher": "Presses Universitaires de France",
    "title": "Modèles et récits",
    "type": "chapter"
  },
  {
    "DOI": "10.5311/josis.2020.20.664",
    "ISSN": "1948-660X",
    "author": [
      {
        "family": "Goodchild",
        "given": "Michael F."
      }
    ],
    "container-title": "Journal of Spatial Information Science",
    "id": "Goodchild2020",
    "issue": "20",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "title": "How well do we really know the world? Uncertainty in GIScience",
    "title-short": "How well do we really know the world?",
    "type": "article-journal"
  },
  {
    "DOI": "10.4000/quaderni.1417",
    "ISSN": "2105-2956",
    "author": [
      {
        "family": "Clavert",
        "given": "Frédéric"
      },
      {
        "family": "Schafer",
        "given": "Valérie"
      }
    ],
    "container-title": "Quaderni",
    "id": "Clavert2019",
    "issue": "98",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "fr-FR",
    "page": "33--49",
    "title": "Les humanités numériques, un enjeu historique",
    "type": "article-journal"
  },
  {
    "DOI": "10.1371/journal.pone.0243300",
    "ISSN": "1932-6203",
    "author": [
      {
        "family": "Vidgen",
        "given": "Bertie"
      },
      {
        "family": "Derczynski",
        "given": "Leon"
      }
    ],
    "container-title": "PLOS ONE",
    "id": "Vidgen2020",
    "issue": "12",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "e0243300",
    "title": "Directions in abusive language training data, a systematic review: Garbage in, garbage out",
    "title-short": "Directions in abusive language training data, a systematic review",
    "type": "article-journal",
    "volume": "15"
  },
  {
    "DOI": "10.1177/0963662520942122",
    "ISSN": "1361-6609",
    "author": [
      {
        "family": "Gustafson",
        "given": "Abel"
      },
      {
        "family": "Rice",
        "given": "Ronald E."
      }
    ],
    "container-title": "Public Understanding of Science",
    "id": "Gustafson_2020",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "614-633",
    "title": "A review of the effects of uncertainty in public science communication",
    "type": "article-journal",
    "volume": "29"
  },
  {
    "DOI": "10.4000/socio.1451",
    "ISSN": "2425-2158",
    "author": [
      {
        "family": "Mounier",
        "given": "Pierre"
      }
    ],
    "container-title": "Socio",
    "id": "Mounier2015",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "fr-FR",
    "page": "97-112",
    "title": "Une “utopie politique” pour les humanités numériques?",
    "type": "article-journal"
  },
  {
    "DOI": "10.4000/socio.1296",
    "ISSN": "2425-2158",
    "author": [
      {
        "family": "Gefen",
        "given": "Alexandre"
      }
    ],
    "container-title": "Socio",
    "id": "Gefen2015",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "fr-FR",
    "page": "61-74",
    "title": "Les enjeux épistémologiques des humanités numériques",
    "type": "article-journal"
  },
  {
    "DOI": "10.4000/socio.1262",
    "ISSN": "2425-2158",
    "author": [
      {
        "family": "Desbois",
        "given": "Henri"
      }
    ],
    "container-title": "Socio",
    "id": "Desbois2015",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "fr-FR",
    "page": "39-60",
    "title": "La carte et le territoire à l’ère numérique",
    "type": "article-journal"
  },
  {
    "DOI": "10.1007/s42520-020-00322-2",
    "ISSN": "2197-6082",
    "author": [
      {
        "family": "König",
        "given": "Mareike"
      }
    ],
    "container-title": "Neue Politische Literatur",
    "id": "Koenig2020",
    "issued": {
      "date-parts": [
        [
          2020,
          11
        ]
      ]
    },
    "language": "de-DE",
    "title": "Die digitale Transformation als reflexiver <i>turn</i>: Einführende Literatur zur digitalen Geschichte im Überblick",
    "title-short": "Die digitale Transformation als reflexiver <i>turn</i>",
    "type": "article-journal"
  },
  {
    "DOI": "10.4135/9781526470546",
    "ISBN": "9781526470546",
    "author": [
      {
        "family": "Barnet",
        "given": "Belinda"
      }
    ],
    "chapter-number": "15",
    "container-title": "The SAGE handbook of Web history",
    "editor": [
      {
        "family": "Brügger",
        "given": "Niels"
      },
      {
        "family": "Milligan",
        "given": "Ian"
      }
    ],
    "id": "Barnet2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "215-226",
    "publisher": "SAGE",
    "title": "Hypertext before the Web – or, what the Web could have been",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/s10502-005-2594-8",
    "ISSN": "1573-7519",
    "author": [
      {
        "family": "Furner",
        "given": "Jonathan"
      }
    ],
    "container-title": "Archival Science",
    "id": "Furner2004",
    "issue": "3–4",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "language": "en-US",
    "page": "233--265",
    "title": "Conceptual analysis: A method for understanding information as evidence, and evidence as information",
    "title-short": "Conceptual analysis",
    "type": "article-journal",
    "volume": "4"
  },
  {
    "ISBN": "978-0-471-73545-8",
    "author": [
      {
        "family": "Fisher",
        "given": "Peter F."
      }
    ],
    "chapter-number": "13",
    "container-title": "Geographical information systems: Principles, techniques, management and applications. Abridged",
    "edition": "2",
    "editor": [
      {
        "family": "Longley",
        "given": "Paul A."
      },
      {
        "family": "Goodchild",
        "given": "Michael F."
      },
      {
        "family": "Maguire",
        "given": "David J."
      },
      {
        "family": "Rhind",
        "given": "David W."
      }
    ],
    "id": "Fisher2005",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "language": "en-US",
    "page": "191-205",
    "publisher": "Wiley",
    "title": "Models of uncertainty in spatial data",
    "type": "chapter"
  },
  {
    "URL": "http://research.gold.ac.uk/id/eprint/7942/",
    "abstract": "This thesis is a study of scientific practice in computational physics. It is based on an 18 month period of ethnographic research at the Imperial College Applied Modelling and Computation Group. Using a theoretical framework drawn from practice theory, science studies, and historical epistemology, I study how simulations are developed and used in science. Emphasising modelling as a process, I explore how software provides a distinctive kind of material for doing science on computers and how images and writings of various kinds are folded into the research process. Through concrete examples the thesis charts how projects are devised and evolve and how they draw together materials and technologies into semi-stable configurations that crystallise around the objects of their concern, what Hans-Jörg Rheinberger dubbed “epistemic things”epistemic things.The main pivot of the research, however, is the connection of practice-theoretical science studies with the philosophy of Gaston Bachelard, whose concept of “phenomenotechnique”phenomenotechnique facilitates a rationalist reading of scientific practice. Rather than treating reason as a singular logic or method, or as a faculty of the mind, Bachelard points us towards processes of change within actual scientific research, a dynamic reason immanent to processes of skilled engagement. Combining this study of reason with the more recent attention to things within research from materialist and semiotic traditions, I also revive a new sense for the term “representation”representation, tracing the multiple relationships and shifting identities and differences that are involved in representing. I thus develop a theory of simulation that implies a non-representationalist concept of representing and a non-teleological concept of reason.",
    "author": [
      {
        "family": "Spencer",
        "given": "Matt"
      }
    ],
    "genre": "PhD thesis",
    "id": "Spencer2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "keyword": "Simulation, modelling, computational physics, computational science, e-science, scientific computing, scientific software, science studies, practice theory, historical epistemology",
    "language": "en-US",
    "publisher": "Goldsmiths, University of London",
    "title": "Reason and representation in scientific simulation",
    "type": "thesis"
  },
  {
    "DOI": "10.1007/978-94-011-3492-7_10",
    "author": [
      {
        "family": "Granger",
        "given": "Gilles-Gaston"
      }
    ],
    "collection-number": "18",
    "collection-title": "Episteme",
    "container-title": "The problem of reductionism in science",
    "editor": [
      {
        "family": "Agazzi",
        "given": "Evandro"
      }
    ],
    "id": "Granger1991",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "language": "en-US",
    "page": "173-182",
    "publisher": "Kluwer",
    "title": "Must a science of artificial intelligence be necessarily reductionist?",
    "type": "chapter"
  },
  {
    "DOI": "10.1038/454034a",
    "ISSN": "1476-4687",
    "author": [
      {
        "family": "Turchin",
        "given": "Peter"
      }
    ],
    "container-title": "Nature",
    "id": "Turchin2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "language": "en-US",
    "page": "34--35",
    "title": "Arise “cliodynamics”",
    "type": "article-journal",
    "volume": "454"
  },
  {
    "DOI": "10.7202/044084ar",
    "ISSN": "1712-8307",
    "author": [
      {
        "family": "Stein",
        "given": "Christian"
      }
    ],
    "container-title": "Nouvelles perspectives en sciences sociales",
    "id": "Stein2010",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "language": "fr-FR",
    "page": "227-279",
    "title": "L’historien et ses modèles",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "ISSN": "0395-2649",
    "URL": "https://persee.fr/doc/ahess_0395-2649_1988_num_43_1_283472",
    "author": [
      {
        "family": "Lepetit",
        "given": "Bernard"
      }
    ],
    "container-title": "Annales ESC",
    "id": "Lepetit1988",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1988
        ]
      ]
    },
    "language": "fr-FR",
    "page": "3-4",
    "title": "Présentation",
    "type": "article-journal",
    "volume": "43"
  },
  {
    "DOI": "10.4000/nda.3453",
    "ISSN": "2425-1941",
    "annote": "Preface to a special issue on Jean-Claude Gardin",
    "author": [
      {
        "family": "Djindjian",
        "given": "François"
      },
      {
        "family": "Moscati",
        "given": "Paola"
      }
    ],
    "container-title": "Les Nouvelles de l’archéologie",
    "id": "Djindjian2016a",
    "issue": "144",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "fr-FR",
    "page": "3",
    "title": "Préface",
    "type": "article-journal"
  },
  {
    "DOI": "10.4000/nda.3456",
    "ISSN": "2425-1941",
    "author": [
      {
        "family": "Djindjian",
        "given": "François"
      }
    ],
    "container-title": "Les Nouvelles de l’archéologie",
    "id": "Djindjian2016b",
    "issue": "144",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "fr-FR",
    "page": "4-9",
    "title": "Jean-Claude Gardin (1925-2013), un archéologue libre!",
    "type": "article-journal"
  },
  {
    "DOI": "10.4000/nda.3457",
    "ISSN": "2425-1941",
    "author": [
      {
        "family": "Moscati",
        "given": "Paola"
      }
    ],
    "container-title": "Les Nouvelles de l’archéologie",
    "id": "Moscati2016",
    "issue": "144",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "page": "10-13",
    "title": "Jean-Claude Gardin and the evolution of archaeological computing",
    "type": "article-journal"
  },
  {
    "DOI": "10.4000/nda.3461",
    "ISSN": "2425-1941",
    "author": [
      {
        "family": "Gallay",
        "given": "Alain"
      }
    ],
    "container-title": "Les Nouvelles de l’archéologie",
    "id": "Gallay2016",
    "issue": "144",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "fr-FR",
    "page": "14-21",
    "title": "Jean-Claude Gardin et les stratégies de recherche en archéologie",
    "type": "article-journal"
  },
  {
    "DOI": "10.4000/nda.3468",
    "ISSN": "2425-1941",
    "author": [
      {
        "family": "Lyonnet",
        "given": "Bertille"
      }
    ],
    "container-title": "Les Nouvelles de l’archéologie",
    "id": "Lyonnet2016",
    "issue": "144",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "fr-FR",
    "page": "21-24",
    "title": "De la théorie à la pratique, les travaux de Jean-Claude Gardin en Asie centrale",
    "type": "article-journal"
  },
  {
    "DOI": "10.4000/nda.3470",
    "ISSN": "2425-1941",
    "author": [
      {
        "family": "Vitali",
        "given": "Vanda"
      }
    ],
    "container-title": "Les Nouvelles de l’archéologie",
    "id": "Vitali2016",
    "issue": "144",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "page": "24-26",
    "title": "On being mentored by Jean-Claude Gardin: A letter to young archaeologists",
    "title-short": "On being mentored by Jean-Claude Gardin",
    "type": "article-journal"
  },
  {
    "DOI": "10.4000/nda.3472",
    "ISSN": "2425-1941",
    "author": [
      {
        "family": "Semeraro",
        "given": "Grazia"
      }
    ],
    "container-title": "Les Nouvelles de l’archéologie",
    "id": "Semeraro2016",
    "issue": "144",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "page": "26-28",
    "title": "Form, function and descriptive analysis in archaeology",
    "type": "article-journal"
  },
  {
    "annote": "Contains reproduction of Warren Weaver’s \"Translation\" memo, http://www.mt-archive.info/Weaver-1949.pdf",
    "collection-title": "Technology press books in the social sciences",
    "editor": [
      {
        "family": "Locke",
        "given": "William N."
      },
      {
        "family": "Booth",
        "given": "A. Donald"
      }
    ],
    "id": "Locke1955",
    "issued": {
      "date-parts": [
        [
          1955
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Technology Press of the MIT and Wiley",
    "publisher-place": "New York, NY",
    "title": "Machine translation of languages: Fourteen essays",
    "title-short": "Machine translation of languages",
    "type": "book"
  },
  {
    "URL": "http://www.mt-archive.info/Weaver-1949.pdf",
    "author": [
      {
        "family": "Weaver",
        "given": "Warren"
      }
    ],
    "id": "Weaver1949",
    "issued": {
      "date-parts": [
        [
          1949,
          7,
          15
        ]
      ]
    },
    "language": "en-US",
    "title": "Translation",
    "type": ""
  },
  {
    "URL": "http://www.mt-archive.info/MTS-1999-Hutchins.pdf",
    "author": [
      {
        "family": "Hutchins",
        "given": "John"
      }
    ],
    "container-title": "Proceedings of MT Summit VII",
    "event": "MT summit VII",
    "event-date": {
      "date-parts": [
        [
          1999,
          9,
          13
        ],
        [
          1999,
          9,
          17
        ]
      ]
    },
    "event-place": "Kent Ridge Digital Labs, Singapore",
    "id": "Hutchins1999",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "language": "en-US",
    "page": "30-34",
    "publisher": "AAMT",
    "title": "Retrospect and prospect in computer-based translation: MT in the great translation era",
    "title-short": "Retrospect and prospect in computer-based translation",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1016/j.resstr.2006.06.005",
    "ISSN": "0734-3310",
    "author": [
      {
        "family": "Johnston",
        "given": "Bill"
      },
      {
        "family": "Webber",
        "given": "Sheila"
      }
    ],
    "container-title": "Research Strategies",
    "id": "Johnston2005",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "language": "en-US",
    "page": "108-121",
    "title": "As we may think: Information literacy as a discipline for the information age",
    "title-short": "As we may think",
    "type": "article-journal",
    "volume": "20"
  },
  {
    "ISBN": "978-3-030-18313-4",
    "author": [
      {
        "family": "Busa",
        "given": "Roberto"
      }
    ],
    "editor": [
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Passarotti",
        "given": "Marco"
      }
    ],
    "id": "Nyhan2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "publisher-place": "Cham",
    "title": "One origin of digital humanities: Fr Roberto Busa in his own words",
    "title-short": "One origin of digital humanities",
    "type": "book"
  },
  {
    "ISBN": "9780674158900",
    "annote": "Available from the Internet Archive https://archive.org/details/concordancetoliv0000pack",
    "author": [
      {
        "family": "Packard",
        "given": "David W."
      }
    ],
    "id": "Packard1968",
    "issued": {
      "date-parts": [
        [
          1968
        ]
      ]
    },
    "language": "en-US",
    "number-of-volumes": "4",
    "publisher": "Harvard University Press",
    "publisher-place": "Cambridge, MA",
    "title": "A concordance to Livy",
    "type": "book"
  },
  {
    "ISBN": "9781732841086",
    "author": [
      {
        "family": "Graham",
        "given": "Shawn"
      }
    ],
    "id": "Graham2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Digital Press at the University of North Dakota",
    "publisher-place": "Grand Forks, ND",
    "title": "Failing gloriously and other essays",
    "type": "book"
  },
  {
    "annote": "http://referentiel.nouvelobs.com/archives_pdf/OBS0182_19680508/OBS0182_19680508_036.pdf, http://referentiel.nouvelobs.com/archives_pdf/OBS0182_19680508/OBS0182_19680508_037.pdf, http://referentiel.nouvelobs.com/archives_pdf/OBS0182_19680508/OBS0182_19680508_037.pdf",
    "author": [
      {
        "family": "Boudon",
        "given": "Raymond"
      }
    ],
    "container-title": "Le Nouvel Observateur",
    "id": "Boudon1968",
    "issue": "182",
    "issued": {
      "date-parts": [
        [
          1968,
          5,
          8
        ]
      ]
    },
    "language": "fr-FR",
    "page": "36-38",
    "title": "La machine à accélérer le temps",
    "type": "article-journal"
  },
  {
    "annote": "http://referentiel.nouvelobs.com/archives_pdf/OBS0182_19680508/OBS0182_19680508_038.pdf, http://referentiel.nouvelobs.com/archives_pdf/OBS0182_19680508/OBS0182_19680508_039.pdf",
    "author": [
      {
        "family": "Le Roy Ladurie",
        "given": "Emmanuel"
      }
    ],
    "container-title": "Le Nouvel Observateur",
    "id": "LeRoyLadurie1968",
    "issue": "182",
    "issued": {
      "date-parts": [
        [
          1968,
          5,
          8
        ]
      ]
    },
    "language": "fr-FR",
    "page": "38-39",
    "title": "La fin des érudits",
    "type": "article-journal"
  },
  {
    "annote": "Available from the Internet Archive: https://archive.org/details/leterritoiredelh0000lero",
    "author": [
      {
        "family": "Le Roy Ladurie",
        "given": "Emmanuel"
      }
    ],
    "id": "LeRoyLadurie1973a",
    "issued": {
      "date-parts": [
        [
          1973
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Gallimard",
    "title": "Le territoire de l’historien",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Le Roy Ladurie",
        "given": "Emmanuel"
      }
    ],
    "container-title": "Le territoire de l’historien",
    "id": "LeRoyLadurie1973b",
    "issued": {
      "date-parts": [
        [
          1973
        ]
      ]
    },
    "language": "fr-FR",
    "page": "11-14",
    "publisher": "Gallimard",
    "title": "L’historien et l’ordinateur",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/bf02400110",
    "author": [
      {
        "family": "Fleury",
        "given": "Philippe"
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Fleury1986",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1986
        ]
      ]
    },
    "language": "fr-FR",
    "page": "239-245",
    "title": "Du texte latin à la concordance imprimée",
    "type": "article-journal",
    "volume": "20"
  },
  {
    "ISBN": "978-2-7606-3203-5",
    "author": [
      {
        "family": "Sinatra",
        "given": "Michael E."
      },
      {
        "family": "Vitali-Rosati",
        "given": "Marcello"
      }
    ],
    "id": "Sinatra2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Les Presses de l’Université de Montréal",
    "title": "Pratiques de l’édition numérique",
    "type": "book"
  },
  {
    "ISBN": "978-2-7606-3203-5",
    "URL": "http://www.parcoursnumeriques-pum.ca/1-pratiques/chapitre3.html",
    "accessed": {
      "date-parts": [
        [
          2022,
          4,
          9
        ]
      ]
    },
    "author": [
      {
        "family": "Sinatra",
        "given": "Michael E."
      },
      {
        "family": "Vitali-Rosati",
        "given": "Marcello"
      }
    ],
    "chapter-number": "3",
    "container-title": "Pratiques de l’édition numérique",
    "editor": [
      {
        "family": "Sinatra",
        "given": "Michael E."
      },
      {
        "family": "Vitali-Rosati",
        "given": "Marcello"
      }
    ],
    "id": "Sinatra2014-ch3",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "fr-FR",
    "page": "49-60",
    "publisher": "Les Presses de l’Université de Montréal",
    "publisher-place": "Montréal",
    "title": "Histoire des humanités numériques",
    "type": "chapter"
  },
  {
    "URL": "https://hal.archives-ouvertes.fr/hal-02403315",
    "author": [
      {
        "family": "Guichard",
        "given": "Éric"
      }
    ],
    "id": "Guichard2019",
    "issued": {
      "date-parts": [
        [
          2019,
          8
        ]
      ]
    },
    "language": "fr-FR",
    "note": "À paraître dans: Ilouz, Charles; Huerta, Antoine, “Amériques-Europe, les humanités numériques en partage? Enjeux, innovations et perspectives”, Les Indes Savantes",
    "title": "Les humanités numériques n’existent pas",
    "type": "manuscript"
  },
  {
    "DOI": "10.4000/socio.791",
    "container-title": "Socio",
    "editor": [
      {
        "family": "Diminescu",
        "given": "Dana"
      },
      {
        "family": "Wieviorka",
        "given": "Michel"
      }
    ],
    "id": "Diminescu2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "fr-FR",
    "title": "Le tournant numérique… et après?",
    "type": "article-journal"
  },
  {
    "author": [
      {
        "family": "Christian",
        "given": "Brian"
      },
      {
        "family": "Griffiths",
        "given": "Tom"
      }
    ],
    "id": "Christian2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Picador",
    "title": "Algorithms to live by: The computer science of human decisions",
    "title-short": "Algorithms to live by",
    "type": "book"
  },
  {
    "ISBN": "978-3-549-07368-1",
    "author": [
      {
        "family": "Demandt",
        "given": "Alexander"
      }
    ],
    "id": "Demandt2010",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Propyläen",
    "title": "Es hätte auch anders kommen können: Wendepunkte deutscher Geschichte",
    "title-short": "Es hätte auch anders kommen können",
    "type": "book"
  },
  {
    "URL": "https://n2t.net/ark:/13960/t4dn48b0g",
    "author": [
      {
        "family": "Weber",
        "given": "Max"
      }
    ],
    "id": "Weber1922",
    "issued": {
      "date-parts": [
        [
          1922
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Mohr",
    "title": "Gesammelte Aufsätze zur Wissenschaftslehre",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Weber",
        "given": "Max"
      }
    ],
    "container-title": "Gesammelte Aufsätze zur Wissenschaftslehre",
    "id": "Weber1922-215",
    "issued": {
      "date-parts": [
        [
          1922
        ]
      ]
    },
    "language": "de-DE",
    "original-date": {
      "date-parts": [
        [
          1905
        ]
      ]
    },
    "page": "215-290",
    "publisher": "Mohr",
    "title": "Kritische Studien auf dem Gebiet der kulturwissenschaftlichen Logik",
    "type": "chapter"
  },
  {
    "DOI": "10.1145/3231587",
    "ISSN": "1557-7317",
    "author": [
      {
        "family": "Nardelli",
        "given": "Enrico"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Nardelli2019",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "32-35",
    "title": "Do we really need computational thinking?",
    "type": "article-journal",
    "volume": "62"
  },
  {
    "ISSN": "0036-8733",
    "author": [
      {
        "family": "Weiser",
        "given": "Mark"
      }
    ],
    "container-title": "Scientific American",
    "id": "Weiser1991",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1991,
          9
        ]
      ]
    },
    "language": "en-US",
    "page": "94-104",
    "title": "The computer for the 21<sup>st</sup> century",
    "type": "article-journal",
    "volume": "265"
  },
  {
    "URL": "http://ceur-ws.org/Vol-2723/short16.pdf",
    "author": [
      {
        "family": "Piotrowski",
        "given": "Michael"
      },
      {
        "family": "Fafinski",
        "given": "Mateusz"
      }
    ],
    "container-title": "CHR2020: Proceedings of the workshop on computational humanities research",
    "editor": [
      {
        "family": "Karsdorp",
        "given": "Folgert"
      },
      {
        "family": "McGillivray",
        "given": "Barbara"
      },
      {
        "family": "Nerghes",
        "given": "Adina"
      },
      {
        "family": "Wevers",
        "given": "Melvin"
      }
    ],
    "event-date": {
      "date-parts": [
        [
          2020,
          11,
          18
        ],
        [
          2020,
          11,
          20
        ]
      ]
    },
    "event-place": "Amsterdam",
    "id": "Piotrowski2020c",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "171-181",
    "publisher": "CEUR Workshop Proceedings",
    "title": "Nothing new under the sun? Computational humanities and the methodology of history",
    "type": "paper-conference"
  },
  {
    "DOI": "10.5749/j.ctt1cn6thb.17",
    "author": [
      {
        "family": "Clement",
        "given": "Tanya E."
      }
    ],
    "chapter-number": "14",
    "container-title": "Debates in the digital humanities 2016",
    "id": "Clement2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "page": "153--175",
    "publisher": "University of Minnesota Press",
    "title": "Where is methodology in digital humanities?",
    "type": "chapter"
  },
  {
    "DOI": "10.1177/2053951714528481",
    "ISSN": "2053-9517",
    "author": [
      {
        "family": "Kitchin",
        "given": "Rob"
      }
    ],
    "container-title": "Big Data & Society",
    "id": "Kitchin2014",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "page": "1-12",
    "title": "Big Data, new epistemologies and paradigm shifts",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "author": [
      {
        "family": "Bachelard",
        "given": "Gaston"
      }
    ],
    "edition": "10",
    "id": "Bachelard1968",
    "issued": {
      "date-parts": [
        [
          1968
        ]
      ]
    },
    "language": "fr-FR",
    "original-date": {
      "date-parts": [
        [
          1934
        ]
      ]
    },
    "publisher": "Les Presses universitaires de France",
    "title": "Le nouvel esprit scientifique",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Bachelard",
        "given": "Gaston"
      }
    ],
    "edition": "1<sup>re</sup> édition critique",
    "id": "Bachelard2020",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "fr-FR",
    "original-date": {
      "date-parts": [
        [
          1934
        ]
      ]
    },
    "publisher": "Les Presses universitaires de France",
    "title": "Le nouvel esprit scientifique",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Bachelard",
        "given": "Gaston"
      }
    ],
    "id": "Bachelard1984",
    "issued": {
      "date-parts": [
        [
          1984
        ]
      ]
    },
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1934
        ]
      ]
    },
    "publisher": "Beacon Press",
    "title": "The new scientific spirit",
    "translator": [
      {
        "family": "Goldhammer",
        "given": "Arthur"
      }
    ],
    "type": "book"
  },
  {
    "ISBN": "978-3-89244-454-1",
    "author": [
      {
        "family": "Rheinberger",
        "given": "Hans-Jörg"
      }
    ],
    "id": "Rheinberger2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Wallstein",
    "publisher-place": "Göttingen",
    "title": "Experimentalsysteme und epistemische Dinge: Eine Geschichte der Proteinsynthese im Reagenzglas",
    "title-short": "Experimentalsysteme und epistemische Dinge",
    "type": "book"
  },
  {
    "DOI": "10.1162/106361405774288026",
    "ISSN": "1530-9274",
    "author": [
      {
        "family": "Rheinberger",
        "given": "Hans-Jörg"
      }
    ],
    "container-title": "Perspectives on Science",
    "id": "Rheinberger2005",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2005
        ]
      ]
    },
    "language": "en-US",
    "page": "313-328",
    "title": "Gaston Bachelard and the notion of “phenomenotechnique”",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "DOI": "10.1017/S0003055402000278",
    "ISSN": "0003-0554",
    "abstract": "Social scientists interested in explaining historical processes can, indeed should, refuse the choice between modeling causal relationships and studying history. Identifying temporality as the defining characteristic of processes that can be meaningfully distinguished as “history,” I show that modeling such phenomena engenders particular difficulties but is both possible and fruitful. Narratives, as a way of presenting empirical information, have distinctive strengths that make them especially suited for historical scholarship, and structuring the narratives based on the model allows us to treat them as data on which to test the model. At the same time, this use of narratives raises methodological problems not identified in recent debates. I specify these problems, analyze their implications, and suggest ways of solving or minimizing them. There is no inherent incompatibility between—but much potential gain from—modeling history and using historical narratives as data.",
    "author": [
      {
        "family": "Büthe",
        "given": "Tim"
      }
    ],
    "container-title": "The American Political Science Review",
    "id": "Buethe2002",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2002
        ]
      ]
    },
    "language": "en-US",
    "page": "481-493",
    "title": "Taking temporality seriously: Modeling history and the use of narratives as evidence",
    "title-short": "Taking temporality seriously",
    "type": "article-journal",
    "volume": "96"
  },
  {
    "DOI": "10.5771/1615-634X-2021-1-46",
    "ISSN": "1615-634X",
    "author": [
      {
        "dropping-particle": "van",
        "family": "Es",
        "given": "Karin"
      },
      {
        "family": "Schäfer",
        "given": "Mirko Tobias"
      },
      {
        "family": "Maranke",
        "given": "Wieringa"
      }
    ],
    "container-title": "M&K Medien & Kommunikationswissenschaft",
    "id": "Es2021",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "page": "46-64",
    "title": "Tool criticism and the computational turn: A “methodological moment” in media and communication studies",
    "title-short": "Tool criticism and the computational turn",
    "type": "article-journal",
    "volume": "69"
  },
  {
    "DOI": "10.1002/1520-6696(197807)14:3<247::aid-jhbs2300140311>3.0.co;2-r",
    "ISSN": "1520-6696",
    "author": [
      {
        "family": "Sedelow",
        "given": "Walter A."
      },
      {
        "family": "Sedelow",
        "given": "Sally Yeates"
      }
    ],
    "container-title": "Journal of the History of the Behavioral Sciences",
    "id": "Sedelow1978",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1978
        ]
      ]
    },
    "language": "en-US",
    "page": "247-263",
    "title": "Formalized historiography, the structure of scientific and literary texts: Part i. Some issues posed by computational methodology",
    "title-short": "Formalized historiography, the structure of scientific and literary texts",
    "type": "article-journal",
    "volume": "14"
  },
  {
    "URL": "http://www.jstor.org/stable/657925",
    "author": [
      {
        "family": "Franzosi",
        "given": "Roberto"
      },
      {
        "family": "Mohr",
        "given": "John W."
      }
    ],
    "container-title": "Theory and Society",
    "id": "Franzosi1997",
    "issue": "2/3",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "language": "en-US",
    "page": "133-160",
    "title": "New directions in formalization and historical analysis",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "DOI": "10.1080/09608788.2014.949217",
    "ISSN": "1469-3526",
    "author": [
      {
        "family": "Betti",
        "given": "Arianna"
      },
      {
        "dropping-particle": "van den",
        "family": "Berg",
        "given": "Hein"
      }
    ],
    "container-title": "British Journal for the History of Philosophy",
    "id": "Betti2014",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "page": "812-835",
    "title": "Modelling the history of ideas",
    "type": "article-journal",
    "volume": "22"
  },
  {
    "DOI": "10.1007/s10955-012-0604-9",
    "ISSN": "1572-9613",
    "author": [
      {
        "family": "Stauffer",
        "given": "Dietrich"
      }
    ],
    "container-title": "Journal of Statistical Physics",
    "id": "Stauffer2012",
    "issue": "1–2",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "page": "9--20",
    "title": "A biased review of sociophysics",
    "type": "article-journal",
    "volume": "151"
  },
  {
    "DOI": "10.3406/ahess.1958.2781",
    "ISSN": "1953-8146",
    "author": [
      {
        "family": "Braudel",
        "given": "Fernand"
      }
    ],
    "container-title": "Annales ESC",
    "id": "Braudel1958",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1958
        ]
      ]
    },
    "language": "fr-FR",
    "page": "725-753",
    "title": "Histoire et Sciences sociales: La longue durée",
    "title-short": "Histoire et Sciences sociales",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "URL": "http://www.jstor.org/stable/40647704",
    "abstract": "This is a new translation of Braudel’s classic article on the longue durée, on the occasion of the 50th anniversary of its original publication. The article lays out Braudel’s concept of the multiplicity of social times and their respective utilities in the analysis of historical phenomena. It specifically criticizes the emphasis on short-term episodic and idiographic history (l’histoire événementielle) on the one hand and the emphasis on the use of eternal concepts in the social sciences, what he calls the très longue durée. He explicates the necessity of analyzing the longue durée (long, but not eternal), as well as what he calls la conjoncture, the cyclical movements within the longue durée.",
    "author": [
      {
        "family": "Braudel",
        "given": "Fernand"
      }
    ],
    "container-title": "Review (Fernand Braudel Center)",
    "id": "Braudel2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1958
        ]
      ]
    },
    "page": "171-203",
    "title": "History and the social sciences: The longue durée",
    "title-short": "History and the social sciences",
    "translator": [
      {
        "family": "Wallerstein",
        "given": "Immanuel"
      }
    ],
    "type": "article-journal",
    "volume": "32"
  },
  {
    "DOI": "10.1002/9781118766804.wbiect111",
    "author": [
      {
        "family": "Brennen",
        "given": "J. Scott"
      },
      {
        "family": "Kreiss",
        "given": "Daniel"
      }
    ],
    "container-title": "The international encyclopedia of communication theory and philosophy",
    "editor": [
      {
        "family": "Jensen",
        "given": "Klaus Bruhn"
      },
      {
        "family": "Rothenbuhler",
        "given": "Eric W."
      },
      {
        "family": "Pooley",
        "given": "Jefferson D."
      },
      {
        "family": "Craig",
        "given": "Robert T."
      }
    ],
    "id": "Brennen2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Wiley",
    "title": "Digitalization",
    "type": "chapter"
  },
  {
    "DOI": "10.1080/23801883.2017.1304162",
    "ISSN": "2380-1891",
    "author": [
      {
        "family": "Hill",
        "given": "Mark J."
      }
    ],
    "container-title": "Global Intellectual History",
    "id": "Hill2016",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "page": "130-150",
    "title": "Invisible interpretations: Reflections on the digital humanities and intellectual history",
    "title-short": "Invisible interpretations",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "author": [
      {
        "family": "Buzzetti",
        "given": "Dino"
      }
    ],
    "container-title": "Rendiconti dell’Accademia Nazionale dei Lincei: Classe di Scienze morali, storiche e filologiche, S. ix",
    "editor": [
      {
        "family": "Moscati",
        "given": "Paola"
      },
      {
        "family": "Orlandi",
        "given": "Tito"
      }
    ],
    "id": "Buzzetti2019",
    "issue": "1–2",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "it-IT",
    "page": "71-103",
    "title": "Alle origini dell’Informatica Umanistica: Humanities Computing e/o Digital Humanities",
    "title-short": "Alle origini dell’Informatica Umanistica",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "DOI": "10.5399/uo/hsda.6.1.3",
    "ISSN": "2158-3846",
    "author": [
      {
        "family": "Buzzetti",
        "given": "Dino"
      }
    ],
    "container-title": "Humanist Studies & the Digital Age",
    "id": "Buzzetti2019-en",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "32--58",
    "title": "The origins of humanities computing and the digital humanities turn",
    "translator": [
      {
        "family": "Lollini",
        "given": "Massimo"
      }
    ],
    "type": "article-journal",
    "volume": "6"
  },
  {
    "edition": "2<sup>e</sup> édition, revue et augmentée",
    "editor": [
      {
        "family": "Gardin",
        "given": "Jean-Claude"
      },
      {
        "family": "Lagrange",
        "given": "Marie-Salomé"
      },
      {
        "family": "Martin",
        "given": "Jean-Maurice"
      },
      {
        "family": "Molino",
        "given": "Jean"
      },
      {
        "family": "Natali-Smit",
        "given": "Johanna"
      }
    ],
    "id": "Gardin1987",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Maison des sciences de l’homme",
    "title": "La Logique du plausible: Essais d’epistemologie pratique en sciences humaines",
    "title-short": "La Logique du plausible",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Abelson",
        "given": "Harold"
      },
      {
        "family": "Sussman",
        "given": "Gerald Jay"
      }
    ],
    "edition": "2",
    "editor": [
      {
        "family": "Sussman",
        "given": "Julie"
      }
    ],
    "id": "Abelson1996",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA",
    "title": "Structure and interpretation of computer programs",
    "type": "book"
  },
  {
    "DOI": "10.1109/mc.1987.1663532",
    "author": [
      {
        "family": "Brooks",
        "given": "Frederick P."
      }
    ],
    "container-title": "Computer",
    "id": "Brooks1987",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "language": "en-US",
    "page": "10-19",
    "title": "No silver bullet: Essence and accidents of software engineering",
    "title-short": "No silver bullet",
    "type": "article-journal",
    "volume": "20"
  },
  {
    "URL": "https://web.media.mit.edu/~minsky/papers/Why programming is--.html",
    "author": [
      {
        "family": "Minsky",
        "given": "Marvin"
      }
    ],
    "container-title": "Design and planning II: Computers in design and communication",
    "editor": [
      {
        "family": "Krampen",
        "given": "N."
      },
      {
        "family": "Seitz",
        "given": "P."
      }
    ],
    "event": "International conference on design and planning",
    "event-date": {
      "date-parts": [
        [
          1966
        ]
      ]
    },
    "event-place": "University of Waterloo, Ontario",
    "id": "Minsky1967",
    "issued": {
      "date-parts": [
        [
          1967
        ]
      ]
    },
    "language": "en-US",
    "page": "120-125",
    "publisher": "Hastings House",
    "title": "Why programming is a good medium for expressing poorly understood and sloppily formulated ideas",
    "type": "paper-conference"
  },
  {
    "DOI": "10.1111/cogs.12065",
    "author": [
      {
        "family": "Pearl",
        "given": "Judea"
      }
    ],
    "container-title": "Cognitive Science",
    "id": "Pearl2013",
    "issue": "6",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "en-US",
    "page": "977-985",
    "title": "Structural counterfactuals: A brief introduction",
    "title-short": "Structural counterfactuals",
    "type": "article-journal",
    "volume": "37"
  },
  {
    "DOI": "10.1080/03080188.2020.1831227",
    "ISSN": "1743-2790",
    "author": [
      {
        "family": "McCarty",
        "given": "Willard"
      }
    ],
    "container-title": "Interdisciplinary Science Reviews",
    "id": "McCarty2021",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "page": "325-362",
    "title": "As perceived, not as known: Digital enquiry and the art of intelligence",
    "title-short": "As perceived, not as known",
    "type": "article-journal",
    "volume": "46"
  },
  {
    "URL": "https://aclanthology.org/W08-0216",
    "author": [
      {
        "family": "Stone",
        "given": "Matthew"
      }
    ],
    "container-title": "Proceedings of the third workshop on issues in teaching computational linguistics",
    "editor": [
      {
        "family": "Palmer",
        "given": "Martha"
      },
      {
        "family": "Brew",
        "given": "Chris"
      },
      {
        "family": "Xia",
        "given": "Fei"
      }
    ],
    "event-date": {
      "date-parts": [
        [
          2008,
          6,
          19
        ],
        [
          2008,
          6,
          20
        ]
      ]
    },
    "event-place": "Columbus, OH",
    "id": "Stone2008",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "language": "en-US",
    "page": "129-136",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Stroudsburg, PA, USA",
    "title": "Support collaboration by teaching fundamentals",
    "type": "paper-conference"
  },
  {
    "ISBN": "978-2-02-023693-5",
    "author": [
      {
        "family": "Moles",
        "given": "Abraham A."
      }
    ],
    "editor": [
      {
        "family": "Rohmer-Moles",
        "given": "Élisabeth"
      }
    ],
    "id": "Moles1995",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "language": "fr-FR",
    "original-date": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "publisher": "Seuil",
    "title": "Les sciences de l’imprécis",
    "type": "book"
  },
  {
    "annote": "BCUL: https://renouvaud1.primo.exlibrisgroup.com/permalink/41BCULAUSA_LIB/1vikse1/alma991000517469702852",
    "author": [
      {
        "family": "Grignon",
        "given": "Claude"
      }
    ],
    "container-title": "Le modèle et le récit",
    "editor": [
      {
        "family": "Grenier",
        "given": "Jean-Yves"
      },
      {
        "family": "Grignon",
        "given": "Claude"
      },
      {
        "family": "Menger",
        "given": "Pierre-Michel"
      }
    ],
    "id": "Grignon2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "language": "fr-FR",
    "page": "7-43",
    "publisher": "Éd. de la Maison des sciences de l’homme",
    "title": "La formalisation et les sciences du récit: le cas de la sociologie",
    "title-short": "La formalisation et les sciences du récit",
    "type": "chapter"
  },
  {
    "DOI": "10.3917/zil.006.0070",
    "ISSN": "2553-6133",
    "author": [
      {
        "family": "Plutniak",
        "given": "Sébastien"
      }
    ],
    "container-title": "Zilsel",
    "id": "Plutniak2019",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "fr-FR",
    "page": "70-115",
    "title": "À distances raisonnables des structuralismes: logique, langage, formalisation et sciences de l’homme",
    "title-short": "À distances raisonnables des structuralismes",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "DOI": "10.1109/mis.2009.36",
    "ISSN": "1541-1672",
    "abstract": "At Brown University, there is excitement of having access to the Brown Corpus, containing one million English words. Since then, we have seen several notable corpora that are about 100 times larger, and in 2006, Google released a trillion-word corpus with frequency counts for all sequences up to five words long. In some ways this corpus is a step backwards from the Brown Corpus: it’s taken from unfiltered Web pages and thus contains incomplete sentences, spelling errors, grammatical errors, and all sorts of other errors. It’s not annotated with carefully hand-corrected part-of-speech tags. But the fact that it’s a million times larger than the Brown Corpus outweighs these drawbacks. A trillion-word corpus - along with other Web-derived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions - captures even very rare aspects of human behavior. So, this corpus could serve as the basis of a complete model for certain tasks - if only we knew how to extract the model from the data.",
    "author": [
      {
        "family": "Halevy",
        "given": "Alon"
      },
      {
        "family": "Norvig",
        "given": "Peter"
      },
      {
        "family": "Pereira",
        "given": "Fernando"
      }
    ],
    "container-title": "IEEE Intelligent Systems",
    "id": "Halevy2009",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "page": "8--12",
    "title": "The unreasonable effectiveness of data",
    "type": "article-journal",
    "volume": "24"
  },
  {
    "DOI": "10.1007/978-94-6265-084-8",
    "ISBN": "978-94-6265-084-8",
    "ISSN": "2215-003X",
    "author": [
      {
        "family": "Viret",
        "given": "Marjolaine"
      }
    ],
    "collection-title": "ASSER international sports law series",
    "id": "Viret2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "publisher": "T.M.C. Asser Press",
    "title": "Evidence in anti-doping at the intersection of science and law",
    "type": "article-journal"
  },
  {
    "DOI": "10.1093/lpr/mgy021",
    "ISSN": "1470-840X",
    "author": [
      {
        "family": "Ben-Haim",
        "given": "Yakov"
      }
    ],
    "container-title": "Law, Probability and Risk",
    "id": "Ben-Haim2019",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "77-95",
    "title": "Assessing “beyond a reasonable doubt” without probability: An info-gap perspective",
    "title-short": "Assessing “beyond a reasonable doubt” without probability",
    "type": "article-journal",
    "volume": "18"
  },
  {
    "DOI": "10.1086/686261",
    "ISSN": "1537-5366",
    "author": [
      {
        "family": "Weisbach",
        "given": "David"
      }
    ],
    "container-title": "Journal of Legal Studies",
    "id": "Weisbach2015",
    "issue": "S2",
    "issued": {
      "date-parts": [
        [
          2015,
          6
        ]
      ]
    },
    "language": "en-US",
    "page": "S319-S335",
    "title": "Introduction: Legal decision making under deep uncertainty",
    "title-short": "Introduction",
    "type": "article-journal",
    "volume": "44"
  },
  {
    "ISBN": "0-300-07477-8",
    "URL": "http://n2t.net/ark:/13960/t43r9764b",
    "author": [
      {
        "family": "Mazlish",
        "given": "Bruce"
      }
    ],
    "id": "Mazlish1998",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Yale University Press",
    "publisher-place": "New Haven, CT",
    "title": "The uncertain sciences",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Mazlish",
        "given": "Bruce"
      }
    ],
    "edition": "With a new introduction by the author",
    "id": "Mazlish2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "publisher": "Routledge",
    "title": "The uncertain sciences",
    "type": "book"
  },
  {
    "DOI": "10.1073/pnas.2102061118",
    "ISSN": "1091-6490",
    "annote": "See https://twitter.com/benmschmidt/status/1419497587296571395",
    "author": [
      {
        "family": "Bollen",
        "given": "Johan"
      },
      {
        "dropping-particle": "ten",
        "family": "Thij",
        "given": "Marijn"
      },
      {
        "family": "Breithaupt",
        "given": "Fritz"
      },
      {
        "family": "Barron",
        "given": "Alexander T. J."
      },
      {
        "family": "Rutter",
        "given": "Lauren A."
      },
      {
        "family": "Lorenzo-Luaces",
        "given": "Lorenzo"
      },
      {
        "family": "Scheffer",
        "given": "Marten"
      }
    ],
    "container-title": "Proceedings of the National Academy of Sciences",
    "id": "Bollen2021",
    "issue": "30",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "page": "e2102061118",
    "title": "Historical language records reveal a surge of cognitive distortions in recent decades",
    "type": "article-journal",
    "volume": "118"
  },
  {
    "DOI": "10.1038/s41467-020-18566-7",
    "ISSN": "2041-1723",
    "annote": "See https://mfafinski.github.io/Historical_data/",
    "author": [
      {
        "family": "Safra",
        "given": "Lou"
      },
      {
        "family": "Chevallier",
        "given": "Coralie"
      },
      {
        "family": "Grèzes",
        "given": "Julie"
      },
      {
        "family": "Baumard",
        "given": "Nicolas"
      }
    ],
    "container-title": "Nature Communications",
    "id": "Safra2020",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "title": "Tracking historical changes in trustworthiness using machine learning analyses of facial cues in paintings",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "ISSN": "1938-4122",
    "URL": "http://digitalhumanities.org/dhq/vol/5/1/000091/000091.html",
    "author": [
      {
        "family": "Drucker",
        "given": "Johanna"
      }
    ],
    "container-title": "Digital Humanities Quarterly",
    "id": "Drucker2011",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "language": "en-US",
    "title": "Humanities approaches to graphical display",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "DOI": "10.4324/9781315576251",
    "abstract": "Digital Humanities is becoming an increasingly popular focus of academic endeavour. There are now hundreds of Digital Humanities centres worldwide and the subject is taught at both postgraduate and undergraduate level. Yet the term ’Digital Humanities’ is much debated. This reader brings together, for the first time, in one core volume the essential readings that have emerged in Digital Humanities. We provide a historical overview of how the term ’Humanities Computing’ developed into the term ’Digital Humanities’, and highlight core readings which explore the meaning, scope, and implementation of the field. To contextualize and frame each included reading, the editors and authors provide a commentary on the original piece. There is also an annotated bibliography of other material not included in the text to provide an essential list of reading in the discipline. This text will be required reading for scholars and students who want to discover the history of Digital Humanities through its core writings, and for those who wish to understand the many possibilities that exist when trying to define Digital Humanities.",
    "author": [
      {
        "family": "Vanhoutte",
        "given": "Edward"
      }
    ],
    "chapter-number": "6",
    "container-title": "Defining digital humanities",
    "editor": [
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Vanhoutte",
        "given": "Edward"
      }
    ],
    "id": "Vanhoutte2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "page": "119-156",
    "publisher": "Ashgate",
    "publisher-place": "Farnham",
    "title": "The gates of hell: History and definition of digital | humanities | computing",
    "title-short": "The gates of hell",
    "type": "chapter"
  },
  {
    "DOI": "10.4324/9781315576251",
    "abstract": "Digital Humanities is becoming an increasingly popular focus of academic endeavour. There are now hundreds of Digital Humanities centres worldwide and the subject is taught at both postgraduate and undergraduate level. Yet the term ’Digital Humanities’ is much debated. This reader brings together, for the first time, in one core volume the essential readings that have emerged in Digital Humanities. We provide a historical overview of how the term ’Humanities Computing’ developed into the term ’Digital Humanities’, and highlight core readings which explore the meaning, scope, and implementation of the field. To contextualize and frame each included reading, the editors and authors provide a commentary on the original piece. There is also an annotated bibliography of other material not included in the text to provide an essential list of reading in the discipline. This text will be required reading for scholars and students who want to discover the history of Digital Humanities through its core writings, and for those who wish to understand the many possibilities that exist when trying to define Digital Humanities.",
    "author": [
      {
        "family": "Rosenbloom",
        "given": "Paul"
      }
    ],
    "chapter-number": "11",
    "container-title": "Defining digital humanities",
    "editor": [
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Nyhan",
        "given": "Julianne"
      },
      {
        "family": "Vanhoutte",
        "given": "Edward"
      }
    ],
    "id": "Rosenbloom2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "keyword": "digital_humanities",
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "page": "119-233",
    "publisher": "Ashgate",
    "publisher-place": "Farnham",
    "title": "Toward a conceptual framework for the digital humanities",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/s10816-015-9241-3",
    "ISSN": "1573-7764",
    "author": [
      {
        "family": "Dallas",
        "given": "Costis"
      }
    ],
    "container-title": "Journal of Archaeological Method and Theory",
    "id": "Dallas2016",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "page": "305-330",
    "title": "Jean-Claude Gardin on archaeological data, representation and knowledge: Implications for digital archaeology",
    "title-short": "Jean-Claude Gardin on archaeological data, representation and knowledge",
    "type": "article-journal",
    "volume": "23"
  },
  {
    "DOI": "10.3998/dh.12544152.0001.001",
    "editor": [
      {
        "family": "Kee",
        "given": "Kevin"
      }
    ],
    "id": "Kee2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "publisher": "University of Michigan Press",
    "title": "Pastplay: Teaching and learning history with technology",
    "title-short": "Pastplay",
    "type": "book"
  },
  {
    "DOI": "10.3998/dh.12544152.0001.001",
    "author": [
      {
        "family": "Ramsay",
        "given": "Stephen"
      }
    ],
    "chapter-number": "5",
    "container-title": "Pastplay: Teaching and learning history with technology",
    "editor": [
      {
        "family": "Kee",
        "given": "Kevin"
      }
    ],
    "id": "Ramsay2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "page": "111-120",
    "publisher": "University of Michigan Press",
    "title": "The hermeneutics of screwing around; or what you do with a million books: Teaching and learning history with technology",
    "title-short": "The hermeneutics of screwing around; or what you do with a million books",
    "type": "chapter"
  },
  {
    "DOI": "10.22148/001c.18509",
    "ISSN": "2371-4549",
    "author": [
      {
        "family": "Algee-Hewitt",
        "given": "Mark"
      },
      {
        "family": "Porter",
        "given": "J. D."
      },
      {
        "family": "Walser",
        "given": "Hannah"
      }
    ],
    "container-title": "Journal of Cultural Analytics",
    "id": "Algee-Hewitt2020",
    "issued": {
      "date-parts": [
        [
          2020,
          12
        ]
      ]
    },
    "language": "en-US",
    "title": "Representing race and ethnicity in American fiction, 1789–1920",
    "type": "article-journal"
  },
  {
    "DOI": "10.22148/001c.21182",
    "ISSN": "2371-4549",
    "author": [
      {
        "family": "Erlin",
        "given": "Matt"
      },
      {
        "family": "Piper",
        "given": "Andrew"
      },
      {
        "family": "Knox",
        "given": "Douglas"
      },
      {
        "family": "Pentecost",
        "given": "Stephen"
      },
      {
        "family": "Drouillard",
        "given": "Michaela"
      },
      {
        "family": "Powell",
        "given": "Brian"
      },
      {
        "family": "Townson",
        "given": "Cienna"
      }
    ],
    "container-title": "Journal of Cultural Analytics",
    "id": "Erlin2021",
    "issued": {
      "date-parts": [
        [
          2021,
          2
        ]
      ]
    },
    "language": "en-US",
    "title": "Cultural capitals: Modeling minor European literature",
    "title-short": "Cultural capitals",
    "type": "article-journal"
  },
  {
    "DOI": "10.22148/001c.18238",
    "ISSN": "2371-4549",
    "author": [
      {
        "family": "Agersnap",
        "given": "Anne"
      },
      {
        "family": "Kristensen-McLachlan",
        "given": "Ross Deans"
      },
      {
        "family": "Johansen",
        "given": "Kirstine Helboe"
      },
      {
        "family": "Schjødt",
        "given": "Uffe"
      },
      {
        "family": "Nielbo",
        "given": "Kristoffer Laigaard"
      }
    ],
    "container-title": "Journal of Cultural Analytics",
    "id": "Agersnap2020",
    "issued": {
      "date-parts": [
        [
          2020,
          12
        ]
      ]
    },
    "language": "en-US",
    "title": "Sermons as data: Introducing a corpus of 11,955 Danish sermons",
    "title-short": "Sermons as data",
    "type": "article-journal"
  },
  {
    "DOI": "10.22148/001c.18841",
    "ISSN": "2371-4549",
    "author": [
      {
        "family": "Soni",
        "given": "Sandeep"
      },
      {
        "family": "Klein",
        "given": "Lauren F."
      },
      {
        "family": "Eisenstein",
        "given": "Jacob"
      }
    ],
    "container-title": "Journal of Cultural Analytics",
    "id": "Soni2021",
    "issued": {
      "date-parts": [
        [
          2021,
          1
        ]
      ]
    },
    "language": "en-US",
    "title": "Abolitionist networks: Modeling language change in nineteenth-century activist newspapers",
    "title-short": "Abolitionist networks",
    "type": "article-journal"
  },
  {
    "DOI": "10.22148/001c.12266",
    "ISSN": "2371-4549",
    "author": [
      {
        "family": "Jofre",
        "given": "Ana"
      },
      {
        "family": "Cole",
        "given": "Josh"
      },
      {
        "family": "Berardi",
        "given": "Vincent"
      },
      {
        "family": "Bennett",
        "given": "Carl"
      },
      {
        "family": "Reale",
        "given": "Michael"
      }
    ],
    "container-title": "Journal of Cultural Analytics",
    "id": "Jofre2020",
    "issued": {
      "date-parts": [
        [
          2020,
          3
        ]
      ]
    },
    "language": "en-US",
    "title": "What’s in a face? Gender representation of faces in Time, 1940s–1990s",
    "type": "article-journal"
  },
  {
    "DOI": "10.7551/mitpress/13618.001.0001",
    "ISBN": "978-0-262-36313-6",
    "author": [
      {
        "family": "McShane",
        "given": "Marjorie"
      },
      {
        "family": "Nirenburg",
        "given": "Sergei"
      }
    ],
    "id": "McShane2021",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "Linguistics for the age of AI",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-662-62989-5",
    "ISBN": "978-3-662-62988-8",
    "editor": [
      {
        "family": "Hauck-Thum",
        "given": "Uta"
      },
      {
        "family": "Noller",
        "given": "Jörg"
      }
    ],
    "id": "Hauck-Thum2021",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Metzler",
    "title": "Was ist Digitalität? Philosophische und pädagogische Perspektiven",
    "title-short": "Was ist Digitalität?",
    "type": "book"
  },
  {
    "DOI": "10.5406/j.ctvfjd0mf",
    "ISBN": "978-0-252-05111-1",
    "author": [
      {
        "family": "Dobson",
        "given": "James E."
      }
    ],
    "id": "Dobson2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "University of Illinois Press",
    "title": "Critical digital humanities: The search for a methodology",
    "title-short": "Critical digital humanities",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Gadamer",
        "given": "Hans-Georg"
      }
    ],
    "container-title": "Gesammelte Werke",
    "edition": "6",
    "id": "Gadamer1990",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "language": "de-DE",
    "original-date": {
      "date-parts": [
        [
          1960
        ]
      ]
    },
    "publisher": "Mohr Siebeck",
    "title": "Wahrheit und Methode: Grundzüge einer philosophischen Hermeneutik",
    "type": "chapter",
    "volume": "1",
    "volume-title": "Hermeneutik"
  },
  {
    "author": [
      {
        "family": "Gadamer",
        "given": "Hans-Georg"
      }
    ],
    "container-title": "Gesammelte Werke: Ergänzungen, Register",
    "edition": "6",
    "id": "Gadamer1993",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "language": "de-DE",
    "original-date": {
      "date-parts": [
        [
          1986
        ]
      ]
    },
    "publisher": "Mohr Siebeck",
    "title": "Wahrheit und Methode",
    "type": "chapter",
    "volume": "2",
    "volume-title": "Hermeneutik: Ergänzungen, Register"
  },
  {
    "author": [
      {
        "family": "Gadamer",
        "given": "Hans-Georg"
      }
    ],
    "edition": "Second, revised edition",
    "id": "Gadamer1989",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Continuum",
    "title": "Truth and method",
    "translator": [
      {
        "family": "Weinsheimer",
        "given": "Joel"
      },
      {
        "family": "Marshall",
        "given": "Donald"
      }
    ],
    "type": "book"
  },
  {
    "DOI": "10.1007/s11841-016-0535-z",
    "ISSN": "1873-930X",
    "author": [
      {
        "family": "Romano",
        "given": "Claude"
      }
    ],
    "container-title": "Sophia",
    "id": "Romano2016",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "page": "393-402",
    "title": "The flexible rule of the hermeneut",
    "type": "article-journal",
    "volume": "56"
  },
  {
    "URL": "https://histanthro.org/notes/nine-pails-of-ashes/",
    "author": [
      {
        "family": "Geoghegan",
        "given": "Bernard Dionysius"
      }
    ],
    "container-title": "History of Anthropology Review",
    "id": "Geoghegan2021",
    "issued": {
      "date-parts": [
        [
          2021,
          8
        ]
      ]
    },
    "language": "en-US",
    "title": "Nine pails of ashes: Social networks, genocide, and the structuralists’ database of language",
    "title-short": "Nine pails of ashes",
    "type": "article-journal",
    "volume": "45"
  },
  {
    "DOI": "10.1177/0952695119864241",
    "ISSN": "1461-720X",
    "author": [
      {
        "family": "Geoghegan",
        "given": "Bernard Dionysius"
      }
    ],
    "container-title": "History of the Human Sciences",
    "id": "Geoghegan2020",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "52-79",
    "title": "Textocracy, or, the cybernetic logic of french theory",
    "type": "article-journal",
    "volume": "33"
  },
  {
    "URL": "http://www.bernardg.com/publications",
    "author": [
      {
        "family": "Geoghegan",
        "given": "Bernard Dionysius"
      }
    ],
    "genre": "PhD thesis",
    "id": "Geoghegan2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Northwestern University and Bauhaus-Universität Weimar",
    "title": "The cybernetic apparatus: Media, liberalism, and the reform of the human sciences",
    "title-short": "The cybernetic apparatus",
    "type": "thesis"
  },
  {
    "DOI": "10.1215/10407391-2420021",
    "ISSN": "1527-1986",
    "author": [
      {
        "family": "Galloway",
        "given": "Alexander R."
      }
    ],
    "container-title": "differences",
    "id": "Galloway2014",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "page": "107-131",
    "title": "The cybernetic hypothesis",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "ISBN": "92-3-201988-4",
    "author": [
      {
        "family": "Apostel",
        "given": "Leo"
      },
      {
        "family": "Benoist",
        "given": "Jean-Marie"
      },
      {
        "family": "Bottomore",
        "given": "Tom Burton"
      },
      {
        "family": "Boulding",
        "given": "Kenneth Ewart"
      },
      {
        "family": "Dufrenne",
        "given": "Mikel"
      },
      {
        "family": "Eliade",
        "given": "Mircea"
      },
      {
        "family": "Furtado",
        "given": "Celso"
      },
      {
        "family": "Gusdorf",
        "given": "Georges"
      },
      {
        "family": "Krishna",
        "given": "Daya"
      },
      {
        "family": "Mommsen",
        "given": "Wolfgang J."
      },
      {
        "family": "Morin",
        "given": "Edgar"
      },
      {
        "family": "Piattelli-Palmarini",
        "given": "Massimo"
      },
      {
        "family": "Sinaceur",
        "given": "Mohammed Allal"
      },
      {
        "family": "Smirnov"
      },
      {
        "family": "Stanislav Nikolaevitch",
        "given": "Smirnov"
      },
      {
        "family": "Ui",
        "given": "Jun"
      }
    ],
    "id": "Apostel1983",
    "issued": {
      "date-parts": [
        [
          1983
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Unesco",
    "publisher-place": "Paris",
    "title": "Interdisciplinarité et sciences humaines",
    "type": "book",
    "volume": "1"
  },
  {
    "DOI": "10.3406/hism.1987.1328",
    "ISSN": "0982-1783",
    "author": [
      {
        "family": "Duncan Baretta",
        "given": "Silvio R."
      },
      {
        "family": "Markoff",
        "given": "John"
      },
      {
        "family": "Shapiro",
        "given": "Gilbert"
      }
    ],
    "container-title": "Histoire & Mesure",
    "id": "Duncan_Baretta1987",
    "issue": "3–4",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "language": "en-US",
    "page": "115-172",
    "title": "The selective transmission of historical documents: The case of the parish <i>cahiers</i> of 1789",
    "title-short": "The selective transmission of historical documents",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.3406/colan.1975.4231",
    "ISSN": "0336-1500",
    "author": [
      {
        "family": "Richaudeau",
        "given": "François"
      }
    ],
    "container-title": "Communication et langages",
    "id": "Richaudeau1975",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1975
        ]
      ]
    },
    "language": "fr-FR",
    "page": "78-91",
    "title": "Marshall McLuhan ou Robert Escarpit? L’avenir du livre",
    "type": "article-journal",
    "volume": "27"
  },
  {
    "ISBN": "3-8253-0312-8",
    "author": [
      {
        "family": "Gadamer",
        "given": "Hans-Georg"
      }
    ],
    "edition": "2",
    "editor": [
      {
        "family": "Dutt",
        "given": "Carsten"
      }
    ],
    "id": "Gadamer1995",
    "issued": {
      "date-parts": [
        [
          1995
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Winter",
    "publisher-place": "Heidelberg",
    "title": "Hermeneutik – Ästhetik – praktische Philosophie: Hans-Georg Gadamer im Gespräch",
    "title-short": "Hermeneutik – Ästhetik – praktische Philosophie",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Tugendhat",
        "given": "Ernst"
      }
    ],
    "container-title": "Times Literary Supplement",
    "id": "Tugendhat1978",
    "issued": {
      "date-parts": [
        [
          1978,
          5,
          19
        ]
      ]
    },
    "language": "en-US",
    "page": "565",
    "title": "The fusion of horizons",
    "type": "article-journal"
  },
  {
    "DOI": "10.1007/978-94-010-1823-4",
    "ISBN": "978-94-010-1823-4",
    "editor": [
      {
        "family": "Manninen",
        "given": "Juha"
      },
      {
        "family": "Tuomela",
        "given": "Raimo"
      }
    ],
    "id": "Manninen1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Reidel",
    "title": "Essays on explanation and understanding: Studies in the foundations of humanities and social sciences",
    "title-short": "Essays on explanation and understanding",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-94-010-1823-4_18",
    "ISBN": "978-94-010-1823-4",
    "author": [
      {
        "dropping-particle": "von",
        "family": "Wright",
        "given": "Georg Henrik"
      }
    ],
    "container-title": "Essays on explanation and understanding: Studies in the foundations of humanities and social sciences",
    "editor": [
      {
        "family": "Manninen",
        "given": "Juha"
      },
      {
        "family": "Tuomela",
        "given": "Raimo"
      }
    ],
    "id": "vonWright1976",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "language": "en-US",
    "page": "415-435",
    "publisher": "Reidel",
    "title": "Determinism and the study of man: Studies in the foundations of humanities and social sciences",
    "title-short": "Determinism and the study of man",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/bf00142755",
    "ISSN": "1572-851X",
    "annote": "Fleming borrows the term \"methodological parallelism\" from vonWright1976",
    "author": [
      {
        "family": "Fleming",
        "given": "Patricia Ann"
      }
    ],
    "container-title": "Human Studies",
    "id": "Fleming1990",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1990
        ]
      ]
    },
    "language": "en-US",
    "page": "221-236",
    "title": "Paul ricœur’s methodological parallelism",
    "type": "article-journal",
    "volume": "13"
  },
  {
    "DOI": "10.3406/rfp.2004.3250",
    "ISSN": "0556-7807",
    "author": [
      {
        "family": "Bressoux",
        "given": "Pascal"
      }
    ],
    "container-title": "Revue française de pédagogie",
    "id": "Bressoux2004",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "language": "fr-FR",
    "page": "61-74",
    "title": "Formalisation et modélisation dans les sciences sociales: une étude de la construction du jugement des enseignants",
    "title-short": "Formalisation et modélisation dans les sciences sociales",
    "type": "article-journal",
    "volume": "148"
  },
  {
    "URL": "http://mimesisbooks.com/index.php/mim/catalog/book/10",
    "collection-title": "Quaderni di «Filosofia»",
    "editor": [
      {
        "family": "Ciracì",
        "given": "Fabio"
      },
      {
        "family": "Fedriga",
        "given": "Riccardo"
      },
      {
        "family": "Marras",
        "given": "Cristina"
      }
    ],
    "id": "Ciraci2021",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "it-IT",
    "publisher": "Mimesis",
    "title": "Filosofia digitale",
    "type": "book"
  },
  {
    "DOI": "10.1163/24523666-00502005",
    "ISSN": "2452-3666",
    "author": [
      {
        "family": "McVeigh",
        "given": "Simon"
      }
    ],
    "container-title": "Research Data Journal for the Humanities and Social Sciences",
    "id": "McVeigh2020",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "50-61",
    "title": "Rescuing a heritage database: Some lessons from london concert life in the eighteenth century",
    "title-short": "Rescuing a heritage database",
    "type": "article-journal",
    "volume": "5"
  },
  {
    "author": [
      {
        "family": "Ceccato",
        "given": "Silvio"
      }
    ],
    "id": "Ceccato1964",
    "issued": {
      "date-parts": [
        [
          1964
        ]
      ]
    },
    "language": "it-IT",
    "number-of-volumes": "2",
    "publisher": "Marsilio",
    "title": "Un tecnico fra i filosofi",
    "type": "book"
  },
  {
    "DOI": "10.1177/20539517211047725",
    "ISSN": "2053-9517",
    "author": [
      {
        "family": "Törnberg",
        "given": "Petter"
      },
      {
        "family": "Uitermark",
        "given": "Justus"
      }
    ],
    "container-title": "Big Data & Society",
    "id": "Toernberg2021",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "title": "For a heterodox computational social science",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "ISBN": "978-0-674-41707-6",
    "author": [
      {
        "family": "Kirschenbaum",
        "given": "Matthew G."
      }
    ],
    "id": "Kirschenbaum2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "publisher": "The Belknap Press of Harvard University Press",
    "publisher-place": "Cambridge, MA",
    "title": "Track changes: A literary history of word processing",
    "title-short": "Track changes",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Sammet",
        "given": "Jean E."
      }
    ],
    "id": "Sammet1969",
    "issued": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Prentice-Hall",
    "title": "Programming languages: History and fundamentals",
    "title-short": "Programming languages",
    "type": "book"
  },
  {
    "DOI": "10.1145/363848.363863",
    "ISSN": "0001-0782",
    "abstract": "An online, interactive system for text editing is described in detail, with remarks on the theoretical and experimental justification for its form. Emphasis throughout the system is on providing maximum convenience and power for the user. Notable features are its ability to handle any piece of text, the content-searching facility, and the character-by-character editing operations. The editor can be programmed to a limited extent.",
    "author": [
      {
        "family": "Deutsch",
        "given": "L. Peter"
      },
      {
        "family": "Lampson",
        "given": "Butler W."
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Deutsch1967",
    "issue": "12",
    "issued": {
      "date-parts": [
        [
          1967
        ]
      ]
    },
    "language": "en-US",
    "page": "793-799",
    "title": "An online editor",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "DOI": "10.1109/MAHC.2009.127",
    "author": [
      {
        "family": "Murphy",
        "given": "Dan"
      }
    ],
    "container-title": "IEEE Annals of the History of Computing",
    "id": "Murphy2009",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "page": "110-115",
    "title": "The beginnings of TECO",
    "type": "article-journal",
    "volume": "31"
  },
  {
    "URL": "http://n2t.net/ark:/13960/t54f6tp7f",
    "author": [
      {
        "family": "Berman",
        "given": "P. I."
      }
    ],
    "genre": "AGARDograph",
    "id": "Berman1977",
    "issued": {
      "date-parts": [
        [
          1977
        ]
      ]
    },
    "language": "en-US",
    "number": "229",
    "publisher": "NATO Advisory Group for Aerospace Research and Development",
    "title": "Survey of computer-assisted writing and editing systems",
    "type": "report"
  },
  {
    "URL": "http://n2t.net/ark:/13960/t7xm1zq53",
    "author": [
      {
        "family": "Tesler",
        "given": "Larry"
      }
    ],
    "container-title": "Byte",
    "id": "Tesler1981",
    "issue": "8",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "language": "en-US",
    "page": "90-147",
    "title": "The Smalltalk environment",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "DOI": "10.1145/2212877.2212896",
    "ISSN": "1558-3449",
    "author": [
      {
        "family": "Tesler",
        "given": "Larry"
      }
    ],
    "container-title": "Interactions",
    "id": "Tesler2012",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "page": "70-75",
    "title": "A personal history of modeless text editing and cut/copy-paste",
    "type": "article-journal",
    "volume": "19"
  },
  {
    "URL": "http://n2t.net/ark:/13960/t0bw04r7r",
    "author": [
      {
        "family": "Smith",
        "given": "David Canfield"
      },
      {
        "family": "Irby",
        "given": "Charles"
      },
      {
        "family": "Kimball",
        "given": "Ralph"
      },
      {
        "family": "Verplank",
        "given": "Bill"
      }
    ],
    "container-title": "Byte",
    "id": "Smith1982",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "language": "en-US",
    "page": "242-282",
    "title": "Designing the Star user interface",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "editor": [
      {
        "family": "Nievergelt",
        "given": "Jürg"
      },
      {
        "family": "Coray",
        "given": "Giovanni"
      },
      {
        "family": "Nicoud",
        "given": "Jean-Daniel"
      },
      {
        "family": "Shaw",
        "given": "Alan C."
      }
    ],
    "id": "Nievergelt1982",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "language": "en-US",
    "publisher": "North-Holland",
    "title": "Document preparation systems",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Kernighan",
        "given": "Brian W."
      },
      {
        "family": "Lesk",
        "given": "Michael E."
      }
    ],
    "container-title": "Document preparation systems",
    "editor": [
      {
        "family": "Nievergelt",
        "given": "Jürg"
      },
      {
        "family": "Coray",
        "given": "Giovanni"
      },
      {
        "family": "Nicoud",
        "given": "Jean-Daniel"
      },
      {
        "family": "Shaw",
        "given": "Alan C."
      }
    ],
    "id": "Kernighan1982a",
    "issued": {
      "date-parts": [
        [
          1982
        ]
      ]
    },
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1978
        ]
      ]
    },
    "page": "1-20",
    "publisher": "North-Holland",
    "title": "UNIX document preparation",
    "type": "chapter"
  },
  {
    "DOI": "10.1515/9783110682106",
    "ISBN": "9783110682106",
    "author": [
      {
        "family": "Kemman",
        "given": "Max"
      }
    ],
    "id": "Kemman2021",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "publisher": "De Gruyter Oldenbourg",
    "title": "Trading zones of digital history",
    "type": "book"
  },
  {
    "URL": "https://www.sfsic.org/wp-inside/uploads/2021/06/questionner-humanites-numeriques.pdf",
    "editor": [
      {
        "family": "Paquienséguy",
        "given": "Françoise"
      },
      {
        "family": "Pélissier",
        "given": "Nicolas"
      }
    ],
    "id": "Paquienseguy2021",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Société Française des Sciences de l’Information et de la Communication (SFSIC), Conférence Permanente des Directeur·trices d’unités de recherche en Sciences de l’Information et de la Communication (CP-DirSIC)",
    "title": "Questionner les humanités numériques",
    "type": "book"
  },
  {
    "ISSN": "0039-324X",
    "URL": "https://press.amu.edu.pl/en/studia-metodologiczne-39.html",
    "author": [
      {
        "family": "Boruszewski",
        "given": "Jarosław"
      },
      {
        "family": "Nowak-Posadzy",
        "given": "Krzysztof"
      }
    ],
    "container-title": "Studia Metodologiczne: Dissertationes Methodologicae",
    "id": "Boruszewski2019",
    "issue": "39",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "253-296",
    "title": "From integration to modelling: On a neglected function of the methodology of humanities",
    "title-short": "From integration to modelling",
    "type": "article-journal"
  },
  {
    "URL": "https://ihsk.de/publikationen/Ulrich_Oevermann-Manifest_der_objektiv_hermeneutischen_Sozialforschung.pdf",
    "accessed": {
      "date-parts": [
        [
          2021,
          10,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Oevermann",
        "given": "Ulrich"
      }
    ],
    "id": "Oevermann2002",
    "issued": {
      "date-parts": [
        [
          2002,
          3
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Institut für hermeneutische Sozial- und Kulturforschung e. V.",
    "title": "Klinische Soziologie auf der Basis der Methodologie der objektiven Hermeneutik: Manifest der objektiv hermeneutischen Sozialforschung",
    "title-short": "Klinische Soziologie auf der Basis der Methodologie der objektiven Hermeneutik",
    "type": "webpage"
  },
  {
    "DOI": "10.1002/pan3.10256",
    "ISSN": "2575-8314",
    "author": [
      {
        "family": "Langer",
        "given": "Lars"
      },
      {
        "family": "Burghardt",
        "given": "Manuel"
      },
      {
        "family": "Borgards",
        "given": "Roland"
      },
      {
        "family": "Böhning‐Gaese",
        "given": "Katrin"
      },
      {
        "family": "Seppelt",
        "given": "Ralf"
      },
      {
        "family": "Wirth",
        "given": "Christian"
      }
    ],
    "container-title": "People and Nature",
    "id": "Langer2021",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "page": "1093-1109",
    "title": "The rise and fall of biodiversity in literature: A comprehensive quantification of historical changes in the use of vernacular labels for biological taxa in western creative literature",
    "title-short": "The rise and fall of biodiversity in literature",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.1109/tews.1968.4322339",
    "ISSN": "0018-9405",
    "author": [
      {
        "family": "Madnick",
        "given": "Stuart E."
      },
      {
        "family": "Moulton",
        "given": "Allen"
      }
    ],
    "container-title": "IEEE Transactions on Engineering Writing and Speech",
    "id": "Madnick1968",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1968
        ]
      ]
    },
    "language": "en-US",
    "page": "92-100",
    "title": "SCRIPT, an on-line manuscript processing system",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "URL": "https://aclanthology.org/www.mt-archive.info/50/TLS-1962-Masterman.pdf",
    "author": [
      {
        "family": "Masterman",
        "given": "Margaret"
      }
    ],
    "container-title": "The Times Literary Supplement",
    "id": "Masterman1962",
    "issued": {
      "date-parts": [
        [
          1962,
          4,
          27
        ]
      ]
    },
    "language": "en-US",
    "title": "The intellect’s new eye",
    "type": "article-journal",
    "volume": "284"
  },
  {
    "DOI": "10.1007/978-3-642-45321-2",
    "collection-number": "8001–8003",
    "collection-title": "LNCS",
    "editor": [
      {
        "family": "Dershowitz",
        "given": "Nachum"
      },
      {
        "family": "Nissan",
        "given": "Ephraim"
      }
    ],
    "id": "Dershowitz2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "number-of-volumes": "3",
    "publisher": "Springer",
    "title": "Language, culture, computation. Essays dedicated to Yaacov Choueka on the occasion of his 75th birthday",
    "title-short": "Language, culture, computation",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-642-45324-3_7",
    "author": [
      {
        "family": "McCarty",
        "given": "Willard"
      }
    ],
    "collection-number": "8002",
    "collection-title": "LNCS",
    "container-title": "Language, culture, computation: Computing of the humanities, law, and narratives. Essays dedicated to yaacov choueka on the occasion of his 75th birthday",
    "editor": [
      {
        "family": "Dershowitz",
        "given": "Nachum"
      },
      {
        "family": "Nissan",
        "given": "Ephraim"
      }
    ],
    "id": "McCarty2014a",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "number-of-volumes": "3",
    "page": "103-117",
    "publisher": "Springer",
    "title": "Special effects; or, the tooling is here. Where are the results? Essays dedicated to Yaacov Choueka on the occasion of his 75th birthday",
    "title-short": "Special effects; or, the tooling is here. Where are the results?",
    "type": "chapter",
    "volume": "2"
  },
  {
    "URL": "http://midwayreview.uchicago.edu/a/9/1/bradley/",
    "accessed": {
      "date-parts": [
        [
          2021,
          10,
          25
        ]
      ]
    },
    "author": [
      {
        "family": "Bradley",
        "given": "Colin"
      }
    ],
    "container-title": "The Midway Review",
    "id": "BradleyC2013",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "en-US",
    "title": "The irony of the humanities: The humanities and the datafication of sentiment",
    "title-short": "The irony of the humanities",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "URL": "https://newrepublic.com/article/114127/",
    "accessed": {
      "date-parts": [
        [
          2021,
          10,
          25
        ]
      ]
    },
    "author": [
      {
        "family": "Pinker",
        "given": "Steven"
      }
    ],
    "container-title": "The New Republic",
    "id": "Pinker2013",
    "issued": {
      "date-parts": [
        [
          2013,
          8,
          7
        ]
      ]
    },
    "language": "en-US",
    "title": "Science is not your enemy",
    "type": "article-journal"
  },
  {
    "DOI": "10.22148/001c.22335",
    "author": [
      {
        "family": "So",
        "given": "Richard Jean"
      }
    ],
    "container-title": "Journal of Cultural Analytics",
    "id": "So2021",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "title": "Introduction: Contemporary culture after data science",
    "title-short": "Introduction",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "DOI": "10.1093/oxfordhb/9780195309799.013.0010",
    "author": [
      {
        "family": "Piccinini",
        "given": "Gualtiero"
      }
    ],
    "container-title": "The oxford handbook of philosophy of cognitive science",
    "editor": [
      {
        "family": "Margolis",
        "given": "Eric"
      },
      {
        "family": "Samuels",
        "given": "Richard"
      },
      {
        "family": "Stich",
        "given": "Stephen P."
      }
    ],
    "id": "Piccinini2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "page": "222-249",
    "publisher": "Oxford University Press",
    "title": "Computationalism",
    "type": "entry-encyclopedia"
  },
  {
    "ISBN": "978-0-674-03292-7",
    "author": [
      {
        "family": "Golumbia",
        "given": "David"
      }
    ],
    "id": "Golumbia2009",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Harvard University Press",
    "title": "The cultural logic of computation",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-95465-3",
    "ISBN": "978-3-319-95465-3",
    "collection-title": "Computational social sciences",
    "editor": [
      {
        "family": "Chen",
        "given": "Shu-Heng"
      }
    ],
    "id": "Chen2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Big data in computational social science and humanities",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-319-95465-3_18",
    "ISBN": "978-3-319-95465-3",
    "author": [
      {
        "family": "Nanetti",
        "given": "Andrea"
      },
      {
        "family": "Cheong",
        "given": "Siew Ann"
      }
    ],
    "collection-title": "Computational social sciences",
    "container-title": "Big data in computational social science and humanities",
    "editor": [
      {
        "family": "Chen",
        "given": "Shu-Heng"
      }
    ],
    "id": "Nanetti2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "page": "337-363",
    "publisher": "Springer",
    "title": "Computational history: From big data to big simulations",
    "title-short": "Computational history",
    "type": "chapter"
  },
  {
    "DOI": "10.1007/978-3-319-95465-3_19",
    "ISBN": "978-3-319-95465-3",
    "author": [
      {
        "family": "Tsao",
        "given": "Chia-Rong"
      }
    ],
    "collection-title": "Computational social sciences",
    "container-title": "Big data in computational social science and humanities",
    "editor": [
      {
        "family": "Chen",
        "given": "Shu-Heng"
      }
    ],
    "id": "Tsao2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "page": "365-377",
    "publisher": "Springer",
    "title": "A posthumanist reflection on the digital humanities and social sciences",
    "type": "chapter"
  },
  {
    "ISBN": "3-534-15076-7",
    "author": [
      {
        "family": "Grondin",
        "given": "Jean"
      }
    ],
    "edition": "2., überarbeitete Auflage",
    "id": "Grondin2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "language": "de-DE",
    "original-date": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "publisher": "Wissenschaftliche Buchgesellschaft",
    "title": "Einführung in die philosophische Hermeneutik",
    "type": "book"
  },
  {
    "ISBN": "0-300-05969-8",
    "author": [
      {
        "family": "Grondin",
        "given": "Jean"
      }
    ],
    "id": "Grondin1994",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Yale University Press",
    "title": "Introduction to philosophical hermeneutics",
    "translator": [
      {
        "family": "Weinsheimer",
        "given": "Joel"
      }
    ],
    "type": "book"
  },
  {
    "DOI": "10.1007/s00146-009-0255-9",
    "author": [
      {
        "family": "Capurro",
        "given": "Rafael"
      }
    ],
    "container-title": "AI & Society",
    "id": "Capurro2010",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "page": "35-42",
    "title": "Digital hermeneutics: An outline",
    "title-short": "Digital hermeneutics",
    "type": "article-journal",
    "volume": "25"
  },
  {
    "DOI": "10.1111/hith.10827",
    "author": [
      {
        "family": "Partner",
        "given": "Nancy"
      }
    ],
    "container-title": "History and Theory",
    "id": "Partner2016",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "page": "35-53",
    "title": "Foucault’s iconic afterlife: The posthumous reach of words and things",
    "title-short": "Foucault’s iconic afterlife",
    "type": "article-journal",
    "volume": "55"
  },
  {
    "DOI": "10.1057/9781137484185",
    "ISBN": "978-1-137-48418-5",
    "author": [
      {
        "family": "Antonijević",
        "given": "Smiljana"
      }
    ],
    "id": "Antonijevic2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Palgrave Macmillan",
    "title": "Amongst digital humanists",
    "type": "book"
  },
  {
    "URL": "https://dhcertificate.org/HIST-680/sites/default/files/pdf/DH_Types_One_Two.pdf",
    "annote": "Original URL was: http://stephenramsay.us/2013/05/03/dh-one-and-two/",
    "author": [
      {
        "family": "Ramsay",
        "given": "Stephen"
      }
    ],
    "id": "Ramsay2013b",
    "issued": {
      "date-parts": [
        [
          2013,
          5,
          3
        ]
      ]
    },
    "language": "en-US",
    "title": "DH types one and two",
    "type": "webpage"
  },
  {
    "DOI": "10.2139/ssrn.1859267",
    "author": [
      {
        "family": "Bulger",
        "given": "Monica E."
      },
      {
        "family": "Meyer",
        "given": "Eric T."
      },
      {
        "dropping-particle": "de la",
        "family": "Flor",
        "given": "Grace"
      },
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Wyatt",
        "given": "Sally"
      },
      {
        "family": "Jirotka",
        "given": "Marina"
      },
      {
        "family": "Eccles",
        "given": "Katherine"
      },
      {
        "family": "Madsen",
        "given": "Christine"
      }
    ],
    "genre": "A Research Information Network Report",
    "id": "Bulger2011",
    "issued": {
      "date-parts": [
        [
          2011,
          4
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Research Information Network",
    "title": "Reinventing research? Information practices in the humanities",
    "title-short": "Reinventing research?",
    "type": "report"
  },
  {
    "URL": "https://mashable.com/article/mother-of-all-demos-douglas-engelbart-50-years",
    "accessed": {
      "date-parts": [
        [
          2021,
          11,
          8
        ]
      ]
    },
    "author": [
      {
        "family": "Wolpin",
        "given": "Stewart"
      }
    ],
    "container-title": "Mashable",
    "id": "Wolpin2018",
    "issued": {
      "date-parts": [
        [
          2018,
          12,
          8
        ]
      ]
    },
    "language": "en-US",
    "title": "50 years ago, douglas engelbart’s “mother of all demos” changed personal technology forever",
    "type": "article-journal"
  },
  {
    "URL": "http://nbn-resolving.de/nbn:de:bvb:19-epub-42413-3",
    "author": [
      {
        "family": "Klinke",
        "given": "Harald"
      }
    ],
    "container-title": "#DigiCampus. Digitale Forschung und Lehre in den Geisteswissenschaften",
    "editor": [
      {
        "family": "Klinke",
        "given": "Harald"
      }
    ],
    "id": "Klinke2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "de-DE",
    "page": "1-8",
    "publisher": "Universitätsbibliothek der Ludwig-Maximilians-Universität",
    "publisher-place": "München",
    "title": "Vorwort: Die digitale Transformation in den Geisteswissenschaften",
    "title-short": "Vorwort",
    "type": "chapter"
  },
  {
    "DOI": "10.29085/9781856049054.010",
    "author": [
      {
        "family": "Warwick",
        "given": "Claire"
      }
    ],
    "container-title": "Digital humanities in practice",
    "editor": [
      {
        "family": "Warwick",
        "given": "Claire"
      },
      {
        "family": "Terras",
        "given": "Melissa"
      },
      {
        "family": "Nyhan",
        "given": "Julianne"
      }
    ],
    "id": "Warwick2012",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "language": "en-US",
    "page": "193-216",
    "publisher": "Facet",
    "title": "Institutional models for digital humanities",
    "type": "chapter"
  },
  {
    "DOI": "10.1108/s0733-558x201646",
    "ISBN": "978-1-78560-830-8",
    "collection-number": "46",
    "collection-title": "Research in the sociology of organizations",
    "editor": [
      {
        "family": "Popp Berman",
        "given": "Elizabeth"
      },
      {
        "family": "Paradeise",
        "given": "Catherine"
      }
    ],
    "id": "Popp2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Emerald",
    "title": "The university under pressure",
    "type": "book"
  },
  {
    "DOI": "10.1108/s0733-558x20160000046011",
    "ISBN": "978-1-78560-830-8",
    "ISSN": "0733-558X",
    "author": [
      {
        "family": "Louvel",
        "given": "Séverine"
      }
    ],
    "collection-number": "46",
    "collection-title": "Research in the sociology of organizations",
    "editor": [
      {
        "family": "Popp Berman",
        "given": "Elizabeth"
      },
      {
        "family": "Paradeise",
        "given": "Catherine"
      }
    ],
    "id": "Louvel2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "page": "329-359",
    "publisher": "Emerald",
    "title": "Going interdisciplinary in French and US universities: Organizational change and university policies",
    "title-short": "Going interdisciplinary in French and US universities",
    "type": "chapter"
  },
  {
    "DOI": "10.3998/dh.12869322.0001.001",
    "author": [
      {
        "family": "Klein",
        "given": "Julie Thompson"
      }
    ],
    "id": "Klein2015",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "language": "en-US",
    "publisher": "University of Michigan Press",
    "title": "Interdisciplining digital humanities: Boundary work in an emerging field",
    "title-short": "Interdisciplining digital humanities",
    "type": "book"
  },
  {
    "DOI": "10.7551/mitpress/12517.001.0001",
    "ISBN": "978-0-262-36323-5",
    "author": [
      {
        "family": "Jaton",
        "given": "Florian"
      }
    ],
    "id": "Jaton2021",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "publisher": "MIT Press",
    "title": "The constitution of algorithms: Ground-truthing, programming, formulating",
    "title-short": "The constitution of algorithms",
    "type": "book"
  },
  {
    "DOI": "10.1007/s11191-006-9038-6",
    "ISSN": "1573-1901",
    "author": [
      {
        "family": "Quale",
        "given": "Andreas"
      }
    ],
    "container-title": "Science & Education",
    "id": "Quale2007",
    "issued": {
      "date-parts": [
        [
          2007
        ]
      ]
    },
    "language": "en-US",
    "page": "231--266",
    "title": "Radical constructivism, and the sin of relativism",
    "type": "article-journal",
    "volume": "16"
  },
  {
    "URL": "https://constructivist.info/6/1/012.quale",
    "abstract": "Context: A number of objections that are frequently raised in the literature against radical constructivism, including: the charge of solipsism, allegations of self-refutation, social and moral reservations, and the accusation that RC cannot explain the success of science. Problem: These four objections are sought to be refuted. Results: 1. Solipsism is only troublesome against the background of a realist ontological perspective. 2. The truth-value of any proposition is only defined relative to some ontological context, thus self-refutation, as constituting a logical problem, does not arise. 3. Any ethical argumentation derives from one’s own personal views on ethical matters: their construction being a personal responsibility such that no one else can tell a person how to construct the “right ethics.” 4. In the relativist ontology of radical constructivism, a scientific theory is regarded as a model imposed on natural phenomena; its success is due to the capabilities of its constructor/scientists. Implications: It is found that the objections are based on an (overt or tacit) adoption of the antithetical viewpoint of scientific realism. In other words, radical constructivism is being criticised for not promoting a realist ontology.",
    "author": [
      {
        "family": "Quale",
        "given": "Andreas"
      }
    ],
    "container-title": "Constructivist Foundations",
    "id": "Quale2010",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "language": "en-US",
    "page": "12-18",
    "title": "Objections to radical constructivism",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "URL": "http://constructivist.info/6/1/019.glasersfeld",
    "abstract": "Problem: Radical constructivism, although having a very successful base in research on mathematics and science education, has not become a generally accepted theory of knowledge. Purpose: This paper discusses possible sources of aversion. Results: The first section makes explicit the unavoidability of accepting the responsibility for one’s thinking and acting, a responsibility that under stressful circumstances one would rather avoid. Another section suggests the origin of the human quest for certain knowledge. The third section introduces the notion of “stickiness of beliefs.” Implications: Constructivism has powerful implications for everyday attitudes and social relations with others. This paper, it is hoped, may induce some readers to investigate these implications.",
    "author": [
      {
        "dropping-particle": "von",
        "family": "Glasersfeld",
        "given": "Ernst"
      }
    ],
    "container-title": "Constructivist Foundations",
    "id": "vonGlasersfeld2010",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "language": "en-US",
    "page": "19-21",
    "title": "Why people dislike radical constructivism",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "ISBN": "0-87589-144-6",
    "author": [
      {
        "family": "Ikenberry",
        "given": "Stanley O."
      },
      {
        "family": "Friedman",
        "given": "Renee C."
      }
    ],
    "id": "Ikenberry1972",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Jossey-Bass",
    "publisher-place": "San Francisco",
    "title": "Beyond academic departments: The story of institutes and centers",
    "title-short": "Beyond academic departments",
    "type": "book"
  },
  {
    "DOI": "10.1057/s41307-018-0092-x",
    "ISSN": "1740-3863",
    "author": [
      {
        "family": "Vienni Baptista",
        "given": "Bianca"
      },
      {
        "family": "Vasen",
        "given": "Federico"
      },
      {
        "family": "Villa Soto",
        "given": "Juan Carlos"
      }
    ],
    "container-title": "Higher Education Policy",
    "id": "Vienni2019",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "461-483",
    "title": "Interdisciplinary centers in latin american universities: The challenges of institutionalization",
    "title-short": "Interdisciplinary centers in latin american universities",
    "type": "article-journal",
    "volume": "32"
  },
  {
    "DOI": "10.1146/annurev-soc-070308-115954",
    "ISSN": "1545-2115",
    "author": [
      {
        "family": "Jacobs",
        "given": "Jerry A."
      },
      {
        "family": "Frickel",
        "given": "Scott"
      }
    ],
    "container-title": "Annual Review of Sociology",
    "id": "Jacobs2009",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "page": "43--65",
    "title": "Interdisciplinarity: A critical assessment",
    "title-short": "Interdisciplinarity",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "DOI": "10.1086/599585",
    "author": [
      {
        "family": "Chandler",
        "given": "James"
      }
    ],
    "container-title": "Critical Inquiry",
    "id": "Chandler2009",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "page": "729-746",
    "title": "Introduction: Doctrines, disciplines, discourses, departments",
    "title-short": "Introduction",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "DOI": "10.1086/599580",
    "author": [
      {
        "family": "Post",
        "given": "Robert"
      }
    ],
    "container-title": "Critical Inquiry",
    "id": "Post2009",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "page": "749-770",
    "title": "Debating disciplinarity",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "DOI": "10.1086/599586",
    "author": [
      {
        "family": "Biagioli",
        "given": "Mario"
      }
    ],
    "container-title": "Critical Inquiry",
    "id": "Biagioli2009",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2009
        ]
      ]
    },
    "language": "en-US",
    "page": "816-833",
    "title": "Postdisciplinary liaisons: Science studies and the humanities",
    "title-short": "Postdisciplinary liaisons",
    "type": "article-journal",
    "volume": "35"
  },
  {
    "author": [
      {
        "family": "Baudrillard",
        "given": "Jean"
      }
    ],
    "id": "Baudrillard1981",
    "issued": {
      "date-parts": [
        [
          1981
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Galilée",
    "title": "Simulacres et simulation",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-030-83411-1",
    "ISBN": "978-3-030-83411-1",
    "author": [
      {
        "dropping-particle": "van",
        "family": "Oorschot",
        "given": "Paul C."
      }
    ],
    "id": "vanOorschot2021",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Computer security and the Internet",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-030-43620-9",
    "ISBN": "978-3-030-43620-9",
    "editor": [
      {
        "family": "Akpan",
        "given": "Ben"
      },
      {
        "family": "Kennedy",
        "given": "Teresa J."
      }
    ],
    "id": "Akpan2020",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "Science education in theory and practice",
    "type": "book"
  },
  {
    "DOI": "10.1007/978-3-476-04753-3",
    "ISBN": "978-3-476-04753-3",
    "editor": [
      {
        "family": "Albrecht",
        "given": "Andrea"
      },
      {
        "family": "Bonitz",
        "given": "Masetto"
      },
      {
        "family": "Skowronski",
        "given": "Alexandra"
      },
      {
        "family": "Zittel",
        "given": "Claus"
      }
    ],
    "id": "Albrecht2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "J. B. Metzler",
    "title": "Max Bense: Werk – Kontext – Wirkung",
    "title-short": "Max Bense",
    "type": "book"
  },
  {
    "DOI": "10.1515/sem-2014-0037",
    "ISSN": "0037-1998",
    "author": [
      {
        "family": "Eckardt",
        "given": "Michael"
      }
    ],
    "container-title": "Semiotica",
    "id": "Eckardt2014",
    "issue": "202",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "title": "Stopovers at logic and cybernetics: Georg Klaus’s road to semiotics",
    "title-short": "Stopovers at logic and cybernetics",
    "type": "article-journal",
    "volume": "2014"
  },
  {
    "DOI": "10.1371/journal.pone.0059030",
    "ISSN": "1932-6203",
    "author": [
      {
        "family": "Acerbi",
        "given": "Alberto"
      },
      {
        "family": "Lampos",
        "given": "Vasileios"
      },
      {
        "family": "Garnett",
        "given": "Philip"
      },
      {
        "family": "Bentley",
        "given": "R. Alexander"
      }
    ],
    "container-title": "PLoS ONE",
    "id": "Acerbi2013",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "en-US",
    "page": "e59030",
    "title": "The expression of emotions in 20th century books",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "DOI": "10.4000/histoiremesure.13534",
    "ISSN": "1957-7745",
    "author": [
      {
        "dropping-particle": "de",
        "family": "Valeriola",
        "given": "Sébastien"
      }
    ],
    "container-title": "Histoire & mesure",
    "id": "deValeriola2020",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "fr-FR",
    "page": "171-196",
    "title": "L’ordinateur au service du dépouillement de sources historiques",
    "type": "article-journal",
    "volume": "XXXV"
  },
  {
    "author": [
      {
        "family": "Günther",
        "given": "Gotthard"
      }
    ],
    "edition": "2",
    "id": "Guenther1963",
    "issued": {
      "date-parts": [
        [
          1963
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Agis",
    "title": "Das Bewußtsein der Maschinen: Eine Metaphysik der Kybernetik",
    "title-short": "Das Bewußtsein der Maschinen",
    "type": "book"
  },
  {
    "DOI": "10.12759/HSR.SUPPL.31.2018.78-95",
    "author": [
      {
        "family": "Bod",
        "given": "Rens"
      }
    ],
    "container-title": "Historical Social Research Supplement",
    "id": "Bod2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "title": "Modelling in the humanities: Linking patterns to principles",
    "title-short": "Modelling in the humanities",
    "type": "article-journal",
    "volume": "31"
  },
  {
    "DOI": "10.1126/science.abl7655",
    "ISSN": "1095-9203",
    "annote": "Web site: https://forgotten-books.netlify.app",
    "author": [
      {
        "family": "Kestemont",
        "given": "Mike"
      },
      {
        "family": "Karsdorp",
        "given": "Folgert"
      },
      {
        "dropping-particle": "de",
        "family": "Bruijn",
        "given": "Elisabeth"
      },
      {
        "family": "Driscoll",
        "given": "Matthew"
      },
      {
        "family": "Kapitan",
        "given": "Katarzyna A."
      },
      {
        "family": "Ó Macháin",
        "given": "Pádraig"
      },
      {
        "family": "Sawyer",
        "given": "Daniel"
      },
      {
        "family": "Sleiderink",
        "given": "Remco"
      },
      {
        "family": "Chao",
        "given": "Anne"
      }
    ],
    "container-title": "Science",
    "id": "Kestemont2022",
    "issue": "6582",
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "language": "en-US",
    "page": "765-769",
    "title": "Forgotten books: The application of unseen species models to the survival of culture",
    "title-short": "Forgotten books",
    "type": "article-journal",
    "volume": "375"
  },
  {
    "DOI": "10.1163/9789004402560",
    "author": [
      {
        "family": "Clivaz",
        "given": "Claire"
      }
    ],
    "id": "Clivaz2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Brill",
    "publisher-place": "Leiden, Boston",
    "title": "Écritures digitales: digital writing, digital scriptures",
    "title-short": "Écritures digitales",
    "type": "book"
  },
  {
    "DOI": "10.1163/9789004402560_006",
    "author": [
      {
        "family": "Clivaz",
        "given": "Claire"
      }
    ],
    "chapter-number": "2",
    "container-title": "Écritures digitales: digital writing, digital scriptures",
    "id": "Clivaz2019_ch02",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "fr-FR",
    "page": "44-81",
    "publisher": "Brill",
    "publisher-place": "Leiden, Boston",
    "title": "Sauf le nom: nommer les <i>Digital Humanities</i>: digital writing, digital scriptures",
    "title-short": "Sauf le nom",
    "type": "chapter"
  },
  {
    "URL": "https://www.jstor.org/stable/40938008",
    "author": [
      {
        "family": "Monod",
        "given": "Gabriel"
      }
    ],
    "container-title": "Revue historique",
    "id": "Monod1889",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1889
        ]
      ]
    },
    "language": "fr-FR",
    "page": "277-285",
    "title": "M. Fustel de Coulanges",
    "type": "article-journal",
    "volume": "42"
  },
  {
    "DOI": "10.1145/3486642",
    "ISSN": "1557-7317",
    "author": [
      {
        "family": "Repenning",
        "given": "Alexander"
      },
      {
        "family": "Basawapatna",
        "given": "Ashok"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Repenning2021",
    "issue": "11",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "page": "30-33",
    "title": "Explicative programming",
    "type": "article-journal",
    "volume": "64"
  },
  {
    "DOI": "10.1145/3491268",
    "ISSN": "1557-7317",
    "author": [
      {
        "family": "Denning",
        "given": "Peter J."
      },
      {
        "family": "Tedre",
        "given": "Matti"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Denning2021",
    "issue": "12",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "page": "30-33",
    "title": "Computational thinking for professionals",
    "type": "article-journal",
    "volume": "64"
  },
  {
    "DOI": "10.1016/0010-0285(76)90022-0",
    "ISSN": "0010-0285",
    "author": [
      {
        "family": "Simon",
        "given": "Herbert A."
      },
      {
        "family": "Hayes",
        "given": "John R."
      }
    ],
    "container-title": "Cognitive Psychology",
    "id": "Simon1976",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1976
        ]
      ]
    },
    "language": "en-US",
    "page": "165-190",
    "title": "The understanding process: Problem isomorphs",
    "title-short": "The understanding process",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "author": [
      {
        "family": "Tognini-Bonelli",
        "given": "Elena"
      }
    ],
    "id": "Tognini-Bonelli2001",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "language": "en-US",
    "publisher": "John Benjamins",
    "title": "Corpus linguistics at work",
    "type": "book"
  },
  {
    "DOI": "10.1075/ijcl.6.1.05kil",
    "ISSN": "1569-9811",
    "author": [
      {
        "family": "Kilgarriff",
        "given": "Adam"
      }
    ],
    "container-title": "International Journal of Corpus Linguistics",
    "id": "Kilgarriff2001",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2001,
          12
        ]
      ]
    },
    "language": "en-US",
    "page": "97-133",
    "title": "Comparing corpora",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "DOI": "10.1515/9783110296075-009",
    "ISBN": "978-3-11-029607-5",
    "author": [
      {
        "family": "Bubenhofer",
        "given": "Noah"
      }
    ],
    "chapter-number": "9",
    "container-title": "Handbuch Diskurs",
    "editor": [
      {
        "family": "Warnke",
        "given": "Ingo H."
      }
    ],
    "id": "Bubenhofer2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "de-DE",
    "page": "208-241",
    "publisher": "De Gruyter",
    "publisher-place": "Berlin, Boston",
    "title": "Diskurslinguistik und Korpora",
    "type": "chapter"
  },
  {
    "URL": "http://www.jstor.org/stable/409987",
    "author": [
      {
        "family": "Harris",
        "given": "Zellig S."
      }
    ],
    "container-title": "Language",
    "id": "Harris1952",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1952
        ]
      ]
    },
    "language": "en-US",
    "page": "1-30",
    "publisher": "Linguistic Society of America",
    "title": "Discourse analysis",
    "type": "article-journal",
    "volume": "28"
  },
  {
    "DOI": "10.1515/9783110229967",
    "ISBN": "978-3-11-021244-0",
    "URL": "https://doi.org/10.5167/uzh-43891",
    "abstract": "Seit den 1990er Jahren hat sich die Diskurslinguistik in der Germanistischen Sprachwissenschaft als neue Teildisziplin zur Analyse textübergreifender Muster fest etablieren können. Die ständig wachsende Zahl diskurstheoretischer Arbeiten und diskurslinguistischer Einzeluntersuchungen bestätigt dies. Eine konzise Einführung in die diskurslinguistische Theorie und Methode gab es aber bislang nicht - und dies, obwohl die Teildisziplin mittlerweile auch einen festen Platz in der Hochschullehre hat.",
    "author": [
      {
        "family": "Spitzmüller",
        "given": "Jürgen"
      },
      {
        "family": "Warnke",
        "given": "Ingo H."
      }
    ],
    "collection-title": "De Gruyter Studium",
    "id": "Spitzmueller2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "De Gruyter",
    "publisher-place": "Berlin",
    "title": "Diskurslinguistik: Eine Einführung in Theorien und Methoden der transtextuellen Sprachanalyse",
    "title-short": "Diskurslinguistik",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Busse",
        "given": "Dietrich"
      }
    ],
    "id": "Busse1987",
    "issued": {
      "date-parts": [
        [
          1987
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Klett-Cotta",
    "title": "Historische Semantik: Analyse eines Programms",
    "title-short": "Historische Semantik",
    "type": "book"
  },
  {
    "DOI": "10.3998/dh.12172434.0001.001",
    "author": [
      {
        "family": "Cohen",
        "given": "Daniel"
      },
      {
        "family": "Scheinfeldt",
        "given": "Tom"
      }
    ],
    "id": "Cohen2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "en-US",
    "publisher": "University of Michigan Press",
    "title": "Hacking the academy: New approaches to scholarship and teaching from digital humanities",
    "title-short": "Hacking the academy",
    "type": "book"
  },
  {
    "DOI": "10.3998/dh.12172434.0001.001",
    "author": [
      {
        "family": "Scheinfeldt",
        "given": "Tom"
      }
    ],
    "container-title": "Hacking the academy: New approaches to scholarship and teaching from digital humanities",
    "editor": [
      {
        "family": "Cohen",
        "given": "Daniel"
      },
      {
        "family": "Scheinfeldt",
        "given": "Tom"
      }
    ],
    "id": "Scheinfeldt2013",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "en-US",
    "page": "55-59",
    "publisher": "University of Michigan Press",
    "title": "Theory, method, and digital humanities: New approaches to scholarship and teaching from digital humanities",
    "title-short": "Theory, method, and digital humanities",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Varenne",
        "given": "Franck"
      }
    ],
    "collection-number": "14",
    "collection-title": "History and philosophy of technoscience",
    "id": "Varenne2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Routledge",
    "title": "From models to simulations",
    "type": "book"
  },
  {
    "DOI": "10.1057/9781137460950.0002",
    "ISBN": "978-1-349-49883-3",
    "author": [
      {
        "family": "Staley",
        "given": "David J."
      }
    ],
    "id": "Staley2014",
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Palgrave Macmillan",
    "publisher-place": "London, New York",
    "title": "Brain, mind and Internet: A deep history and future",
    "title-short": "Brain, mind and Internet",
    "type": "book"
  },
  {
    "ISBN": "978-1-138-68938-1",
    "author": [
      {
        "family": "Staley",
        "given": "David J."
      }
    ],
    "id": "Staley2021",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Routledge",
    "publisher-place": "London, New York",
    "title": "Historical imagination",
    "type": "book"
  },
  {
    "DOI": "10.17516/1997-1370-2016-9-7-1546-1552",
    "ISSN": "2313-6014",
    "author": [
      {
        "family": "Liu",
        "given": "Alan"
      }
    ],
    "container-title": "Journal of Siberian Federal University. Humanities & Social Sciences",
    "id": "Liu2016",
    "issue": "7",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "page": "1546-1552",
    "title": "Is digital humanities a field? An answer from the point of view of language",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "DOI": "10.1002/asi.24533",
    "ISSN": "2330-1643",
    "author": [
      {
        "family": "Luhmann",
        "given": "Jan"
      },
      {
        "family": "Burghardt",
        "given": "Manuel"
      }
    ],
    "container-title": "Journal of the Association for Information Science and Technology",
    "id": "Luhmann2021",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "page": "148-171",
    "title": "Digital humanities—a discipline in its own right? An analysis of the role and position of digital humanities in the academic landscape",
    "type": "article-journal",
    "volume": "73"
  },
  {
    "DOI": "10.5749/j.ctvg251hk",
    "ISBN": "978-1-5179-0693-1",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      },
      {
        "family": "Klein",
        "given": "Lauren F."
      }
    ],
    "id": "Gold2019",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "publisher": "University of Minnesota Press",
    "publisher-place": "Minneapolis, MN, USA",
    "title": "Debates in the digital humanities 2019",
    "type": "book"
  },
  {
    "DOI": "10.5749/j.ctvg251hk.3",
    "ISBN": "978-1-5179-0693-1",
    "author": [
      {
        "family": "Gold",
        "given": "Matthew K."
      },
      {
        "family": "Klein",
        "given": "Lauren F."
      }
    ],
    "container-title": "Debates in the digital humanities 2019",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      },
      {
        "family": "Klein",
        "given": "Lauren F."
      }
    ],
    "id": "Gold2019-intro",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "language": "en-US",
    "page": "ix-xiv",
    "publisher": "University of Minnesota Press",
    "publisher-place": "Minneapolis, MN, USA",
    "title": "A DH that matters",
    "type": "chapter"
  },
  {
    "DOI": "10.1126/science.92.2394.437",
    "URL": "https://www.jstor.org/stable/1666589",
    "author": [
      {
        "family": "Weyl",
        "given": "Hermann"
      }
    ],
    "container-title": "Science",
    "id": "Weyl1940",
    "issue": "2394",
    "issued": {
      "date-parts": [
        [
          1940
        ]
      ]
    },
    "language": "en-US",
    "page": "437-446",
    "title": "The mathematical way of thinking",
    "type": "article-journal",
    "volume": "92"
  },
  {
    "DOI": "10.2753/RSH1061-1983030213",
    "ISSN": "0038-5867",
    "abstract": "The distinguishing characteristic of the recent development of science has been the radical improvement in the methods of accumulating, processing, and analyzing factual data by large-scale employment of computers and mathematical techniques.",
    "author": [
      {
        "family": "Kovalʹchenko",
        "given": "Ivan Dmitrievich"
      }
    ],
    "container-title": "Soviet Studies in History",
    "id": "Kovalchenko1964",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1964
        ]
      ]
    },
    "language": "en-US",
    "page": "13-20",
    "title": "Analysis of historical statistics by mathematical means",
    "type": "article-journal",
    "volume": "3"
  },
  {
    "DOI": "10.2753/rsh1061-198317023",
    "ISSN": "0038-5867",
    "author": [
      {
        "family": "Kovalʹchenko",
        "given": "Ivan Dmitrievich"
      },
      {
        "family": "Sivachev",
        "given": "Nikolai Vasilʹevich"
      }
    ],
    "container-title": "Soviet Studies in History",
    "id": "Kovalchenko1978",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          1978
        ]
      ]
    },
    "language": "en-US",
    "page": "3-56",
    "title": "Structuralism and structural-quantitative methods in current historical research",
    "type": "article-journal",
    "volume": "17"
  },
  {
    "DOI": "10.1080/00182494.1974.10594030",
    "author": [
      {
        "family": "Kahk",
        "given": "Juhan Matthias"
      },
      {
        "family": "Kovalʹchenko",
        "given": "Ivan Dmitrievich"
      }
    ],
    "container-title": "Historical Methods Newsletter",
    "id": "Kahk1974",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          1974
        ]
      ]
    },
    "language": "en-US",
    "page": "217-224",
    "title": "Methodological problems of the application of mathematical methods in historical research",
    "type": "article-journal",
    "volume": "7"
  },
  {
    "DOI": "10.1007/BF02402320",
    "ISSN": "1572-8412",
    "author": [
      {
        "family": "Putnam",
        "given": "George F."
      }
    ],
    "container-title": "Computers and the Humanities",
    "id": "Putnam1971",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "language": "en-US",
    "page": "23-29",
    "title": "Soviet historians, quantitative methods, and digital computers",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "DOI": "10.1093/llc/fqab017",
    "ISSN": "2055-768X",
    "author": [
      {
        "family": "Fafinski",
        "given": "Mateusz"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Fafinski2021",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "page": "94-108",
    "title": "Facsimile narratives: Researching the past in the age of digital reproduction",
    "title-short": "Facsimile narratives",
    "type": "article-journal",
    "volume": "37"
  },
  {
    "DOI": "10.1093/llc/fqab079",
    "ISSN": "2055-768X",
    "author": [
      {
        "family": "Dobson",
        "given": "James E."
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Dobson2021",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "page": "81-93",
    "title": "Vector hermeneutics: On the interpretation of vector space models of text",
    "title-short": "Vector hermeneutics",
    "type": "article-journal",
    "volume": "37"
  },
  {
    "DOI": "10.1093/llc/fqab046",
    "ISSN": "2055-768X",
    "author": [
      {
        "family": "Savoy",
        "given": "Jacques"
      },
      {
        "family": "Wehren",
        "given": "Marylène"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Savoy2021",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "page": "229-241",
    "title": "Trump’s and Biden’s styles during the 2020 US presidential election",
    "type": "article-journal",
    "volume": "37"
  },
  {
    "DOI": "10.1163/18722636-12341446",
    "ISSN": "1872-2636",
    "author": [
      {
        "family": "Virmajoki",
        "given": "Veli"
      }
    ],
    "container-title": "Journal of the Philosophy of History",
    "id": "Virmajoki2022",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "22-53",
    "title": "What should we require from an account of explanation in historiography?",
    "type": "article-journal",
    "volume": "16"
  },
  {
    "DOI": "10.1163/18722636-12341437",
    "ISSN": "1872-2636",
    "author": [
      {
        "family": "Fear",
        "given": "Christopher"
      }
    ],
    "container-title": "Journal of the Philosophy of History",
    "id": "Fear2022",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "language": "en-US",
    "page": "1-21",
    "title": "R. G. Collingwood’s overlapping ideas of history",
    "type": "article-journal",
    "volume": "16"
  },
  {
    "DOI": "10.1525/res.2021.2.4.523",
    "ISSN": "2688-867X",
    "author": [
      {
        "family": "Bell",
        "given": "Eamonn"
      }
    ],
    "container-title": "Resonance",
    "id": "Bell2021",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "page": "523-558",
    "title": "Cybernetics, listening, and sound-studio phenomenotechnique in Abraham Moles’s <i>théorie de l’information et perception esthétique</i> (1958)",
    "type": "article-journal",
    "volume": "2"
  },
  {
    "DOI": "10.1038/s41467-018-07634-8",
    "ISSN": "2041-1723",
    "author": [
      {
        "family": "AlShebli",
        "given": "Bedoor K."
      },
      {
        "family": "Rahwan",
        "given": "Talal"
      },
      {
        "family": "Woon",
        "given": "Wei Lee"
      }
    ],
    "container-title": "Nature Communications",
    "id": "AlShebli2018",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "title": "The preeminence of ethnic diversity in scientific collaboration",
    "type": "article-journal",
    "volume": "9"
  },
  {
    "ISBN": "978-3-540-54359-6",
    "author": [
      {
        "family": "Zemanek",
        "given": "Heinz"
      }
    ],
    "id": "Zemanek1992",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Springer-Verlag",
    "publisher-place": "Berlin/Heidelberg",
    "title": "Das geistige Umfeld der Informationstechnik",
    "type": "book"
  },
  {
    "DOI": "10.1037/pspa0000098",
    "ISSN": "0022-3514",
    "author": [
      {
        "family": "Wang",
        "given": "Yilun"
      },
      {
        "family": "Kosinski",
        "given": "Michal"
      }
    ],
    "container-title": "Journal of Personality and Social Psychology",
    "id": "Wang2018",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "page": "246-257",
    "title": "Deep neural networks are more accurate than humans at detecting sexual orientation from facial images",
    "type": "article-journal",
    "volume": "114"
  },
  {
    "DOI": "10.1038/s41598-020-79310-1",
    "ISSN": "2045-2322",
    "author": [
      {
        "family": "Kosinski",
        "given": "Michal"
      }
    ],
    "container-title": "Scientific Reports",
    "id": "Kosinski2021",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "en-US",
    "title": "Facial recognition technology can expose political orientation from naturalistic facial images",
    "type": "article-journal",
    "volume": "11"
  },
  {
    "DOI": "10.1145/367593.367617",
    "ISSN": "1557-7317",
    "author": [
      {
        "family": "Weizenbaum",
        "given": "Joseph"
      }
    ],
    "container-title": "Communications of the ACM",
    "id": "Weizenbaum1963",
    "issue": "9",
    "issued": {
      "date-parts": [
        [
          1963
        ]
      ]
    },
    "language": "en-US",
    "page": "524-536",
    "title": "Symmetric list processor",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "DOI": "10.22364/bjmc.2018.6.3.05",
    "ISSN": "2255-8950",
    "author": [
      {
        "family": "Podnieks",
        "given": "Karlis"
      }
    ],
    "container-title": "Baltic Journal of Modern Computing",
    "id": "Podnieks2018",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "title": "Philosophy of modeling: Neglected pages of history",
    "title-short": "Philosophy of modeling",
    "type": "article-journal",
    "volume": "6"
  },
  {
    "URL": "https://n2t.net/ark:/13960/t4vj3zr74",
    "author": [
      {
        "family": "Nelson",
        "given": "Theodor H."
      }
    ],
    "id": "Nelson1974",
    "issued": {
      "date-parts": [
        [
          1974
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Self-published",
    "title": "Computer lib/dream machines",
    "type": "book"
  },
  {
    "DOI": "10.4324/9781315107493",
    "author": [
      {
        "family": "Rumelhart",
        "given": "David E."
      }
    ],
    "container-title": "Theoretical issues in reading comprehension",
    "editor": [
      {
        "family": "Spiro",
        "given": "R."
      },
      {
        "family": "Bruce",
        "given": "B."
      },
      {
        "family": "Brewer",
        "given": "W."
      }
    ],
    "id": "Rumelhart1980",
    "issued": {
      "date-parts": [
        [
          1980
        ]
      ]
    },
    "language": "en-US",
    "page": "33-58",
    "publisher": "Erlbaum",
    "title": "Schemata: The building blocks of cognition",
    "title-short": "Schemata",
    "type": "chapter"
  },
  {
    "URL": "https://web.media.mit.edu/~minsky/papers/Frames/frames.html",
    "accessed": {
      "date-parts": [
        [
          2022,
          4,
          21
        ]
      ]
    },
    "author": [
      {
        "family": "Minsky",
        "given": "Marvin"
      }
    ],
    "genre": "Memo",
    "id": "Minsky1974",
    "issued": {
      "date-parts": [
        [
          1974,
          6
        ]
      ]
    },
    "language": "en-US",
    "number": "306",
    "publisher": "MIT-AI Laboratory",
    "title": "A framework for representing knowledge",
    "type": "report"
  },
  {
    "URL": "https://n2t.net/ark:/13960/t6c261n93",
    "author": [
      {
        "family": "Korzybski",
        "given": "Alfred"
      }
    ],
    "id": "Korzybski1933",
    "issued": {
      "date-parts": [
        [
          1933
        ]
      ]
    },
    "language": "en-US",
    "publisher": "International Non-Aristotelian Library Publishing Company",
    "title": "Science and sanity: An introduction to non-aristotelian systems and general semantics",
    "title-short": "Science and sanity",
    "type": "book"
  },
  {
    "DOI": "10.1162/posc_a_00268",
    "ISSN": "1530-9274",
    "author": [
      {
        "family": "Hof",
        "given": "Barbara E."
      }
    ],
    "container-title": "Perspectives on Science",
    "id": "Hof2018",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "page": "76-96",
    "title": "The cybernetic “general model theory”: Unifying science or epistemic change?",
    "title-short": "The cybernetic “general model theory”",
    "type": "article-journal",
    "volume": "26"
  },
  {
    "DOI": "10.4000/hrc.5799",
    "ISSN": "2265-786X",
    "author": [
      {
        "family": "Bories",
        "given": "Anne-Sophie"
      }
    ],
    "container-title": "Histoire de la recherche contemporaine",
    "id": "Bories2021",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "fr-FR",
    "title": "Lectures appareillées: les études littéraires computationnelles entre rupture et continuité",
    "title-short": "Lectures appareillées",
    "type": "article-journal",
    "volume": "X"
  },
  {
    "DOI": "10.1111/1468-229X.12969",
    "ISSN": "1468-229X",
    "abstract": "Computing and the use of digital sources and resources is an everyday and essential practice in current academic scholarship. The present article gives a concise overview of approaches and methods within digital historical scholarship, focusing on the question “How have the digital humanities evolved and what has that evolution brought to historical scholarship?” We begin by discussing techniques in which data are generated and machine searchable, such as OCR/HTR, born-digital archives, computer vision, scholarly editions and linked data. In the second section, we provide examples of how data is made more accessible through quantitative text and network analysis. The third section considers the need for hermeneutics and data-awareness in digital historical scholarship. The technologies described in this article have had varying degrees of effect on historical scholarship, usually in indirect ways. With this article we aim to take stock of the digital approaches and methods used in historical scholarship in order to provide starting points for scholars seeking to understand the digital turn in the field and how and when to implement such approaches in their work.",
    "author": [
      {
        "family": "Romein",
        "given": "C. Annemieke"
      },
      {
        "family": "Kemman",
        "given": "Max"
      },
      {
        "family": "Birkholz",
        "given": "Julie M."
      },
      {
        "family": "Baker",
        "given": "James"
      },
      {
        "family": "Gruijter",
        "given": "Michel De"
      },
      {
        "family": "Meroño-Peñuela",
        "given": "Albert"
      },
      {
        "family": "Ries",
        "given": "Thorsten"
      },
      {
        "family": "Ros",
        "given": "Ruben"
      },
      {
        "family": "Scagliola",
        "given": "Stefania"
      }
    ],
    "container-title": "History",
    "id": "Romein2020",
    "issue": "365",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "291-312",
    "title": "State of the field: Digital history",
    "title-short": "State of the field",
    "type": "article-journal",
    "volume": "105"
  },
  {
    "DOI": "10.1080/00182370.2020.1725720",
    "ISSN": "0018-2370",
    "abstract": "Digital History is a relatively new sub-discipline within the academy. Understanding the nature of digital history, especially whence its core principles have evolved and how key practitioners have applied digital tools in generating historical knowledge, is the focus of this work. If Digital History is to grow into a more meaningful part of the historical discourse, we must make certain changes within academic culture and adapt to longer term technical realities.",
    "author": [
      {
        "family": "Decker",
        "given": "Michael J."
      }
    ],
    "container-title": "The Historian",
    "id": "Decker2020",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "keyword": "Digital Humanities, historiography, theory",
    "page": "7-21",
    "title": "The finger of God is here! Past, present, and future of digital history",
    "type": "article-journal",
    "volume": "82"
  },
  {
    "DOI": "10.1038/474420b",
    "ISSN": "1476-4687",
    "author": [
      {
        "family": "Anonymous"
      }
    ],
    "container-title": "Nature",
    "id": "Nature2011",
    "issue": "474",
    "issued": {
      "date-parts": [
        [
          2011,
          6
        ]
      ]
    },
    "page": "420",
    "title": "Poetry in motion",
    "type": "article-journal"
  },
  {
    "author": [
      {
        "family": "Andrae",
        "given": "Carl Göran"
      },
      {
        "family": "Lundkvist",
        "given": "Sven"
      }
    ],
    "container-title": "Proceedings of the XIII international congress of historical sciences",
    "event": "XIII international congress of historical sciences",
    "event-date": {
      "date-parts": [
        [
          1970,
          8,
          16
        ],
        [
          1970,
          8,
          23
        ]
      ]
    },
    "event-place": "Moscow",
    "id": "Andrae1970",
    "issued": {
      "date-parts": [
        [
          1970
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Nauka",
    "publisher-place": "Moscow",
    "title": "The use of historical mass-data: Experiences from a project on Swedish popular movements",
    "title-short": "The use of historical mass-data",
    "type": "paper-conference"
  },
  {
    "DOI": "10.2307/2856091",
    "ISSN": "0038-7134",
    "author": [
      {
        "family": "Bullough",
        "given": "Vern L."
      },
      {
        "family": "Lusignan",
        "given": "Serge"
      },
      {
        "family": "Ohlgren",
        "given": "Thomas H."
      }
    ],
    "container-title": "Speculum",
    "id": "Bullough1974",
    "issued": {
      "date-parts": [
        [
          1974
        ]
      ]
    },
    "language": "en-US",
    "page": "392-402",
    "title": "Report: Computers and the medievalist",
    "title-short": "Report",
    "type": "article-journal",
    "volume": "49"
  },
  {
    "DOI": "10.2307/2937409",
    "ISSN": "0021-8723",
    "author": [
      {
        "family": "Clubb",
        "given": "Jerome M."
      },
      {
        "family": "Allen",
        "given": "Howard"
      }
    ],
    "container-title": "Journal of American History",
    "id": "Clubb1967",
    "issued": {
      "date-parts": [
        [
          1967
        ]
      ]
    },
    "language": "en-US",
    "page": "599-607",
    "title": "Computers and historical studies",
    "type": "article-journal",
    "volume": "54"
  },
  {
    "ISBN": "978-0-415-60759-9",
    "abstract": "Many statements made by historians are quantitative statements, involving the use of measurable historical evidence. The historian who uses quantitative methods to analyse and interpret such information needs to be well acquainted with the particular methods and techniques of analysis and to be able to make the best use of the data that are available. There is an increasing need for training in such methods and in the interpretation of the large volume of literature now using quantitative techniques. Dr Floud’s text, which is relevant to all branches of historical inquiry, provides a straightforward and intelligible introduction for all students and research workers. The simpler and more useful techniques of descriptive and analytical statistics are described, up to the level of simple linear regression. Historical examples are used throughout, and great attention is paid to the need to ensure that the techniques are consistent with the quality of the data and with the historical problems they are intended to solve. Attention is paid to problems of the analysis of time series, which are of particular use to historians. No previous knowledge of statistics is assumed, and the simple mathematical techniques that are used are fully and clearly explained, without the use of more mathematical knowledge than is provided by an O-level course. A bibliography is provided to guide historians towards the most useful further reading. This student friendly text was first published in 1973.",
    "author": [
      {
        "family": "Floud",
        "given": "Roderick"
      }
    ],
    "id": "Floud2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Routledge",
    "title": "An introduction to quantitative methods for historians",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Fogel",
        "given": "Robert W."
      }
    ],
    "id": "Fogel1964",
    "issued": {
      "date-parts": [
        [
          1964
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Johns Hopkins University Press",
    "publisher-place": "Baltimore",
    "title": "Railroads and American economic growth: Essays in econometric history",
    "title-short": "Railroads and American economic growth",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Fogel",
        "given": "Robert W."
      },
      {
        "family": "Engerman",
        "given": "Stanley L."
      }
    ],
    "id": "Fogel1974",
    "issued": {
      "date-parts": [
        [
          1974
        ]
      ]
    },
    "language": "en-US",
    "number-of-volumes": "2",
    "publisher": "Little Brown",
    "publisher-place": "Boston",
    "title": "Time on the cross",
    "type": "book"
  },
  {
    "ISBN": "978-0-252-07151-5",
    "abstract": "This detailed analysis of slavery in the antebellum South was written in 1975 in response to the prior year’s publication of Robert Fogel and Stanley Engerman’s controversial Time on the Cross, which argued that slavery was an efficient and dynamic engine for the southern economy and that its success was due largely to the willing cooperation of the slaves themselves. Noted labor historian Herbert G. Gutman was unconvinced, even outraged, by Fogel and Engerman’s arguments. In this book he offers a systematic dissection of Time on the Cross, drawing on a wealth of data to contest that book’s most fundamental assertions. A benchmark work of historical inquiry, Gutman’s critique sheds light on a range of crucial aspects of slavery and its economic effectiveness. Gutman emphasizes the slaves’ responses to their treatment at the hands of slaveowners. He shows that slaves labored, not because they shared values and goals with their masters, but because of the omnipresent threat of ’negative incentives, ’ primarily physical violence. In his introduction to this new edition, Bruce Levine provides a historical analysis of the debate over Time on the Cross. Levine reminds us of the continuing influence of the latter book, demonstrated by Robert W. Fogel’s 1993 Nobel Prize in Economic Sciences, and hence the importance and timeliness of Gutman’s critique.",
    "author": [
      {
        "family": "Gutman",
        "given": "Herbert George"
      }
    ],
    "id": "Gutman2003",
    "issued": {
      "date-parts": [
        [
          2003
        ]
      ]
    },
    "language": "en-US",
    "publisher": "University of Illinois Press",
    "publisher-place": "Champagne, IL",
    "title": "Slavery and the numbers game: A critique of <i>time on the Cross</i>",
    "title-short": "Slavery and the numbers game",
    "type": "book"
  },
  {
    "abstract": "Computational digital historians have tended to elucidate their methods rather than advance interpretative arguments. While this attention to method is salutary, given the absence of methodological discussion in history generally, it is not clear how computational historians can advance historical arguments while also explaining methods. Drawing on a classic essay by David Hackett Fischer, \"The Braided Narrative: Substance and Form in Social History,\" this essay proposes a model for argumentative writing in computational digital history. Rather than using models such a methods section drawn from other disciplines, a braided narrative weaves together methodology and interpretation. The two strands strengthen one another when digital historians can elucidate how their methods and interpretations are mutually constitutive.",
    "author": [
      {
        "family": "Mullen",
        "given": "Lincoln"
      }
    ],
    "chapter-number": "31",
    "container-title": "Debates in the digital humanities 2019",
    "editor": [
      {
        "family": "Gold",
        "given": "Matthew K."
      },
      {
        "family": "Klein",
        "given": "Lauren F."
      }
    ],
    "id": "Mullen2018",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "language": "en-US",
    "page": "382-388",
    "publisher": "University of Minnesota Press",
    "publisher-place": "Minneapolis, MN",
    "title": "A braided narrative for digital history",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Thomas",
        "given": "William G."
      }
    ],
    "container-title": "A companion to digital humanities",
    "editor": [
      {
        "family": "Schreibman",
        "given": "Susan"
      },
      {
        "family": "Siemens",
        "given": "Ray"
      },
      {
        "family": "Unsworth",
        "given": "John"
      }
    ],
    "id": "Thomas2004",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "language": "en-US",
    "page": "56-68",
    "publisher": "Blackwell",
    "publisher-place": "Oxford",
    "title": "Computing and the historical imagination",
    "type": "chapter"
  },
  {
    "ISBN": "978-83-01-04987-4",
    "author": [
      {
        "family": "Topolski",
        "given": "Jerzy"
      }
    ],
    "edition": "3",
    "id": "Topolski1984",
    "issued": {
      "date-parts": [
        [
          1984
        ]
      ]
    },
    "language": "pl-PL",
    "publisher": "Państwowe Wydawnictwo Naukowe",
    "publisher-place": "Warsaw",
    "title": "Metodologia historii",
    "type": "book"
  },
  {
    "ISBN": "978-1-4008-3885-1",
    "abstract": "The Poverty of Clio challenges the hold that cliometrics–an approach to economic history that employs the analytical tools of economists–has exerted on the study of our economic past. In this provocative book, Francesco Boldizzoni calls for the reconstruction of economic history, one in which history and the social sciences are brought to bear on economics, and not the other way around. Boldizzoni questions the appeal of economics over history–which he identifies as a distinctly American attitude–exposing its errors and hidden ideologies, and revealing how it fails to explain economic behavior itself. He shows how the misguided reliance on economic reasoning to interpret history has come at the expense of insights from the humanities and has led to a rejection of valuable past historical research. Developing a better alternative to new institutional economics and the rational choice approach, Boldizzoni builds on the extraordinary accomplishments of twentieth-century European historians and social thinkers to offer fresh ideas for the renewal of the field. Economic history needs to rediscover the true relationship between economy and culture, and promote an authentic alliance with the social sciences, starting with sociology and anthropology. It must resume its dialogue with the humanities, but without shrinking away from theory when constructing its models. The Poverty of Clio demonstrates why history must exert its own creative power on economics.",
    "author": [
      {
        "family": "Boldizzoni",
        "given": "Francesco"
      }
    ],
    "id": "Boldizzoni2011",
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Princeton University Press",
    "publisher-place": "Princeton, NJ, USA",
    "title": "The poverty of Clio: Resurrecting economic history",
    "title-short": "The poverty of Clio",
    "type": "book"
  },
  {
    "annote": "https://books.google.ch/books?id=r1jBN5iehKsC&lpg=PP1&hl=de&pg=PA792#v=onepage&q=atheist&f=false",
    "author": [
      {
        "family": "Russell",
        "given": "Bertrand"
      }
    ],
    "container-title": "The collected papers of Bertrand Russell: Last philosophical testament, 1943–68",
    "editor": [
      {
        "family": "Slater",
        "given": "John G."
      }
    ],
    "id": "Russell1949-1997",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "language": "en-US",
    "original-date": {
      "date-parts": [
        [
          1949
        ]
      ]
    },
    "page": "89-92",
    "publisher": "Routledge",
    "title": "Am I an atheist or an agnostic? A plea for tolerance in the face of new dogmas",
    "title-short": "Am I an atheist or an agnostic?",
    "type": "chapter",
    "volume": "11"
  },
  {
    "URL": "https://n2t.net/ark:/13960/t0dw01s44",
    "author": [
      {
        "family": "Russel",
        "given": "Bertrand"
      }
    ],
    "id": "Russell1948",
    "issued": {
      "date-parts": [
        [
          1948
        ]
      ]
    },
    "language": "en-US",
    "publisher": "George Allen & Unwin",
    "title": "Human knowledge: Its scope and limits",
    "title-short": "Human knowledge",
    "type": "book"
  },
  {
    "annote": "https://books.google.fr/books?id=rDUJAAAAIAAJ&printsec=frontcover&hl=fr&&pg=PA2",
    "author": [
      {
        "family": "Laplace",
        "given": "Pierre-Simon"
      }
    ],
    "id": "Laplace1814",
    "issued": {
      "date-parts": [
        [
          1814
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Courcier",
    "title": "Essai philosophique sur les probabilités",
    "type": "book"
  },
  {
    "DOI": "10.1080/00437956.1954.11659520",
    "ISSN": "2373-5112",
    "author": [
      {
        "family": "Harris",
        "given": "Zellig S."
      }
    ],
    "container-title": "Word",
    "id": "Harris1954",
    "issue": "2–3",
    "issued": {
      "date-parts": [
        [
          1954
        ]
      ]
    },
    "language": "en-US",
    "page": "146-162",
    "title": "Distributional structure",
    "type": "article-journal",
    "volume": "10"
  },
  {
    "annote": "Reprinted in Palmer, F.R., ed. (1968). Selected Papers of J.R. Firth 1952-1959. London: Longman.",
    "author": [
      {
        "family": "Firth",
        "given": "John R."
      }
    ],
    "collection-title": "Special volume of the philological society",
    "container-title": "Studies in linguistic analysis",
    "id": "Firth1957",
    "issued": {
      "date-parts": [
        [
          1957
        ]
      ]
    },
    "language": "en-US",
    "page": "1-32",
    "publisher": "Blackwell",
    "title": "A synopsis of linguistic theory 1930–1955",
    "type": "chapter"
  },
  {
    "ISBN": "950-04-0217-3",
    "author": [
      {
        "family": "Borges",
        "given": "Jorge Luis"
      }
    ],
    "container-title": "Obras completas 1923–1972",
    "editor": [
      {
        "family": "Frías",
        "given": "Carlos V."
      }
    ],
    "id": "Borges1974-avatares",
    "issued": {
      "date-parts": [
        [
          1974
        ]
      ]
    },
    "language": "es-ES",
    "original-date": {
      "date-parts": [
        [
          1939
        ]
      ]
    },
    "page": "254-258",
    "publisher": "Emecé",
    "title": "Avatares de la tortuga",
    "type": "chapter"
  },
  {
    "ISBN": "950-04-0217-3",
    "author": [
      {
        "family": "Borges",
        "given": "Jorge Luis"
      }
    ],
    "container-title": "Obras completas 1923–1972",
    "editor": [
      {
        "family": "Frías",
        "given": "Carlos V."
      }
    ],
    "id": "Borges1974-zahir",
    "issued": {
      "date-parts": [
        [
          1974
        ]
      ]
    },
    "language": "es-ES",
    "original-date": {
      "date-parts": [
        [
          1949
        ]
      ]
    },
    "page": "589-595",
    "publisher": "Emecé",
    "title": "El Zahir",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Gebser",
        "given": "Jean"
      }
    ],
    "chapter-number": "9",
    "container-title": "In der Bewährung: Zehn Hinweise auf das neue Bewußtsein",
    "id": "Gebser1962-ch9",
    "issued": {
      "date-parts": [
        [
          1962
        ]
      ]
    },
    "language": "de-DE",
    "original-date": {
      "date-parts": [
        [
          1958
        ]
      ]
    },
    "page": "119-134",
    "publisher": "Francke",
    "title": "Die Welt ohne Gegenüber",
    "type": "chapter"
  },
  {
    "DOI": "10.3406/roman.1978.5216",
    "ISSN": "0048-8593",
    "author": [
      {
        "family": "Carbonell",
        "given": "Charles-Olivier"
      }
    ],
    "container-title": "Romantisme",
    "id": "Carbonell1978",
    "issue": "21",
    "issued": {
      "date-parts": [
        [
          1978
        ]
      ]
    },
    "language": "fr-FR",
    "page": "173-185",
    "title": "L’histoire dite “positiviste” en France",
    "type": "article-journal",
    "volume": "8"
  },
  {
    "DOI": "10.1145/1709424.1709462",
    "ISSN": "0097-8418",
    "author": [
      {
        "family": "Isbell",
        "given": "Charles L."
      },
      {
        "family": "Stein",
        "given": "Lynn Andrea"
      },
      {
        "family": "Cutler",
        "given": "Robb"
      },
      {
        "family": "Forbes",
        "given": "Jeffrey"
      },
      {
        "family": "Fraser",
        "given": "Linda"
      },
      {
        "family": "Impagliazzo",
        "given": "John"
      },
      {
        "family": "Proulx",
        "given": "Viera"
      },
      {
        "family": "Russ",
        "given": "Steve"
      },
      {
        "family": "Thomas",
        "given": "Richard"
      },
      {
        "family": "Xu",
        "given": "Yan"
      }
    ],
    "container-title": "ACM SIGCSE Bulletin",
    "id": "2010",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "language": "en-US",
    "page": "195-207",
    "title": "(Re)defining computing curricula by (re)defining computing",
    "type": "article-journal",
    "volume": "41"
  },
  {
    "DOI": "10.1093/jhc/fhz039",
    "ISSN": "1477-8564",
    "author": [
      {
        "family": "Etter",
        "given": "Walter"
      },
      {
        "family": "Schmidt",
        "given": "Olivier"
      }
    ],
    "container-title": "Journal of the History of Collections",
    "id": "Etter2020",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "411-430",
    "title": "Gessner’s fossil crab",
    "type": "article-journal",
    "volume": "32"
  },
  {
    "DOI": "10.26298/MELUSINA.8F8W-Y749-EITD",
    "author": [
      {
        "family": "Lang",
        "given": "Sarah"
      }
    ],
    "container-title": "Fabrikation von erkenntnis: Experimente in den digital humanities",
    "editor": [
      {
        "family": "Burghardt",
        "given": "Manuel"
      },
      {
        "family": "Dieckmann",
        "given": "Lisa"
      },
      {
        "family": "Steyer",
        "given": "Timo"
      },
      {
        "family": "Trilcke",
        "given": "Peer"
      },
      {
        "family": "Walkowski",
        "given": "Niels-Oliver"
      },
      {
        "family": "Weis",
        "given": "Joëlle"
      },
      {
        "family": "Wuttke",
        "given": "Ulrike"
      }
    ],
    "id": "Lang2022",
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Melusina Press",
    "title": "Experiments in the digital laboratory: What the computational humanities can learn about their definition and terminology from the history of science",
    "title-short": "Experiments in the digital laboratory",
    "type": "chapter"
  },
  {
    "DOI": "10.26298/MELUSINA.8F8W-Y749",
    "editor": [
      {
        "family": "Burghardt",
        "given": "Manuel"
      },
      {
        "family": "Dieckmann",
        "given": "Lisa"
      },
      {
        "family": "Steyer",
        "given": "Timo"
      },
      {
        "family": "Trilcke",
        "given": "Peer"
      },
      {
        "family": "Walkowski",
        "given": "Niels-Oliver"
      },
      {
        "family": "Weis",
        "given": "Joëlle"
      },
      {
        "family": "Wuttke",
        "given": "Ulrike"
      }
    ],
    "id": "Burghardt2021",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "language": "de-DE",
    "publisher": "Melusina Press",
    "title": "Fabrikation von Erkenntnis",
    "type": "book"
  },
  {
    "DOI": "10.4000/books.editionsehess.20041",
    "ISBN": "978-2-71323111-7",
    "editor": [
      {
        "family": "Boutier",
        "given": "Jean"
      },
      {
        "family": "Passeron",
        "given": "Jean-Claude"
      },
      {
        "family": "Revel",
        "given": "Jacques"
      }
    ],
    "id": "Boutier2006",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "language": "fr-FR",
    "publisher": "Éditions de l’École des hautes études en sciences sociales",
    "title": "Qu’est-ce qu’une discipline?",
    "type": "book"
  },
  {
    "URL": "http://hdl.handle.net/10323/4480",
    "author": [
      {
        "family": "Klein",
        "given": "Julie Thompson"
      }
    ],
    "container-title": "Issues in Interdisciplinary Studies",
    "id": "Klein2013",
    "issue": "31",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "language": "en-US",
    "page": "66-74",
    "title": "The state of the field: Institutionalization of interdisciplinarity",
    "title-short": "The state of the field",
    "type": "article-journal"
  },
  {
    "DOI": "10.1093/oxfordhb/9780198733522.001.0001",
    "ISBN": "978-0-19873352-2",
    "edition": "2",
    "editor": [
      {
        "family": "Frodeman",
        "given": "Robert"
      },
      {
        "family": "Klein",
        "given": "Julie Thompson"
      },
      {
        "family": "Pacheco",
        "given": "Roberto C. S."
      }
    ],
    "id": "Frodeman2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Oxford University Press",
    "title": "The Oxford handbook of interdisciplinarity",
    "type": "book"
  },
  {
    "DOI": "10.1093/oxfordhb/9780198733522.013.14",
    "ISBN": "978-0-19873352-2",
    "author": [
      {
        "family": "Davidson",
        "given": "Cathy N."
      },
      {
        "family": "Savonick",
        "given": "Danica"
      }
    ],
    "chapter-number": "12",
    "container-title": "The Oxford handbook of interdisciplinarity",
    "edition": "2",
    "editor": [
      {
        "family": "Frodeman",
        "given": "Robert"
      }
    ],
    "id": "Davidson2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "page": "159-172",
    "publisher": "Oxford University Press",
    "title": "Digital humanities: The emergence of science and technology studies",
    "title-short": "Digital humanities",
    "type": "chapter"
  },
  {
    "DOI": "10.1093/oxfordhb/9780198733522.013.15",
    "ISBN": "978-0-19873352-2",
    "author": [
      {
        "family": "Jasanoff",
        "given": "Sheila"
      }
    ],
    "chapter-number": "13",
    "container-title": "The Oxford handbook of interdisciplinarity",
    "edition": "2",
    "editor": [
      {
        "family": "Frodeman",
        "given": "Robert"
      }
    ],
    "id": "Jasanoff2017",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "language": "en-US",
    "page": "173-187",
    "publisher": "Oxford University Press",
    "title": "A field of its own: The emergence of science and technology studies",
    "title-short": "A field of its own",
    "type": "chapter"
  },
  {
    "DOI": "10.1002/asi.24565",
    "abstract": "This article considers the interdisciplinary opportunities and challenges of working with digital cultural heritage, such as digitized historical newspapers, and proposes an integrated digital hermeneutics workflow to combine purely disciplinary research approaches from computer science, humanities, and library work. Common interests and motivations of the above-mentioned disciplines have resulted in interdisciplinary projects and collaborations such as the NewsEye project, which is working on novel solutions on how digital heritage data is (re)searched, accessed, used, and analyzed. We argue that collaborations of different disciplines can benefit from a good understanding of the workflows and traditions of each of the disciplines involved but must find integrated approaches to successfully exploit the full potential of digitized sources. The paper is furthermore providing an insight into digital tools, methods, and hermeneutics in action, showing that integrated interdisciplinary research needs to build something in between the disciplines while respecting and understanding each other’s expertise and expectations.",
    "author": [
      {
        "family": "Oberbichler",
        "given": "Sarah"
      },
      {
        "family": "Boroş",
        "given": "Emanuela"
      },
      {
        "family": "Doucet",
        "given": "Antoine"
      },
      {
        "family": "Marjanen",
        "given": "Jani"
      },
      {
        "family": "Pfanzelter",
        "given": "Eva"
      },
      {
        "family": "Rautiainen",
        "given": "Juha"
      },
      {
        "family": "Toivonen",
        "given": "Hannu"
      },
      {
        "family": "Tolonen",
        "given": "Mikko"
      }
    ],
    "container-title": "Journal of the Association for Information Science and Technology",
    "id": "Oberbichler2021",
    "issued": {
      "date-parts": [
        [
          2021,
          8
        ]
      ]
    },
    "language": "en-US",
    "title": "Integrated interdisciplinary workflows for research on historical newspapers: Perspectives from humanities scholars, computer scientists, and librarians",
    "title-short": "Integrated interdisciplinary workflows for research on historical newspapers",
    "type": "article-journal"
  },
  {
    "DOI": "10.1007/978-3-319-22683-5",
    "ISBN": "978-3-319-22683-5",
    "ISSN": "2215-1796",
    "collection-number": "29",
    "collection-title": "Sociology of the sciences yearbook",
    "editor": [
      {
        "family": "Merz",
        "given": "Martina"
      },
      {
        "family": "Sormani",
        "given": "Philippe"
      }
    ],
    "id": "Merz2016",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Springer",
    "title": "The local configuration of new research fields: On regional and national diversity",
    "title-short": "The local configuration of new research fields",
    "type": "book"
  },
  {
    "DOI": "10.1093/llc/fqac016",
    "ISSN": "2055-768X",
    "author": [
      {
        "family": "Spinaci",
        "given": "Gianmarco"
      },
      {
        "family": "Colavizza",
        "given": "Giovanni"
      },
      {
        "family": "Peroni",
        "given": "Silvio"
      }
    ],
    "container-title": "Digital Scholarship in the Humanities",
    "id": "Spinaci2022",
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "language": "en-US",
    "title": "A map of digital humanities research across bibliographic data sources",
    "type": "article-journal"
  },
  {
    "URL": "http://eprints.ncrm.ac.uk/783/",
    "abstract": "This paper examines disciplines and disciplinarity through the lenses of certain academic disciplines including philosophy, anthropology, sociology, history, management and education.",
    "author": [
      {
        "family": "Krishnan",
        "given": "Armin"
      }
    ],
    "genre": "NCRM Working Paper",
    "id": "Krishnan2009",
    "issued": {
      "date-parts": [
        [
          2009,
          1
        ]
      ]
    },
    "language": "en-US",
    "number": "03/09",
    "publisher": "ESRC National Centre for Research Methods",
    "publisher-place": "Southampton",
    "title": "What are academic disciplines? Some observations on the disciplinarity vs. Interdisciplinarity debate",
    "title-short": "What are academic disciplines?",
    "type": "report"
  },
  {
    "DOI": "10.1016/s0169-7552(98)00110-x",
    "ISSN": "0169-7552",
    "author": [
      {
        "family": "Brin",
        "given": "Sergey"
      },
      {
        "family": "Page",
        "given": "Lawrence"
      }
    ],
    "container-title": "Computer Networks and ISDN Systems",
    "id": "Brin1998",
    "issue": "1–7",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "language": "en-US",
    "page": "107-117",
    "title": "The anatomy of a large-scale hypertextual web search engine",
    "type": "article-journal",
    "volume": "30"
  },
  {
    "author": [
      {
        "family": "Harman",
        "given": "Donna"
      }
    ],
    "container-title": "Information retrieval",
    "editor": [
      {
        "family": "Frakes",
        "given": "William B."
      },
      {
        "family": "Baeza-Yates",
        "given": "Rircardo"
      }
    ],
    "id": "har92",
    "issued": {
      "date-parts": [
        [
          1992
        ]
      ]
    },
    "language": "en-US",
    "page": "363-392",
    "publisher": "Prentice Hall",
    "title": "Ranking algorithms",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Salton",
        "given": "Gerard"
      },
      {
        "family": "McGill",
        "given": "Michael"
      }
    ],
    "id": "sm83",
    "issued": {
      "date-parts": [
        [
          1983
        ]
      ]
    },
    "language": "en-US",
    "publisher": "McGraw-Hill",
    "title": "Introduction to modern information retrieval",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Smeaton",
        "given": "Alan F."
      }
    ],
    "container-title": "Information retrieval and hypertext",
    "id": "sme96",
    "issued": {
      "date-parts": [
        [
          1996
        ]
      ]
    },
    "language": "en-US",
    "page": "3-25",
    "publisher": "Kluwer",
    "title": "An overview of information retrieval",
    "type": "chapter"
  },
  {
    "author": [
      {
        "family": "Spärck Jones",
        "given": "Karen"
      }
    ],
    "container-title": "Journal of Documentation",
    "id": "spa72",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "language": "en-US",
    "page": "11-20",
    "title": "A statistical interpretaton of term specificity and its applicaton in retrieval",
    "type": "article-journal",
    "volume": "28"
  },
  {
    "editor": [
      {
        "family": "Sherif",
        "given": "Muzafer"
      },
      {
        "family": "Sherif",
        "given": "Carolyn W."
      }
    ],
    "id": "Sherif1969",
    "issued": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "language": "en-US",
    "publisher": "Transaction",
    "title": "Interdisciplinary relationships in the social sciences",
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Campbell",
        "given": "Donald T."
      }
    ],
    "chapter-number": "19",
    "container-title": "Interdisciplinary relationships in the social sciences",
    "editor": [
      {
        "family": "Sherif",
        "given": "Muzafer"
      },
      {
        "family": "Sherif",
        "given": "Carolyn W."
      }
    ],
    "id": "Campbell1969",
    "issued": {
      "date-parts": [
        [
          1969
        ]
      ]
    },
    "language": "en-US",
    "page": "328-348",
    "publisher": "Transaction",
    "title": "Ethnocentrism of disciplines and the fish-scale model of omniscience",
    "type": "chapter"
  },
  {
    "DOI": "10.1162/qss_a_00062",
    "ISSN": "2641-3337",
    "author": [
      {
        "family": "Marres",
        "given": "Noortje"
      },
      {
        "dropping-particle": "de",
        "family": "Rijcke",
        "given": "Sarah"
      }
    ],
    "container-title": "Quantitative Science Studies",
    "id": "Marres2020",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "language": "en-US",
    "page": "1041-1055",
    "title": "From indicators to indicating interdisciplinarity: A participatory mapping methodology for research communities in-the-making",
    "title-short": "From indicators to indicating interdisciplinarity",
    "type": "article-journal",
    "volume": "1"
  },
  {
    "DOI": "10.1177/05390184221077787",
    "ISSN": "1461-7412",
    "author": [
      {
        "family": "Benz",
        "given": "Pierre"
      },
      {
        "family": "Rossier",
        "given": "Thierry"
      }
    ],
    "container-title": "Social Science Information",
    "id": "2022",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "language": "en-US",
    "page": "179-214",
    "title": "Is interdisciplinarity distinctive? Scientific collaborations through research projects in natural sciences",
    "type": "article-journal",
    "volume": "61"
  }
]
